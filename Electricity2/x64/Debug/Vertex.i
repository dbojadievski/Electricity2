#line 1 "C:\\Users\\dboja\\source\\Electricity2\\Electricity2\\Vertex.cpp"
#line 1 "C:\\Users\\dboja\\source\\Electricity2\\Electricity2\\Vertex.h"
#pragma once

#line 1 "C:\\Users\\dboja\\source\\Electricity2\\Electricity2\\Math.h"
#pragma once

#pragma external_header(push)
#line 1 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
//-------------------------------------------------------------------------------------
// DirectXMath.h -- SIMD C++ Math library
//
// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
//
// http://go.microsoft.com/fwlink/?LinkID=615560
//-------------------------------------------------------------------------------------

#pragma once









#line 21 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"



#line 25 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"





#line 31 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"




#line 36 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"


#line 39 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"



#line 43 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"



#line 47 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"



#line 51 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"



#line 55 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"



#line 59 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"



#line 63 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"



#line 67 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"



#line 71 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"



#line 75 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"



#line 79 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"



#line 83 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"








#line 92 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
#line 93 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"



#line 97 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

#pragma warning(push)
#pragma warning(disable:4514 4820)
// C4514/4820: Off by default noise
#pragma external_header(push)
#line 1 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\math.h"
//
// math.h
//
//      Copyright (c) Microsoft Corporation. All rights reserved.
//
// The C Standard Library <math.h> header.  This header consists of two parts:
// <corecrt_math.h> contains the math library; <corecrt_math_defines.h> contains
// the nonstandard but useful constant definitions.  The headers are divided in
// this way for modularity (to support the C++ modules feature).
//
#pragma external_header(push)
#line 1 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"
//
// corecrt_math.h
//
//      Copyright (c) Microsoft Corporation. All rights reserved.
//
// The majority of the C Standard Library <math.h> functionality.
//
#pragma once



#pragma external_header(push)
#line 1 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
//
// corecrt.h
//
//      Copyright (c) Microsoft Corporation. All rights reserved.
//
// Declarations used throughout the CoreCRT library.
//
#pragma once

#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
//
// vcruntime.h
//
//      Copyright (c) Microsoft Corporation. All rights reserved.
//
// Declarations used throughout the VCRuntime library.
//
#pragma once
//
// Note on use of "deprecate":
//
// Various places in this header and other headers use
// __declspec(deprecate) or macros that have the term DEPRECATE in them.
// We use "deprecate" here ONLY to signal the compiler to emit a warning
// about these items. The use of "deprecate" should NOT be taken to imply
// that any standard committee has deprecated these functions from the
// relevant standards.  In fact, these functions are NOT deprecated from
// the standard.
//
// Full details can be found in our documentation by searching for
// "Security Enhancements in the CRT".
//




// Many VCRuntime headers avoid exposing their contents to non-compilers like
// the Windows resource compiler and Qt's meta-object compiler (moc).


#line 32 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"

#line 34 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
#line 35 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


    
#line 39 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"

// The _CRTIMP macro is not used in the VCRuntime or the CoreCRT anymore, but
// there is a lot of existing code that declares CRT functions using this macro,
// and if we remove its definition, we break that existing code.  It is thus
// defined here only for compatibility.

    
    

#line 49 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
        
            
        

#line 54 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
    #line 55 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
#line 56 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"

#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"
/***
*sal.h - markers for documenting the semantics of APIs
*
*       Copyright (c) Microsoft Corporation. All rights reserved.
*
*Purpose:
*       sal.h provides a set of annotations to describe how a function uses its
*       parameters - the assumptions it makes about them, and the guarantees it makes
*       upon finishing.
*
*       [Public]
*
****/
#pragma once

/*==========================================================================

   The comments in this file are intended to give basic understanding of
   the usage of SAL, the Microsoft Source Code Annotation Language.
   For more details, please see https://go.microsoft.com/fwlink/?LinkID=242134

   The macros are defined in 3 layers, plus the structural set:

   _In_/_Out_/_Ret_ Layer:
   ----------------------
   This layer provides the highest abstraction and its macros should be used
   in most cases. These macros typically start with:
      _In_     : input parameter to a function, unmodified by called function
      _Out_    : output parameter, written to by called function, pointed-to
                 location not expected to be initialized prior to call
      _Outptr_ : like _Out_ when returned variable is a pointer type
                 (so param is pointer-to-pointer type). Called function
                 provides/allocated space.
      _Outref_ : like _Outptr_, except param is reference-to-pointer type.
      _Inout_  : inout parameter, read from and potentially modified by
                 called function.
      _Ret_    : for return values
      _Field_  : class/struct field invariants
   For common usage, this class of SAL provides the most concise annotations.
   Note that _In_/_Out_/_Inout_/_Outptr_ annotations are designed to be used
   with a parameter target. Using them with _At_ to specify non-parameter
   targets may yield unexpected results.

   This layer also includes a number of other properties that can be specified
   to extend the ability of code analysis, most notably:
      -- Designating parameters as format strings for printf/scanf/scanf_s
      -- Requesting stricter type checking for C enum parameters

   _Pre_/_Post_ Layer:
   ------------------
   The macros of this layer only should be used when there is no suitable macro
   in the _In_/_Out_ layer. Its macros start with _Pre_ or _Post_.
   This layer provides the most flexibility for annotations.

   Implementation Abstraction Layer:
   --------------------------------
   Macros from this layer should never be used directly. The layer only exists
   to hide the implementation of the annotation macros.

   Structural Layer:
   ----------------
   These annotations, like _At_ and _When_, are used with annotations from
   any of the other layers as modifiers, indicating exactly when and where
   the annotations apply.


   Common syntactic conventions:
   ----------------------------

   Usage:
   -----
   _In_, _Out_, _Inout_, _Pre_, _Post_, are for formal parameters.
   _Ret_, _Deref_ret_ must be used for return values.

   Nullness:
   --------
   If the parameter can be NULL as a precondition to the function, the
   annotation contains _opt. If the macro does not contain '_opt' the
   parameter cannot be NULL.

   If an out/inout parameter returns a null pointer as a postcondition, this is
   indicated by _Ret_maybenull_ or _result_maybenull_. If the macro is not
   of this form, then the result will not be NULL as a postcondition.
     _Outptr_ - output value is not NULL
     _Outptr_result_maybenull_ - output value might be NULL

   String Type:
   -----------
   _z: NullTerminated string
   for _In_ parameters the buffer must have the specified stringtype before the call
   for _Out_ parameters the buffer must have the specified stringtype after the call
   for _Inout_ parameters both conditions apply

   Extent Syntax:
   -------------
   Buffer sizes are expressed as element counts, unless the macro explicitly
   contains _byte_ or _bytes_. Some annotations specify two buffer sizes, in
   which case the second is used to indicate how much of the buffer is valid
   as a postcondition. This table outlines the precondition buffer allocation
   size, precondition number of valid elements, postcondition allocation size,
   and postcondition number of valid elements for representative buffer size
   annotations:
                                     Pre    |  Pre    |  Post   |  Post
                                     alloc  |  valid  |  alloc  |  valid
      Annotation                     elems  |  elems  |  elems  |  elems
      ----------                     ------------------------------------
      _In_reads_(s)                    s    |   s     |   s     |   s
      _Inout_updates_(s)               s    |   s     |   s     |   s
      _Inout_updates_to_(s,c)          s    |   s     |   s     |   c
      _Out_writes_(s)                  s    |   0     |   s     |   s
      _Out_writes_to_(s,c)             s    |   0     |   s     |   c
      _Outptr_result_buffer_(s)        ?    |   ?     |   s     |   s
      _Outptr_result_buffer_to_(s,c)   ?    |   ?     |   s     |   c

   For the _Outptr_ annotations, the buffer in question is at one level of
   dereference. The called function is responsible for supplying the buffer.

   Success and failure:
   -------------------
   The SAL concept of success allows functions to define expressions that can
   be tested by the caller, which if it evaluates to non-zero, indicates the
   function succeeded, which means that its postconditions are guaranteed to
   hold.  Otherwise, if the expression evaluates to zero, the function is
   considered to have failed, and the postconditions are not guaranteed.

   The success criteria can be specified with the _Success_(expr) annotation:
     _Success_(return != FALSE) BOOL
     PathCanonicalizeA(_Out_writes_(MAX_PATH) LPSTR pszBuf, LPCSTR pszPath) :
        pszBuf is only guaranteed to be NULL-terminated when TRUE is returned,
        and FALSE indicates failure. In common practice, callers check for zero
        vs. non-zero returns, so it is preferable to express the success
        criteria in terms of zero/non-zero, not checked for exactly TRUE.

   Functions can specify that some postconditions will still hold, even when
   the function fails, using _On_failure_(anno-list), or postconditions that
   hold regardless of success or failure using _Always_(anno-list).

   The annotation _Return_type_success_(expr) may be used with a typedef to
   give a default _Success_ criteria to all functions returning that type.
   This is the case for common Windows API status types, including
   HRESULT and NTSTATUS.  This may be overridden on a per-function basis by
   specifying a _Success_ annotation locally.

============================================================================*/





#line 151 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"



#line 155 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"

























// Disable expansion of SAL macros in non-Prefast mode to
// improve compiler throughput.


#line 185 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"


#line 188 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"

#line 190 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"

// safeguard for MIDL and RC builds



#line 196 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"



#line 200 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"






#line 207 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"











#line 219 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"






// Some annotations aren't officially SAL2 yet.

#line 228 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"
#line 229 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"


//============================================================================
//   Structural SAL:
//     These annotations modify the use of other annotations.  They may
//     express the annotation target (i.e. what parameter/field the annotation
//     applies to) or the condition under which the annotation is applicable.
//============================================================================

// _At_(target, annos) specifies that the annotations listed in 'annos' is to
// be applied to 'target' rather than to the identifier which is the current
// lexical target.


// _At_buffer_(target, iter, bound, annos) is similar to _At_, except that
// target names a buffer, and each annotation in annos is applied to each
// element of target up to bound, with the variable named in iter usable
// by the annotations to refer to relevant offsets within target.


// _When_(expr, annos) specifies that the annotations listed in 'annos' only
// apply when 'expr' evaluates to non-zero.




// <expr> indicates whether normal post conditions apply to a function


// <expr> indicates whether post conditions apply to a function returning
// the type that this annotation is applied to


// Establish postconditions that apply only if the function does not succeed


// Establish postconditions that apply in both success and failure cases.
// Only applicable with functions that have  _Success_ or _Return_type_succss_.


// Usable on a function defintion. Asserts that a function declaration is
// in scope, and its annotations are to be used. There are no other annotations
// allowed on the function definition.


// _Notref_ may precede a _Deref_ or "real" annotation, and removes one
// level of dereference if the parameter is a C++ reference (&).  If the
// net deref on a "real" annotation is negative, it is simply discarded.


// Annotations for defensive programming styles.







//============================================================================
//   _In_/_Out_ Layer:
//============================================================================

// Reserved pointer parameters, must always be NULL.


// _Const_ allows specification that any namable memory location is considered
// readonly for a given call.



// Input parameters --------------------------

//   _In_ - Annotations for parameters where data is passed into the function, but not modified.
//          _In_ by itself can be used with non-pointer types (although it is redundant).

// e.g. void SetPoint( _In_ const POINT* pPT );



// nullterminated 'in' parameters.
// e.g. void CopyStr( _In_z_ const char* szFrom, _Out_z_cap_(cchTo) char* szTo, size_t cchTo );




// 'input' buffers with given size











// 'input' buffers valid to the given end pointer








// Output parameters --------------------------

//   _Out_ - Annotations for pointer or reference parameters where data passed back to the caller.
//           These are mostly used where the pointer/reference is to a non-pointer type.
//           _Outptr_/_Outref) (see below) are typically used to return pointers via parameters.

// e.g. void GetPoint( _Out_ POINT* pPT );


























// Inout parameters ----------------------------

//   _Inout_ - Annotations for pointer or reference parameters where data is passed in and
//        potentially modified.
//          void ModifyPoint( _Inout_ POINT* pPT );
//          void ModifyPointByRef( _Inout_ POINT& pPT );




// For modifying string buffers
//   void toupper( _Inout_z_ char* sz );



// For modifying buffers with explicit element size











// For modifying buffers with explicit byte size










// Pointer to pointer parameters -------------------------

//   _Outptr_ - Annotations for output params returning pointers
//      These describe parameters where the called function provides the buffer:
//        HRESULT SHStrDupW(_In_ LPCWSTR psz, _Outptr_ LPWSTR *ppwsz);
//      The caller passes the address of an LPWSTR variable as ppwsz, and SHStrDupW allocates
//      and initializes memory and returns the pointer to the new LPWSTR in *ppwsz.
//
//    _Outptr_opt_ - describes parameters that are allowed to be NULL.
//    _Outptr_*_result_maybenull_ - describes parameters where the called function might return NULL to the caller.
//
//    Example:
//       void MyFunc(_Outptr_opt_ int **ppData1, _Outptr_result_maybenull_ int **ppData2);
//    Callers:
//       MyFunc(NULL, NULL);           // error: parameter 2, ppData2, should not be NULL
//       MyFunc(&pData1, &pData2);     // ok: both non-NULL
//       if (*pData1 == *pData2) ...   // error: pData2 might be NULL after call






// Annotations for _Outptr_ parameters returning pointers to null terminated strings.






// Annotations for _Outptr_ parameters where the output pointer is set to NULL if the function fails.




// Annotations for _Outptr_ parameters which return a pointer to a ref-counted COM object,
// following the COM convention of setting the output to NULL on failure.
// The current implementation is identical to _Outptr_result_nullonfailure_.
// For pointers to types that are not COM objects, _Outptr_result_nullonfailure_ is preferred.






// Annotations for _Outptr_ parameters returning a pointer to buffer with a specified number of elements/bytes

































// Annotations for output reference to pointer parameters.


















// Annotations for output reference to pointer parameters that guarantee
// that the pointer is set to NULL on failure.


// Generic annotations to set output value of a by-pointer or by-reference parameter to null/zero on failure.




// return values -------------------------------

//
// _Ret_ annotations
//
// describing conditions that hold for return values after the call

// e.g. _Ret_z_ CString::operator const wchar_t*() const noexcept;



// used with allocated but not yet initialized objects




// used with allocated and initialized objects
//    returns single valid object


//    returns pointer to initialized buffer of specified size







//    returns pointer to partially initialized buffer, with total size 'size' and initialized size 'count'






// Annotations for strict type checking




// Check the return value of a function e.g. _Check_return_ ErrorCode Foo();



// e.g. MyPrintF( _Printf_format_string_ const wchar_t* wzFormat, ... );









// annotations to express value of integral or pointer parameter









// annotation to express that a value (usually a field of a mutable class)
// is not changed by a function call


// Annotations to allow expressing generalized pre and post conditions.
// 'cond' may be any valid SAL expression that is considered to be true as a precondition
// or postcondition (respsectively).



// Annotations to express struct, class and field invariants




















//============================================================================
//   _Pre_/_Post_ Layer:
//============================================================================

//
// Raw Pre/Post for declaring custom pre/post conditions
//




//
// Validity property
//





//
// Buffer size properties
//

// Expressing buffer sizes without specifying pre or post condition








// Expressing buffer size as pre or post condition










//
// Pointer null-ness properties
//




//
// _Pre_ annotations ---
//
// describing conditions that must be met before the call of the function

// e.g. int strlen( _Pre_z_ const char* sz );
// buffer is a zero terminated string


// valid size unknown or indicated by type (e.g.:LPSTR)





// Overrides recursive valid when some field is not yet initialized when using _Inout_


// used with allocated but not yet initialized objects




//
// _Post_ annotations ---
//
// describing conditions that hold after the function call

// void CopyStr( _In_z_ const char* szFrom, _Pre_cap_(cch) _Post_z_ char* szFrom, size_t cchFrom );
// buffer will be a zero-terminated string after the call


// e.g. HRESULT InitStruct( _Post_valid_ Struct* pobj );



// e.g. void free( _Post_ptr_invalid_ void* pv );


// e.g. void ThrowExceptionIfNull( _Post_notnull_ const void* pv );


// e.g. HRESULT GetObject(_Outptr_ _On_failure_(_At_(*p, _Post_null_)) T **p);







#pragma region Input Buffer SAL 1 compatibility macros

/*==========================================================================

   This section contains definitions for macros defined for VS2010 and earlier.
   Usage of these macros is still supported, but the SAL 2 macros defined above
   are recommended instead.  This comment block is retained to assist in
   understanding SAL that still uses the older syntax.

   The macros are defined in 3 layers:

   _In_/_Out_ Layer:
   ----------------
   This layer provides the highest abstraction and its macros should be used
   in most cases. Its macros start with _In_, _Out_ or _Inout_. For the
   typical case they provide the most concise annotations.

   _Pre_/_Post_ Layer:
   ------------------
   The macros of this layer only should be used when there is no suitable macro
   in the _In_/_Out_ layer. Its macros start with _Pre_, _Post_, _Ret_,
   _Deref_pre_ _Deref_post_ and _Deref_ret_. This layer provides the most
   flexibility for annotations.

   Implementation Abstraction Layer:
   --------------------------------
   Macros from this layer should never be used directly. The layer only exists
   to hide the implementation of the annotation macros.


   Annotation Syntax:
   |--------------|----------|----------------|-----------------------------|
   |   Usage      | Nullness | ZeroTerminated |  Extent                     |
   |--------------|----------|----------------|-----------------------------|
   | _In_         | <>       | <>             | <>                          |
   | _Out_        | opt_     | z_             | [byte]cap_[c_|x_]( size )   |
   | _Inout_      |          |                | [byte]count_[c_|x_]( size ) |
   | _Deref_out_  |          |                | ptrdiff_cap_( ptr )         |
   |--------------|          |                | ptrdiff_count_( ptr )       |
   | _Ret_        |          |                |                             |
   | _Deref_ret_  |          |                |                             |
   |--------------|          |                |                             |
   | _Pre_        |          |                |                             |
   | _Post_       |          |                |                             |
   | _Deref_pre_  |          |                |                             |
   | _Deref_post_ |          |                |                             |
   |--------------|----------|----------------|-----------------------------|

   Usage:
   -----
   _In_, _Out_, _Inout_, _Pre_, _Post_, _Deref_pre_, _Deref_post_ are for
   formal parameters.
   _Ret_, _Deref_ret_ must be used for return values.

   Nullness:
   --------
   If the pointer can be NULL the annotation contains _opt. If the macro
   does not contain '_opt' the pointer may not be NULL.

   String Type:
   -----------
   _z: NullTerminated string
   for _In_ parameters the buffer must have the specified stringtype before the call
   for _Out_ parameters the buffer must have the specified stringtype after the call
   for _Inout_ parameters both conditions apply

   Extent Syntax:
   |------|---------------|---------------|
   | Unit | Writ/Readable | Argument Type |
   |------|---------------|---------------|
   |  <>  | cap_          | <>            |
   | byte | count_        | c_            |
   |      |               | x_            |
   |------|---------------|---------------|

   'cap' (capacity) describes the writable size of the buffer and is typically used
   with _Out_. The default unit is elements. Use 'bytecap' if the size is given in bytes
   'count' describes the readable size of the buffer and is typically used with _In_.
   The default unit is elements. Use 'bytecount' if the size is given in bytes.

   Argument syntax for cap_, bytecap_, count_, bytecount_:
   (<parameter>|return)[+n]  e.g. cch, return, cb+2

   If the buffer size is a constant expression use the c_ postfix.
   E.g. cap_c_(20), count_c_(MAX_PATH), bytecount_c_(16)

   If the buffer size is given by a limiting pointer use the ptrdiff_ versions
   of the macros.

   If the buffer size is neither a parameter nor a constant expression use the x_
   postfix. e.g. bytecount_x_(num*size) x_ annotations accept any arbitrary string.
   No analysis can be done for x_ annotations but they at least tell the tool that
   the buffer has some sort of extent description. x_ annotations might be supported
   by future compiler versions.

============================================================================*/

// e.g. void SetCharRange( _In_count_(cch) const char* rgch, size_t cch )
// valid buffer extent described by another parameter





// valid buffer extent described by a constant extression





// nullterminated  'input' buffers with given size

// e.g. void SetCharRange( _In_count_(cch) const char* rgch, size_t cch )
// nullterminated valid buffer extent described by another parameter





// nullterminated valid buffer extent described by a constant extression





// buffer capacity is described by another pointer
// e.g. void Foo( _In_ptrdiff_count_(pchMax) const char* pch, const char* pchMax ) { while pch < pchMax ) pch++; }



// 'x' version for complex expressions that are not supported by the current compiler version
// e.g. void Set3ColMatrix( _In_count_x_(3*cRows) const Elem* matrix, int cRows );






// 'out' with buffer size
// e.g. void GetIndices( _Out_cap_(cIndices) int* rgIndices, size_t cIndices );
// buffer capacity is described by another parameter





// buffer capacity is described by a constant expression





// buffer capacity is described by another parameter multiplied by a constant expression





// buffer capacity is described by another pointer
// e.g. void Foo( _Out_ptrdiff_cap_(pchMax) char* pch, const char* pchMax ) { while pch < pchMax ) pch++; }



// buffer capacity is described by a complex expression





// a zero terminated string is filled into a buffer of given capacity
// e.g. void CopyStr( _In_z_ const char* szFrom, _Out_z_cap_(cchTo) char* szTo, size_t cchTo );
// buffer capacity is described by another parameter





// buffer capacity is described by a constant expression





// buffer capacity is described by a complex expression





// a zero terminated string is filled into a buffer of given capacity
// e.g. size_t CopyCharRange( _In_count_(cchFrom) const char* rgchFrom, size_t cchFrom, _Out_cap_post_count_(cchTo,return)) char* rgchTo, size_t cchTo );





// a zero terminated string is filled into a buffer of given capacity
// e.g. size_t CopyStr( _In_z_ const char* szFrom, _Out_z_cap_post_count_(cchTo,return+1) char* szTo, size_t cchTo );





// only use with dereferenced arguments e.g. '*pcch'










// e.g. GetString( _Out_z_capcount_(*pLen+1) char* sz, size_t* pLen );






// 'inout' buffers with initialized elements before and after the call
// e.g. void ModifyIndices( _Inout_count_(cIndices) int* rgIndices, size_t cIndices );










// nullterminated 'inout' buffers with initialized elements before and after the call
// e.g. void ModifyIndices( _Inout_count_(cIndices) int* rgIndices, size_t cIndices );


















// e.g. void AppendToLPSTR( _In_ LPCSTR szFrom, _Inout_cap_(cchTo) LPSTR* szTo, size_t cchTo );















// inout string buffers with writable size
// e.g. void AppendStr( _In_z_ const char* szFrom, _Inout_z_cap_(cchTo) char* szTo, size_t cchTo );
















// returning pointers to valid objects



// annotations to express 'boundedness' of integral value parameter








// e.g.  HRESULT HrCreatePoint( _Deref_out_opt_ POINT** ppPT );





// e.g.  void CloneString( _In_z_ const wchar_t* wzFrom, _Deref_out_z_ wchar_t** pWzTo );





//
// _Deref_pre_ ---
//
// describing conditions for array elements of dereferenced pointer parameters that must be met before the call

// e.g. void SaveStringArray( _In_count_(cStrings) _Deref_pre_z_ const wchar_t* const rgpwch[] );



// e.g. void FillInArrayOfStr32( _In_count_(cStrings) _Deref_pre_cap_c_(32) _Deref_post_z_ wchar_t* const rgpwch[] );
// buffer capacity is described by another parameter





// buffer capacity is described by a constant expression





// buffer capacity is described by a complex condition





// convenience macros for nullterminated buffers with given capacity















// known capacity and valid but unknown readable extent















// e.g. void SaveMatrix( _In_count_(n) _Deref_pre_count_(n) const Elem** matrix, size_t n );
// valid buffer extent is described by another parameter





// valid buffer extent is described by a constant expression





// valid buffer extent is described by a complex expression





// e.g. void PrintStringArray( _In_count_(cElems) _Deref_pre_valid_ LPCSTR rgStr[], size_t cElems );








// restrict access rights



//
// _Deref_post_ ---
//
// describing conditions for array elements or dereferenced pointer parameters that hold after the call

// e.g. void CloneString( _In_z_ const Wchar_t* wzIn _Out_ _Deref_post_z_ wchar_t** pWzOut );



// e.g. HRESULT HrAllocateMemory( size_t cb, _Out_ _Deref_post_bytecap_(cb) void** ppv );
// buffer capacity is described by another parameter





// buffer capacity is described by a constant expression





// buffer capacity is described by a complex expression





// convenience macros for nullterminated buffers with given capacity















// known capacity and valid but unknown readable extent















// e.g. HRESULT HrAllocateZeroInitializedMemory( size_t cb, _Out_ _Deref_post_bytecount_(cb) void** ppv );
// valid buffer extent is described by another parameter





// buffer capacity is described by a constant expression





// buffer capacity is described by a complex expression





// e.g. void GetStrings( _Out_count_(cElems) _Deref_post_valid_ LPSTR const rgStr[], size_t cElems );







//
// _Deref_ret_ ---
//




//
// special _Deref_ ---
//


//
// _Ret_ ---
//

// e.g. _Ret_opt_valid_ LPSTR void* CloneSTR( _Pre_valid_ LPSTR src );



// e.g. _Ret_opt_bytecap_(cb) void* AllocateMemory( size_t cb );
// Buffer capacity is described by another parameter





// Buffer capacity is described by a constant expression





// Buffer capacity is described by a complex condition





// return value is nullterminated and capacity is given by another parameter





// e.g. _Ret_opt_bytecount_(cb) void* AllocateZeroInitializedMemory( size_t cb );
// Valid Buffer extent is described by another parameter





// Valid Buffer extent is described by a constant expression





// Valid Buffer extent is described by a complex expression





// return value is nullterminated and length is given by another parameter






// _Pre_ annotations ---


// restrict access rights



// e.g. void FreeMemory( _Pre_bytecap_(cb) _Post_ptr_invalid_ void* pv, size_t cb );
// buffer capacity described by another parameter





// buffer capacity described by a constant expression







// buffer capacity is described by another parameter multiplied by a constant expression



// buffer capacity described by size of other buffer, only used by dangerous legacy APIs
// e.g. int strcpy(_Pre_cap_for_(src) char* dst, const char* src);



// buffer capacity described by a complex condition





// buffer capacity described by the difference to another pointer parameter



// e.g. void AppendStr( _Pre_z_ const char* szFrom, _Pre_z_cap_(cchTo) _Post_z_ char* szTo, size_t cchTo );















// known capacity and valid but unknown readable extent















// e.g. void AppendCharRange( _Pre_count_(cchFrom) const char* rgFrom, size_t cchFrom, _Out_z_cap_(cchTo) char* szTo, size_t cchTo );
// Valid buffer extent described by another parameter





// Valid buffer extent described by a constant expression





// Valid buffer extent described by a complex expression





// Valid buffer extent described by the difference to another pointer parameter




// char * strncpy(_Out_cap_(_Count) _Post_maybez_ char * _Dest, _In_z_ const char * _Source, _In_ size_t _Count)
// buffer maybe zero-terminated after the call


// e.g. SIZE_T HeapSize( _In_ HANDLE hHeap, DWORD dwFlags, _Pre_notnull_ _Post_bytecap_(return) LPCVOID lpMem );



// e.g. int strlen( _In_z_ _Post_count_(return+1) const char* sz );







// e.g. size_t CopyStr( _In_z_ const char* szFrom, _Pre_cap_(cch) _Post_z_count_(return+1) char* szFrom, size_t cchFrom );







//
// _Prepost_ ---
//
// describing conditions that hold before and after the function call



















//
// _Deref_<both> ---
//
// short version for _Deref_pre_<ann> _Deref_post_<ann>
// describing conditions for array elements or dereferenced pointer parameters that hold before and after the call










































//
// _Deref_<miscellaneous>
//
// used with references to arrays







#pragma endregion Input Buffer SAL 1 compatibility macros


//============================================================================
//   Implementation Layer:
//============================================================================


// Naming conventions:
// A symbol the begins with _SA_ is for the machinery of creating any
// annotations; many of those come from sourceannotations.h in the case
// of attributes.

// A symbol that ends with _impl is the very lowest level macro.  It is
// not required to be a legal standalone annotation, and in the case
// of attribute annotations, usually is not.  (In the case of some declspec
// annotations, it might be, but it should not be assumed so.)  Those
// symbols will be used in the _PreN..., _PostN... and _RetN... annotations
// to build up more complete annotations.

// A symbol ending in _impl_ is reserved to the implementation as well,
// but it does form a complete annotation; usually they are used to build
// up even higher level annotations.



























































#line 1555 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"






























#line 1586 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"
























#line 1611 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"

// Using "nothing" for sal










#line 1624 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"






































#line 1663 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"















































































































#line 1775 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"






































































































#line 1878 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"








































































































































































#line 2047 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"

































































































// Obsolete -- may be needed for transition to attributes.



#line 2149 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"

// This section contains the deprecated annotations

/*
 -------------------------------------------------------------------------------
 Introduction

 sal.h provides a set of annotations to describe how a function uses its
 parameters - the assumptions it makes about them, and the guarantees it makes
 upon finishing.

 Annotations may be placed before either a function parameter's type or its return
 type, and describe the function's behavior regarding the parameter or return value.
 There are two classes of annotations: buffer annotations and advanced annotations.
 Buffer annotations describe how functions use their pointer parameters, and
 advanced annotations either describe complex/unusual buffer behavior, or provide
 additional information about a parameter that is not otherwise expressible.

 -------------------------------------------------------------------------------
 Buffer Annotations

 The most important annotations in sal.h provide a consistent way to annotate
 buffer parameters or return values for a function. Each of these annotations describes
 a single buffer (which could be a string, a fixed-length or variable-length array,
 or just a pointer) that the function interacts with: where it is, how large it is,
 how much is initialized, and what the function does with it.

 The appropriate macro for a given buffer can be constructed using the table below.
 Just pick the appropriate values from each category, and combine them together
 with a leading underscore. Some combinations of values do not make sense as buffer
 annotations. Only meaningful annotations can be added to your code; for a list of
 these, see the buffer annotation definitions section.

 Only a single buffer annotation should be used for each parameter.

 |------------|------------|---------|--------|----------|----------|---------------|
 |   Level    |   Usage    |  Size   | Output | NullTerm | Optional |  Parameters   |
 |------------|------------|---------|--------|----------|----------|---------------|
 | <>         | <>         | <>      | <>     | _z       | <>       | <>            |
 | _deref     | _in        | _ecount | _full  | _nz      | _opt     | (size)        |
 | _deref_opt | _out       | _bcount | _part  |          |          | (size,length) |
 |            | _inout     |         |        |          |          |               |
 |            |            |         |        |          |          |               |
 |------------|------------|---------|--------|----------|----------|---------------|

 Level: Describes the buffer pointer's level of indirection from the parameter or
          return value 'p'.

 <>         : p is the buffer pointer.
 _deref     : *p is the buffer pointer. p must not be NULL.
 _deref_opt : *p may be the buffer pointer. p may be NULL, in which case the rest of
                the annotation is ignored.

 Usage: Describes how the function uses the buffer.

 <>     : The buffer is not accessed. If used on the return value or with _deref, the
            function will provide the buffer, and it will be uninitialized at exit.
            Otherwise, the caller must provide the buffer. This should only be used
            for alloc and free functions.
 _in    : The function will only read from the buffer. The caller must provide the
            buffer and initialize it. Cannot be used with _deref.
 _out   : The function will only write to the buffer. If used on the return value or
            with _deref, the function will provide the buffer and initialize it.
            Otherwise, the caller must provide the buffer, and the function will
            initialize it.
 _inout : The function may freely read from and write to the buffer. The caller must
            provide the buffer and initialize it. If used with _deref, the buffer may
            be reallocated by the function.

 Size: Describes the total size of the buffer. This may be less than the space actually
         allocated for the buffer, in which case it describes the accessible amount.

 <>      : No buffer size is given. If the type specifies the buffer size (such as
             with LPSTR and LPWSTR), that amount is used. Otherwise, the buffer is one
             element long. Must be used with _in, _out, or _inout.
 _ecount : The buffer size is an explicit element count.
 _bcount : The buffer size is an explicit byte count.

 Output: Describes how much of the buffer will be initialized by the function. For
           _inout buffers, this also describes how much is initialized at entry. Omit this
           category for _in buffers; they must be fully initialized by the caller.

 <>    : The type specifies how much is initialized. For instance, a function initializing
           an LPWSTR must NULL-terminate the string.
 _full : The function initializes the entire buffer.
 _part : The function initializes part of the buffer, and explicitly indicates how much.

 NullTerm: States if the present of a '\0' marks the end of valid elements in the buffer.
 _z    : A '\0' indicated the end of the buffer
 _nz     : The buffer may not be null terminated and a '\0' does not indicate the end of the
          buffer.
 Optional: Describes if the buffer itself is optional.

 <>   : The pointer to the buffer must not be NULL.
 _opt : The pointer to the buffer might be NULL. It will be checked before being dereferenced.

 Parameters: Gives explicit counts for the size and length of the buffer.

 <>            : There is no explicit count. Use when neither _ecount nor _bcount is used.
 (size)        : Only the buffer's total size is given. Use with _ecount or _bcount but not _part.
 (size,length) : The buffer's total size and initialized length are given. Use with _ecount_part
                   and _bcount_part.

 -------------------------------------------------------------------------------
 Buffer Annotation Examples

 LWSTDAPI_(BOOL) StrToIntExA(
     __in LPCSTR pszString,
     DWORD dwFlags,
     __out int *piRet                     -- A pointer whose dereference will be filled in.
 );

 void MyPaintingFunction(
     __in HWND hwndControl,               -- An initialized read-only parameter.
     __in_opt HDC hdcOptional,            -- An initialized read-only parameter that might be NULL.
     __inout IPropertyStore *ppsStore     -- An initialized parameter that may be freely used
                                          --   and modified.
 );

 LWSTDAPI_(BOOL) PathCompactPathExA(
     __out_ecount(cchMax) LPSTR pszOut,   -- A string buffer with cch elements that will
                                          --   be NULL terminated on exit.
     __in LPCSTR pszSrc,
     UINT cchMax,
     DWORD dwFlags
 );

 HRESULT SHLocalAllocBytes(
     size_t cb,
     __deref_bcount(cb) T **ppv           -- A pointer whose dereference will be set to an
                                          --   uninitialized buffer with cb bytes.
 );

 __inout_bcount_full(cb) : A buffer with cb elements that is fully initialized at
     entry and exit, and may be written to by this function.

 __out_ecount_part(count, *countOut) : A buffer with count elements that will be
     partially initialized by this function. The function indicates how much it
     initialized by setting *countOut.

 -------------------------------------------------------------------------------
 Advanced Annotations

 Advanced annotations describe behavior that is not expressible with the regular
 buffer macros. These may be used either to annotate buffer parameters that involve
 complex or conditional behavior, or to enrich existing annotations with additional
 information.

 __success(expr) f :
     <expr> indicates whether function f succeeded or not. If <expr> is true at exit,
     all the function's guarantees (as given by other annotations) must hold. If <expr>
     is false at exit, the caller should not expect any of the function's guarantees
     to hold. If not used, the function must always satisfy its guarantees. Added
     automatically to functions that indicate success in standard ways, such as by
     returning an HRESULT.

 __nullterminated p :
     Pointer p is a buffer that may be read or written up to and including the first
     NULL character or pointer. May be used on typedefs, which marks valid (properly
     initialized) instances of that type as being NULL-terminated.

 __nullnullterminated p :
     Pointer p is a buffer that may be read or written up to and including the first
     sequence of two NULL characters or pointers. May be used on typedefs, which marks
     valid instances of that type as being double-NULL terminated.

 __reserved v :
     Value v must be 0/NULL, reserved for future use.

 __checkReturn v :
     Return value v must not be ignored by callers of this function.

 __typefix(ctype) v :
     Value v should be treated as an instance of ctype, rather than its declared type.

 __override f :
     Specify C#-style 'override' behaviour for overriding virtual methods.

 __callback f :
     Function f can be used as a function pointer.

 __format_string p :
     Pointer p is a string that contains % markers in the style of printf.

 __blocksOn(resource) f :
     Function f blocks on the resource 'resource'.

 __fallthrough :
     Annotates switch statement labels where fall-through is desired, to distinguish
     from forgotten break statements.

 -------------------------------------------------------------------------------
 Advanced Annotation Examples

 __success(return != FALSE) LWSTDAPI_(BOOL)
 PathCanonicalizeA(__out_ecount(MAX_PATH) LPSTR pszBuf, LPCSTR pszPath) :
    pszBuf is only guaranteed to be NULL-terminated when TRUE is returned.

 typedef __nullterminated WCHAR* LPWSTR : Initialized LPWSTRs are NULL-terminated strings.

 __out_ecount(cch) __typefix(LPWSTR) void *psz : psz is a buffer parameter which will be
     a NULL-terminated WCHAR string at exit, and which initially contains cch WCHARs.

 -------------------------------------------------------------------------------
*/






#line 2361 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"
extern "C" {




#line 2367 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"


/*
 -------------------------------------------------------------------------------
 Helper Macro Definitions

 These express behavior common to many of the high-level annotations.
 DO NOT USE THESE IN YOUR CODE.
 -------------------------------------------------------------------------------
*/

/*
    The helper annotations are only understood by the compiler version used by
    various defect detection tools. When the regular compiler is running, they
    are defined into nothing, and do not affect the compiled code.
*/



















































































































































































































#line 2595 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    

    
    

#line 2634 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"

/*
-------------------------------------------------------------------------------
Buffer Annotation Definitions

Any of these may be used to directly annotate functions, but only one should
be used for each parameter. To determine which annotation to use for a given
buffer, use the table in the buffer annotations section.
-------------------------------------------------------------------------------
*/
































































































































































































/*
-------------------------------------------------------------------------------
Advanced Annotation Definitions

Any of these may be used to directly annotate functions, and may be used in
combination with each other or with regular buffer macros. For an explanation
of each annotation, see the advanced annotations section.
-------------------------------------------------------------------------------
*/






















#line 2868 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"









#line 2878 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"





#line 2884 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"
    
#line 2886 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"
#line 2887 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"






#line 2894 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"
#line 2895 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"






#line 2902 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"
#line 2903 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"











#line 2915 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"

//
// Set the analysis mode (global flags to analysis).
// They take effect at the point of declaration; use at global scope
// as a declaration.
//

// Synthesize a unique symbol.








//
// Floating point warnings are only meaningful in kernel-mode on x86
// so avoid reporting them on other platforms.
//













#line 2949 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"

// The following are predefined:
//  _Analysis_operator_new_throw_   (operator new throws)
//  _Analysis_operator_new_null_        (operator new returns null)
//  _Analysis_operator_new_never_fails_ (operator new never fails)
//

// Function class annotations.














}
#line 2973 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"

#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\concurrencysal.h"
/***
*concurrencysal.h - markers for documenting the concurrent semantics of APIs
*
*       Copyright (c) Microsoft Corporation. All rights reserved.
*
*Purpose:
*       This file contains macros for Concurrency SAL annotations. Definitions
*       starting with _Internal are low level macros that are subject to change.
*       Users should not use those low level macros directly.
*       [ANSI]
*
*       [Public]
*
****/




#pragma once


extern "C" {
#line 24 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\concurrencysal.h"











































































































































































































































































#line 292 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\concurrencysal.h"



#line 296 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\concurrencysal.h"
















































#line 345 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\concurrencysal.h"






/*
 * Old spelling: will be deprecated
 */


































#line 389 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\concurrencysal.h"


}
#line 393 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\concurrencysal.h"

#line 395 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\concurrencysal.h"
#pragma external_header(pop)
#line 2975 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\sal.h"
#pragma external_header(pop)
#line 58 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"
//
// vadefs.h
//
//      Copyright (c) Microsoft Corporation. All rights reserved.
//
// Definitions of macro helpers used by <stdarg.h>.  This is the topmost header
// in the CRT header lattice, and is always the first CRT header to be included,
// explicitly or implicitly.  Therefore, this header also has several definitions
// that are used throughout the CRT.
//
#pragma once



#pragma pack(push, 8)

// C4339: '__type_info_node': use of undefined type detected in CLR meta-data (/Wall)

    


        
    #line 24 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"
#line 25 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"

// C4412: function signature contains type '<typename>';
//        C++ objects are unsafe to pass between pure code and mixed or native. (/Wall)

    


        
    #line 34 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"
#line 35 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"

// Use _VCRUNTIME_EXTRA_DISABLED_WARNINGS to add additional warning suppressions to VCRuntime headers.

    
#line 40 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"

// C4514: unreferenced inline function has been removed (/Wall)
// C4820: '<typename>' : 'N' bytes padding added after data member (/Wall)

    
#line 46 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"

#pragma warning(push)
#pragma warning(disable:   4514 4820 )


extern "C" {
#line 53 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"



#line 57 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"


    
    
        typedef unsigned __int64  uintptr_t;
    

#line 65 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"
#line 66 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"


    
    


        typedef char* va_list;
    #line 74 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"
#line 75 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"


    


#line 81 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"





#line 87 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"



#line 91 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"
    
    
#line 94 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"











#line 106 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"







#line 114 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"











#line 126 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"





#line 132 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"










#line 143 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"










#line 154 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"

    void __cdecl __va_start(va_list* , ...);

    
    



    

#line 165 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"


} // extern "C"
#line 169 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"


    extern "C++"
    {
        template <typename _Ty>
        struct __vcrt_va_list_is_reference
        {
            enum : bool { __the_value = false };
        };

        template <typename _Ty>
        struct __vcrt_va_list_is_reference<_Ty&>
        {
            enum : bool { __the_value = true };
        };

        template <typename _Ty>
        struct __vcrt_va_list_is_reference<_Ty&&>
        {
            enum : bool { __the_value = true };
        };

        template <typename _Ty>
        struct __vcrt_assert_va_start_is_not_reference
        {
            static_assert(!__vcrt_va_list_is_reference<_Ty>::__the_value,
                "va_start argument must not have reference type and must not be parenthesized");
        };
    } // extern "C++"

    





#line 206 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vadefs.h"

#pragma warning(pop) 
#pragma pack(pop)
#pragma external_header(pop)
#line 59 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"

#pragma warning(push)
#pragma warning(disable:   4514 4820 )

// All C headers have a common prologue and epilogue, to enclose the header in
// an extern "C" declaration when the header is #included in a C++ translation
// unit and to push/pop the packing.


    



    





















#line 95 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"

__pragma(pack(push, 8)) extern "C" {




    


        
    #line 106 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
#line 107 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
















    

#line 126 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"

#line 128 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
        
    #line 130 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
#line 131 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


    

#line 136 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
        
    #line 138 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
#line 139 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"

// Definitions of calling conventions used code sometimes compiled as managed



#line 145 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
    
    
#line 148 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"




    
#line 154 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"



// Definitions of common __declspecs




    


#line 166 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"



#line 170 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
    
#line 172 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"




    
#line 178 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


    
        
        
    

#line 186 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
#line 187 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"

// For backwards compatibility


// Definitions of common types

    typedef unsigned __int64 size_t;
    typedef __int64          ptrdiff_t;
    typedef __int64          intptr_t;




#line 201 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


    typedef bool  __vcrt_bool;






#line 211 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"

// Indicate that these common types are defined

    
#line 216 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


    
#line 220 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


    
#line 224 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"

// Provide a typedef for wchar_t for use under /Zc:wchar_t-






    
        
    

#line 237 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
#line 238 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


    


#line 244 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"



#line 248 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


    extern "C++"
    {
        template <typename _CountofType, size_t _SizeOfArray>
        char (*__countof_helper(__unaligned _CountofType (&_Array)[_SizeOfArray]))[_SizeOfArray];

        
    }


#line 260 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"



#line 264 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


    




#line 272 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


    
        
    

#line 279 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
#line 280 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


    

#line 285 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
        
    #line 287 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
#line 288 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


    

#line 293 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
        
    #line 295 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
#line 296 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"





#line 302 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"



#line 306 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"

// [[nodiscard]] attributes on STL functions

    


        
    

#line 316 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
#line 317 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


    


#line 323 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"

// See note on use of "deprecate" at the top of this file




#line 330 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


    


        




    #line 341 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
#line 342 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"



#line 346 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


    
        
    


#line 354 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
#line 355 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


    void __cdecl __security_init_cookie(void);

    


#line 363 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"


#line 366 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
        void __cdecl __security_check_cookie(  uintptr_t _StackCookie);
        __declspec(noreturn) void __cdecl __report_gsfailure(  uintptr_t _StackCookie);
    #line 369 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
#line 370 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"

extern uintptr_t __security_cookie;


    
    
    
#line 378 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"

} __pragma(pack(pop))

#pragma warning(pop) 

#line 384 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\vcruntime.h"
#pragma external_header(pop)
#line 11 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
// Warning Suppression
//
//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

// C4412: function signature contains type '_locale_t';
//        C++ objects are unsafe to pass between pure code and mixed or native. (/Wall)

    


        
    #line 26 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 27 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

// Use _UCRT_EXTRA_DISABLED_WARNINGS to add additional warning suppressions to UCRT headers.

    
#line 32 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

// C4324: structure was padded due to __declspec(align()) (/W4)
// C4514: unreferenced inline function has been removed (/Wall)
// C4574: 'MACRO' is defined to be '0': did you mean to use '#if MACRO'? (/Wall)
// C4710: function not inlined (/Wall)
// C4793: 'function' is compiled as native code (/Wall and /W1 under /clr:pure)
// C4820: padding after data member (/Wall)
// C4995: name was marked #pragma deprecated
// C4996: __declspec(deprecated)
// C28719: Banned API, use a more robust and secure replacement.
// C28726: Banned or deprecated API, use a more robust and secure replacement.
// C28727: Banned API.

    
#line 47 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    











        
    #line 63 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 64 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    


        
    #line 71 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 72 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

#pragma warning(push)
#pragma warning(disable: 4324  4514 4574 4710 4793 4820 4995 4996 28719 28726 28727 )


__pragma(pack(push, 8)) extern "C" {

//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
// Annotation Macros
//
//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

    

#line 88 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
        
    

#line 92 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 93 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

// If you need the ability to remove __declspec(import) from an API, to support static replacement,
// declare the API using _ACRTIMP_ALT instead of _ACRTIMP.

    
#line 99 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    

#line 104 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
        
    

#line 108 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 109 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"



#line 113 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
    
#line 115 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    


#line 121 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"





#line 127 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
    
#line 129 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

// __declspec(guard(overflow)) enabled by /sdl compiler switch for CRT allocators



    
#line 136 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"



#line 140 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
    
#line 142 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

// The CLR requires code calling other SecurityCritical code or using SecurityCritical types
// to be marked as SecurityCritical.
// _CRT_SECURITYCRITICAL_ATTRIBUTE covers this for internal function definitions.
// _CRT_INLINE_PURE_SECURITYCRITICAL_ATTRIBUTE is for inline pure functions defined in the header.
// This is clr:pure-only because for mixed mode we compile inline functions as native.



    
#line 153 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"













    


        
    #line 171 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 172 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"



#line 176 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
    
#line 178 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"



#line 182 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
    
#line 184 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"



#line 188 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
    
#line 190 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"



//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
// Miscellaneous Stuff
//
//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

extern "C++"
{
    template<bool _Enable, typename _Ty>
    struct _CrtEnableIf;

    template<typename _Ty>
    struct _CrtEnableIf<true, _Ty>
    {
        typedef _Ty _Type;
    };
}
#line 211 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    typedef bool  __crt_bool;






#line 221 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"










    
        
    #line 234 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"








#line 243 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"









// CRT headers are included into some kinds of source files where only data type
// definitions and macro definitions are required but function declarations and
// inline function definitions are not.  These files include assembly files, IDL
// files, and resource files.  The tools that process these files often have a
// limited ability to process C and C++ code.  The _CRT_FUNCTIONS_REQUIRED macro
// is defined to 1 when we are compiling a file that actually needs functions to
// be declared (and defined, where applicable), and to 0 when we are compiling a
// file that does not.  This allows us to suppress declarations and definitions
// that are not compilable with the aforementioned tools.

    

#line 265 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
        
    #line 267 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 268 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"



#line 272 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    
#line 276 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


 
  
   
  

#line 284 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
 





#line 291 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 292 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
// Windows API Partitioning and ARM Desktop Support
//
//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

    

















        
    #line 319 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 320 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    
#line 324 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    
        
    

#line 331 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 332 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

// Verify that the ARM Desktop SDK is available when building an ARM Desktop app








//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
// Invalid Parameter Handler
//
//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

    __declspec(dllimport) void __cdecl _invalid_parameter(
          wchar_t const*,
          wchar_t const*,
          wchar_t const*,
                unsigned int,
                uintptr_t
        );
#line 356 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

__declspec(dllimport) void __cdecl _invalid_parameter_noinfo(void);
__declspec(dllimport) __declspec(noreturn) void __cdecl _invalid_parameter_noinfo_noreturn(void);

__declspec(noreturn)
__declspec(dllimport) void __cdecl _invoke_watson(
      wchar_t const* _Expression,
      wchar_t const* _FunctionName,
      wchar_t const* _FileName,
            unsigned int _LineNo,
            uintptr_t _Reserved);


    
        

    













#line 387 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 388 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"



//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
// Deprecation and Warnings
//
//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+




    


#line 405 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"



#line 409 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    


        


    #line 418 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 419 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"



//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
// Managed CRT Support
//
//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

    






        
    #line 437 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 438 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    


        
    #line 445 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 446 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"



#line 450 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"



//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
// SecureCRT Configuration
//
//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+





#line 464 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"















#line 480 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"





    
#line 487 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"



#line 491 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    

#line 496 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 497 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    


        


            
        #line 507 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
    #line 508 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 509 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"



#line 513 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"





#line 519 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    


        



    #line 529 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 530 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    
        
    



#line 539 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

    
        // _CRT_SECURE_CPP_OVERLOAD_STANDARD_NAMES_COUNT is ignored if
        // _CRT_SECURE_CPP_OVERLOAD_STANDARD_NAMES is set to 0
        
    



#line 549 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

    
        
              
        

#line 556 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
    



#line 561 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

    
        
    



#line 569 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

    
        
    



#line 577 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 578 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    
#line 582 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"



//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
// Basic Types
//
//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
typedef int                           errno_t;
typedef unsigned short                wint_t;
typedef unsigned short                wctype_t;
typedef long                          __time32_t;
typedef __int64                       __time64_t;

typedef struct __crt_locale_data_public
{
      unsigned short const* _locale_pctype;
      int _locale_mb_cur_max;
               unsigned int _locale_lc_codepage;
} __crt_locale_data_public;

typedef struct __crt_locale_pointers
{
    struct __crt_locale_data*    locinfo;
    struct __crt_multibyte_data* mbcinfo;
} __crt_locale_pointers;

typedef __crt_locale_pointers* _locale_t;

typedef struct _Mbstatet
{ // state of a multibyte translation
    unsigned long _Wchar;
    unsigned short _Byte, _State;
} _Mbstatet;

typedef _Mbstatet mbstate_t;



#line 622 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"



#line 626 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    


        typedef __time64_t time_t;
    #line 633 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 634 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

// Indicate that these common types are defined

    
#line 639 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"


    typedef size_t rsize_t;
#line 643 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"




//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
// C++ Secure Overload Generation Macros
//
//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

    

        










        










        










        










        










        










        










        










        










        












        












        
















    














#line 813 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 814 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"






































































//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
// C++ Standard Overload Generation Macros
//
//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

    













































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































#line 1865 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

        
        
        
        

        

            


            


            


            


            


            


            


            


            



            



            


            


            


            


            


            


            


            


            


            


            



            



            



            


            



            




            

            




            

            




            

            




            

            




            

            




            

            




            

            




            

        











































#line 2055 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
    #line 2056 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"
#line 2057 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt.h"

} __pragma(pack(pop))


#pragma warning(pop) 
#pragma external_header(pop)
#line 13 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"

#pragma warning(push)
#pragma warning(disable: 4324  4514 4574 4710 4793 4820 4995 4996 28719 28726 28727 )


__pragma(pack(push, 8)) extern "C" {


    // Definition of the _exception struct, which is passed to the matherr function
    // when a floating point exception is detected:
    struct _exception
    {
        int    type;   // exception type - see below
        char*  name;   // name of function where error occurred
        double arg1;   // first argument to function
        double arg2;   // second argument (if any) to function
        double retval; // value to be returned by function
    };

    // Definition of the _complex struct to be used by those who use the complex
    // functions and want type checking.
    
        

        struct _complex
        {
            double x, y; // real and imaginary parts
        };

        


#line 46 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"
    #line 47 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"
#line 48 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"



// On x86, when not using /arch:SSE2 or greater, floating point operations
// are performed using the x87 instruction set and FLT_EVAL_METHOD is 2.
// (When /fp:fast is used, floating point operations may be consistent, so
// we use the default types.)



#line 59 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"
    typedef float  float_t;
    typedef double double_t;
#line 62 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"



// Constant definitions for the exception type passed in the _exception struct







// Definitions of _HUGE and HUGE_VAL - respectively the XENIX and ANSI names
// for a value returned in case of error by a number of the floating point
// math routines.

    
        extern double const _HUGE;
    

#line 82 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"
#line 83 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"


    
#line 87 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"


























// Values for use as arguments to the _fperrraise function





























// IEEE 754 double properties





// IEEE 754 float properties





// IEEE 754 long double properties













void __cdecl _fperrraise(  int _Except);

  __declspec(dllimport) short __cdecl _dclass(  double _X);
  __declspec(dllimport) short __cdecl _ldclass(  long double _X);
  __declspec(dllimport) short __cdecl _fdclass(  float _X);

  __declspec(dllimport) int __cdecl _dsign(  double _X);
  __declspec(dllimport) int __cdecl _ldsign(  long double _X);
  __declspec(dllimport) int __cdecl _fdsign(  float _X);

  __declspec(dllimport) int __cdecl _dpcomp(  double _X,   double _Y);
  __declspec(dllimport) int __cdecl _ldpcomp(  long double _X,   long double _Y);
  __declspec(dllimport) int __cdecl _fdpcomp(  float _X,   float _Y);

  __declspec(dllimport) short __cdecl _dtest(  double* _Px);
  __declspec(dllimport) short __cdecl _ldtest(  long double* _Px);
  __declspec(dllimport) short __cdecl _fdtest(  float* _Px);

__declspec(dllimport) short __cdecl _d_int(  double* _Px,   short _Xexp);
__declspec(dllimport) short __cdecl _ld_int(  long double* _Px,   short _Xexp);
__declspec(dllimport) short __cdecl _fd_int(  float* _Px,   short _Xexp);

__declspec(dllimport) short __cdecl _dscale(  double* _Px,   long _Lexp);
__declspec(dllimport) short __cdecl _ldscale(  long double* _Px,   long _Lexp);
__declspec(dllimport) short __cdecl _fdscale(  float* _Px,   long _Lexp);

__declspec(dllimport) short __cdecl _dunscale(  short* _Pex,   double* _Px);
__declspec(dllimport) short __cdecl _ldunscale(  short* _Pex,   long double* _Px);
__declspec(dllimport) short __cdecl _fdunscale(  short* _Pex,   float* _Px);

  __declspec(dllimport) short __cdecl _dexp(  double* _Px,   double _Y,   long _Eoff);
  __declspec(dllimport) short __cdecl _ldexp(  long double* _Px,   long double _Y,   long _Eoff);
  __declspec(dllimport) short __cdecl _fdexp(  float* _Px,   float _Y,   long _Eoff);

  __declspec(dllimport) short __cdecl _dnorm(  unsigned short* _Ps);
  __declspec(dllimport) short __cdecl _fdnorm(  unsigned short* _Ps);

  __declspec(dllimport) double __cdecl _dpoly(  double _X,   double const* _Tab,   int _N);
  __declspec(dllimport) long double __cdecl _ldpoly(  long double _X,   long double const* _Tab,   int _N);
  __declspec(dllimport) float __cdecl _fdpoly(  float _X,   float const* _Tab,   int _N);

  __declspec(dllimport) double __cdecl _dlog(  double _X,   int _Baseflag);
  __declspec(dllimport) long double __cdecl _ldlog(  long double _X,   int _Baseflag);
  __declspec(dllimport) float __cdecl _fdlog(  float _X,   int _Baseflag);

  __declspec(dllimport) double __cdecl _dsin(  double _X,   unsigned int _Qoff);
  __declspec(dllimport) long double __cdecl _ldsin(  long double _X,   unsigned int _Qoff);
  __declspec(dllimport) float __cdecl _fdsin(  float _X,   unsigned int _Qoff);

// double declarations
typedef union
{   // pun floating type as integer array
    unsigned short _Sh[4];
    double _Val;
} _double_val;

// float declarations
typedef union
{   // pun floating type as integer array
    unsigned short _Sh[2];
    float _Val;
} _float_val;

// long double declarations
typedef union
{   // pun floating type as integer array
    unsigned short _Sh[4];
    long double _Val;
} _ldouble_val;

typedef union
{   // pun float types as integer array
    unsigned short _Word[4];
    float _Float;
    double _Double;
    long double _Long_double;
} _float_const;

extern const _float_const _Denorm_C,  _Inf_C,  _Nan_C,  _Snan_C, _Hugeval_C;
extern const _float_const _FDenorm_C, _FInf_C, _FNan_C, _FSnan_C;
extern const _float_const _LDenorm_C, _LInf_C, _LNan_C, _LSnan_C;

extern const _float_const _Eps_C,  _Rteps_C;
extern const _float_const _FEps_C, _FRteps_C;
extern const _float_const _LEps_C, _LRteps_C;

extern const double      _Zero_C,  _Xbig_C;
extern const float       _FZero_C, _FXbig_C;
extern const long double _LZero_C, _LXbig_C;




























extern "C++"
{
      inline int fpclassify(  float _X) throw()
    {
        return _fdtest(&_X);
    }

      inline int fpclassify(  double _X) throw()
    {
        return _dtest(&_X);
    }

      inline int fpclassify(  long double _X) throw()
    {
        return _ldtest(&_X);
    }

      inline bool signbit(  float _X) throw()
    {
        return _fdsign(_X) != 0;
    }

      inline bool signbit(  double _X) throw()
    {
        return _dsign(_X) != 0;
    }

      inline bool signbit(  long double _X) throw()
    {
        return _ldsign(_X) != 0;
    }

      inline int _fpcomp(  float _X,   float _Y) throw()
    {
        return _fdpcomp(_X, _Y);
    }

      inline int _fpcomp(  double _X,   double _Y) throw()
    {
        return _dpcomp(_X, _Y);
    }

      inline int _fpcomp(  long double _X,   long double _Y) throw()
    {
        return _ldpcomp(_X, _Y);
    }

    template <class _Trc, class _Tre> struct _Combined_type
    {   // determine combined type
        typedef float _Type;
    };

    template <> struct _Combined_type<float, double>
    {   // determine combined type
        typedef double _Type;
    };

    template <> struct _Combined_type<float, long double>
    {   // determine combined type
        typedef long double _Type;
    };

    template <class _Ty, class _T2> struct _Real_widened
    {   // determine widened real type
        typedef long double _Type;
    };

    template <> struct _Real_widened<float, float>
    {   // determine widened real type
        typedef float _Type;
    };

    template <> struct _Real_widened<float, double>
    {   // determine widened real type
        typedef double _Type;
    };

    template <> struct _Real_widened<double, float>
    {   // determine widened real type
        typedef double _Type;
    };

    template <> struct _Real_widened<double, double>
    {   // determine widened real type
        typedef double _Type;
    };

    template <class _Ty> struct _Real_type
    {   // determine equivalent real type
        typedef double _Type;   // default is double
    };

    template <> struct _Real_type<float>
    {   // determine equivalent real type
        typedef float _Type;
    };

    template <> struct _Real_type<long double>
    {   // determine equivalent real type
        typedef long double _Type;
    };

    template <class _T1, class _T2>
      inline int _fpcomp(  _T1 _X,   _T2 _Y) throw()
    {   // compare _Left and _Right
        typedef typename _Combined_type<float,
            typename _Real_widened<
            typename _Real_type<_T1>::_Type,
            typename _Real_type<_T2>::_Type>::_Type>::_Type _Tw;
        return _fpcomp((_Tw)_X, (_Tw)_Y);
    }

    template <class _Ty>
      inline bool isfinite(  _Ty _X) throw()
    {
        return fpclassify(_X) <= 0;
    }

    template <class _Ty>
      inline bool isinf(  _Ty _X) throw()
    {
        return fpclassify(_X) == 1;
    }

    template <class _Ty>
      inline bool isnan(  _Ty _X) throw()
    {
        return fpclassify(_X) == 2;
    }

    template <class _Ty>
      inline bool isnormal(  _Ty _X) throw()
    {
        return fpclassify(_X) == (-1);
    }

    template <class _Ty1, class _Ty2>
      inline bool isgreater(  _Ty1 _X,   _Ty2 _Y) throw()
    {
        return (_fpcomp(_X, _Y) & 4) != 0;
    }

    template <class _Ty1, class _Ty2>
      inline bool isgreaterequal(  _Ty1 _X,   _Ty2 _Y) throw()
    {
        return (_fpcomp(_X, _Y) & (2 | 4)) != 0;
    }

    template <class _Ty1, class _Ty2>
      inline bool isless(  _Ty1 _X,   _Ty2 _Y) throw()
    {
        return (_fpcomp(_X, _Y) & 1) != 0;
    }

    template <class _Ty1, class _Ty2>
      inline bool islessequal(  _Ty1 _X,   _Ty2 _Y) throw()
    {
        return (_fpcomp(_X, _Y) & (1 | 2)) != 0;
    }

    template <class _Ty1, class _Ty2>
      inline bool islessgreater(  _Ty1 _X,   _Ty2 _Y) throw()
    {
        return (_fpcomp(_X, _Y) & (1 | 4)) != 0;
    }

    template <class _Ty1, class _Ty2>
      inline bool isunordered(  _Ty1 _X,   _Ty2 _Y) throw()
    {
        return _fpcomp(_X, _Y) == 0;
    }
}  // extern "C++"
#line 459 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"





      int       __cdecl abs(  int _X);
      long      __cdecl labs(  long _X);
      long long __cdecl llabs(  long long _X);

      double __cdecl acos(  double _X);
      double __cdecl asin(  double _X);
      double __cdecl atan(  double _X);
      double __cdecl atan2(  double _Y,   double _X);

      double __cdecl cos(  double _X);
      double __cdecl cosh(  double _X);
      double __cdecl exp(  double _X);
       double __cdecl fabs(  double _X);
      double __cdecl fmod(  double _X,   double _Y);
      double __cdecl log(  double _X);
      double __cdecl log10(  double _X);
      double __cdecl pow(  double _X,   double _Y);
      double __cdecl sin(  double _X);
      double __cdecl sinh(  double _X);
       double __cdecl sqrt(  double _X);
      double __cdecl tan(  double _X);
      double __cdecl tanh(  double _X);

      __declspec(dllimport) double    __cdecl acosh(  double _X);
      __declspec(dllimport) double    __cdecl asinh(  double _X);
      __declspec(dllimport) double    __cdecl atanh(  double _X);
      __declspec(dllimport)  double    __cdecl atof(  char const* _String);
      __declspec(dllimport)  double    __cdecl _atof_l(  char const* _String,   _locale_t _Locale);
      __declspec(dllimport) double    __cdecl _cabs(  struct _complex _Complex_value);
      __declspec(dllimport) double    __cdecl cbrt(  double _X);
      __declspec(dllimport) double    __cdecl ceil(  double _X);
      __declspec(dllimport) double    __cdecl _chgsign(  double _X);
      __declspec(dllimport) double    __cdecl copysign(  double _Number,   double _Sign);
      __declspec(dllimport) double    __cdecl _copysign(  double _Number,   double _Sign);
      __declspec(dllimport) double    __cdecl erf(  double _X);
      __declspec(dllimport) double    __cdecl erfc(  double _X);
      __declspec(dllimport) double    __cdecl exp2(  double _X);
      __declspec(dllimport) double    __cdecl expm1(  double _X);
      __declspec(dllimport) double    __cdecl fdim(  double _X,   double _Y);
      __declspec(dllimport) double    __cdecl floor(  double _X);
      __declspec(dllimport) double    __cdecl fma(  double _X,   double _Y,   double _Z);
      __declspec(dllimport) double    __cdecl fmax(  double _X,   double _Y);
      __declspec(dllimport) double    __cdecl fmin(  double _X,   double _Y);
      __declspec(dllimport) double    __cdecl frexp(  double _X,   int* _Y);
      __declspec(dllimport) double    __cdecl hypot(  double _X,   double _Y);
      __declspec(dllimport) double    __cdecl _hypot(  double _X,   double _Y);
      __declspec(dllimport) int       __cdecl ilogb(  double _X);
      __declspec(dllimport) double    __cdecl ldexp(  double _X,   int _Y);
      __declspec(dllimport) double    __cdecl lgamma(  double _X);
      __declspec(dllimport) long long __cdecl llrint(  double _X);
      __declspec(dllimport) long long __cdecl llround(  double _X);
      __declspec(dllimport) double    __cdecl log1p(  double _X);
      __declspec(dllimport) double    __cdecl log2(  double _X);
      __declspec(dllimport) double    __cdecl logb(  double _X);
      __declspec(dllimport) long      __cdecl lrint(  double _X);
      __declspec(dllimport) long      __cdecl lround(  double _X);

    int __cdecl _matherr(  struct _exception* _Except);

      __declspec(dllimport) double __cdecl modf(  double _X,   double* _Y);
      __declspec(dllimport) double __cdecl nan(  char const* _X);
      __declspec(dllimport) double __cdecl nearbyint(  double _X);
      __declspec(dllimport) double __cdecl nextafter(  double _X,   double _Y);
      __declspec(dllimport) double __cdecl nexttoward(  double _X,   long double _Y);
      __declspec(dllimport) double __cdecl remainder(  double _X,   double _Y);
      __declspec(dllimport) double __cdecl remquo(  double _X,   double _Y,   int* _Z);
      __declspec(dllimport) double __cdecl rint(  double _X);
      __declspec(dllimport) double __cdecl round(  double _X);
      __declspec(dllimport) double __cdecl scalbln(  double _X,   long _Y);
      __declspec(dllimport) double __cdecl scalbn(  double _X,   int _Y);
      __declspec(dllimport) double __cdecl tgamma(  double _X);
      __declspec(dllimport) double __cdecl trunc(  double _X);
      __declspec(dllimport) double __cdecl _j0(  double _X );
      __declspec(dllimport) double __cdecl _j1(  double _X );
      __declspec(dllimport) double __cdecl _jn(int _X,   double _Y);
      __declspec(dllimport) double __cdecl _y0(  double _X);
      __declspec(dllimport) double __cdecl _y1(  double _X);
      __declspec(dllimport) double __cdecl _yn(  int _X,   double _Y);

      __declspec(dllimport) float     __cdecl acoshf(  float _X);
      __declspec(dllimport) float     __cdecl asinhf(  float _X);
      __declspec(dllimport) float     __cdecl atanhf(  float _X);
      __declspec(dllimport) float     __cdecl cbrtf(  float _X);
      __declspec(dllimport) float     __cdecl _chgsignf(  float _X);
      __declspec(dllimport) float     __cdecl copysignf(  float _Number,   float _Sign);
      __declspec(dllimport) float     __cdecl _copysignf(  float _Number,   float _Sign);
      __declspec(dllimport) float     __cdecl erff(  float _X);
      __declspec(dllimport) float     __cdecl erfcf(  float _X);
      __declspec(dllimport) float     __cdecl expm1f(  float _X);
      __declspec(dllimport) float     __cdecl exp2f(  float _X);
      __declspec(dllimport) float     __cdecl fdimf(  float _X,   float _Y);
      __declspec(dllimport) float     __cdecl fmaf(  float _X,   float _Y,   float _Z);
      __declspec(dllimport) float     __cdecl fmaxf(  float _X,   float _Y);
      __declspec(dllimport) float     __cdecl fminf(  float _X,   float _Y);
      __declspec(dllimport) float     __cdecl _hypotf(  float _X,   float _Y);
      __declspec(dllimport) int       __cdecl ilogbf(  float _X);
      __declspec(dllimport) float     __cdecl lgammaf(  float _X);
      __declspec(dllimport) long long __cdecl llrintf(  float _X);
      __declspec(dllimport) long long __cdecl llroundf(  float _X);
      __declspec(dllimport) float     __cdecl log1pf(  float _X);
      __declspec(dllimport) float     __cdecl log2f(  float _X);
      __declspec(dllimport) float     __cdecl logbf(  float _X);
      __declspec(dllimport) long      __cdecl lrintf(  float _X);
      __declspec(dllimport) long      __cdecl lroundf(  float _X);
      __declspec(dllimport) float     __cdecl nanf(  char const* _X);
      __declspec(dllimport) float     __cdecl nearbyintf(  float _X);
      __declspec(dllimport) float     __cdecl nextafterf(  float _X,   float _Y);
      __declspec(dllimport) float     __cdecl nexttowardf(  float _X,   long double _Y);
      __declspec(dllimport) float     __cdecl remainderf(  float _X,   float _Y);
      __declspec(dllimport) float     __cdecl remquof(  float _X,   float _Y,   int* _Z);
      __declspec(dllimport) float     __cdecl rintf(  float _X);
      __declspec(dllimport) float     __cdecl roundf(  float _X);
      __declspec(dllimport) float     __cdecl scalblnf(  float _X,   long _Y);
      __declspec(dllimport) float     __cdecl scalbnf(  float _X,   int _Y);
      __declspec(dllimport) float     __cdecl tgammaf(  float _X);
      __declspec(dllimport) float     __cdecl truncf(  float _X);

    



#line 586 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"

    

          __declspec(dllimport) float __cdecl _logbf(  float _X);
          __declspec(dllimport) float __cdecl _nextafterf(  float _X,   float _Y);
          __declspec(dllimport) int   __cdecl _finitef(  float _X);
          __declspec(dllimport) int   __cdecl _isnanf(  float _X);
          __declspec(dllimport) int   __cdecl _fpclassf(  float _X);

          __declspec(dllimport) int   __cdecl _set_FMA3_enable(  int _Flag);
          __declspec(dllimport) int   __cdecl _get_FMA3_enable(void);

    




#line 604 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"



    

          __declspec(dllimport) float __cdecl acosf(  float _X);
          __declspec(dllimport) float __cdecl asinf(  float _X);
          __declspec(dllimport) float __cdecl atan2f(  float _Y,   float _X);
          __declspec(dllimport) float __cdecl atanf(  float _X);
          __declspec(dllimport) float __cdecl ceilf(  float _X);
          __declspec(dllimport) float __cdecl cosf(  float _X);
          __declspec(dllimport) float __cdecl coshf(  float _X);
          __declspec(dllimport) float __cdecl expf(  float _X);

    









































#line 661 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"

    



#line 667 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"

          __inline float __cdecl fabsf(  float _X)
        {
            return (float)fabs(_X);
        }

    #line 674 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"

    

          __declspec(dllimport) float __cdecl floorf(  float _X);
          __declspec(dllimport) float __cdecl fmodf(  float _X,   float _Y);

    











#line 693 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"

      __inline float __cdecl frexpf(  float _X,   int *_Y)
    {
        return (float)frexp(_X, _Y);
    }

      __inline float __cdecl hypotf(  float _X,   float _Y)
    {
        return _hypotf(_X, _Y);
    }

      __inline float __cdecl ldexpf(  float _X,   int _Y)
    {
        return (float)ldexp(_X, _Y);
    }

    

          __declspec(dllimport) float  __cdecl log10f(  float _X);
          __declspec(dllimport) float  __cdecl logf(  float _X);
          __declspec(dllimport) float  __cdecl modff(  float _X,   float *_Y);
          __declspec(dllimport) float  __cdecl powf(  float _X,   float _Y);
          __declspec(dllimport) float  __cdecl sinf(  float _X);
          __declspec(dllimport) float  __cdecl sinhf(  float _X);
          __declspec(dllimport) float  __cdecl sqrtf(  float _X);
          __declspec(dllimport) float  __cdecl tanf(  float _X);
          __declspec(dllimport) float  __cdecl tanhf(  float _X);

    

















































#line 772 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"

      __declspec(dllimport) long double __cdecl acoshl(  long double _X);

      __inline long double __cdecl acosl(  long double _X)
    {
        return acos((double)_X);
    }

      __declspec(dllimport) long double __cdecl asinhl(  long double _X);

      __inline long double __cdecl asinl(  long double _X)
    {
        return asin((double)_X);
    }

      __inline long double __cdecl atan2l(  long double _Y,   long double _X)
    {
        return atan2((double)_Y, (double)_X);
    }

      __declspec(dllimport) long double __cdecl atanhl(  long double _X);

      __inline long double __cdecl atanl(  long double _X)
    {
        return atan((double)_X);
    }

      __declspec(dllimport) long double __cdecl cbrtl(  long double _X);

      __inline long double __cdecl ceill(  long double _X)
    {
        return ceil((double)_X);
    }

      __inline long double __cdecl _chgsignl(  long double _X)
    {
        return _chgsign((double)_X);
    }

      __declspec(dllimport) long double __cdecl copysignl(  long double _Number,   long double _Sign);

      __inline long double __cdecl _copysignl(  long double _Number,   long double _Sign)
    {
        return _copysign((double)_Number, (double)_Sign);
    }

      __inline long double __cdecl coshl(  long double _X)
    {
        return cosh((double)_X);
    }

      __inline long double __cdecl cosl(  long double _X)
    {
        return cos((double)_X);
    }

      __declspec(dllimport) long double __cdecl erfl(  long double _X);
      __declspec(dllimport) long double __cdecl erfcl(  long double _X);

      __inline long double __cdecl expl(  long double _X)
    {
        return exp((double)_X);
    }

      __declspec(dllimport) long double __cdecl exp2l(  long double _X);
      __declspec(dllimport) long double __cdecl expm1l(  long double _X);

      __inline long double __cdecl fabsl(  long double _X)
    {
        return fabs((double)_X);
    }

      __declspec(dllimport) long double __cdecl fdiml(  long double _X,   long double _Y);

      __inline long double __cdecl floorl(  long double _X)
    {
        return floor((double)_X);
    }

      __declspec(dllimport) long double __cdecl fmal(  long double _X,   long double _Y,   long double _Z);
      __declspec(dllimport) long double __cdecl fmaxl(  long double _X,   long double _Y);
      __declspec(dllimport) long double __cdecl fminl(  long double _X,   long double _Y);

      __inline long double __cdecl fmodl(  long double _X,   long double _Y)
    {
        return fmod((double)_X, (double)_Y);
    }

      __inline long double __cdecl frexpl(  long double _X,   int *_Y)
    {
        return frexp((double)_X, _Y);
    }

      __declspec(dllimport) int __cdecl ilogbl(  long double _X);

      __inline long double __cdecl _hypotl(  long double _X,   long double _Y)
    {
        return _hypot((double)_X, (double)_Y);
    }

      __inline long double __cdecl hypotl(  long double _X,   long double _Y)
    {
        return _hypot((double)_X, (double)_Y);
    }

      __inline long double __cdecl ldexpl(  long double _X,   int _Y)
    {
        return ldexp((double)_X, _Y);
    }

      __declspec(dllimport) long double __cdecl lgammal(  long double _X);
      __declspec(dllimport) long long __cdecl llrintl(  long double _X);
      __declspec(dllimport) long long __cdecl llroundl(  long double _X);

      __inline long double __cdecl logl(  long double _X)
    {
        return log((double)_X);
    }

      __inline long double __cdecl log10l(  long double _X)
    {
        return log10((double)_X);
    }

      __declspec(dllimport) long double __cdecl log1pl(  long double _X);
      __declspec(dllimport) long double __cdecl log2l(  long double _X);
      __declspec(dllimport) long double __cdecl logbl(  long double _X);
      __declspec(dllimport) long __cdecl lrintl(  long double _X);
      __declspec(dllimport) long __cdecl lroundl(  long double _X);

      __inline long double __cdecl modfl(  long double _X,   long double* _Y)
    {
        double _F, _I;
        _F = modf((double)_X, &_I);
        *_Y = _I;
        return _F;
    }

      __declspec(dllimport) long double __cdecl nanl(  char const* _X);
      __declspec(dllimport) long double __cdecl nearbyintl(  long double _X);
      __declspec(dllimport) long double __cdecl nextafterl(  long double _X,   long double _Y);
      __declspec(dllimport) long double __cdecl nexttowardl(  long double _X,   long double _Y);

      __inline long double __cdecl powl(  long double _X,   long double _Y)
    {
        return pow((double)_X, (double)_Y);
    }

      __declspec(dllimport) long double __cdecl remainderl(  long double _X,   long double _Y);
      __declspec(dllimport) long double __cdecl remquol(  long double _X,   long double _Y,   int* _Z);
      __declspec(dllimport) long double __cdecl rintl(  long double _X);
      __declspec(dllimport) long double __cdecl roundl(  long double _X);
      __declspec(dllimport) long double __cdecl scalblnl(  long double _X,   long _Y);
      __declspec(dllimport) long double __cdecl scalbnl(  long double _X,   int _Y);

      __inline long double __cdecl sinhl(  long double _X)
    {
        return sinh((double)_X);
    }

      __inline long double __cdecl sinl(  long double _X)
    {
        return sin((double)_X);
    }

      __inline long double __cdecl sqrtl(  long double _X)
    {
        return sqrt((double)_X);
    }

      __inline long double __cdecl tanhl(  long double _X)
    {
        return tanh((double)_X);
    }

      __inline long double __cdecl tanl(  long double _X)
    {
        return tan((double)_X);
    }

      __declspec(dllimport) long double __cdecl tgammal(  long double _X);
      __declspec(dllimport) long double __cdecl truncl(  long double _X);

    



#line 960 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"



    
    
    
    
    
    

    

    
        
            extern double HUGE;
        

#line 978 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"

        __declspec(deprecated("The POSIX name for this item is deprecated. Instead, use the ISO C " "and C++ conformant name: " "_j0" ". See online help for details."))   __declspec(dllimport) double __cdecl j0(  double _X);
        __declspec(deprecated("The POSIX name for this item is deprecated. Instead, use the ISO C " "and C++ conformant name: " "_j1" ". See online help for details."))   __declspec(dllimport) double __cdecl j1(  double _X);
        __declspec(deprecated("The POSIX name for this item is deprecated. Instead, use the ISO C " "and C++ conformant name: " "_jn" ". See online help for details."))   __declspec(dllimport) double __cdecl jn(  int _X,   double _Y);
        __declspec(deprecated("The POSIX name for this item is deprecated. Instead, use the ISO C " "and C++ conformant name: " "_y0" ". See online help for details."))   __declspec(dllimport) double __cdecl y0(  double _X);
        __declspec(deprecated("The POSIX name for this item is deprecated. Instead, use the ISO C " "and C++ conformant name: " "_y1" ". See online help for details."))   __declspec(dllimport) double __cdecl y1(  double _X);
        __declspec(deprecated("The POSIX name for this item is deprecated. Instead, use the ISO C " "and C++ conformant name: " "_yn" ". See online help for details."))   __declspec(dllimport) double __cdecl yn(  int _X,   double _Y);
    #line 986 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"

#line 988 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"

} __pragma(pack(pop))

#pragma warning(pop) 
#line 993 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_math.h"
#pragma external_header(pop)
#line 12 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\math.h"




#pragma external_header(pop)
#line 102 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
#pragma external_header(push)
#line 1 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\float.h"
//
// float.h
//
//      Copyright (c) Microsoft Corporation. All rights reserved.
//
// Implementation-defined values commonly used by sophisticated numerical
// (floating point) programs.
//
#pragma once





#pragma warning(push)
#pragma warning(disable: 4324  4514 4574 4710 4793 4820 4995 4996 28719 28726 28727 )


__pragma(pack(push, 8)) extern "C" {




    


        


            
        #line 32 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\float.h"
    #line 33 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\float.h"
#line 34 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\float.h"


// Define the floating point precision used.
//
// For x86, results are in double precision (unless /arch:sse2 is used, in which
// case results are in source precision.
//
// For x64 and ARM, results are in source precision.
//
// If the compiler is invoked with /fp:fast, the compiler is allowed to use the
// fastest precision and even mix within a single function, so precision is
// indeterminable.
//
// Note that manipulating the floating point behavior using the float_control/
// fenv_access/fp_contract #pragmas may alter the actual floating point evaluation
// method, which may in turn invalidate the value of FLT_EVAL_METHOD.



    






        
    #line 62 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\float.h"
#line 63 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\float.h"



//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
// Constants
//
//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+















































//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
// Flags
//
//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+







// New Control Bit that specifies the ambiguity in control word.



// Abstract User Control Word Mask and bit definitions














// i386 specific definitions









// RISC specific definitions








// Invalid subconditions (_SW_INVALID also set)







// Floating point error signals and return codes














// On x86 with arch:SSE2, the OS returns these exceptions


















// Initial Control Word value




#line 216 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\float.h"

    

#line 220 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\float.h"



//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
// State Manipulation
//
//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
// Note that reading or writing the floating point control or status words is
// not supported in managed code.

__declspec(dllimport) unsigned int __cdecl _clearfp(void);

#pragma warning(push)
#pragma warning(disable: 4141) 

 __declspec(deprecated("This function or variable may be unsafe. Consider using " "_controlfp_s" " instead. To disable deprecation, use _CRT_SECURE_NO_WARNINGS. " "See online help for details."))
__declspec(dllimport) unsigned int __cdecl _controlfp(
      unsigned int _NewValue,
      unsigned int _Mask
    );

#pragma warning(pop)


__declspec(dllimport) void __cdecl _set_controlfp(
      unsigned int _NewValue,
      unsigned int _Mask
    );


__declspec(dllimport) errno_t __cdecl _controlfp_s(
      unsigned int* _CurrentState,
           unsigned int  _NewValue,
           unsigned int  _Mask
    );


__declspec(dllimport) unsigned int __cdecl _statusfp(void);


__declspec(dllimport) void __cdecl _fpreset(void);















__declspec(dllimport) unsigned int __cdecl _control87(
      unsigned int _NewValue,
      unsigned int _Mask
    );











// Global variable holding floating point error code
 
__declspec(dllimport) int* __cdecl __fpecode(void);



 
__declspec(dllimport) int __cdecl __fpe_flt_rounds(void);







//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
// IEEE Recommended Functions
//
//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
  __declspec(dllimport) double __cdecl _copysign(  double _Number,   double _Sign);
  __declspec(dllimport) double __cdecl _chgsign(  double _X);
  __declspec(dllimport) double __cdecl _scalb(  double _X,   long _Y);
  __declspec(dllimport) double __cdecl _logb(  double _X);
  __declspec(dllimport) double __cdecl _nextafter(  double _X,   double _Y);
  __declspec(dllimport) int    __cdecl _finite(  double _X);
  __declspec(dllimport) int    __cdecl _isnan(  double _X);
  __declspec(dllimport) int    __cdecl _fpclass(  double _X);


      __declspec(dllimport) float __cdecl _scalbf(  float _X,   long _Y);
#line 324 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\float.h"



//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
//
// Nonstandard Names for Compatibility
//
//-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+


    
    
    

    
    __declspec(dllimport) void __cdecl fpreset(void);

    
    

    
    

    // For backwards compatibility with the old spelling
    
    

    
    
    
    
    
    
    

    
    
    

    
    
    
    
    

    
    
    
    

    

    
    
    
    
    
    

    
    
    
    

    
    
    
    
    
    

    
    
    
    

    

#line 403 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\float.h"



} __pragma(pack(pop))

#pragma warning(pop) 
#line 410 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\float.h"
#pragma external_header(pop)
#line 103 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
#pragma warning(pop)




#pragma warning(push)
#pragma warning(disable : 4987)
// C4987: Off by default noise
#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"
/***
*   intrin.h - declarations/definitions for platform specific intrinsic functions.
*
*       Copyright (c) Microsoft Corporation. All rights reserved.
*Purpose:
*   This include file contains the declarations for platform specific intrinsic
*   functions, or will include other files that have declaration of intrinsic
*   functions. Also there will be some platform specific macros to be used with
*   intrinsic functions.
*
****/

#pragma once





#pragma warning(push)
#pragma warning(disable:   4514 4820 )

#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"
/***
*   intrin0.inl.h - declarations of compiler intrinsics used by the C++
*                   Standard Library.
*
*       Copyright (c) Microsoft Corporation. All rights reserved.
*
*Purpose:
*   This header file declares compiler intrinsics that are used by the
*   C++ Standard Library, especially <atomic>. Compiler throughput is
*   the only reason that intrin0.inl.h is separate from intrin.h.
*
****/

#pragma once





#pragma warning(push)
#pragma warning(disable:   4514 4820 )


extern "C" {
#line 26 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"

/*
** __MACHINE              : everything
** __MACHINEX86           : x86 only
** __MACHINEX64           : x64 only
** __MACHINEX86_X64       : x86 and x64 only
** __MACHINEARM           : ARM only
** __MACHINEARM64         : ARM64 only
** __MACHINEARM_ARM64     : ARM and ARM64 only
** __MACHINEARM_ARM64_X64 : ARM and 64-bit Arch only
** __MACHINEARM64_X64     : ARM64 and x64 only
** __MACHINECHPEX86ARM64  : CHPE x86 on arm64 only
** __MACHINEWVMPURE       : /clr:pure only
** __MACHINEZ             : nothing
*/











/* Most intrinsics not available to pure managed code */



#line 57 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"


#line 60 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"






#line 67 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"




#line 72 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"




#line 77 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"




#line 82 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"

/* For compatibility with <winnt.h>, some intrinsics are __cdecl except on x64 */




#line 89 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"




#line 94 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"




#line 99 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"




#line 104 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"




#line 109 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"




#line 114 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"

/*******************************************************************
* Note: New intrinsics should be added here IF AND ONLY IF they're *
* being used by the C++ Standard Library.                          *
* OTHERWISE, new intrinsics should be added to intrin.h.           *
*******************************************************************/



unsigned char _BitScanForward(unsigned long * _Index, unsigned long _Mask);
unsigned char _BitScanForward64(unsigned long * _Index, unsigned __int64 _Mask);

unsigned char _BitScanReverse(unsigned long * _Index, unsigned long _Mask);
unsigned char _BitScanReverse64(unsigned long * _Index, unsigned __int64 _Mask);

unsigned char _bittest(long const *, long);


long _InterlockedAnd(long volatile * _Value, long _Mask);
short _InterlockedAnd16(short volatile * _Value, short _Mask);



__int64 _InterlockedAnd64(__int64 volatile * _Value, __int64 _Mask);
__int64 _interlockedand64(__int64 volatile * _Value, __int64 _Mask);



char _InterlockedAnd8(char volatile * _Value, char _Mask);






long  _InterlockedCompareExchange(long volatile * _Destination, long _Exchange, long _Comparand);

short _InterlockedCompareExchange16(short volatile * _Destination, short _Exchange, short _Comparand);



__int64 _InterlockedCompareExchange64(__int64 volatile * _Destination, __int64 _Exchange, __int64 _Comparand);



char _InterlockedCompareExchange8(char volatile * _Destination, char _Exchange, char _Comparand);






unsigned char _InterlockedCompareExchange128(__int64 volatile * _Destination, __int64 _ExchangeHigh, __int64 _ExchangeLow, __int64 * _ComparandResult);



long  _InterlockedDecrement(long volatile * _Addend);

short _InterlockedDecrement16(short volatile * _Addend);
__int64 _InterlockedDecrement64(__int64 volatile * _Addend);
__int64 _interlockeddecrement64(__int64 volatile * _Addend);
long  _InterlockedExchange(long volatile * _Target, long _Value);

short _InterlockedExchange16(short volatile * _Target, short _Value);



__int64 _InterlockedExchange64(__int64 volatile * _Target, __int64 _Value);
__int64 _interlockedexchange64(__int64 volatile * _Target, __int64 _Value);



char _InterlockedExchange8(char volatile * _Target, char _Value);



long  _InterlockedExchangeAdd(long volatile * _Addend, long _Value);
short _InterlockedExchangeAdd16(short volatile * _Addend, short _Value);



__int64 _InterlockedExchangeAdd64(__int64 volatile * _Addend, __int64 _Value);
__int64 _interlockedexchangeadd64(__int64 volatile * _Addend, __int64 _Value);



char _InterlockedExchangeAdd8(char volatile * _Addend, char _Value);









long  _InterlockedIncrement(long volatile * _Addend);

short _InterlockedIncrement16(short volatile * _Addend);
__int64 _InterlockedIncrement64(__int64 volatile * _Addend);
__int64 _interlockedincrement64(__int64 volatile * _Addend);

long _InterlockedOr(long volatile * _Value, long _Mask);
short _InterlockedOr16(short volatile * _Value, short _Mask);



__int64 _InterlockedOr64(__int64 volatile * _Value, __int64 _Mask);
__int64 _interlockedor64(__int64 volatile * _Value, __int64 _Mask);



char _InterlockedOr8(char volatile * _Value, char _Mask);






long _InterlockedXor(long volatile * _Value, long _Mask);
short _InterlockedXor16(short volatile * _Value, short _Mask);



__int64 _InterlockedXor64(__int64 volatile * _Value, __int64 _Mask);
__int64 _interlockedxor64(__int64 volatile * _Value, __int64 _Mask);



char _InterlockedXor8(char volatile * _Value, char _Mask);






void _ReadWriteBarrier(void);
__int16 __iso_volatile_load16(const volatile __int16 *);
__int32 __iso_volatile_load32(const volatile __int32 *);
__int64 __iso_volatile_load64(const volatile __int64 *);
__int8 __iso_volatile_load8(const volatile __int8 *);
void __iso_volatile_store16(volatile __int16 *, __int16);
void __iso_volatile_store32(volatile __int32 *, __int32);
void __iso_volatile_store64(volatile __int64 *, __int64);
void __iso_volatile_store8(volatile __int8 *, __int8);


unsigned char _interlockedbittestandset(long volatile *, long);



void _mm_pause(void);
unsigned int __lzcnt(unsigned int);
unsigned short __lzcnt16(unsigned short);
unsigned __int64 __lzcnt64(unsigned __int64);
unsigned int __popcnt(unsigned int);
unsigned short __popcnt16(unsigned short);
unsigned __int64 __popcnt64(unsigned __int64);
unsigned __int64 __shiftright128(unsigned __int64 _LowPart, unsigned __int64 _HighPart, unsigned char _Shift);

unsigned int _tzcnt_u32(unsigned int);
unsigned __int64 _tzcnt_u64(unsigned __int64);
#line 277 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"
unsigned __int64 _umul128(unsigned __int64 _Multiplier, unsigned __int64 _Multiplicand, unsigned __int64 * _HighProduct);
double __ceil(double);
float __ceilf(float);
double __floor(double);
float __floorf(float);
double __round(double);
float __roundf(float);
double __trunc(double);
float __truncf(float);
double __copysign(double, double);
float __copysignf(float, float);
unsigned __signbitvalue(double);
unsigned __signbitvaluef(float);
int _cvt_ftoi_sat (float a);
unsigned _cvt_ftoui_sat (float a);
long long _cvt_ftoll_sat (float a);
unsigned long long _cvt_ftoull_sat (float a);
int _cvt_ftoi_sent (float a);
unsigned _cvt_ftoui_sent (float a);
long long _cvt_ftoll_sent (float a);
unsigned long long _cvt_ftoull_sent (float a);
int _cvt_dtoi_sat (double a);
unsigned _cvt_dtoui_sat (double a);
long long _cvt_dtoll_sat (double a);
unsigned long long _cvt_dtoull_sat (double a);
int _cvt_dtoi_sent (double a);
unsigned _cvt_dtoui_sent (double a);
long long _cvt_dtoll_sent (double a);
unsigned long long _cvt_dtoull_sent (double a);


constexpr void * __cdecl __builtin_assume_aligned(const void *, size_t, ...) noexcept;


#line 312 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"
#line 313 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"

/*******************************************************************
* Note: New intrinsics should be added here IF AND ONLY IF they're *
* being used by the C++ Standard Library.                          *
* OTHERWISE, new intrinsics should be added to intrin.h.           *
*******************************************************************/


}
#line 323 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"
#pragma warning(pop) 
#line 325 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin0.inl.h"
#pragma external_header(pop)
#line 23 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"
#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\setjmp.h"
//
// setjmp.h
//
//      Copyright (c) Microsoft Corporation. All rights reserved.
//
// The C Standard Library <setjmp.h> header.
//
#pragma once












#pragma warning(push)
#pragma warning(disable:   4514 4820 )

__pragma(pack(push, 8)) extern "C" {



// Definitions specific to particular setjmp implementations.




















#line 50 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\setjmp.h"

    typedef struct __declspec(align(16)) _SETJMP_FLOAT128
    {
        unsigned __int64 Part[2];
    } SETJMP_FLOAT128;

    
    typedef SETJMP_FLOAT128 _JBTYPE;

    typedef struct _JUMP_BUFFER
    {
        unsigned __int64 Frame;
        unsigned __int64 Rbx;
        unsigned __int64 Rsp;
        unsigned __int64 Rbp;
        unsigned __int64 Rsi;
        unsigned __int64 Rdi;
        unsigned __int64 R12;
        unsigned __int64 R13;
        unsigned __int64 R14;
        unsigned __int64 R15;
        unsigned __int64 Rip;
        unsigned long MxCsr;
        unsigned short FpCsr;
        unsigned short Spare;

        SETJMP_FLOAT128 Xmm6;
        SETJMP_FLOAT128 Xmm7;
        SETJMP_FLOAT128 Xmm8;
        SETJMP_FLOAT128 Xmm9;
        SETJMP_FLOAT128 Xmm10;
        SETJMP_FLOAT128 Xmm11;
        SETJMP_FLOAT128 Xmm12;
        SETJMP_FLOAT128 Xmm13;
        SETJMP_FLOAT128 Xmm14;
        SETJMP_FLOAT128 Xmm15;
    } _JUMP_BUFFER;






















































#line 142 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\setjmp.h"



// Define the buffer type for holding the state information

    
    typedef _JBTYPE jmp_buf[16];
#line 150 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\setjmp.h"




    
#line 156 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\setjmp.h"



// Function prototypes
int __cdecl _setjmp(
      jmp_buf _Buf
    );


    __declspec(noreturn) void __cdecl longjmp(
          jmp_buf _Buf,
          int     _Value
        ) noexcept(false);





#line 175 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\setjmp.h"


} __pragma(pack(pop))

#pragma warning(pop) 
#pragma external_header(pop)
#line 24 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"



#line 28 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"



    
        #pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"
/*
 *  Copyright (C) 1985-2020 Intel Corporation.
 *
 *  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 */

/***
* immintrin.h - Meta Header file for Intel(R) Architecture intrinsic functions.
*
*******************************************************************************/

#pragma once



#line 17 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"







#line 25 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\wmmintrin.h"
/*
 *  Copyright (C) 1985-2015 Intel Corporation.
 *
 *  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 */

/*
 * wmmintrin.h
 *
 * Principal header file for Intel(R) AES and PCLMULQDQ intrinsics.
 */

#pragma once



#line 18 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\wmmintrin.h"







#line 26 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\wmmintrin.h"

#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\nmmintrin.h"
/*
 *  Copyright (C) 1985-2015 Intel Corporation.
 *
 *  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 */

/*
 * nmmintrin.h
 *
 * Principal header file for Intel(R) Core(TM) 2 Duo processor
 * SSE4.2 intrinsics.
 */

#pragma once



#line 19 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\nmmintrin.h"







#line 27 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\nmmintrin.h"

#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\smmintrin.h"
/*
 *  Copyright (C) 1985-2015 Intel Corporation.
 *
 *  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 */

/*
 * smmintrin.h
 *
 * Principal header file for Intel(R) Core(TM) 2 Duo processor
 * SSE4.1 intrinsics
 */

#pragma once



#line 19 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\smmintrin.h"







#line 27 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\smmintrin.h"

#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"
/*
 *  Copyright (C) 1985-2015 Intel Corporation.
 *
 *  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 */

#pragma once



#line 12 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"







#line 20 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"

#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\pmmintrin.h"
/*
 *  Copyright (C) 1985-2015 Intel Corporation.
 *
 *  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 */

/*
 * pmmintrin.h
 *
 * Principal header file for Intel(R) Pentium(R) 4 processor SSE3 intrinsics
 */

#pragma once



#line 18 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\pmmintrin.h"







#line 26 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\pmmintrin.h"

/*
 * We need emmintrin.h for the basic type declarations.
 */
#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"
/*
 *  Copyright (C) 1985-2015 Intel Corporation.
 *
 *  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 */

/*
 * emmintrin.h
 *
 * Principal header file for Willamette New Instruction intrinsics
 *
 * The intrinsics package can be used in 2 ways, based whether or not
 * _EMM_FUNCTIONALITY is defined; if it is, the C implementation
 * will be used (the "functional intrinsics").
 */

#pragma once



#line 22 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"







#line 30 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"



#line 34 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"

/*
 * the __m128 & __m64 types are required for the intrinsics
 */
#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"
/*
 *  Copyright (C) 1985-2015 Intel Corporation.
 *
 *  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 */

/*
 * xmmintrin.h
 *
 * Principal header file for Streaming SIMD Extensions intrinsics
 *
 * The intrinsics package can be used in 2 ways, based whether or not
 * _MM_FUNCTIONALITY is defined; if it is, the C/x87 implementation
 * will be used (the "faux intrinsics").
 *
 *
 * Note that the m128 datatype provided using _MM2_FUNCTIONALITY mode is
 *   implemented as struct, will not be 128b aligned, will be passed
 *   via the stack, etc.  MM_FUNCTIONALITY mode is not intended for
 *   performance, just semantics.
 *
 */

#pragma once



#line 29 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"







#line 37 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"

/*
 * the m64 type is required for the integer Streaming SIMD Extensions intrinsics
 */

#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\mmintrin.h"
/*
 *  Copyright (C) 1985-2015 Intel Corporation.
 *
 *  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 */

/*
 * Definitions and declarations for use with compiler intrinsics.
 */

#pragma once






#line 19 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\mmintrin.h"



#line 23 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\mmintrin.h"


extern "C" { /* Begin "C" */
/* Intrinsics use C name-mangling.
 */
#line 29 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\mmintrin.h"

typedef union __declspec(intrin_type) __declspec(align(8)) __m64
{
    unsigned __int64    m64_u64;
    float               m64_f32[2];
    __int8              m64_i8[8];
    __int16             m64_i16[4];
    __int32             m64_i32[2];
    __int64             m64_i64;
    unsigned __int8     m64_u8[8];
    unsigned __int16    m64_u16[4];
    unsigned __int32    m64_u32[2];
} __m64;









































































































































#line 180 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\mmintrin.h"


}; /* End "C" */
#line 184 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\mmintrin.h"

#line 186 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\mmintrin.h"
#line 187 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\mmintrin.h"
#line 188 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\mmintrin.h"
#pragma external_header(pop)
#line 43 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"
#line 44 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"

















typedef union __declspec(intrin_type) __declspec(align(16)) __m128 {
     float               m128_f32[4];
     unsigned __int64    m128_u64[2];
     __int8              m128_i8[16];
     __int16             m128_i16[8];
     __int32             m128_i32[4];
     __int64             m128_i64[2];
     unsigned __int8     m128_u8[16];
     unsigned __int16    m128_u16[8];
     unsigned __int32    m128_u32[4];
 } __m128;


/* pick up _mm_malloc() and _mm_free() */
#pragma external_header(push)
#line 1 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\malloc.h"
//
// malloc.h
//
//      Copyright (c) Microsoft Corporation. All rights reserved.
//
// The memory allocation library.
//
#pragma once




#pragma external_header(push)
#line 1 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_malloc.h"
//
// corecrt_malloc.h
//
//      Copyright (c) Microsoft Corporation. All rights reserved.
//
// The memory allocation library.  These pieces of the allocation library are
// shared by both <stdlib.h> and <malloc.h>.
//
#pragma once



#pragma warning(push)
#pragma warning(disable: 4324  4514 4574 4710 4793 4820 4995 4996 28719 28726 28727 )


__pragma(pack(push, 8)) extern "C" {





































#line 56 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_malloc.h"

     
__declspec(dllimport) __declspec(allocator) __declspec(restrict)
void* __cdecl _calloc_base(
      size_t _Count,
      size_t _Size
    );

     
__declspec(dllimport)  __declspec(allocator) __declspec(restrict) 
void* __cdecl calloc(
       size_t _Count,
       size_t _Size
    );

 
__declspec(dllimport) int __cdecl _callnewh(
      size_t _Size
    );

     
__declspec(dllimport) __declspec(allocator) 
void* __cdecl _expand(
                void*  _Block,
       size_t _Size
    );

__declspec(dllimport)
void __cdecl _free_base(
        void* _Block
    );

__declspec(dllimport) 
void __cdecl free(
        void* _Block
    );

     
__declspec(dllimport) __declspec(allocator) __declspec(restrict)
void* __cdecl _malloc_base(
      size_t _Size
    );

     
__declspec(dllimport) __declspec(allocator)  __declspec(restrict) 
void* __cdecl malloc(
       size_t _Size
    );

 
__declspec(dllimport)
size_t __cdecl _msize_base(
      void* _Block
    );

 
__declspec(dllimport) 
size_t __cdecl _msize(
      void* _Block
    );

       
__declspec(dllimport) __declspec(allocator) __declspec(restrict)
void* __cdecl _realloc_base(
         void*  _Block,
                                 size_t _Size
    );

       
__declspec(dllimport) __declspec(allocator) __declspec(restrict) 
void* __cdecl realloc(
        void*  _Block,
              size_t _Size
    );

       
__declspec(dllimport) __declspec(allocator) __declspec(restrict)
void* __cdecl _recalloc_base(
        void*  _Block,
                                size_t _Count,
                                size_t _Size
    );

       
__declspec(dllimport) __declspec(allocator) __declspec(restrict)
void* __cdecl _recalloc(
        void*  _Block,
              size_t _Count,
              size_t _Size
    );

__declspec(dllimport)
void __cdecl _aligned_free(
        void* _Block
    );

     
__declspec(dllimport) __declspec(allocator) __declspec(restrict)
void* __cdecl _aligned_malloc(
       size_t _Size,
                         size_t _Alignment
    );

     
__declspec(dllimport) __declspec(allocator) __declspec(restrict)
void* __cdecl _aligned_offset_malloc(
       size_t _Size,
                         size_t _Alignment,
                         size_t _Offset
    );

 
__declspec(dllimport)
size_t __cdecl _aligned_msize(
      void*  _Block,
               size_t _Alignment,
               size_t _Offset
    );

       
__declspec(dllimport) __declspec(allocator) __declspec(restrict)
void* __cdecl _aligned_offset_realloc(
        void*  _Block,
              size_t _Size,
                                size_t _Alignment,
                                size_t _Offset
    );

       
__declspec(dllimport) __declspec(allocator) __declspec(restrict)
void* __cdecl _aligned_offset_recalloc(
        void*  _Block,
              size_t _Count,
              size_t _Size,
                                size_t _Alignment,
                                size_t _Offset
    );

       
__declspec(dllimport) __declspec(allocator) __declspec(restrict)
void* __cdecl _aligned_realloc(
        void*  _Block,
              size_t _Size,
                                size_t _Alignment
    );

       
__declspec(dllimport) __declspec(allocator) __declspec(restrict)
void* __cdecl _aligned_recalloc(
        void*  _Block,
              size_t _Count,
              size_t _Size,
                                size_t _Alignment
    );


















#line 229 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\corecrt_malloc.h"



} __pragma(pack(pop))

#pragma warning(pop) 
#pragma external_header(pop)
#line 14 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\malloc.h"

#pragma warning(push)
#pragma warning(disable: 4324  4514 4574 4710 4793 4820 4995 4996 28719 28726 28727 )


__pragma(pack(push, 8)) extern "C" {



// Maximum heap request the heap manager will attempt

    


#line 29 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\malloc.h"



// Constants for _heapchk and _heapwalk routines









typedef struct _heapinfo
{
    int* _pentry;
    size_t _size;
    int _useflag;
} _HEAPINFO;






   
void* __cdecl _alloca(  size_t _Size);





    __declspec(dllimport) intptr_t __cdecl _get_heap_handle(void);

     
    __declspec(dllimport) int __cdecl _heapmin(void);

    
        __declspec(dllimport) int __cdecl _heapwalk(  _HEAPINFO* _EntryInfo);
    #line 69 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\malloc.h"

    
          __declspec(dllimport) int __cdecl _heapchk(void);
    #line 73 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\malloc.h"

    __declspec(dllimport) int __cdecl _resetstkoflw(void);

    
    
    

    
        
    

#line 85 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\malloc.h"

    static_assert((sizeof(unsigned int) <= 16), "sizeof(unsigned int) <= _ALLOCA_S_MARKER_SIZE");


    #pragma warning(push)
    #pragma warning(disable: 6540) 
                                   // of its existing __declspec annotations

    __inline void* _MarkAllocaS(   void* _Ptr, unsigned int _Marker)
    {
        if (_Ptr)
        {
            *((unsigned int*)_Ptr) = _Marker;
            _Ptr = (char*)_Ptr + 16;
        }
        return _Ptr;
    }

    __inline size_t _MallocaComputeSize(size_t _Size)
    {
        size_t _MarkedSize = _Size + 16;
        return _MarkedSize > _Size ? _MarkedSize : 0;
    }

    #pragma warning(pop)

#line 112 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\malloc.h"




// C6255: _alloca indicates failure by raising a stack overflow exception
// C6386: buffer overrun
    
        
        




    #line 126 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\malloc.h"












#line 139 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\malloc.h"




#line 144 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\malloc.h"
#line 145 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\malloc.h"

    

    #pragma warning(push)
    #pragma warning(disable: 6014) 
    __inline void __cdecl _freea(    void* _Memory)
    {
        unsigned int _Marker;
        if (_Memory)
        {
            _Memory = (char*)_Memory - 16;
            _Marker = *(unsigned int*)_Memory;
            if (_Marker == 0xDDDD)
            {
                free(_Memory);
            }
            





        }
    }
    #pragma warning(pop)

#line 172 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\malloc.h"




    
#line 178 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\malloc.h"



} __pragma(pack(pop))

#pragma warning(pop) 
#line 185 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\malloc.h"
#pragma external_header(pop)
#line 76 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"
#line 77 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"
#line 78 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"

 /*******************************************************/
 /* MACRO for shuffle parameter for _mm_shuffle_ps().   */
 /* Argument fp3 is a digit[0123] that represents the fp*/
 /* from argument "b" of mm_shuffle_ps that will be     */
 /* placed in fp3 of result. fp2 is the same for fp2 in */
 /* result. fp1 is a digit[0123] that represents the fp */
 /* from argument "a" of mm_shuffle_ps that will be     */
 /* places in fp1 of result. fp0 is the same for fp0 of */
 /* result                                              */
 /*******************************************************/




 /*******************************************************/
 /* MACRO for performing the transpose of a 4x4 matrix  */
 /* of single precision floating point values.          */
 /* Arguments row0, row1, row2, and row3 are __m128     */
 /* values whose elements form the corresponding rows   */
 /* of a 4x4 matrix.  The matrix transpose is returned  */
 /* in arguments row0, row1, row2, and row3 where row0  */
 /* now holds column 0 of the original matrix, row1 now */
 /* holds column 1 of the original matrix, etc.         */
 /*******************************************************/















/* constants for use with _mm_prefetch */





// The values below are not yet supported.
//#define _MM_HINT_ET0    5
//#define _MM_HINT_ET1    6
//#define _MM_HINT_ET2    7

/* (this declspec not supported with 0.A or 0.B) */


/* MACRO functions for setting and reading the MXCSR */














































 /*****************************************************/
 /*     INTRINSICS FUNCTION PROTOTYPES START HERE     */
 /*****************************************************/


extern "C" { /* Begin "C" */
  /* Intrinsics use C name-mangling. */
#line 187 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"

/*
 * FP, arithmetic
 */

extern __m128 _mm_add_ss(__m128 _A, __m128 _B);
extern __m128 _mm_add_ps(__m128 _A, __m128 _B);
extern __m128 _mm_sub_ss(__m128 _A, __m128 _B);
extern __m128 _mm_sub_ps(__m128 _A, __m128 _B);
extern __m128 _mm_mul_ss(__m128 _A, __m128 _B);
extern __m128 _mm_mul_ps(__m128 _A, __m128 _B);
extern __m128 _mm_div_ss(__m128 _A, __m128 _B);
extern __m128 _mm_div_ps(__m128 _A, __m128 _B);
extern __m128 _mm_sqrt_ss(__m128 _A);
extern __m128 _mm_sqrt_ps(__m128 _A);
extern __m128 _mm_rcp_ss(__m128 _A);
extern __m128 _mm_rcp_ps(__m128 _A);
extern __m128 _mm_rsqrt_ss(__m128 _A);
extern __m128 _mm_rsqrt_ps(__m128 _A);
extern __m128 _mm_min_ss(__m128 _A, __m128 _B);
extern __m128 _mm_min_ps(__m128 _A, __m128 _B);
extern __m128 _mm_max_ss(__m128 _A, __m128 _B);
extern __m128 _mm_max_ps(__m128 _A, __m128 _B);

/*
 * FP, logical
 */

extern __m128 _mm_and_ps(__m128 _A, __m128 _B);
extern __m128 _mm_andnot_ps(__m128 _A, __m128 _B);
extern __m128 _mm_or_ps(__m128 _A, __m128 _B);
extern __m128 _mm_xor_ps(__m128 _A, __m128 _B);

/*
 * FP, comparison
 */

extern __m128 _mm_cmpeq_ss(__m128 _A, __m128 _B);
extern __m128 _mm_cmpeq_ps(__m128 _A, __m128 _B);
extern __m128 _mm_cmplt_ss(__m128 _A, __m128 _B);
extern __m128 _mm_cmplt_ps(__m128 _A, __m128 _B);
extern __m128 _mm_cmple_ss(__m128 _A, __m128 _B);
extern __m128 _mm_cmple_ps(__m128 _A, __m128 _B);
extern __m128 _mm_cmpgt_ss(__m128 _A, __m128 _B);
extern __m128 _mm_cmpgt_ps(__m128 _A, __m128 _B);
extern __m128 _mm_cmpge_ss(__m128 _A, __m128 _B);
extern __m128 _mm_cmpge_ps(__m128 _A, __m128 _B);
extern __m128 _mm_cmpneq_ss(__m128 _A, __m128 _B);
extern __m128 _mm_cmpneq_ps(__m128 _A, __m128 _B);
extern __m128 _mm_cmpnlt_ss(__m128 _A, __m128 _B);
extern __m128 _mm_cmpnlt_ps(__m128 _A, __m128 _B);
extern __m128 _mm_cmpnle_ss(__m128 _A, __m128 _B);
extern __m128 _mm_cmpnle_ps(__m128 _A, __m128 _B);
extern __m128 _mm_cmpngt_ss(__m128 _A, __m128 _B);
extern __m128 _mm_cmpngt_ps(__m128 _A, __m128 _B);
extern __m128 _mm_cmpnge_ss(__m128 _A, __m128 _B);
extern __m128 _mm_cmpnge_ps(__m128 _A, __m128 _B);
extern __m128 _mm_cmpord_ss(__m128 _A, __m128 _B);
extern __m128 _mm_cmpord_ps(__m128 _A, __m128 _B);
extern __m128 _mm_cmpunord_ss(__m128 _A, __m128 _B);
extern __m128 _mm_cmpunord_ps(__m128 _A, __m128 _B);
extern int _mm_comieq_ss(__m128 _A, __m128 _B);
extern int _mm_comilt_ss(__m128 _A, __m128 _B);
extern int _mm_comile_ss(__m128 _A, __m128 _B);
extern int _mm_comigt_ss(__m128 _A, __m128 _B);
extern int _mm_comige_ss(__m128 _A, __m128 _B);
extern int _mm_comineq_ss(__m128 _A, __m128 _B);
extern int _mm_ucomieq_ss(__m128 _A, __m128 _B);
extern int _mm_ucomilt_ss(__m128 _A, __m128 _B);
extern int _mm_ucomile_ss(__m128 _A, __m128 _B);
extern int _mm_ucomigt_ss(__m128 _A, __m128 _B);
extern int _mm_ucomige_ss(__m128 _A, __m128 _B);
extern int _mm_ucomineq_ss(__m128 _A, __m128 _B);

/*
 * FP, conversions
 */

extern int _mm_cvt_ss2si(__m128 _A);
extern int _mm_cvtt_ss2si(__m128 _A);
extern __m128 _mm_cvt_si2ss(__m128, int);
extern float _mm_cvtss_f32(__m128 _A);








#line 278 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"


/*
 * Support for 64-bit intrinsics
 */
extern __int64 _mm_cvtss_si64(__m128 _A);
extern __int64 _mm_cvttss_si64(__m128 _A);
extern __m128  _mm_cvtsi64_ss(__m128 _A, __int64 _B);
#line 287 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"

/*
 * FP, misc
 */

extern __m128 _mm_shuffle_ps(__m128 _A, __m128 _B, unsigned int _Imm8);
extern __m128 _mm_unpackhi_ps(__m128 _A, __m128 _B);
extern __m128 _mm_unpacklo_ps(__m128 _A, __m128 _B);
extern __m128 _mm_loadh_pi(__m128, __m64 const*);
extern __m128 _mm_movehl_ps(__m128, __m128);
extern __m128 _mm_movelh_ps(__m128, __m128);
extern void _mm_storeh_pi(__m64 *, __m128);
extern __m128 _mm_loadl_pi(__m128, __m64 const*);
extern void _mm_storel_pi(__m64 *, __m128);
extern int _mm_movemask_ps(__m128 _A);



















#line 322 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"

/*
 * memory & initialization
 */

extern __m128 _mm_set_ss(float _A);
extern __m128 _mm_set_ps1(float _A);
extern __m128 _mm_set_ps(float _A, float _B, float _C, float _D);
extern __m128 _mm_setr_ps(float _A, float _B, float _C, float _D);
extern __m128 _mm_setzero_ps(void);
extern __m128 _mm_load_ss(float const*_A);
extern __m128 _mm_load_ps1(float const*_A);
extern __m128 _mm_load_ps(float const*_A);
extern __m128 _mm_loadr_ps(float const*_A);
extern __m128 _mm_loadu_ps(float const*_A);
extern void _mm_store_ss(float *_V, __m128 _A);
extern void _mm_store_ps1(float *_V, __m128 _A);
extern void _mm_store_ps(float *_V, __m128 _A);
extern void _mm_storer_ps(float *_V, __m128 _A);
extern void _mm_storeu_ps(float *_V, __m128 _A);
extern void _mm_prefetch(char const*_A, int _Sel);


#line 346 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"
extern void _mm_stream_ps(float *, __m128);
extern __m128 _mm_move_ss(__m128 _A, __m128 _B);

extern void _mm_sfence(void);
extern unsigned int _mm_getcsr(void);
extern void _mm_setcsr(unsigned int);






/* Alternate intrinsic names definition */

















#line 377 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"







 /******************************************************/
 /* UTILITY INTRINSICS FUNCTION DEFINITIONS START HERE */
 /******************************************************/





















































































































#line 505 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"



}; /* End "C" */
#line 510 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"

#line 512 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"
#line 513 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"
#line 514 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\xmmintrin.h"
#pragma external_header(pop)
#line 39 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"

typedef union __declspec(intrin_type) __declspec(align(16)) __m128i {
    __int8              m128i_i8[16];
    __int16             m128i_i16[8];
    __int32             m128i_i32[4];
    __int64             m128i_i64[2];
    unsigned __int8     m128i_u8[16];
    unsigned __int16    m128i_u16[8];
    unsigned __int32    m128i_u32[4];
    unsigned __int64    m128i_u64[2];
} __m128i;

typedef struct __declspec(intrin_type) __declspec(align(16)) __m128d {
    double              m128d_f64[2];
} __m128d;

/*
 * Macro function for shuffle
 */


 /*****************************************************/
 /*     INTRINSICS FUNCTION PROTOTYPES START HERE     */
 /*****************************************************/


extern "C" { /* Begin "C" */
  /* Intrinsics use C name-mangling. */
#line 68 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"

/*
 * DP, arithmetic
 */

extern __m128d _mm_add_sd(__m128d _A, __m128d _B);
extern __m128d _mm_add_pd(__m128d _A, __m128d _B);
extern __m128d _mm_sub_sd(__m128d _A, __m128d _B);
extern __m128d _mm_sub_pd(__m128d _A, __m128d _B);
extern __m128d _mm_mul_sd(__m128d _A, __m128d _B);
extern __m128d _mm_mul_pd(__m128d _A, __m128d _B);
extern __m128d _mm_sqrt_sd(__m128d _A, __m128d _B);
extern __m128d _mm_sqrt_pd(__m128d _A);
extern __m128d _mm_div_sd(__m128d _A, __m128d _B);
extern __m128d _mm_div_pd(__m128d _A, __m128d _B);
extern __m128d _mm_min_sd(__m128d _A, __m128d _B);
extern __m128d _mm_min_pd(__m128d _A, __m128d _B);
extern __m128d _mm_max_sd(__m128d _A, __m128d _B);
extern __m128d _mm_max_pd(__m128d _A, __m128d _B);

/*
 * DP, logicals
 */

extern __m128d _mm_and_pd(__m128d _A, __m128d _B);
extern __m128d _mm_andnot_pd(__m128d _A, __m128d _B);
extern __m128d _mm_or_pd(__m128d _A, __m128d _B);
extern __m128d _mm_xor_pd(__m128d _A, __m128d _B);

/*
 * DP, comparisons
 */

extern __m128d _mm_cmpeq_sd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpeq_pd(__m128d _A, __m128d _B);
extern __m128d _mm_cmplt_sd(__m128d _A, __m128d _B);
extern __m128d _mm_cmplt_pd(__m128d _A, __m128d _B);
extern __m128d _mm_cmple_sd(__m128d _A, __m128d _B);
extern __m128d _mm_cmple_pd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpgt_sd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpgt_pd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpge_sd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpge_pd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpneq_sd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpneq_pd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpnlt_sd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpnlt_pd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpnle_sd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpnle_pd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpngt_sd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpngt_pd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpnge_sd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpnge_pd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpord_pd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpord_sd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpunord_pd(__m128d _A, __m128d _B);
extern __m128d _mm_cmpunord_sd(__m128d _A, __m128d _B);
extern int _mm_comieq_sd(__m128d _A, __m128d _B);
extern int _mm_comilt_sd(__m128d _A, __m128d _B);
extern int _mm_comile_sd(__m128d _A, __m128d _B);
extern int _mm_comigt_sd(__m128d _A, __m128d _B);
extern int _mm_comige_sd(__m128d _A, __m128d _B);
extern int _mm_comineq_sd(__m128d _A, __m128d _B);
extern int _mm_ucomieq_sd(__m128d _A, __m128d _B);
extern int _mm_ucomilt_sd(__m128d _A, __m128d _B);
extern int _mm_ucomile_sd(__m128d _A, __m128d _B);
extern int _mm_ucomigt_sd(__m128d _A, __m128d _B);
extern int _mm_ucomige_sd(__m128d _A, __m128d _B);
extern int _mm_ucomineq_sd(__m128d _A, __m128d _B);

/*
 * DP, converts
 */

extern __m128d _mm_cvtepi32_pd(__m128i _A);
extern __m128i _mm_cvtpd_epi32(__m128d _A);
extern __m128i _mm_cvttpd_epi32(__m128d _A);
extern __m128 _mm_cvtepi32_ps(__m128i _A);
extern __m128i _mm_cvtps_epi32(__m128 _A);
extern __m128i _mm_cvttps_epi32(__m128 _A);
extern __m128 _mm_cvtpd_ps(__m128d _A);
extern __m128d _mm_cvtps_pd(__m128 _A);
extern __m128 _mm_cvtsd_ss(__m128 _A, __m128d _B);
extern __m128d _mm_cvtss_sd(__m128d _A, __m128 _B);

extern int _mm_cvtsd_si32(__m128d _A);
extern int _mm_cvttsd_si32(__m128d _A);
extern __m128d _mm_cvtsi32_sd(__m128d _A, int _B);





#line 162 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"

/*
 * DP, misc
 */

extern __m128d _mm_unpackhi_pd(__m128d _A, __m128d _B);
extern __m128d _mm_unpacklo_pd(__m128d _A, __m128d _B);
extern int _mm_movemask_pd(__m128d _A);
extern __m128d _mm_shuffle_pd(__m128d _A, __m128d _B, int _I);

/*
 * DP, loads
 */

extern __m128d _mm_load_pd(double const*_Dp);
extern __m128d _mm_load1_pd(double const*_Dp);
extern __m128d _mm_loadr_pd(double const*_Dp);
extern __m128d _mm_loadu_pd(double const*_Dp);
extern __m128d _mm_load_sd(double const*_Dp);
extern __m128d _mm_loadh_pd(__m128d _A, double const*_Dp);
extern __m128d _mm_loadl_pd(__m128d _A, double const*_Dp);

/*
 * DP, sets
 */

extern __m128d _mm_set_sd(double _W);
extern __m128d _mm_set1_pd(double _A);
extern __m128d _mm_set_pd(double _Z, double _Y);
extern __m128d _mm_setr_pd(double _Y, double _Z);
extern __m128d _mm_setzero_pd(void);
extern __m128d _mm_move_sd(__m128d _A, __m128d _B);

/*
 * DP, stores
 */

extern void _mm_store_sd(double *_Dp, __m128d _A);
extern void _mm_store1_pd(double *_Dp, __m128d _A);
extern void _mm_store_pd(double *_Dp, __m128d _A);
extern void _mm_storeu_pd(double *_Dp, __m128d _A);
extern void _mm_storer_pd(double *_Dp, __m128d _A);
extern void _mm_storeh_pd(double *_Dp, __m128d _A);
extern void _mm_storel_pd(double *_Dp, __m128d _A);

/* Alternate intrinsic names definition */




/*
 * Integer, arithmetic
 */

extern __m128i _mm_add_epi8(__m128i _A, __m128i _B);
extern __m128i _mm_add_epi16(__m128i _A, __m128i _B);
extern __m128i _mm_add_epi32(__m128i _A, __m128i _B);


#line 222 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"
extern __m128i _mm_add_epi64(__m128i _A, __m128i _B);
extern __m128i _mm_adds_epi8(__m128i _A, __m128i _B);
extern __m128i _mm_adds_epi16(__m128i _A, __m128i _B);
extern __m128i _mm_adds_epu8(__m128i _A, __m128i _B);
extern __m128i _mm_adds_epu16(__m128i _A, __m128i _B);
extern __m128i _mm_avg_epu8(__m128i _A, __m128i _B);
extern __m128i _mm_avg_epu16(__m128i _A, __m128i _B);
extern __m128i _mm_madd_epi16(__m128i _A, __m128i _B);
extern __m128i _mm_max_epi16(__m128i _A, __m128i _B);
extern __m128i _mm_max_epu8(__m128i _A, __m128i _B);
extern __m128i _mm_min_epi16(__m128i _A, __m128i _B);
extern __m128i _mm_min_epu8(__m128i _A, __m128i _B);
extern __m128i _mm_mulhi_epi16(__m128i _A, __m128i _B);
extern __m128i _mm_mulhi_epu16(__m128i _A, __m128i _B);
extern __m128i _mm_mullo_epi16(__m128i _A, __m128i _B);


#line 240 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"
extern __m128i _mm_mul_epu32(__m128i _A, __m128i _B);
extern __m128i _mm_sad_epu8(__m128i _A, __m128i _B);
extern __m128i _mm_sub_epi8(__m128i _A, __m128i _B);
extern __m128i _mm_sub_epi16(__m128i _A, __m128i _B);
extern __m128i _mm_sub_epi32(__m128i _A, __m128i _B);


#line 248 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"
extern __m128i _mm_sub_epi64(__m128i _A, __m128i _B);
extern __m128i _mm_subs_epi8(__m128i _A, __m128i _B);
extern __m128i _mm_subs_epi16(__m128i _A, __m128i _B);
extern __m128i _mm_subs_epu8(__m128i _A, __m128i _B);
extern __m128i _mm_subs_epu16(__m128i _A, __m128i _B);

/*
 * Integer, logicals
 */

extern __m128i _mm_and_si128(__m128i _A, __m128i _B);
extern __m128i _mm_andnot_si128(__m128i _A, __m128i _B);
extern __m128i _mm_or_si128(__m128i _A, __m128i _B);
extern __m128i _mm_xor_si128(__m128i _A, __m128i _B);

/*
 * Integer, shifts
 */

extern __m128i _mm_slli_si128(__m128i _A, int _Imm);

extern __m128i _mm_slli_epi16(__m128i _A, int _Count);
extern __m128i _mm_sll_epi16(__m128i _A, __m128i _Count);
extern __m128i _mm_slli_epi32(__m128i _A, int _Count);
extern __m128i _mm_sll_epi32(__m128i _A, __m128i _Count);
extern __m128i _mm_slli_epi64(__m128i _A, int _Count);
extern __m128i _mm_sll_epi64(__m128i _A, __m128i _Count);
extern __m128i _mm_srai_epi16(__m128i _A, int _Count);
extern __m128i _mm_sra_epi16(__m128i _A, __m128i _Count);
extern __m128i _mm_srai_epi32(__m128i _A, int _Count);
extern __m128i _mm_sra_epi32(__m128i _A, __m128i _Count);
extern __m128i _mm_srli_si128(__m128i _A, int _Imm);

extern __m128i _mm_srli_epi16(__m128i _A, int _Count);
extern __m128i _mm_srl_epi16(__m128i _A, __m128i _Count);
extern __m128i _mm_srli_epi32(__m128i _A, int _Count);
extern __m128i _mm_srl_epi32(__m128i _A, __m128i _Count);
extern __m128i _mm_srli_epi64(__m128i _A, int _Count);
extern __m128i _mm_srl_epi64(__m128i _A, __m128i _Count);

/*
 * Integer, comparisons
 */

extern __m128i _mm_cmpeq_epi8(__m128i _A, __m128i _B);
extern __m128i _mm_cmpeq_epi16(__m128i _A, __m128i _B);
extern __m128i _mm_cmpeq_epi32(__m128i _A, __m128i _B);
extern __m128i _mm_cmpgt_epi8(__m128i _A, __m128i _B);
extern __m128i _mm_cmpgt_epi16(__m128i _A, __m128i _B);
extern __m128i _mm_cmpgt_epi32(__m128i _A, __m128i _B);
extern __m128i _mm_cmplt_epi8(__m128i _A, __m128i _B);
extern __m128i _mm_cmplt_epi16(__m128i _A, __m128i _B);
extern __m128i _mm_cmplt_epi32(__m128i _A, __m128i _B);

/*
 * Integer, converts
 */

extern __m128i _mm_cvtsi32_si128(int _A);
extern int _mm_cvtsi128_si32(__m128i _A);

/*
 * Integer, misc
 */

extern __m128i _mm_packs_epi16(__m128i _A, __m128i _B);
extern __m128i _mm_packs_epi32(__m128i _A, __m128i _B);
extern __m128i _mm_packus_epi16(__m128i _A, __m128i _B);
extern int _mm_extract_epi16(__m128i _A, int _Imm);
extern __m128i _mm_insert_epi16(__m128i _A, int _B, int _Imm);
extern int _mm_movemask_epi8(__m128i _A);
extern __m128i _mm_shuffle_epi32(__m128i _A, int _Imm);
extern __m128i _mm_shufflehi_epi16(__m128i _A, int _Imm);
extern __m128i _mm_shufflelo_epi16(__m128i _A, int _Imm);
extern __m128i _mm_unpackhi_epi8(__m128i _A, __m128i _B);
extern __m128i _mm_unpackhi_epi16(__m128i _A, __m128i _B);
extern __m128i _mm_unpackhi_epi32(__m128i _A, __m128i _B);
extern __m128i _mm_unpackhi_epi64(__m128i _A, __m128i _B);
extern __m128i _mm_unpacklo_epi8(__m128i _A, __m128i _B);
extern __m128i _mm_unpacklo_epi16(__m128i _A, __m128i _B);
extern __m128i _mm_unpacklo_epi32(__m128i _A, __m128i _B);
extern __m128i _mm_unpacklo_epi64(__m128i _A, __m128i _B);

/*
 * Integer, loads
 */

extern __m128i _mm_load_si128(__m128i const*_P);
extern __m128i _mm_loadu_si128(__m128i const*_P);
extern __m128i _mm_loadl_epi64(__m128i const*_P);

/*
 * Integer, sets
 */



#line 346 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"
extern __m128i _mm_set_epi64x(__int64 _I1,__int64 _I0);
extern __m128i _mm_set_epi32(int _I3, int _I2, int _I1, int _I0);
extern __m128i _mm_set_epi16(short _W7, short _W6, short _W5, short _W4,
                             short _W3, short _W2, short _W1, short _W0);
extern __m128i _mm_set_epi8(char _B15, char _B14, char _B13, char _B12,
                            char _B11, char _B10, char _B9, char _B8,
                            char _B7, char _B6, char _B5, char _B4,
                            char _B3, char _B2, char _B1, char _B0);


#line 357 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"
extern __m128i _mm_set1_epi64x(__int64 i);
extern __m128i _mm_set1_epi32(int _I);
extern __m128i _mm_set1_epi16(short _W);
extern __m128i _mm_set1_epi8(char _B);
extern __m128i _mm_setl_epi64(__m128i _Q);


#line 365 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"
extern __m128i _mm_setr_epi32(int _I0, int _I1, int _I2, int _I3);
extern __m128i _mm_setr_epi16(short _W0, short _W1, short _W2, short _W3,
                              short _W4, short _W5, short _W6, short _W7);
extern __m128i _mm_setr_epi8(char _B15, char _B14, char _B13, char _B12,
                             char _B11, char _B10, char _B9, char _B8,
                             char _B7, char _B6, char _B5, char _B4,
                             char _B3, char _B2, char _B1, char _B0);
extern __m128i _mm_setzero_si128(void);

/*
 * Integer, stores
 */

extern void _mm_store_si128(__m128i *_P, __m128i _B);
extern void _mm_storeu_si128(__m128i *_P, __m128i _B);
extern void _mm_storel_epi64(__m128i *_P, __m128i _Q);
extern void _mm_maskmoveu_si128(__m128i _D, __m128i _N, char *_P);

/*
 * Integer, moves
 */

extern __m128i _mm_move_epi64(__m128i _Q);



#line 392 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"

/*
 * Cacheability support
 */

extern void _mm_stream_pd(double *_Dp, __m128d _A);
extern void _mm_stream_si128(__m128i *_P, __m128i _A);
extern void _mm_clflush(void const*_P);
extern void _mm_lfence(void);
extern void _mm_mfence(void);
extern void _mm_stream_si32(int *_P, int _I);
extern void _mm_pause(void);

/*
 * New convert to float
 */

extern double _mm_cvtsd_f64(__m128d _A);

/*
 * Support for casting between various SP, DP, INT vector types.
 * Note that these do no conversion of values, they just change
 * the type.
 */

extern __m128  _mm_castpd_ps(__m128d);
extern __m128i _mm_castpd_si128(__m128d);
extern __m128d _mm_castps_pd(__m128);
extern __m128i _mm_castps_si128(__m128);
extern __m128  _mm_castsi128_ps(__m128i);
extern __m128d _mm_castsi128_pd(__m128i);

/*
 * Support for 64-bit extension intrinsics
 */


extern __int64 _mm_cvtsd_si64(__m128d);
extern __int64 _mm_cvttsd_si64(__m128d);
extern __m128d _mm_cvtsi64_sd(__m128d, __int64);
extern __m128i _mm_cvtsi64_si128(__int64);
extern __int64 _mm_cvtsi128_si64(__m128i);
/* Alternate intrinsic name definitions */

#line 437 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"


}; /* End "C" */
#line 441 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"

#line 443 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"
#line 444 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"
#line 445 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\emmintrin.h"
#pragma external_header(pop)
#line 31 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\pmmintrin.h"

 /*****************************************************/
 /*     MACROS FOR USE WITH INTRINSICS                */
 /*****************************************************/

/*
 * MACRO functions for setting and reading the DAZ bit in the MXCSR
 */










 /*****************************************************/
 /*     INTRINSICS FUNCTION PROTOTYPES START HERE     */
 /*****************************************************/


extern "C" { /* Begin "C" */
  /* Intrinsics use C name-mangling. */
#line 57 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\pmmintrin.h"

/*
 * New Single precision vector instructions.
 */

extern __m128 _mm_addsub_ps(__m128 /* a */, __m128 /* b */);
extern __m128 _mm_hadd_ps(__m128 /* a */, __m128 /* b */);
extern __m128 _mm_hsub_ps(__m128 /* a */, __m128 /* b */);
extern __m128 _mm_movehdup_ps(__m128 /* a */);
extern __m128 _mm_moveldup_ps(__m128 /* a */);

/*
 * New double precision vector instructions.
 */

extern __m128d _mm_addsub_pd(__m128d /* a */, __m128d /* b */);
extern __m128d _mm_hadd_pd(__m128d /* a */, __m128d /* b */);
extern __m128d _mm_hsub_pd(__m128d /* a */, __m128d /* b */);
extern __m128d _mm_loaddup_pd(double const * /* dp */);
extern __m128d _mm_movedup_pd(__m128d /* a */);

/*
 * New unaligned integer vector load instruction.
 */
extern __m128i _mm_lddqu_si128(__m128i const * /* p */);

/*
 * Miscellaneous new instructions.
 */
/*
 * For _mm_monitor p goes in eax, extensions goes in ecx, hints goes in edx.
 */
extern void _mm_monitor(void const * /* p */, unsigned /* extensions */, unsigned /* hints */);

/*
 * For _mm_mwait, extensions goes in ecx, hints goes in eax.
 */
extern void _mm_mwait(unsigned /* extensions */, unsigned /* hints */);


}; /* End "C" */
#line 99 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\pmmintrin.h"

#line 101 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\pmmintrin.h"
#line 102 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\pmmintrin.h"
#line 103 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\pmmintrin.h"
#pragma external_header(pop)
#line 22 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"









extern "C" {
#line 33 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"

    // Horizontal Add: add pairs of adjacent words or double words.
    // Each field in the result is the sum of two adjacent fields
    // from the arguments, with the lower result fields coming from
    // the first argument and the upper result fields coming from
    // the second argument. The "hadds" forms saturate the signed
    // addition rather than wrapping.

    extern __m128i _mm_hadd_epi16 (__m128i, __m128i);
    extern __m128i _mm_hadd_epi32 (__m128i, __m128i);
    extern __m128i _mm_hadds_epi16 (__m128i, __m128i);





#line 50 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"

    // Horizontal Subtract: subtract pairs of adjacent words or double
    // words. Each field in the result is the difference of two adjacent
    // fields from the arguments, where the upper field is subtracted
    // from the lower field. The lower result fields come from
    // the first argument and the upper result fields come from
    // the second argument. The "hsubs" forms saturate the signed
    // subtraction rather than wrapping.

    extern __m128i _mm_hsub_epi16 (__m128i, __m128i);
    extern __m128i _mm_hsub_epi32 (__m128i, __m128i);
    extern __m128i _mm_hsubs_epi16 (__m128i, __m128i);





#line 68 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"

    // Multiply unsigned bytes by signed bytes and sum the word
    // results in pairs with saturation. Each byte of the first
    // argument is zero-extended to a word field and each byte
    // of the second argument is sign-extended to a word field,
    // then each pair of words is multiplied together to give
    // signed word intermediate results. Pairs of words from
    // that result are added horizontally with saturation
    // to give the final result.

    extern __m128i _mm_maddubs_epi16 (__m128i, __m128i);



#line 83 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"

    // Packed multiply high integers with round and scaling,
    // {X,}MM2/m{128,64} (b) to {X,}MM1 (a).

    extern __m128i _mm_mulhrs_epi16 (__m128i, __m128i);



#line 92 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"

    // Packed shuffle bytes
    // {X,}MM2/m{128,64} (b) by {X,}MM1 (a).

    extern __m128i _mm_shuffle_epi8 (__m128i, __m128i);



#line 101 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"

    // Packed byte, word, double word sign, {X,}MM2/m{128,64} (b) to
    // {X,}MM1 (a).

    extern __m128i _mm_sign_epi8 (__m128i, __m128i);
    extern __m128i _mm_sign_epi16 (__m128i, __m128i);
    extern __m128i _mm_sign_epi32 (__m128i, __m128i);





#line 114 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"

    // Packed align and shift right by n*8 bits,
    // {X,}MM2/m{128,64} (b) to {X,}MM1 (a).

    extern __m128i _mm_alignr_epi8 (__m128i, __m128i, int);



#line 123 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"

    // Packed byte, word, double word absolute value,
    // {X,}MM2/m{128,64} (b) to {X,}MM1 (a).

    extern __m128i _mm_abs_epi8 (__m128i);
    extern __m128i _mm_abs_epi16 (__m128i);
    extern __m128i _mm_abs_epi32 (__m128i);





#line 136 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"


};
#line 140 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"

#line 142 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"
#line 143 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"
#line 144 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\tmmintrin.h"
#pragma external_header(pop)
#line 29 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\smmintrin.h"


/*
 * Rounding mode macros
 */

















/*
 * MACRO functions for ceil/floor intrinsics
 */















/*
 * MACRO functions for packed integer 128-bit comparison intrinsics.
 */







extern "C" {
#line 81 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\smmintrin.h"

        // Integer blend instructions - select data from 2 sources
        // using constant or variable mask

        extern __m128i _mm_blend_epi16 (__m128i, __m128i, const int /* mask */);
        extern __m128i _mm_blendv_epi8 (__m128i, __m128i, __m128i mask);

        // Float single precision blend instructions - select data
        // from 2 sources using constant/variable mask

        extern __m128  _mm_blend_ps (__m128, __m128, const int /* mask */);
        extern __m128  _mm_blendv_ps(__m128, __m128, __m128 /* mask */);

        // Float double precision blend instructions - select data
        // from 2 sources using constant/variable mask

        extern __m128d _mm_blend_pd (__m128d, __m128d, const int /* mask */);
        extern __m128d _mm_blendv_pd(__m128d, __m128d, __m128d /* mask */);

        // Dot product instructions with mask-defined summing and zeroing
        // of result's parts

        extern __m128  _mm_dp_ps(__m128, __m128, const int /* mask */);
        extern __m128d _mm_dp_pd(__m128d, __m128d, const int /* mask */);

        // Packed integer 64-bit comparison, zeroing or filling with ones
        // corresponding parts of result

        extern __m128i _mm_cmpeq_epi64(__m128i, __m128i);

        // Min/max packed integer instructions

        extern __m128i _mm_min_epi8 (__m128i, __m128i);
        extern __m128i _mm_max_epi8 (__m128i, __m128i);

        extern __m128i _mm_min_epu16(__m128i, __m128i);
        extern __m128i _mm_max_epu16(__m128i, __m128i);

        extern __m128i _mm_min_epi32(__m128i, __m128i);
        extern __m128i _mm_max_epi32(__m128i, __m128i);
        extern __m128i _mm_min_epu32(__m128i, __m128i);
        extern __m128i _mm_max_epu32(__m128i, __m128i);

        // Packed integer 32-bit multiplication with truncation
        // of upper halves of results

        extern __m128i _mm_mullo_epi32(__m128i, __m128i);

        // Packed integer 32-bit multiplication of 2 pairs of operands
        // producing two 64-bit results

        extern __m128i _mm_mul_epi32(__m128i, __m128i);

        // Packed integer 128-bit bitwise comparison.
        // return 1 if (val 'and' mask) == 0

        extern int _mm_testz_si128(__m128i /* mask */, __m128i /* val */);

        // Packed integer 128-bit bitwise comparison.
        // return 1 if (val 'and_not' mask) == 0

        extern int _mm_testc_si128(__m128i /* mask */, __m128i /* val */);

        // Packed integer 128-bit bitwise comparison
        // ZF = ((val 'and' mask) == 0)  CF = ((val 'and_not' mask) == 0)
        // return 1 if both ZF and CF are 0

        extern int _mm_testnzc_si128(__m128i /* mask */, __m128i /* val */);

        // Insert single precision float into packed single precision
        // array element selected by index.
        // The bits [7-6] of the 3d parameter define src index,
        // the bits [5-4] define dst index, and bits [3-0] define zeroing
        // mask for dst

        extern __m128 _mm_insert_ps(__m128 /* dst */, __m128 /* src */, const int /* index */);

        // Helper macro to create index-parameter value for _mm_insert_ps




        // Extract binary representation of single precision float from
        // packed single precision array element selected by index

        extern int _mm_extract_ps(__m128 /* src */, const int /* index */);

        // Extract single precision float from packed single precision
        // array element selected by index into dest




        // Extract specified single precision float element
        // into the lower part of __m128





        // Insert integer into packed integer array element
        // selected by index

        extern __m128i _mm_insert_epi8 (__m128i /* dst */, int /* src */, const int /* index */);
        extern __m128i _mm_insert_epi32(__m128i /* dst */, int /* src */, const int /* index */);


        extern __m128i _mm_insert_epi64(__m128i /* dst */, __int64 /* src */, const int /* index */);
#line 190 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\smmintrin.h"
        // Extract integer from packed integer array element
        // selected by index

        extern int   _mm_extract_epi8 (__m128i /* src */, const int /* index */);
        extern int   _mm_extract_epi32(__m128i /* src */, const int /* index */);


        extern __int64 _mm_extract_epi64(__m128i /* src */, const int /* index */);
#line 199 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\smmintrin.h"

        // Horizontal packed word minimum and its index in
        // result[15:0] and result[18:16] respectively

        extern __m128i _mm_minpos_epu16(__m128i);

        // Packed/single float double precision rounding

        extern __m128d _mm_round_pd(__m128d /* val */, int /* iRoundMode */);
        extern __m128d _mm_round_sd(__m128d /* dst */, __m128d /* val */, int /* iRoundMode */);

        // Packed/single float single precision rounding

        extern __m128  _mm_round_ps(__m128  /* val */, int /* iRoundMode */);
        extern __m128  _mm_round_ss(__m128 /* dst */, __m128  /* val */, int /* iRoundMode */);

        // Packed integer sign-extension

        extern __m128i _mm_cvtepi8_epi32 (__m128i);
        extern __m128i _mm_cvtepi16_epi32(__m128i);
        extern __m128i _mm_cvtepi8_epi64 (__m128i);
        extern __m128i _mm_cvtepi32_epi64(__m128i);
        extern __m128i _mm_cvtepi16_epi64(__m128i);
        extern __m128i _mm_cvtepi8_epi16 (__m128i);

        // Packed integer zero-extension

        extern __m128i _mm_cvtepu8_epi32 (__m128i);
        extern __m128i _mm_cvtepu16_epi32(__m128i);
        extern __m128i _mm_cvtepu8_epi64 (__m128i);
        extern __m128i _mm_cvtepu32_epi64(__m128i);
        extern __m128i _mm_cvtepu16_epi64(__m128i);
        extern __m128i _mm_cvtepu8_epi16 (__m128i);


        // Pack 8 double words from 2 operands into 8 words of result
        // with unsigned saturation

        extern __m128i _mm_packus_epi32(__m128i, __m128i);

        // Sum absolute 8-bit integer difference of adjacent groups of 4 byte
        // integers in operands. Starting offsets within operands are
        // determined by mask

        extern __m128i _mm_mpsadbw_epu8(__m128i /* s1 */, __m128i /* s2 */, const int /* mask */);

        /*
         * Load double quadword using non-temporal aligned hint
         */



#line 252 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\smmintrin.h"
        extern __m128i _mm_stream_load_si128(const __m128i*);
#line 254 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\smmintrin.h"


}; /* End "C" */
#line 258 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\smmintrin.h"

#line 260 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\smmintrin.h"
#line 261 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\smmintrin.h"
#line 262 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\smmintrin.h"
#pragma external_header(pop)
#line 29 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\nmmintrin.h"



extern "C" {
#line 34 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\nmmintrin.h"

/*
 * These defines are used to determine the kind of units to be compared
 */






/*
 * These defines are used to determine the comparison operation
 */






/*
 * These defines are used to determine the polarity
 */






/*
 * These defines are used in _mm_cmpXstri()
 */



/*
 * These defines are used _mm_cmpXstrm()
 */




/*
 * Intrinsics for text/string processing.
 */

    extern __m128i _mm_cmpistrm (__m128i /* a */, __m128i /* b */, const int /* mode */);
    extern int     _mm_cmpistri (__m128i /* a */, __m128i /* b */, const int /* mode */);

    extern __m128i _mm_cmpestrm (__m128i /* a */, int /* la */, __m128i /* b */, int /* lb */, const int /* mode */);
    extern int     _mm_cmpestri (__m128i /* a */, int /* la */, __m128i /* b */, int /* lb */, const int /* mode */);

/*
 * Intrinsics for text/string processing and reading values of EFlags.
 */

    extern int     _mm_cmpistrz (__m128i /* a */, __m128i /* b */, const int /* mode */);
    extern int     _mm_cmpistrc (__m128i /* a */, __m128i /* b */, const int /* mode */);
    extern int     _mm_cmpistrs (__m128i /* a */, __m128i /* b */, const int /* mode */);
    extern int     _mm_cmpistro (__m128i /* a */, __m128i /* b */, const int /* mode */);
    extern int     _mm_cmpistra (__m128i /* a */, __m128i /* b */, const int /* mode */);

    extern int     _mm_cmpestrz (__m128i /* a */, int /* la */, __m128i /* b */, int /* lb */, const int /* mode */);
    extern int     _mm_cmpestrc (__m128i /* a */, int /* la */, __m128i /* b */, int /* lb */, const int /* mode */);
    extern int     _mm_cmpestrs (__m128i /* a */, int /* la */, __m128i /* b */, int /* lb */, const int /* mode */);
    extern int     _mm_cmpestro (__m128i /* a */, int /* la */, __m128i /* b */, int /* lb */, const int /* mode */);
    extern int     _mm_cmpestra (__m128i /* a */, int /* la */, __m128i /* b */, int /* lb */, const int /* mode */);

/*
 * Packed integer 64-bit comparison, zeroing or filling with ones
 * corresponding parts of result
 */

    extern __m128i _mm_cmpgt_epi64(__m128i /* val1 */, __m128i /* val2 */);

/*
 * Calculate a number of bits set to 1
 */

    extern int _mm_popcnt_u32(unsigned int /* v */);


    extern __int64 _mm_popcnt_u64(unsigned __int64 /* v */);
#line 117 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\nmmintrin.h"

/*
 * Accumulate CRC32 (polynomial 0x11EDC6F41) value
 */

    extern unsigned int _mm_crc32_u8 (unsigned int /* crc */, unsigned char /* v */);
    extern unsigned int _mm_crc32_u16(unsigned int /* crc */, unsigned short /* v */);
    extern unsigned int _mm_crc32_u32(unsigned int /* crc */, unsigned int /* v */);


    extern unsigned __int64 _mm_crc32_u64(unsigned __int64 /* crc */, unsigned __int64 /* v */);
#line 129 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\nmmintrin.h"


}; /* End "C" */
#line 133 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\nmmintrin.h"

#line 135 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\nmmintrin.h"
#line 136 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\nmmintrin.h"
#line 137 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\nmmintrin.h"
#pragma external_header(pop)
#line 28 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\wmmintrin.h"



extern "C" {
#line 33 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\wmmintrin.h"

/*
 * Performs 1 round of AES decryption of the first m128i using
 * the second m128i as a round key.
 */
extern __m128i _mm_aesdec_si128(__m128i /* v */, __m128i /* rkey */);

/*
 * Performs the last round of AES decryption of the first m128i
 * using the second m128i as a round key.
 */
extern __m128i _mm_aesdeclast_si128(__m128i /* v */, __m128i /* rkey */);

/*
 * Performs 1 round of AES encryption of the first m128i using
 * the second m128i as a round key.
 */
extern __m128i _mm_aesenc_si128(__m128i /* v */, __m128i /* rkey */);

/*
 * Performs the last round of AES encryption of the first m128i
 * using the second m128i as a round key.
 */
extern __m128i _mm_aesenclast_si128(__m128i /* v */, __m128i /* rkey */);

/*
 * Performs the InverseMixColumn operation on the source m128i
 * and stores the result into m128i destination.
 */
extern __m128i _mm_aesimc_si128(__m128i /* v */);

/*
 * Generates a m128i round key for the input m128i
 * AES cipher key and byte round constant.
 * The second parameter must be a compile time constant.
 */
extern __m128i _mm_aeskeygenassist_si128(__m128i /* ckey */, const int /* rcon */);

/*
 * Performs carry-less integer multiplication of 64-bit halves
 * of 128-bit input operands.
 * The third parameter indicates which 64-bit halves of the input parameters
 * v1 and v2 should be used. It must be a compile time constant.
 */
extern __m128i _mm_clmulepi64_si128(__m128i /* v1 */, __m128i /* v2 */,
                                            const int /* imm8 */);



}; /* End "C" */
#line 84 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\wmmintrin.h"

#line 86 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\wmmintrin.h"
#line 87 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\wmmintrin.h"
#line 88 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\wmmintrin.h"
#pragma external_header(pop)
#line 27 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"


extern "C" {
#line 31 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

/*
 * Intel(R) AVX compiler intrinsic functions.
 */
typedef union __declspec(intrin_type) __declspec(align(32)) __m256 {
    float m256_f32[8];
} __m256;

typedef struct __declspec(intrin_type) __declspec(align(32)) __m256d {
    double m256d_f64[4];
} __m256d;

typedef union  __declspec(intrin_type) __declspec(align(32)) __m256i {
    __int8              m256i_i8[32];
    __int16             m256i_i16[16];
    __int32             m256i_i32[8];
    __int64             m256i_i64[4];
    unsigned __int8     m256i_u8[32];
    unsigned __int16    m256i_u16[16];
    unsigned __int32    m256i_u32[8];
    unsigned __int64    m256i_u64[4];
} __m256i;


/*
 * Compare predicates for scalar and packed compare intrinsic functions
 */






































/*
 * Add Packed Double Precision Floating-Point Values
 * **** VADDPD ymm1, ymm2, ymm3/m256
 * Performs an SIMD add of the four packed double-precision floating-point
 * values from the first source operand to the second source operand, and
 * stores the packed double-precision floating-point results in the
 * destination
 */
extern __m256d __cdecl _mm256_add_pd(__m256d, __m256d);

/*
 * Add Packed Single Precision Floating-Point Values
 * **** VADDPS ymm1, ymm2, ymm3/m256
 * Performs an SIMD add of the eight packed single-precision floating-point
 * values from the first source operand to the second source operand, and
 * stores the packed single-precision floating-point results in the
 * destination
 */
extern __m256 __cdecl _mm256_add_ps(__m256, __m256);

/*
 * Add/Subtract Double Precision Floating-Point Values
 * **** VADDSUBPD ymm1, ymm2, ymm3/m256
 * Adds odd-numbered double-precision floating-point values of the first
 * source operand with the corresponding double-precision floating-point
 * values from the second source operand; stores the result in the odd-numbered
 * values of the destination. Subtracts the even-numbered double-precision
 * floating-point values from the second source operand from the corresponding
 * double-precision floating values in the first source operand; stores the
 * result into the even-numbered values of the destination
 */
extern __m256d __cdecl _mm256_addsub_pd(__m256d, __m256d);

/*
 * Add/Subtract Packed Single Precision Floating-Point Values
 * **** VADDSUBPS ymm1, ymm2, ymm3/m256
 * Adds odd-numbered single-precision floating-point values of the first source
 * operand with the corresponding single-precision floating-point values from
 * the second source operand; stores the result in the odd-numbered values of
 * the destination. Subtracts the even-numbered single-precision floating-point
 * values from the second source operand from the corresponding
 * single-precision floating values in the first source operand; stores the
 * result into the even-numbered values of the destination
 */
extern __m256 __cdecl _mm256_addsub_ps(__m256, __m256);

/*
 * Bitwise Logical AND of Packed Double Precision Floating-Point Values
 * **** VANDPD ymm1, ymm2, ymm3/m256
 * Performs a bitwise logical AND of the four packed double-precision
 * floating-point values from the first source operand and the second
 * source operand, and stores the result in the destination
 */
extern __m256d __cdecl _mm256_and_pd(__m256d, __m256d);

/*
 * Bitwise Logical AND of Packed Single Precision Floating-Point Values
 * **** VANDPS ymm1, ymm2, ymm3/m256
 * Performs a bitwise logical AND of the eight packed single-precision
 * floating-point values from the first source operand and the second
 * source operand, and stores the result in the destination
 */
extern __m256 __cdecl _mm256_and_ps(__m256, __m256);

/*
 * Bitwise Logical AND NOT of Packed Double Precision Floating-Point Values
 * **** VANDNPD ymm1, ymm2, ymm3/m256
 * Performs a bitwise logical AND NOT of the four packed double-precision
 * floating-point values from the first source operand and the second source
 * operand, and stores the result in the destination
 */
extern __m256d __cdecl _mm256_andnot_pd(__m256d, __m256d);

/*
 * Bitwise Logical AND NOT of Packed Single Precision Floating-Point Values
 * **** VANDNPS ymm1, ymm2, ymm3/m256
 * Performs a bitwise logical AND NOT of the eight packed single-precision
 * floating-point values from the first source operand and the second source
 * operand, and stores the result in the destination
 */
extern __m256 __cdecl _mm256_andnot_ps(__m256, __m256);

/*
 * Blend Packed Double Precision Floating-Point Values
 * **** VBLENDPD ymm1, ymm2, ymm3/m256, imm8
 * Double-Precision Floating-Point values from the second source operand are
 * conditionally merged with values from the first source operand and written
 * to the destination. The immediate bits [3:0] determine whether the
 * corresponding Double-Precision Floating Point value in the destination is
 * copied from the second source or first source. If a bit in the mask,
 * corresponding to a word, is "1", then the Double-Precision Floating-Point
 * value in the second source operand is copied, else the value in the first
 * source operand is copied
 */
extern __m256d __cdecl _mm256_blend_pd(__m256d, __m256d, const int);

/*
 * Blend Packed Single Precision Floating-Point Values
 * **** VBLENDPS ymm1, ymm2, ymm3/m256, imm8
 * Single precision floating point values from the second source operand are
 * conditionally merged with values from the first source operand and written
 * to the destination. The immediate bits [7:0] determine whether the
 * corresponding single precision floating-point value in the destination is
 * copied from the second source or first source. If a bit in the mask,
 * corresponding to a word, is "1", then the single-precision floating-point
 * value in the second source operand is copied, else the value in the first
 * source operand is copied
 */
extern __m256 __cdecl _mm256_blend_ps(__m256, __m256, const int);

/*
 * Blend Packed Double Precision Floating-Point Values
 * **** VBLENDVPD ymm1, ymm2, ymm3/m256, ymm4
 * Conditionally copy each quadword data element of double-precision
 * floating-point value from the second source operand (third operand) and the
 * first source operand (second operand) depending on mask bits defined in the
 * mask register operand (fourth operand).
 */
extern __m256d __cdecl _mm256_blendv_pd(__m256d, __m256d, __m256d);

/*
 * Blend Packed Single Precision Floating-Point Values
 * **** VBLENDVPS ymm1, ymm2, ymm3/m256, ymm4
 * Conditionally copy each dword data element of single-precision
 * floating-point value from the second source operand (third operand) and the
 * first source operand (second operand) depending on mask bits defined in the
 * mask register operand (fourth operand).
 */
extern __m256 __cdecl _mm256_blendv_ps(__m256, __m256, __m256);

/*
 * Divide Packed Double-Precision Floating-Point Values
 * **** VDIVPD ymm1, ymm2, ymm3/m256
 * Performs an SIMD divide of the four packed double-precision floating-point
 * values in the first source operand by the four packed double-precision
 * floating-point values in the second source operand
 */
extern __m256d __cdecl _mm256_div_pd(__m256d, __m256d);

/*
 * Divide Packed Single-Precision Floating-Point Values
 * **** VDIVPS ymm1, ymm2, ymm3/m256
 * Performs an SIMD divide of the eight packed single-precision
 * floating-point values in the first source operand by the eight packed
 * single-precision floating-point values in the second source operand
 */
extern __m256 __cdecl _mm256_div_ps(__m256, __m256);

/*
 * Dot Product of Packed Single-Precision Floating-Point Values
 * **** VDPPS ymm1, ymm2, ymm3/m256, imm8
 * Multiplies the packed single precision floating point values in the
 * first source operand with the packed single-precision floats in the
 * second source. Each of the four resulting single-precision values is
 * conditionally summed depending on a mask extracted from the high 4 bits
 * of the immediate operand. This sum is broadcast to each of 4 positions
 * in the destination if the corresponding bit of the mask selected from
 * the low 4 bits of the immediate operand is "1". If the corresponding
 * low bit 0-3 of the mask is zero, the destination is set to zero.
 * The process is replicated for the high elements of the destination.
 */
extern __m256 __cdecl _mm256_dp_ps(__m256, __m256, const int);

/*
 * Add Horizontal Double Precision Floating-Point Values
 * **** VHADDPD ymm1, ymm2, ymm3/m256
 * Adds pairs of adjacent double-precision floating-point values in the
 * first source operand and second source operand and stores results in
 * the destination
 */
extern __m256d __cdecl _mm256_hadd_pd(__m256d, __m256d);

/*
 * Add Horizontal Single Precision Floating-Point Values
 * **** VHADDPS ymm1, ymm2, ymm3/m256
 * Adds pairs of adjacent single-precision floating-point values in the
 * first source operand and second source operand and stores results in
 * the destination
 */
extern __m256 __cdecl _mm256_hadd_ps(__m256, __m256);

/*
 * Subtract Horizontal Double Precision Floating-Point Values
 * **** VHSUBPD ymm1, ymm2, ymm3/m256
 * Subtract pairs of adjacent double-precision floating-point values in
 * the first source operand and second source operand and stores results
 * in the destination
 */
extern __m256d __cdecl _mm256_hsub_pd(__m256d, __m256d);

/*
 * Subtract Horizontal Single Precision Floating-Point Values
 * **** VHSUBPS ymm1, ymm2, ymm3/m256
 * Subtract pairs of adjacent single-precision floating-point values in
 * the first source operand and second source operand and stores results
 * in the destination.
 */
extern __m256 __cdecl _mm256_hsub_ps(__m256, __m256);

/*
 * Maximum of Packed Double Precision Floating-Point Values
 * **** VMAXPD ymm1, ymm2, ymm3/m256
 * Performs an SIMD compare of the packed double-precision floating-point
 * values in the first source operand and the second source operand and
 * returns the maximum value for each pair of values to the destination
 */
extern __m256d __cdecl _mm256_max_pd(__m256d, __m256d);

/*
 * Maximum of Packed Single Precision Floating-Point Values
 * **** VMAXPS ymm1, ymm2, ymm3/m256
 * Performs an SIMD compare of the packed single-precision floating-point
 * values in the first source operand and the second source operand and
 * returns the maximum value for each pair of values to the destination
 */
extern __m256 __cdecl _mm256_max_ps(__m256, __m256);

/*
 * Minimum of Packed Double Precision Floating-Point Values
 * **** VMINPD ymm1, ymm2, ymm3/m256
 * Performs an SIMD compare of the packed double-precision floating-point
 * values in the first source operand and the second source operand and
 * returns the minimum value for each pair of values to the destination
 */
extern __m256d __cdecl _mm256_min_pd(__m256d, __m256d);

/*
 * Minimum of Packed Single Precision Floating-Point Values
 * **** VMINPS ymm1, ymm2, ymm3/m256
 * Performs an SIMD compare of the packed single-precision floating-point
 * values in the first source operand and the second source operand and
 * returns the minimum value for each pair of values to the destination
 */
extern __m256 __cdecl _mm256_min_ps(__m256, __m256);

/*
 * Multiply Packed Double Precision Floating-Point Values
 * **** VMULPD ymm1, ymm2, ymm3/m256
 * Performs a SIMD multiply of the four packed double-precision floating-point
 * values from the first Source operand to the Second Source operand, and
 * stores the packed double-precision floating-point results in the
 * destination
 */
extern __m256d __cdecl _mm256_mul_pd(__m256d, __m256d);

/*
 * Multiply Packed Single Precision Floating-Point Values
 * **** VMULPS ymm1, ymm2, ymm3/m256
 * Performs an SIMD multiply of the eight packed single-precision
 * floating-point values from the first source operand to the second source
 * operand, and stores the packed double-precision floating-point results in
 * the destination
 */
extern __m256 __cdecl _mm256_mul_ps(__m256, __m256);

/*
 * Bitwise Logical OR of Packed Double Precision Floating-Point Values
 * **** VORPD ymm1, ymm2, ymm3/m256
 * Performs a bitwise logical OR of the four packed double-precision
 * floating-point values from the first source operand and the second
 * source operand, and stores the result in the destination
 */
extern __m256d __cdecl _mm256_or_pd(__m256d, __m256d);

/*
 * Bitwise Logical OR of Packed Single Precision Floating-Point Values
 * **** VORPS ymm1, ymm2, ymm3/m256
 * Performs a bitwise logical OR of the eight packed single-precision
 * floating-point values from the first source operand and the second
 * source operand, and stores the result in the destination
 */
extern __m256 __cdecl _mm256_or_ps(__m256, __m256);

/*
 * Shuffle Packed Double Precision Floating-Point Values
 * **** VSHUFPD ymm1, ymm2, ymm3/m256, imm8
 * Moves either of the two packed double-precision floating-point values from
 * each double quadword in the first source operand into the low quadword
 * of each double quadword of the destination; moves either of the two packed
 * double-precision floating-point values from the second source operand into
 * the high quadword of each double quadword of the destination operand.
 * The selector operand determines which values are moved to the destination
 */
extern __m256d __cdecl _mm256_shuffle_pd(__m256d, __m256d, const int);

/*
 * Shuffle Packed Single Precision Floating-Point Values
 * **** VSHUFPS ymm1, ymm2, ymm3/m256, imm8
 * Moves two of the four packed single-precision floating-point values
 * from each double qword of the first source operand into the low
 * quadword of each double qword of the destination; moves two of the four
 * packed single-precision floating-point values from each double qword of
 * the second source operand into to the high quadword of each double qword
 * of the destination. The selector operand determines which values are moved
 * to the destination.
 */
extern __m256 __cdecl _mm256_shuffle_ps(__m256, __m256, const int);

/*
 * Subtract Packed Double Precision Floating-Point Values
 * **** VSUBPD ymm1, ymm2, ymm3/m256
 * Performs an SIMD subtract of the four packed double-precision floating-point
 * values of the second Source operand from the first Source operand, and
 * stores the packed double-precision floating-point results in the destination
 */
extern __m256d __cdecl _mm256_sub_pd(__m256d, __m256d);

/*
 * Subtract Packed Single Precision Floating-Point Values
 * **** VSUBPS ymm1, ymm2, ymm3/m256
 * Performs an SIMD subtract of the eight packed single-precision
 * floating-point values in the second Source operand from the First Source
 * operand, and stores the packed single-precision floating-point results in
 * the destination
 */
extern __m256 __cdecl _mm256_sub_ps(__m256, __m256);

/*
 * Bitwise Logical XOR of Packed Double Precision Floating-Point Values
 * **** VXORPD ymm1, ymm2, ymm3/m256
 * Performs a bitwise logical XOR of the four packed double-precision
 * floating-point values from the first source operand and the second
 * source operand, and stores the result in the destination
 */
extern __m256d __cdecl _mm256_xor_pd(__m256d, __m256d);

/*
 * Bitwise Logical XOR of Packed Single Precision Floating-Point Values
 * **** VXORPS ymm1, ymm2, ymm3/m256
 * Performs a bitwise logical XOR of the eight packed single-precision
 * floating-point values from the first source operand and the second
 * source operand, and stores the result in the destination
 */
extern __m256 __cdecl _mm256_xor_ps(__m256, __m256);

/*
 * Compare Packed Double-Precision Floating-Point Values
 * **** VCMPPD xmm1, xmm2, xmm3/m128, imm8
 * **** VCMPPD ymm1, ymm2, ymm3/m256, imm8
 * Performs an SIMD compare of the four packed double-precision floating-point
 * values in the second source operand (third operand) and the first source
 * operand (second operand) and returns the results of the comparison to the
 * destination operand (first operand). The comparison predicate operand
 * (immediate) specifies the type of comparison performed on each of the pairs
 * of packed values.
 * For 128-bit intrinsic function with compare predicate values in range 0-7
 * compiler may generate SSE2 instructions if it is warranted for performance
 * reasons.
 */
extern __m128d __cdecl _mm_cmp_pd(__m128d, __m128d, const int);
extern __m256d __cdecl _mm256_cmp_pd(__m256d, __m256d, const int);

/*
 * Compare Packed Single-Precision Floating-Point Values
 * **** VCMPPS xmm1, xmm2, xmm3/m256, imm8
 * **** VCMPPS ymm1, ymm2, ymm3/m256, imm8
 * Performs a SIMD compare of the packed single-precision floating-point values
 * in the second source operand (third operand) and the first source operand
 * (second operand) and returns the results of the comparison to the
 * destination operand (first operand). The comparison predicate operand
 * (immediate) specifies the type of comparison performed on each of the pairs
 * of packed values.
 * For 128-bit intrinsic function with compare predicate values in range 0-7
 * compiler may generate SSE2 instructions if it is warranted for performance
 * reasons.
 */
extern __m128 __cdecl _mm_cmp_ps(__m128, __m128, const int);
extern __m256 __cdecl _mm256_cmp_ps(__m256, __m256, const int);

/*
 * Compare Scalar Double-Precision Floating-Point Values
 * **** VCMPSD xmm1, xmm2, xmm3/m64, imm8
 * Compares the low double-precision floating-point values in the second source
 * operand (third operand) and the first source operand (second operand) and
 * returns the results in of the comparison to the destination operand (first
 * operand). The comparison predicate operand (immediate operand) specifies the
 * type of comparison performed.
 * For compare predicate values in range 0-7 compiler may generate SSE2
 * instructions if it is warranted for performance reasons.
 */
extern __m128d __cdecl _mm_cmp_sd(__m128d, __m128d, const int);

/* Compare Scalar Double-Precision Floating-Point Values with Integer Result
 * This is similar to _mm_cmp_sd, except it returns the result as an integer
 * and it supports all predicate values even when AVX support is not available.
 */
extern int __cdecl _mm_comi_sd(__m128d, __m128d, const int);

/*
 * Compare Scalar Single-Precision Floating-Point Values
 * **** VCMPSS xmm1, xmm2, xmm3/m64, imm8
 * Compares the low single-precision floating-point values in the second source
 * operand (third operand) and the first source operand (second operand) and
 * returns the results of the comparison to the destination operand (first
 * operand). The comparison predicate operand (immediate operand) specifies
 * the type of comparison performed.
 * For compare predicate values in range 0-7 compiler may generate SSE2
 * instructions if it is warranted for performance reasons.
 */
extern __m128 __cdecl _mm_cmp_ss(__m128, __m128, const int);

/* Compare Scalar Single-Precision Floating-Point Values with Integer Result
 * This is similar to _mm_cmp_ss, except it returns the result as an integer
 * and it supports all predicate values even when AVX support is not available.
 */
extern int __cdecl _mm_comi_ss(__m128, __m128, const int);

/*
 * Convert Packed Doubleword Integers to
 * Packed Double-Precision Floating-Point Values
 * **** VCVTDQ2PD ymm1, xmm2/m128
 * Converts four packed signed doubleword integers in the source operand to
 * four packed double-precision floating-point values in the destination
 */
extern __m256d __cdecl _mm256_cvtepi32_pd(__m128i);

/*
 * Convert Packed Doubleword Integers to
 * Packed Single-Precision Floating-Point Values
 * **** VCVTDQ2PS ymm1, ymm2/m256
 * Converts eight packed signed doubleword integers in the source operand to
 * eight packed double-precision floating-point values in the destination
 */
extern __m256  __cdecl _mm256_cvtepi32_ps(__m256i);

/*
 * Convert Packed Double-Precision Floating-point values to
 * Packed Single-Precision Floating-Point Values
 * **** VCVTPD2PS xmm1, ymm2/m256
 * Converts four packed double-precision floating-point values in the source
 * operand to four packed single-precision floating-point values in the
 * destination
 */
extern __m128  __cdecl _mm256_cvtpd_ps(__m256d);

/*
 * Convert Packed Single Precision Floating-Point Values to
 * Packed Singed Doubleword Integer Values
 * **** VCVTPS2DQ ymm1, ymm2/m256
 * Converts eight packed single-precision floating-point values in the source
 * operand to eight signed doubleword integers in the destination
 */
extern __m256i __cdecl _mm256_cvtps_epi32(__m256);

/*
 * Convert Packed Single Precision Floating-point values to
 * Packed Double Precision Floating-Point Values
 * **** VCVTPS2PD ymm1, xmm2/m128
 * Converts four packed single-precision floating-point values in the source
 * operand to four packed double-precision floating-point values in the
 * destination
 */
extern __m256d __cdecl _mm256_cvtps_pd(__m128);

/*
 * Convert with Truncation Packed Double-Precision Floating-Point values to
 * Packed Doubleword Integers
 * **** VCVTTPD2DQ xmm1, ymm2/m256
 * Converts four packed double-precision floating-point values in the source
 * operand to four packed signed doubleword integers in the destination.
 * When a conversion is inexact, a truncated (round toward zero) value is
 * returned. If a converted result is larger than the maximum signed doubleword
 * integer, the floating-point invalid exception is raised, and if this
 * exception is masked, the indefinite integer value (80000000H) is returned
*/
extern __m128i __cdecl _mm256_cvttpd_epi32(__m256d);

/*
 * Convert Packed Double-Precision Floating-point values to
 * Packed Doubleword Integers
 * **** VCVTPD2DQ xmm1, ymm2/m256
 * Converts four packed double-precision floating-point values in the source
 * operand to four packed signed doubleword integers in the destination
 */
extern __m128i __cdecl _mm256_cvtpd_epi32(__m256d);

/*
 * Convert with Truncation Packed Single Precision Floating-Point Values to
 * Packed Singed Doubleword Integer Values
 * **** VCVTTPS2DQ ymm1, ymm2/m256
 * Converts eight packed single-precision floating-point values in the source
 * operand to eight signed doubleword integers in the destination.
 * When a conversion is inexact, a truncated (round toward zero) value is
 * returned. If a converted result is larger than the maximum signed doubleword
 * integer, the floating-point invalid exception is raised, and if this
 * exception is masked, the indefinite integer value (80000000H) is returned
 */
extern __m256i __cdecl _mm256_cvttps_epi32(__m256);

/*
 * Convert Scalar Single-Precision Floating-point value in 256-bit vector to
 * equivalent C/C++ float type.
 */
// float _mm256_cvtss_f32 (__m256 a)


/*
 * Convert Scalar Double-Precision Floating-point value in 256-bit vector to
 * equivalent C/C++ double type.
 */
// double _mm256_cvtsd_f64 (__m256d a)


/*
 * Convert 32-bit Scalar integer in 256-bit vector to equivalent C/C++ int type.
 */
// int _mm256_cvtsi256_si32 (__m256i a)



/*
 * Convert 64-bit Scalar integer in 256-bit vector to equivalent C/C++ __int64 type.
 */
// __int64 _mm256_cvtsi256_si64 (__m256i a)

#line 612 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"


/*
 * Extract packed floating-point values
 * **** VEXTRACTF128 xmm1/m128, ymm2, imm8
 * Extracts 128-bits of packed floating-point values from the source operand
 * at an 128-bit offset from imm8[0] into the destination
 */
extern __m128  __cdecl _mm256_extractf128_ps(__m256, const int);
extern __m128d __cdecl _mm256_extractf128_pd(__m256d, const int);
extern __m128i __cdecl _mm256_extractf128_si256(__m256i, const int);

/*
 * Zero All YMM registers
 * **** VZEROALL
 * Zeros contents of all YMM registers
 */
extern void __cdecl _mm256_zeroall(void);

/*
 * Zero Upper bits of YMM registers
 * **** VZEROUPPER
 * Zeros the upper 128 bits of all YMM registers. The lower 128-bits of the
 * registers (the corresponding XMM registers) are unmodified
 */
extern void __cdecl _mm256_zeroupper(void);

/*
 * Permute Single-Precision Floating-Point Values
 * **** VPERMILPS ymm1, ymm2, ymm3/m256
 * **** VPERMILPS xmm1, xmm2, xmm3/m128
 * Permute Single-Precision Floating-Point values in the first source operand
 * using 8-bit control fields in the low bytes of corresponding elements the
 * shuffle control and store results in the destination
 */
extern __m256  __cdecl _mm256_permutevar_ps(__m256, __m256i);
extern __m128  __cdecl _mm_permutevar_ps(__m128, __m128i);

/*
 * Permute Single-Precision Floating-Point Values
 * **** VPERMILPS ymm1, ymm2/m256, imm8
 * **** VPERMILPS xmm1, xmm2/m128, imm8
 * Permute Single-Precision Floating-Point values in the first source operand
 * using four 2-bit control fields in the 8-bit immediate and store results
 * in the destination
 */
extern __m256  __cdecl _mm256_permute_ps(__m256, int);
extern __m128  __cdecl _mm_permute_ps(__m128, int);

/*
 * Permute Double-Precision Floating-Point Values
 * **** VPERMILPD ymm1, ymm2, ymm3/m256
 * **** VPERMILPD xmm1, xmm2, xmm3/m128
 * Permute Double-Precision Floating-Point values in the first source operand
 * using 8-bit control fields in the low bytes of the second source operand
 * and store results in the destination
 */
extern __m256d __cdecl _mm256_permutevar_pd(__m256d, __m256i);
extern __m128d __cdecl _mm_permutevar_pd(__m128d, __m128i);

/*
 * Permute Double-Precision Floating-Point Values
 * **** VPERMILPD ymm1, ymm2/m256, imm8
 * **** VPERMILPD xmm1, xmm2/m128, imm8
 * Permute Double-Precision Floating-Point values in the first source operand
 * using two, 1-bit control fields in the low 2 bits of the 8-bit immediate
 * and store results in the destination
 */
extern __m256d __cdecl _mm256_permute_pd(__m256d, int);
extern __m128d __cdecl _mm_permute_pd(__m128d, int);

/*
 * Permute Floating-Point Values
 * **** VPERM2F128 ymm1, ymm2, ymm3/m256, imm8
 * Permute 128 bit floating-point-containing fields from the first source
 * operand and second source operand using bits in the 8-bit immediate and
 * store results in the destination
 */
extern __m256  __cdecl _mm256_permute2f128_ps(__m256, __m256, int);
extern __m256d __cdecl _mm256_permute2f128_pd(__m256d, __m256d, int);
extern __m256i __cdecl _mm256_permute2f128_si256(__m256i, __m256i, int);

/*
 * Load with Broadcast
 * **** VBROADCASTSS ymm1, m32
 * **** VBROADCASTSS xmm1, m32
 * Load floating point values from the source operand and broadcast to all
 * elements of the destination
 */
extern __m256  __cdecl _mm256_broadcast_ss(float const *);
extern __m128  __cdecl _mm_broadcast_ss(float const *);

/*
 * Load with Broadcast
 * **** VBROADCASTSD ymm1, m64
 * Load floating point values from the source operand and broadcast to all
 * elements of the destination
 */
extern __m256d __cdecl _mm256_broadcast_sd(double const *);

/*
 * Load with Broadcast
 * **** VBROADCASTF128 ymm1, m128
 * Load floating point values from the source operand and broadcast to all
 * elements of the destination
 */
extern __m256  __cdecl _mm256_broadcast_ps(__m128 const *);
extern __m256d __cdecl _mm256_broadcast_pd(__m128d const *);

/*
 * Insert packed floating-point values
 * **** VINSERTF128 ymm1, ymm2, xmm3/m128, imm8
 * Performs an insertion of 128-bits of packed floating-point values from the
 * second source operand into an the destination at an 128-bit offset from
 * imm8[0]. The remaining portions of the destination are written by the
 * corresponding fields of the first source operand
 */
extern __m256  __cdecl _mm256_insertf128_ps(__m256, __m128, int);
extern __m256d __cdecl _mm256_insertf128_pd(__m256d, __m128d, int);
extern __m256i __cdecl _mm256_insertf128_si256(__m256i, __m128i, int);

/*
 * Move Aligned Packed Double-Precision Floating-Point Values
 * **** VMOVAPD ymm1, m256
 * **** VMOVAPD m256, ymm1
 * Moves 4 double-precision floating-point values from the source operand to
 * the destination
 */
extern __m256d __cdecl _mm256_load_pd(double const *);
extern void    __cdecl _mm256_store_pd(double *, __m256d);

/*
 * Move Aligned Packed Single-Precision Floating-Point Values
 * **** VMOVAPS ymm1, m256
 * **** VMOVAPS m256, ymm1
 * Moves 8 single-precision floating-point values from the source operand to
 * the destination
 */
extern __m256  __cdecl _mm256_load_ps(float const *);
extern void    __cdecl _mm256_store_ps(float *, __m256);

/*
 * Move Unaligned Packed Double-Precision Floating-Point Values
 * **** VMOVUPD ymm1, m256
 * **** VMOVUPD m256, ymm1
 * Moves 256 bits of packed double-precision floating-point values from the
 * source operand to the destination
 */
extern __m256d __cdecl _mm256_loadu_pd(double const *);
extern void    __cdecl _mm256_storeu_pd(double *, __m256d);

/*
 * Move Unaligned Packed Single-Precision Floating-Point Values
 * **** VMOVUPS ymm1, m256
 * **** VMOVUPS m256, ymm1
 * Moves 256 bits of packed single-precision floating-point values from the
 * source operand to the destination
 */
extern __m256  __cdecl _mm256_loadu_ps(float const *);
extern void    __cdecl _mm256_storeu_ps(float *, __m256);

/*
 * Move Aligned Packed Integer Values
 * **** VMOVDQA ymm1, m256
 * **** VMOVDQA m256, ymm1
 * Moves 256 bits of packed integer values from the source operand to the
 * destination
 */
extern __m256i __cdecl _mm256_load_si256(__m256i const *);
extern void    __cdecl _mm256_store_si256(__m256i *, __m256i);

/*
 * Move Unaligned Packed Integer Values
 * **** VMOVDQU ymm1, m256
 * **** VMOVDQU m256, ymm1
 * Moves 256 bits of packed integer values from the source operand to the
 * destination
 */
extern __m256i __cdecl _mm256_loadu_si256(__m256i const *);
extern void    __cdecl _mm256_storeu_si256(__m256i *, __m256i);

/*
 * Load Two Unaligned Packed 128-bit Values
 * Loads two potentially unaligned 128-bit values
 * and combines them into one 256-bit value.
 *
 * The data types here (float const*, double const* and __m128i const*)
 * were chosen for consistency with the underlying _mm_loadu_{ps,pd,si128}
 * intrinsics.
 */













/*
 * Store 256-bit Value To Two Unaligned 128-bit Locations
 * Stores the high and low 128-bit halves of a 256-bit value
 * to two different potentially unaligned addresses.
 */

























/*
 * Conditional SIMD Packed Loads and Stores
 * **** VMASKMOVPD xmm1, xmm2, m128
 * **** VMASKMOVPD ymm1, ymm2, m256
 * **** VMASKMOVPD m128, xmm1, xmm2
 * **** VMASKMOVPD m256, ymm1, ymm2
 *
 * Load forms:
 * Load packed values from the 128-bit (XMM forms) or 256-bit (YMM forms)
 * memory location (third operand) into the destination XMM or YMM register
 * (first operand) using a mask in the first source operand (second operand).
 *
 * Store forms:
 * Stores packed values from the XMM or YMM register in the second source
 * operand (third operand) into the 128-bit (XMM forms) or 256-bit (YMM forms)
 * memory location using a mask in first source operand (second operand).
 * Stores are atomic.
 */
extern __m256d __cdecl _mm256_maskload_pd(double const *, __m256i);
extern void    __cdecl _mm256_maskstore_pd(double *, __m256i, __m256d);
extern __m128d __cdecl _mm_maskload_pd(double const *, __m128i);
extern void    __cdecl _mm_maskstore_pd(double *, __m128i, __m128d);

/*
 * Conditional SIMD Packed Loads and Stores
 * **** VMASKMOVPS xmm1, xmm2, m128
 * **** VMASKMOVPS ymm1, ymm2, m256
 * **** VMASKMOVPS m128, xmm1, xmm2
 * **** VMASKMOVPS m256, ymm1, ymm2
 *
 * Load forms:
 * Load packed values from the 128-bit (XMM forms) or 256-bit (YMM forms)
 * memory location (third operand) into the destination XMM or YMM register
 * (first operand) using a mask in the first source operand (second operand).
 *
 * Store forms:
 * Stores packed values from the XMM or YMM register in the second source
 * operand (third operand) into the 128-bit (XMM forms) or 256-bit (YMM forms)
 * memory location using a mask in first source operand (second operand).
 * Stores are atomic.
 */
extern __m256  __cdecl _mm256_maskload_ps(float const *, __m256i);
extern void    __cdecl _mm256_maskstore_ps(float *, __m256i, __m256);
extern __m128  __cdecl _mm_maskload_ps(float const *, __m128i);
extern void    __cdecl _mm_maskstore_ps(float *, __m128i, __m128);

/*
 * Replicate Single-Precision Floating-Point Values
 * **** VMOVSHDUP ymm1, ymm2/m256
 * Duplicates odd-indexed single-precision floating-point values from the
 * source operand
 */
extern __m256  __cdecl _mm256_movehdup_ps(__m256);

/*
 * Replicate Single-Precision Floating-Point Values
 * **** VMOVSLDUP ymm1, ymm2/m256
 * Duplicates even-indexed single-precision floating-point values from the
 * source operand
 */
extern __m256  __cdecl _mm256_moveldup_ps(__m256);

/*
 * Replicate Double-Precision Floating-Point Values
 * **** VMOVDDUP ymm1, ymm2/m256
 * Duplicates even-indexed double-precision floating-point values from the
 * source operand
 */
extern __m256d __cdecl _mm256_movedup_pd(__m256d);

/*
 * Move Unaligned Integer
 * **** VLDDQU ymm1, m256
 * The instruction is functionally similar to VMOVDQU YMM, m256 for loading
 * from memory. That is: 32 bytes of data starting at an address specified by
 * the source memory operand are fetched from memory and placed in a
 * destination
 */
extern __m256i __cdecl _mm256_lddqu_si256(__m256i const *);

/*
 * Store Packed Integers Using Non-Temporal Hint
 * **** VMOVNTDQ m256, ymm1
 * Moves the packed integers in the source operand to the destination using a
 * non-temporal hint to prevent caching of the data during the write to memory
 */
extern void    __cdecl _mm256_stream_si256(__m256i *, __m256i);

/*
 * Store Packed Double-Precision Floating-Point Values Using Non-Temporal Hint
 * **** VMOVNTPD m256, ymm1
 * Moves the packed double-precision floating-point values in the source
 * operand to the destination operand using a non-temporal hint to prevent
 * caching of the data during the write to memory
 */
extern void    __cdecl _mm256_stream_pd(double *, __m256d);

/*
 * Store Packed Single-Precision Floating-Point Values Using Non-Temporal Hint
 * **** VMOVNTPS m256, ymm1
 * Moves the packed single-precision floating-point values in the source
 * operand to the destination operand using a non-temporal hint to prevent
 * caching of the data during the write to memory
 */
extern void    __cdecl _mm256_stream_ps(float *, __m256);

/*
 * Compute Approximate Reciprocals of Packed Single-Precision Floating-Point
 * Values
 * **** VRCPPS ymm1, ymm2/m256
 * Performs an SIMD computation of the approximate reciprocals of the eight
 * packed single precision floating-point values in the source operand and
 * stores the packed single-precision floating-point results in the destination
 */
extern __m256  __cdecl _mm256_rcp_ps(__m256);

/*
 * Compute Approximate Reciprocals of Square Roots of
 * Packed Single-Precision Floating-point Values
 * **** VRSQRTPS ymm1, ymm2/m256
 * Performs an SIMD computation of the approximate reciprocals of the square
 * roots of the eight packed single precision floating-point values in the
 * source operand and stores the packed single-precision floating-point results
 * in the destination
 */
extern __m256  __cdecl _mm256_rsqrt_ps(__m256);

/*
 * Square Root of Double-Precision Floating-Point Values
 * **** VSQRTPD ymm1, ymm2/m256
 * Performs an SIMD computation of the square roots of the two or four packed
 * double-precision floating-point values in the source operand and stores
 * the packed double-precision floating-point results in the destination
 */
extern __m256d __cdecl _mm256_sqrt_pd(__m256d);

/*
 * Square Root of Single-Precision Floating-Point Values
 * **** VSQRTPS ymm1, ymm2/m256
 * Performs an SIMD computation of the square roots of the eight packed
 * single-precision floating-point values in the source operand stores the
 * packed double-precision floating-point results in the destination
 */
extern __m256  __cdecl _mm256_sqrt_ps(__m256);

/*
 * Round Packed Double-Precision Floating-Point Values
 * **** VROUNDPD ymm1,ymm2/m256,imm8
 * Round the four Double-Precision Floating-Point Values values in the source
 * operand by the rounding mode specified in the immediate operand and place
 * the result in the destination. The rounding process rounds the input to an
 * integral value and returns the result as a double-precision floating-point
 * value. The Precision Floating Point Exception is signaled according to the
 * immediate operand. If any source operand is an SNaN then it will be
 * converted to a QNaN.
 */
extern __m256d __cdecl _mm256_round_pd(__m256d, int);



/*
 * Round Packed Single-Precision Floating-Point Values
 * **** VROUNDPS ymm1,ymm2/m256,imm8
 * Round the four single-precision floating-point values values in the source
 * operand by the rounding mode specified in the immediate operand and place
 * the result in the destination. The rounding process rounds the input to an
 * integral value and returns the result as a double-precision floating-point
 * value. The Precision Floating Point Exception is signaled according to the
 * immediate operand. If any source operand is an SNaN then it will be
 * converted to a QNaN.
 */
extern __m256  __cdecl _mm256_round_ps(__m256, int);



/*
 * Unpack and Interleave High Packed Double-Precision Floating-Point Values
 * **** VUNPCKHPD ymm1,ymm2,ymm3/m256
 * Performs an interleaved unpack of the high double-precision floating-point
 * values from the first source operand and the second source operand.
 */
extern __m256d __cdecl _mm256_unpackhi_pd(__m256d, __m256d);

/*
 * Unpack and Interleave High Packed Single-Precision Floating-Point Values
 * **** VUNPCKHPS ymm1,ymm2,ymm3
 * Performs an interleaved unpack of the high single-precision floating-point
 * values from the first source operand and the second source operand
 */
extern __m256  __cdecl _mm256_unpackhi_ps(__m256, __m256);

/*
 * Unpack and Interleave Low Packed Double-Precision Floating-Point Values
 * **** VUNPCKLPD ymm1,ymm2,ymm3/m256
 * Performs an interleaved unpack of the low double-precision floating-point
 * values from the first source operand and the second source operand
 */
extern __m256d __cdecl _mm256_unpacklo_pd(__m256d, __m256d);

/*
 * Unpack and Interleave Low Packed Single-Precision Floating-Point Values
 * **** VUNPCKLPS ymm1,ymm2,ymm3
 * Performs an interleaved unpack of the low single-precision floating-point
 * values from the first source operand and the second source operand
 */
extern __m256  __cdecl _mm256_unpacklo_ps(__m256, __m256);

/*
 * Packed Bit Test
 * **** VPTEST ymm1, ymm2/m256
 * VPTEST set the ZF flag if all bits in the result are 0 of the bitwise AND
 * of the first source operand and the second source operand. VPTEST sets the
 * CF flag if all bits in the result are 0 of the bitwise AND of the second
 * source operand and the logical NOT of the first source operand.
 */
extern int     __cdecl _mm256_testz_si256(__m256i, __m256i);



extern int     __cdecl _mm256_testc_si256(__m256i, __m256i);



extern int     __cdecl _mm256_testnzc_si256(__m256i, __m256i);



/*
 * Packed Bit Test
 * **** VTESTPD ymm1, ymm2/m256
 * **** VTESTPD xmm1, xmm2/m128
 * VTESTPD performs a bitwise comparison of all the sign bits of the
 * double-precision elements in the first source operation and corresponding
 * sign bits in the second source operand. If the AND of the two sets of bits
 * produces all zeros, the ZF is set else the ZF is clear. If the AND NOT of
 * the source sign bits with the dest sign bits produces all zeros the CF is
 * set else the CF is clear
 */
extern int     __cdecl _mm256_testz_pd(__m256d, __m256d);
extern int     __cdecl _mm256_testc_pd(__m256d, __m256d);
extern int     __cdecl _mm256_testnzc_pd(__m256d, __m256d);
extern int     __cdecl _mm_testz_pd(__m128d, __m128d);
extern int     __cdecl _mm_testc_pd(__m128d, __m128d);
extern int     __cdecl _mm_testnzc_pd(__m128d, __m128d);

/*
 * Packed Bit Test
 * **** VTESTPS ymm1, ymm2/m256
 * **** VTESTPS xmm1, xmm2/m128
 * VTESTPS performs a bitwise comparison of all the sign bits of the packed
 * single-precision elements in the first source operation and corresponding
 * sign bits in the second source operand. If the AND of the two sets of bits
 * produces all zeros, the ZF is set else the ZF is clear. If the AND NOT of
 * the source sign bits with the dest sign bits produces all zeros the CF is
 * set else the CF is clear
 */
extern int     __cdecl _mm256_testz_ps(__m256, __m256);
extern int     __cdecl _mm256_testc_ps(__m256, __m256);
extern int     __cdecl _mm256_testnzc_ps(__m256, __m256);
extern int     __cdecl _mm_testz_ps(__m128, __m128);
extern int     __cdecl _mm_testc_ps(__m128, __m128);
extern int     __cdecl _mm_testnzc_ps(__m128, __m128);

/*
 * Extract Double-Precision Floating-Point Sign mask
 * **** VMOVMSKPD r32, ymm2
 * Extracts the sign bits from the packed double-precision floating-point
 * values in the source operand, formats them into a 4-bit mask, and stores
 * the mask in the destination
 */
extern int     __cdecl _mm256_movemask_pd(__m256d);

/*
 * Extract Single-Precision Floating-Point Sign mask
 * **** VMOVMSKPS r32, ymm2
 * Extracts the sign bits from the packed single-precision floating-point
 * values in the source operand, formats them into a 8-bit mask, and stores
 * the mask in the destination
 */
extern int     __cdecl _mm256_movemask_ps(__m256);

/*
 * Return 256-bit vector with all elements set to 0
 */
extern __m256d __cdecl _mm256_setzero_pd(void);
extern __m256  __cdecl _mm256_setzero_ps(void);
extern __m256i __cdecl _mm256_setzero_si256(void);

/*
 * Return 256-bit vector initialized to specified arguments
 */
extern __m256d __cdecl _mm256_set_pd(double, double, double, double);
extern __m256  __cdecl _mm256_set_ps(float, float, float, float,
                                            float, float, float, float);
extern __m256i __cdecl _mm256_set_epi8(char, char, char, char,
                                              char, char, char, char,
                                              char, char, char, char,
                                              char, char, char, char,
                                              char, char, char, char,
                                              char, char, char, char,
                                              char, char, char, char,
                                              char, char, char, char);
extern __m256i __cdecl _mm256_set_epi16(short, short, short, short,
                                               short, short, short, short,
                                               short, short, short, short,
                                               short, short, short, short);
extern __m256i __cdecl _mm256_set_epi32(int, int, int, int,
                                               int, int, int, int);
extern __m256i __cdecl _mm256_set_epi64x(__int64, __int64,
                                                __int64, __int64);










extern __m256d __cdecl _mm256_setr_pd(double, double, double, double);
extern __m256  __cdecl _mm256_setr_ps(float, float, float, float,
                                             float, float, float, float);
extern __m256i __cdecl _mm256_setr_epi8(char, char, char, char,
                                               char, char, char, char,
                                               char, char, char, char,
                                               char, char, char, char,
                                               char, char, char, char,
                                               char, char, char, char,
                                               char, char, char, char,
                                               char, char, char, char);
extern __m256i __cdecl _mm256_setr_epi16(short, short, short, short,
                                                short, short, short, short,
                                                short, short, short, short,
                                                short, short, short, short);
extern __m256i __cdecl _mm256_setr_epi32(int, int, int, int,
                                                int, int, int, int);
extern __m256i __cdecl _mm256_setr_epi64x(__int64, __int64,
                                                 __int64, __int64);




/*
 * Return 256-bit vector with all elements initialized to specified scalar
 */
extern __m256d __cdecl _mm256_set1_pd(double);
extern __m256  __cdecl _mm256_set1_ps(float);
extern __m256i __cdecl _mm256_set1_epi8(char);
extern __m256i __cdecl _mm256_set1_epi16(short);
extern __m256i __cdecl _mm256_set1_epi32(int);
extern __m256i __cdecl _mm256_set1_epi64x(long long);

/*
 * Support intrinsic functions to do vector type casts. These functions do
 * not introduce extra moves to generated code. When cast is done from a 128
 * to 256-bit type the low 128 bits of the 256-bit result contain source
 * parameter value; the upper 128 bits of the result are undefined.
 */
extern __m256  __cdecl _mm256_castpd_ps(__m256d);
extern __m256d __cdecl _mm256_castps_pd(__m256);
extern __m256i __cdecl _mm256_castps_si256(__m256);
extern __m256i __cdecl _mm256_castpd_si256(__m256d);
extern __m256  __cdecl _mm256_castsi256_ps(__m256i);
extern __m256d __cdecl _mm256_castsi256_pd(__m256i);
extern __m128  __cdecl _mm256_castps256_ps128(__m256);
extern __m128d __cdecl _mm256_castpd256_pd128(__m256d);
extern __m128i __cdecl _mm256_castsi256_si128(__m256i);
extern __m256  __cdecl _mm256_castps128_ps256(__m128);
extern __m256d __cdecl _mm256_castpd128_pd256(__m128d);
extern __m256i __cdecl _mm256_castsi128_si256(__m128i);


/*
 * Support for half-float conversions to/from normal float.
 * Immediate argument is used for special MXCSR overrides.
 */
extern __m128  __cdecl _mm_cvtph_ps(__m128i);
extern __m256  __cdecl _mm256_cvtph_ps(__m128i);
extern __m128i __cdecl _mm_cvtps_ph(__m128 /* m1 */, const int /* imm */);
extern __m128i __cdecl _mm256_cvtps_ph(__m256, int);

/*
 * Return a vector with all elements set to zero. It is recommended to use the
 * result of this intrinsic as an input argument to another intrinsic when the
 * initial value is irrelevant.
 */







/*
 * The list of extended control registers.
 * Currently, the list includes only one register.
 */


/* Returns the content of the specified extended control register */
extern unsigned __int64 __cdecl _xgetbv(unsigned int);

/* Writes the value to the specified extended control register */
extern void __cdecl _xsetbv(unsigned int, unsigned __int64);


/*
 * Performs a full or partial save of the enabled processor state components
 * using the specified memory address location and a mask.
 */
extern void __cdecl _xsave(void *, unsigned __int64);

extern void __cdecl _xsave64(void *, unsigned __int64);
#line 1260 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

/*
 * Performs a full or partial save of the enabled processor state components
 * using the specified memory address location and a mask.
 * Optimize the state save operation if possible.
 */
extern void __cdecl _xsaveopt(void *, unsigned __int64);

extern void __cdecl _xsaveopt64(void *, unsigned __int64);
#line 1270 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

/*
 * Performs a full or compressed partial save of the enabled processor state
 * components using the specified memory address location and a mask.
 */
extern void __cdecl _xsavec(void *, unsigned __int64);

extern void __cdecl _xsavec64(void *, unsigned __int64);
#line 1279 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

/*
 * Performs a full or partial restore of the enabled processor states
 * using the state information stored in the specified memory address location
 * and a mask.
 */
extern void __cdecl _xrstor(void const *, unsigned __int64);

extern void __cdecl _xrstor64(void const *, unsigned __int64);
#line 1289 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

/*
 * Performs a full or partial save of the enabled processor extended
 * and supervisor state components in compacted form using the
 * specified memory address location and masks in XCR0 and IA32_XSS MSR.
 */
extern void __cdecl _xsaves(void *, unsigned __int64);

extern void __cdecl _xsaves64(void *, unsigned __int64);
#line 1299 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

/*
 * Performs a full or partial restore of the enabled processor extended
 * and supervisor states using the state information stored in the
 * specified memory address location and masks in XCR0 and IA32_XSS MSR.
 */
extern void __cdecl _xrstors(void const *, unsigned __int64);

extern void __cdecl _xrstors64(void const *, unsigned __int64);
#line 1309 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

/*
 * Saves the current state of the x87 FPU, MMX technology, XMM,
 * and MXCSR registers to the specified 512-byte memory location.
 */
extern void __cdecl _fxsave(void *);

extern void __cdecl _fxsave64(void *);
#line 1318 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

/*
 * Restore the current state of the x87 FPU, MMX technology, XMM,
 * and MXCSR registers from the specified 512-byte memory location.
 */
extern void __cdecl _fxrstor(void const *);

extern void __cdecl _fxrstor64(void const *);
#line 1327 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

/*
 * Perform one attempt to generate a hardware generated random value.
 * The generated value is written to the given memory location and the success
 * status is returned: 1 if the hardware could generate a valid random number
 * and 0 otherwise.
 */
extern int __cdecl _rdrand16_step(unsigned short *);
extern int __cdecl _rdrand32_step(unsigned int *);

extern int __cdecl _rdrand64_step(unsigned __int64 *);
#line 1339 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"


/*
 * Return the value of the FS/GS segment base register.
 */
extern unsigned int     __cdecl _readfsbase_u32(void);
extern unsigned int     __cdecl _readgsbase_u32(void);
extern unsigned __int64 __cdecl _readfsbase_u64(void);
extern unsigned __int64 __cdecl _readgsbase_u64(void);

/*
 * Write the value to the FS/GS segment base register.
 */
extern void __cdecl _writefsbase_u32(unsigned int);
extern void __cdecl _writegsbase_u32(unsigned int);
extern void __cdecl _writefsbase_u64(unsigned __int64);
extern void __cdecl _writegsbase_u64(unsigned __int64);
#line 1357 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

/*
 * Perform FMA (Fused Multiply-and-Add) operations.
 */
extern __m128  __cdecl _mm_fmadd_ps(__m128, __m128, __m128);
extern __m128d __cdecl _mm_fmadd_pd(__m128d, __m128d, __m128d);
extern __m128  __cdecl _mm_fmadd_ss(__m128, __m128, __m128);
extern __m128d __cdecl _mm_fmadd_sd(__m128d, __m128d, __m128d);
extern __m128  __cdecl _mm_fmsub_ps(__m128, __m128, __m128);
extern __m128d __cdecl _mm_fmsub_pd(__m128d, __m128d, __m128d);
extern __m128  __cdecl _mm_fmsub_ss(__m128, __m128, __m128);
extern __m128d __cdecl _mm_fmsub_sd(__m128d, __m128d, __m128d);
extern __m128  __cdecl _mm_fnmadd_ps(__m128, __m128, __m128);
extern __m128d __cdecl _mm_fnmadd_pd(__m128d, __m128d, __m128d);
extern __m128  __cdecl _mm_fnmadd_ss(__m128, __m128, __m128);
extern __m128d __cdecl _mm_fnmadd_sd(__m128d, __m128d, __m128d);
extern __m128  __cdecl _mm_fnmsub_ps(__m128, __m128, __m128);
extern __m128d __cdecl _mm_fnmsub_pd(__m128d, __m128d, __m128d);
extern __m128  __cdecl _mm_fnmsub_ss(__m128, __m128, __m128);
extern __m128d __cdecl _mm_fnmsub_sd(__m128d, __m128d, __m128d);

extern __m256  __cdecl _mm256_fmadd_ps(__m256, __m256, __m256);
extern __m256d __cdecl _mm256_fmadd_pd(__m256d, __m256d, __m256d);
extern __m256  __cdecl _mm256_fmsub_ps(__m256, __m256, __m256);
extern __m256d __cdecl _mm256_fmsub_pd(__m256d, __m256d, __m256d);
extern __m256  __cdecl _mm256_fnmadd_ps(__m256, __m256, __m256);
extern __m256d __cdecl _mm256_fnmadd_pd(__m256d, __m256d, __m256d);
extern __m256  __cdecl _mm256_fnmsub_ps(__m256, __m256, __m256);
extern __m256d __cdecl _mm256_fnmsub_pd(__m256d, __m256d, __m256d);


/*
 * Fused Multiply-and-Add/Subtract__and Multiply-and-Subtract/Add operations.
 */
extern __m128  __cdecl _mm_fmaddsub_ps(__m128, __m128, __m128);
extern __m128d __cdecl _mm_fmaddsub_pd(__m128d, __m128d, __m128d);
extern __m128  __cdecl _mm_fmsubadd_ps(__m128, __m128, __m128);
extern __m128d __cdecl _mm_fmsubadd_pd(__m128d, __m128d, __m128d);

extern __m256  __cdecl _mm256_fmaddsub_ps(__m256, __m256, __m256);
extern __m256d __cdecl _mm256_fmaddsub_pd(__m256d, __m256d, __m256d);
extern __m256  __cdecl _mm256_fmsubadd_ps(__m256, __m256, __m256);
extern __m256d __cdecl _mm256_fmsubadd_pd(__m256d, __m256d, __m256d);


/*
 * Integer 256-bit vector comparison operations.
 */
extern __m256i __cdecl _mm256_cmpeq_epi8(__m256i, __m256i);
extern __m256i __cdecl _mm256_cmpeq_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_cmpeq_epi32(__m256i, __m256i);
extern __m256i __cdecl _mm256_cmpeq_epi64(__m256i, __m256i);

extern __m256i __cdecl _mm256_cmpgt_epi8(__m256i, __m256i);
extern __m256i __cdecl _mm256_cmpgt_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_cmpgt_epi32(__m256i, __m256i);
extern __m256i __cdecl _mm256_cmpgt_epi64(__m256i, __m256i);


/*
 * Integer 256-bit vector MIN/MAX operations.
 */
extern __m256i __cdecl _mm256_max_epi8(__m256i, __m256i);
extern __m256i __cdecl _mm256_max_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_max_epi32(__m256i, __m256i);
extern __m256i __cdecl _mm256_max_epu8(__m256i, __m256i);
extern __m256i __cdecl _mm256_max_epu16(__m256i, __m256i);
extern __m256i __cdecl _mm256_max_epu32(__m256i, __m256i);

extern __m256i __cdecl _mm256_min_epi8(__m256i, __m256i);
extern __m256i __cdecl _mm256_min_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_min_epi32(__m256i, __m256i);
extern __m256i __cdecl _mm256_min_epu8(__m256i, __m256i);
extern __m256i __cdecl _mm256_min_epu16(__m256i, __m256i);
extern __m256i __cdecl _mm256_min_epu32(__m256i, __m256i);


/*
 * Integer 256-bit vector logical operations.
 */
extern __m256i __cdecl _mm256_and_si256(__m256i, __m256i);
extern __m256i __cdecl _mm256_andnot_si256(__m256i, __m256i);
extern __m256i __cdecl _mm256_or_si256(__m256i, __m256i);
extern __m256i __cdecl _mm256_xor_si256(__m256i, __m256i);


/*
 * Integer 256-bit vector arithmetic operations.
 */
extern __m256i __cdecl _mm256_abs_epi8(__m256i);
extern __m256i __cdecl _mm256_abs_epi16(__m256i);
extern __m256i __cdecl _mm256_abs_epi32(__m256i);

extern __m256i __cdecl _mm256_add_epi8(__m256i, __m256i);
extern __m256i __cdecl _mm256_add_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_add_epi32(__m256i, __m256i);
extern __m256i __cdecl _mm256_add_epi64(__m256i, __m256i);

extern __m256i __cdecl _mm256_adds_epi8(__m256i, __m256i);
extern __m256i __cdecl _mm256_adds_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_adds_epu8(__m256i, __m256i);
extern __m256i __cdecl _mm256_adds_epu16(__m256i, __m256i);

extern __m256i __cdecl _mm256_sub_epi8(__m256i, __m256i);
extern __m256i __cdecl _mm256_sub_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_sub_epi32(__m256i, __m256i);
extern __m256i __cdecl _mm256_sub_epi64(__m256i, __m256i);

extern __m256i __cdecl _mm256_subs_epi8(__m256i, __m256i);
extern __m256i __cdecl _mm256_subs_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_subs_epu8(__m256i, __m256i);
extern __m256i __cdecl _mm256_subs_epu16(__m256i, __m256i);

extern __m256i __cdecl _mm256_avg_epu8(__m256i, __m256i);
extern __m256i __cdecl _mm256_avg_epu16(__m256i, __m256i);

extern __m256i __cdecl _mm256_hadd_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_hadd_epi32(__m256i, __m256i);
extern __m256i __cdecl _mm256_hadds_epi16(__m256i, __m256i);

extern __m256i __cdecl _mm256_hsub_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_hsub_epi32(__m256i, __m256i);
extern __m256i __cdecl _mm256_hsubs_epi16(__m256i, __m256i);

extern __m256i __cdecl _mm256_madd_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_maddubs_epi16(__m256i, __m256i);

extern __m256i __cdecl _mm256_mulhi_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_mulhi_epu16(__m256i, __m256i);

extern __m256i __cdecl _mm256_mullo_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_mullo_epi32(__m256i, __m256i);

extern __m256i __cdecl _mm256_mul_epu32(__m256i, __m256i);
extern __m256i __cdecl _mm256_mul_epi32(__m256i, __m256i);

extern __m256i __cdecl _mm256_sign_epi8(__m256i, __m256i);
extern __m256i __cdecl _mm256_sign_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_sign_epi32(__m256i, __m256i);

extern __m256i __cdecl _mm256_mulhrs_epi16(__m256i, __m256i);

extern __m256i __cdecl _mm256_sad_epu8(__m256i, __m256i);
extern __m256i __cdecl _mm256_mpsadbw_epu8(__m256i, __m256i, const int);


/*
 * Integer 256-bit vector arithmetic/logical shift operations.
 */
extern __m256i __cdecl _mm256_slli_si256(__m256i, const int);

extern __m256i __cdecl _mm256_srli_si256(__m256i, const int);


extern __m256i __cdecl _mm256_sll_epi16(__m256i, __m128i);
extern __m256i __cdecl _mm256_sll_epi32(__m256i, __m128i);
extern __m256i __cdecl _mm256_sll_epi64(__m256i, __m128i);

extern __m256i __cdecl _mm256_slli_epi16(__m256i, int);
extern __m256i __cdecl _mm256_slli_epi32(__m256i, int);
extern __m256i __cdecl _mm256_slli_epi64(__m256i, int);

extern __m256i __cdecl _mm256_sllv_epi32(__m256i, __m256i);
extern __m256i __cdecl _mm256_sllv_epi64(__m256i, __m256i);

extern __m128i __cdecl _mm_sllv_epi32(__m128i, __m128i);
extern __m128i __cdecl _mm_sllv_epi64(__m128i, __m128i);

extern __m256i __cdecl _mm256_sra_epi16(__m256i, __m128i);
extern __m256i __cdecl _mm256_sra_epi32(__m256i, __m128i);

extern __m256i __cdecl _mm256_srai_epi16(__m256i, int);
extern __m256i __cdecl _mm256_srai_epi32(__m256i, int);

extern __m256i __cdecl _mm256_srav_epi32(__m256i, __m256i);

extern __m128i __cdecl _mm_srav_epi32(__m128i, __m128i);

extern __m256i __cdecl _mm256_srl_epi16(__m256i, __m128i);
extern __m256i __cdecl _mm256_srl_epi32(__m256i, __m128i);
extern __m256i __cdecl _mm256_srl_epi64(__m256i, __m128i);

extern __m256i __cdecl _mm256_srli_epi16(__m256i, int);
extern __m256i __cdecl _mm256_srli_epi32(__m256i, int);
extern __m256i __cdecl _mm256_srli_epi64(__m256i, int);

extern __m256i __cdecl _mm256_srlv_epi32(__m256i, __m256i);
extern __m256i __cdecl _mm256_srlv_epi64(__m256i, __m256i);

extern __m128i __cdecl _mm_srlv_epi32(__m128i, __m128i);
extern __m128i __cdecl _mm_srlv_epi64(__m128i, __m128i);


/*
 * Integer 128/256-bit vector pack/blend/shuffle/insert/extract operations.
 */
extern __m128i __cdecl _mm_blend_epi32(__m128i, __m128i, const int);

extern __m256i __cdecl _mm256_blend_epi32(__m256i,__m256i, const int);

extern __m256i __cdecl _mm256_alignr_epi8(__m256i, __m256i, const int);

extern __m256i __cdecl _mm256_blendv_epi8(__m256i, __m256i, __m256i);
extern __m256i __cdecl _mm256_blend_epi16(__m256i, __m256i, const int);

extern __m256i __cdecl _mm256_packs_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_packs_epi32(__m256i, __m256i);
extern __m256i __cdecl _mm256_packus_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_packus_epi32(__m256i, __m256i);

extern __m256i __cdecl _mm256_unpackhi_epi8(__m256i, __m256i);
extern __m256i __cdecl _mm256_unpackhi_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_unpackhi_epi32(__m256i, __m256i);
extern __m256i __cdecl _mm256_unpackhi_epi64(__m256i, __m256i);

extern __m256i __cdecl _mm256_unpacklo_epi8(__m256i, __m256i);
extern __m256i __cdecl _mm256_unpacklo_epi16(__m256i, __m256i);
extern __m256i __cdecl _mm256_unpacklo_epi32(__m256i, __m256i);
extern __m256i __cdecl _mm256_unpacklo_epi64(__m256i, __m256i);

extern __m256i __cdecl _mm256_shuffle_epi8(__m256i, __m256i);
extern __m256i __cdecl _mm256_shuffle_epi32(__m256i, const int);

extern __m256i __cdecl _mm256_shufflehi_epi16(__m256i, const int);
extern __m256i __cdecl _mm256_shufflelo_epi16(__m256i, const int);

extern __m128i __cdecl _mm256_extracti128_si256(__m256i, const int);
extern __m256i __cdecl _mm256_inserti128_si256(__m256i, __m128i, const int);


/*
 * Scalar to 128/256-bit vector broadcast operations.
 */
extern __m128  __cdecl _mm_broadcastss_ps(__m128);
extern __m128d __cdecl _mm_broadcastsd_pd(__m128d);

extern __m128i __cdecl _mm_broadcastb_epi8(__m128i);
extern __m128i __cdecl _mm_broadcastw_epi16(__m128i);
extern __m128i __cdecl _mm_broadcastd_epi32(__m128i);
extern __m128i __cdecl _mm_broadcastq_epi64(__m128i);

extern __m256  __cdecl _mm256_broadcastss_ps(__m128);
extern __m256d __cdecl _mm256_broadcastsd_pd(__m128d);

extern __m256i __cdecl _mm256_broadcastb_epi8(__m128i);
extern __m256i __cdecl _mm256_broadcastw_epi16(__m128i);
extern __m256i __cdecl _mm256_broadcastd_epi32(__m128i);
extern __m256i __cdecl _mm256_broadcastq_epi64(__m128i);

extern __m256i __cdecl _mm256_broadcastsi128_si256(__m128i);



/*
 * Integer 256-bit vector signed/unsigned extension operations.
 */
extern __m256i __cdecl _mm256_cvtepi8_epi16(__m128i);
extern __m256i __cdecl _mm256_cvtepi8_epi32(__m128i);
extern __m256i __cdecl _mm256_cvtepi8_epi64(__m128i);
extern __m256i __cdecl _mm256_cvtepi16_epi32(__m128i);
extern __m256i __cdecl _mm256_cvtepi16_epi64(__m128i);
extern __m256i __cdecl _mm256_cvtepi32_epi64(__m128i);

extern __m256i __cdecl _mm256_cvtepu8_epi16(__m128i);
extern __m256i __cdecl _mm256_cvtepu8_epi32(__m128i);
extern __m256i __cdecl _mm256_cvtepu8_epi64(__m128i);
extern __m256i __cdecl _mm256_cvtepu16_epi32(__m128i);
extern __m256i __cdecl _mm256_cvtepu16_epi64(__m128i);
extern __m256i __cdecl _mm256_cvtepu32_epi64(__m128i);


/*
 * Returns a 32-bit mask made up of the most significant bit of each byte
 * of the 256-bit vector source operand.
 */
extern int __cdecl _mm256_movemask_epi8(__m256i);


/*
 * Masked load/store operations.
 */
extern __m128i __cdecl _mm_maskload_epi32(int const * /* ptr */,
                                          __m128i     /* vmask */);
extern __m128i __cdecl _mm_maskload_epi64(__int64 const * /* ptr */,
                                          __m128i         /* vmask */);

extern void __cdecl _mm_maskstore_epi32(int *   /* ptr */,
                                        __m128i /* vmask */,
                                        __m128i /* val */);
extern void __cdecl _mm_maskstore_epi64(__int64 * /* ptr */,
                                        __m128i   /* vmask */,
                                        __m128i   /* val */);

extern __m256i __cdecl _mm256_maskload_epi32(int const * /* ptr */,
                                             __m256i     /* vmask */);
extern __m256i __cdecl _mm256_maskload_epi64(__int64 const * /* ptr */,
                                             __m256i         /* vmask */);

extern void __cdecl _mm256_maskstore_epi32(int *   /* ptr */,
                                           __m256i /* vmask */,
                                           __m256i /* val */);
extern void __cdecl _mm256_maskstore_epi64(__int64 * /* ptr */,
                                           __m256i   /* vmask */,
                                           __m256i   /* val */);


/*
 * Permute elements in vector operations.
 */
extern __m256i __cdecl _mm256_permutevar8x32_epi32(__m256i, __m256i);
extern __m256  __cdecl _mm256_permutevar8x32_ps(__m256, __m256i);

extern __m256i __cdecl _mm256_permute4x64_epi64(__m256i, const int);
extern __m256d __cdecl _mm256_permute4x64_pd(__m256d, const int);

extern __m256i __cdecl _mm256_permute2x128_si256(__m256i, __m256i, const int);


/*
 * Load 32-bytes from memory using non-temporal aligned hint.
 */
extern __m256i  __cdecl _mm256_stream_load_si256(__m256i const *);



/*
 * Masked GATHER from memory to vector register operations.
 */
extern __m256d __cdecl _mm256_mask_i32gather_pd(__m256d        /* old_dst */,
                                                double const * /* ptr */,
                                                __m128i        /* vindex */,
                                                __m256d        /* vmask */,
                                                const int      /* scale */);
extern __m256  __cdecl _mm256_mask_i32gather_ps(__m256         /* old_dst */,
                                                float const *  /* ptr */,
                                                __m256i        /* vindex */,
                                                __m256         /* vmask */,
                                                const int      /* scale */);
extern __m256d __cdecl _mm256_mask_i64gather_pd(__m256d        /* old_dst */,
                                                double const * /* ptr */,
                                                __m256i        /* vindex */,
                                                __m256d        /* vmask */,
                                                const int      /* scale */);
extern __m128  __cdecl _mm256_mask_i64gather_ps(__m128         /* old_dst */,
                                                float const *  /* ptr */,
                                                __m256i        /* vindex */,
                                                __m128         /* vmask */,
                                                const int      /* scale */);

extern __m128d __cdecl _mm_mask_i32gather_pd(__m128d        /* old_dst */,
                                             double const * /* ptr */,
                                             __m128i        /* vindex */,
                                             __m128d        /* vmask */,
                                             const int      /* scale */);
extern __m128  __cdecl _mm_mask_i32gather_ps(__m128         /* old_dst */,
                                             float const *  /* ptr */,
                                             __m128i        /* vindex */,
                                             __m128         /* vmask */,
                                             const int      /* scale */);
extern __m128d __cdecl _mm_mask_i64gather_pd(__m128d        /* old_dst */,
                                             double const * /* ptr */,
                                             __m128i        /* vindex */,
                                             __m128d        /* vmask */,
                                             const int      /* scale */);
extern __m128  __cdecl _mm_mask_i64gather_ps(__m128         /* old_dst */,
                                             float const *  /* ptr */,
                                             __m128i        /* vindex */,
                                             __m128         /* vmask */,
                                             const int      /* scale */);


extern __m256i __cdecl _mm256_mask_i32gather_epi32(__m256i     /* old_dst */,
                                                   int const * /* ptr */,
                                                   __m256i     /* vindex */,
                                                   __m256i     /* vmask */,
                                                   const int   /* scale */);
extern __m256i __cdecl _mm256_mask_i32gather_epi64(__m256i     /* old_dst */,
                                                   __int64 const * /* ptr */,
                                                   __m128i     /* vindex */,
                                                   __m256i     /* vmask */,
                                                   const int   /* scale */);
extern __m128i __cdecl _mm256_mask_i64gather_epi32(__m128i     /* old_dst */,
                                                   int     const * /* ptr */,
                                                   __m256i     /* vindex */,
                                                   __m128i     /* vmask */,
                                                   const int   /* scale */);
extern __m256i __cdecl _mm256_mask_i64gather_epi64(__m256i     /* old_dst */,
                                                   __int64 const * /* ptr */,
                                                   __m256i     /* vindex */,
                                                   __m256i     /* vmask */,
                                                   const int   /* scale */);

extern __m128i __cdecl _mm_mask_i32gather_epi32(__m128i         /* old_dst */,
                                                int const *     /* ptr */,
                                                __m128i         /* vindex */,
                                                __m128i         /* vmask */,
                                                const int       /* scale */);
extern __m128i __cdecl _mm_mask_i32gather_epi64(__m128i         /* old_dst */,
                                                __int64 const * /* ptr */,
                                                __m128i         /* vindex */,
                                                __m128i         /* vmask */,
                                                const int       /* scale */);
extern __m128i __cdecl _mm_mask_i64gather_epi32(__m128i         /* old_dst */,
                                                int     const * /* ptr */,
                                                __m128i         /* vindex */,
                                                __m128i         /* vmask */,
                                                const int       /* scale */);
extern __m128i __cdecl _mm_mask_i64gather_epi64(__m128i         /* old_dst */,
                                                __int64 const * /* ptr */,
                                                __m128i         /* vindex */,
                                                __m128i         /* vmask */,
                                                const int       /* scale */);


/*
 * GATHER from memory to vector register operations.
 */
extern __m256d __cdecl _mm256_i32gather_pd(double const * /* ptr */,
                                           __m128i        /* vindex */,
                                           const int      /* index_scale */);
extern __m256  __cdecl _mm256_i32gather_ps(float  const * /* ptr */,
                                           __m256i        /* vindex */,
                                           const int      /* index_scale */);
extern __m256d __cdecl _mm256_i64gather_pd(double const * /* ptr */,
                                           __m256i        /* vindex */,
                                           const int      /* index_scale */);
extern __m128  __cdecl _mm256_i64gather_ps(float  const * /* ptr */,
                                           __m256i        /* vindex */,
                                           const int      /* index_scale */);

extern __m128d __cdecl _mm_i32gather_pd(double const * /* ptr */,
                                        __m128i        /* vindex */,
                                        const int      /* index_scale */);
extern __m128  __cdecl _mm_i32gather_ps(float  const * /* ptr */,
                                        __m128i        /* vindex */,
                                        const int      /* index_scale */);
extern __m128d __cdecl _mm_i64gather_pd(double const * /* ptr */,
                                        __m128i        /* vindex */,
                                        const int      /* index_scale */);
extern __m128  __cdecl _mm_i64gather_ps(float  const * /* ptr */,
                                        __m128i        /* vindex */,
                                        const int      /* index_scale */);

extern __m256i __cdecl _mm256_i32gather_epi32(int const *     /* ptr */,
                                              __m256i         /* vindex */,
                                              const int       /* scale */);
extern __m256i __cdecl _mm256_i32gather_epi64(__int64 const * /* ptr */,
                                              __m128i         /* vindex */,
                                              const int       /* scale */);
extern __m128i __cdecl _mm256_i64gather_epi32(int const *     /* ptr */,
                                              __m256i         /* vindex */,
                                              const int       /* scale */);
extern __m256i __cdecl _mm256_i64gather_epi64(__int64 const * /* ptr */,
                                              __m256i         /* vindex */,
                                              const int       /* scale */);

extern __m128i __cdecl _mm_i32gather_epi32(int const *     /* ptr */,
                                           __m128i         /* vindex */,
                                           const int       /* index_scale */);
extern __m128i __cdecl _mm_i32gather_epi64(__int64 const * /* ptr */,
                                           __m128i         /* vindex */,
                                           const int       /* index_scale */);
extern __m128i __cdecl _mm_i64gather_epi32(int     const * /* ptr */,
                                           __m128i         /* vindex */,
                                           const int       /* index_scale */);
extern __m128i __cdecl _mm_i64gather_epi64(__int64 const * /* ptr */,
                                           __m128i         /* vindex */,
                                           const int       /* index_scale */);


// unaligned load and store functions








/*
 * A collection of operations to manipulate integer data at bit-granularity.
 * The names of these functions are formed from the instruction mnemonic and
 * the operand data type used to implement them.
 */
extern unsigned int     _bextr_u32(unsigned int /* src */,
                                   unsigned int /* start_bit */,
                                   unsigned int /* len_in_bits */);
extern unsigned int     _bextr2_u32(unsigned int /* src */,
                                    unsigned int /* start_and_len_in_bits */);
extern unsigned int     _blsi_u32(unsigned int);
extern unsigned int     _blsmsk_u32(unsigned int);
extern unsigned int     _blsr_u32(unsigned int);
extern unsigned int     _bzhi_u32(unsigned int /* src */,
                                  unsigned int /* index */);
extern unsigned int     _mulx_u32(unsigned int /* src1 */,
                                  unsigned int /* src2 */,
                                  unsigned int * /* high_bits */);
extern unsigned int     _pdep_u32(unsigned int /* src */,
                                  unsigned int /* mask */);
extern unsigned int     _pext_u32(unsigned int /* src */,
                                  unsigned int /* mask */);
extern unsigned int     _rorx_u32(unsigned int /* src */,
                                  const unsigned int /* shift_count */);
extern int              _sarx_i32(int /* src */,
                                  unsigned int /* shift_count */);
extern unsigned int     _shlx_u32(unsigned int /* src */,
                                  unsigned int /* shift_count */);
extern unsigned int     _shrx_u32(unsigned int /* src */,
                                          unsigned int /* shift_count */);


extern unsigned __int64 _bextr_u64(unsigned __int64 /* src */,
                                   unsigned int /* start_bit */,
                                   unsigned int /* len_in_bits */);
extern unsigned __int64 _bextr2_u64(unsigned __int64 /* src */,
                                    unsigned __int64 /* start_and_len_in_bits */);
extern unsigned __int64 _blsi_u64(unsigned __int64);
extern unsigned __int64 _blsmsk_u64(unsigned __int64);
extern unsigned __int64 _blsr_u64(unsigned __int64);
extern unsigned __int64 _bzhi_u64(unsigned __int64 /* src */,
                                  unsigned int /* index */);
extern unsigned __int64 _mulx_u64(unsigned __int64 /* src1 */,
                                  unsigned __int64 /* src2 */,
                                  unsigned __int64 * /* high_bits */);
extern unsigned __int64 _pdep_u64(unsigned __int64 /* src */,
                                  unsigned __int64 /* mask */);
extern unsigned __int64 _pext_u64(unsigned __int64 /* src */,
                                  unsigned __int64 /* mask */);
extern unsigned __int64 _rorx_u64(unsigned __int64 /* src */,
                                  const unsigned int /* shift_count */);
extern __int64          _sarx_i64(__int64 /* src */,
                                  unsigned int /* shift_count */);
extern unsigned __int64 _shlx_u64(unsigned __int64 /* src */,
                                  unsigned int /* shift_count */);
extern unsigned __int64 _shrx_u64(unsigned __int64 /* src */,
                                          unsigned int /* shift_count */);
#line 1894 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"


/*
 * Leading zero bit count.
 *
 *    Counts the number of leading zero bits in a source operand.
 *    Returns operand size as output when source operand is zero.
 */
extern unsigned int     _lzcnt_u32(unsigned int);

extern unsigned __int64 _lzcnt_u64(unsigned __int64);
#line 1906 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

/*
 * Trailing zero bit count.
 *
 *    Searches the source operand (r2) for the least significant set bit
 *    (1 bit).  If a least significant 1 bit is found, its bit index is
 *    returned, otherwise the result is the number of bits in the operand size.
 */
extern unsigned int     _tzcnt_u32(unsigned int);

extern unsigned __int64 _tzcnt_u64(unsigned __int64);
#line 1918 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"



/*
 * Operation targeted to system software that manages processor context IDs.
 */
extern void __cdecl _invpcid(unsigned int /* type */, void * /* descriptor */);

// Hardware Lock Elision
extern void _Store_HLERelease(long volatile *,long);
extern void _StorePointer_HLERelease(void * volatile *,void *);

extern long _InterlockedExchange_HLEAcquire(long volatile *,long);
extern long _InterlockedExchange_HLERelease(long volatile *,long);
extern void * _InterlockedExchangePointer_HLEAcquire(void *volatile *,void *);
extern void * _InterlockedExchangePointer_HLERelease(void *volatile *,void *);

extern long _InterlockedCompareExchange_HLEAcquire(long volatile *,long,long);
extern long _InterlockedCompareExchange_HLERelease(long volatile *,long,long);
extern __int64 _InterlockedCompareExchange64_HLEAcquire(__int64 volatile *,__int64,__int64);
extern __int64 _InterlockedCompareExchange64_HLERelease(__int64 volatile *,__int64,__int64);
extern void * _InterlockedCompareExchangePointer_HLEAcquire(void *volatile *,void *,void *);
extern void * _InterlockedCompareExchangePointer_HLERelease(void *volatile *,void *,void *);

extern long _InterlockedExchangeAdd_HLEAcquire(long volatile *,long);
extern long _InterlockedExchangeAdd_HLERelease(long volatile *,long);

extern long _InterlockedAnd_HLEAcquire(long volatile *,long);
extern long _InterlockedAnd_HLERelease(long volatile *,long);
extern long _InterlockedOr_HLEAcquire(long volatile *,long);
extern long _InterlockedOr_HLERelease(long volatile *,long);
extern long _InterlockedXor_HLEAcquire(long volatile *,long);
extern long _InterlockedXor_HLERelease(long volatile *,long);

extern unsigned char _interlockedbittestandset_HLEAcquire(long *,long);
extern unsigned char _interlockedbittestandset_HLERelease(long *,long);
extern unsigned char _interlockedbittestandreset_HLEAcquire(long *,long);
extern unsigned char _interlockedbittestandreset_HLERelease(long *,long);


extern void _Store64_HLERelease(__int64 volatile *,__int64);
extern __int64 _InterlockedExchange64_HLEAcquire(__int64 volatile *,__int64);
extern __int64 _InterlockedExchange64_HLERelease(__int64 volatile *,__int64);

extern __int64 _InterlockedExchangeAdd64_HLEAcquire(__int64 volatile *,__int64);
extern __int64 _InterlockedExchangeAdd64_HLERelease(__int64 volatile *,__int64);

extern __int64 _InterlockedAnd64_HLEAcquire(__int64 volatile *,__int64);
extern __int64 _InterlockedAnd64_HLERelease(__int64 volatile *,__int64);
extern __int64 _InterlockedOr64_HLEAcquire(__int64 volatile *,__int64);
extern __int64 _InterlockedOr64_HLERelease(__int64 volatile *,__int64);
extern __int64 _InterlockedXor64_HLEAcquire(__int64 volatile *,__int64);
extern __int64 _InterlockedXor64_HLERelease(__int64 volatile *,__int64);

extern unsigned char _interlockedbittestandset64_HLEAcquire(__int64 *,__int64);
extern unsigned char _interlockedbittestandset64_HLERelease(__int64 *,__int64);
extern unsigned char _interlockedbittestandreset64_HLEAcquire(__int64 *,__int64);
extern unsigned char _interlockedbittestandreset64_HLERelease(__int64 *,__int64);
#line 1977 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

//  Restricted Transactional Memory









extern unsigned int     __cdecl _xbegin(void);
extern void             __cdecl _xend(void);
extern void             __cdecl _xabort(const unsigned int);
extern unsigned char    __cdecl _xtest(void);

/*
 * Perform one attempt to generate a hardware generated random value
 * accordingly to the NIST SP 800-90B/C standards.
 * The generated value is written to the given memory location and the success
 * status is returned: 1 if the hardware could generate a valid random number
 * and 0 otherwise.
 */
extern int __cdecl _rdseed16_step(unsigned short *);
extern int __cdecl _rdseed32_step(unsigned int *);

extern int __cdecl _rdseed64_step(unsigned __int64 *);
#line 2005 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

/*
 * The _addcarryx... functions generate ADCX and ADOX instructions which
 * use CF and OF (in the flags register) respectively to propagate carry.
 * Because this allows two add-with-carry sequences to be interleaved
 * without having to save and restore the carry flag this is useful in
 * multiprecision multiply for example. These functions return
 * the carry-out, which is convenient for chaining multiple operations.
 * The sum is written using the given reference.
 */
extern unsigned char __cdecl _addcarryx_u32(unsigned char /*c_in*/,
                                                   unsigned int /*src1*/,
                                                   unsigned int /*src2*/,
                                                   unsigned int * /*out*/);



extern unsigned char __cdecl _addcarryx_u64(unsigned char /*c_in*/,
                                                   unsigned __int64 /*src1*/,
                                                   unsigned __int64 /*src2*/,
                                                   unsigned __int64 * /*out*/);
#line 2027 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"


/*
 * Perform load a big-endian value from memory.
 */
extern unsigned short   __cdecl _load_be_u16(void const*);
extern unsigned int     __cdecl _load_be_u32(void const*);
extern unsigned __int64 __cdecl _load_be_u64(void const*);




/*
 * Perform store a value to memory as big-endian.
 */
extern void __cdecl _store_be_u16(void *, unsigned short);
extern void __cdecl _store_be_u32(void *, unsigned int);
extern void __cdecl _store_be_u64(void *, unsigned __int64);




/*
 * The Secure Hash Algorithm (SHA) New Instructions.
*/
extern __m128i __cdecl _mm_sha1msg1_epu32(__m128i, __m128i);
extern __m128i __cdecl _mm_sha1msg2_epu32(__m128i, __m128i);
extern __m128i __cdecl _mm_sha1nexte_epu32(__m128i, __m128i);
extern __m128i __cdecl _mm_sha1rnds4_epu32(__m128i, __m128i, const int);

extern __m128i __cdecl _mm_sha256msg1_epu32(__m128i, __m128i);
extern __m128i __cdecl _mm_sha256msg2_epu32(__m128i, __m128i);
extern __m128i __cdecl _mm_sha256rnds2_epu32(__m128i, __m128i, __m128i);

/*
 * Intel(R) Memory Protection Extensions (Intel(R) MPX) intrinsic functions
 */
extern void * __cdecl _bnd_set_ptr_bounds(const void *, size_t);
extern void * __cdecl _bnd_narrow_ptr_bounds(const void *, const void *, size_t);
extern void * __cdecl _bnd_copy_ptr_bounds(const void *, const void *);
extern void * __cdecl _bnd_init_ptr_bounds(const void *);
extern void __cdecl _bnd_store_ptr_bounds(const void **, const void *);
extern void __cdecl _bnd_chk_ptr_lbounds(const void *);
extern void __cdecl _bnd_chk_ptr_ubounds(const void *);
extern void __cdecl _bnd_chk_ptr_bounds(const void *, size_t);
extern void * __cdecl _bnd_load_ptr_bounds(const void **, const void *);
extern const void * __cdecl _bnd_get_ptr_lbound(const void *);
extern const void * __cdecl _bnd_get_ptr_ubound(const void *);

// Insert integer into 256-bit packed integer array at element selected by index
extern __m256i __cdecl _mm256_insert_epi8 (__m256i /* dst */, int /* src */, const int /* index */);
extern __m256i __cdecl _mm256_insert_epi16(__m256i /* dst */, int /* src */, const int /* index */);
extern __m256i __cdecl _mm256_insert_epi32(__m256i /* dst */, int /* src */, const int /* index */);

extern __m256i __cdecl _mm256_insert_epi64(__m256i /* dst */, __int64 /* src */, const int /* index */);
#line 2083 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

// Extract integer element selected by index from 256-bit packed integer array
extern int __cdecl _mm256_extract_epi8 (__m256i /* src */, const int /* index */);
extern int __cdecl _mm256_extract_epi16(__m256i /* src */, const int /* index */);
extern int __cdecl _mm256_extract_epi32(__m256i /* src */, const int /* index */);

extern __int64 __cdecl _mm256_extract_epi64(__m256i /* src */, const int /* index */);
#line 2091 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

// Zero-extended cast functions
extern __m256d __cdecl _mm256_zextpd128_pd256(__m128d);
extern __m256  __cdecl _mm256_zextps128_ps256(__m128);
extern __m256i __cdecl _mm256_zextsi128_si256(__m128i);

// RDPID
extern unsigned int __cdecl _rdpid_u32(void);
// PTWRITE
extern void         __cdecl _ptwrite32(unsigned int);

extern void         __cdecl _ptwrite64(unsigned __int64);
#line 2104 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

// AVX_VNNI intrinsics
extern __m128i __cdecl _mm_dpbusd_avx_epi32(__m128i, __m128i, __m128i);
extern __m256i __cdecl _mm256_dpbusd_avx_epi32(__m256i, __m256i, __m256i);
extern __m128i __cdecl _mm_dpbusds_avx_epi32(__m128i, __m128i, __m128i);
extern __m256i __cdecl _mm256_dpbusds_avx_epi32(__m256i, __m256i, __m256i);
extern __m128i __cdecl _mm_dpwssd_avx_epi32(__m128i, __m128i, __m128i);
extern __m256i __cdecl _mm256_dpwssd_avx_epi32(__m256i, __m256i, __m256i);
extern __m128i __cdecl _mm_dpwssds_avx_epi32(__m128i, __m128i, __m128i);
extern __m256i __cdecl _mm256_dpwssds_avx_epi32(__m256i, __m256i, __m256i);

// MKTME
extern unsigned int __cdecl _pconfig_u32(const int, size_t __data[]);
extern void __cdecl _wbnoinvd(void);

// SGX
extern unsigned int __cdecl _encls_u32(const int, size_t __data[]);
extern unsigned int __cdecl _enclu_u32(const int, size_t __data[]);
extern unsigned int __cdecl _enclv_u32(const int, size_t __data[]);

// Div & idiv

extern unsigned __int64 __cdecl _udiv128(unsigned __int64 /* highdividend */, unsigned __int64 /* lowdividend */, unsigned __int64 /* divisor */, unsigned __int64* /* remainder */);
extern __int64          __cdecl _div128(__int64 /* highdividend */, __int64 /* lowdividend */, __int64 /* divisor */, __int64* /* remainder */);
#line 2129 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"
extern unsigned         __cdecl _udiv64(unsigned __int64 /* dividend */, unsigned /* divisor */, unsigned* /* remainder */);
extern int              __cdecl _div64(__int64 /* dividend */, int /* divisor */, int* /* remainder */);

// Keylocker
extern unsigned char _mm_aesdec128kl_u8(__m128i* /* odata */, __m128i /* idata */, const void* /* h */);
extern unsigned char _mm_aesdec256kl_u8(__m128i* /* odata */, __m128i /* idata */, const void* /* h */);
extern unsigned char _mm_aesdecwide128kl_u8(__m128i* /* odata */, const __m128i* /* idata */, const void* /* h */);
extern unsigned char _mm_aesdecwide256kl_u8(__m128i* /* odata */, const __m128i* /* idata */, const void* /* h */);
extern unsigned char _mm_aesenc128kl_u8(__m128i* /* odata */, __m128i /* idata */, const void* /* h */);
extern unsigned char _mm_aesenc256kl_u8(__m128i* /* odata */, __m128i /* idata */, const void* /* h */);
extern unsigned char _mm_aesencwide128kl_u8(__m128i* /* odata */, const __m128i* /* idata */, const void* /* h */);
extern unsigned char _mm_aesencwide256kl_u8(__m128i* /* odata */, const __m128i* /* idata */, const void* /* h */);
extern unsigned int  _mm_encodekey128_u32(unsigned int /* htype */, __m128i /* key */, void* /* h */);
extern unsigned int  _mm_encodekey256_u32(unsigned int /* htype */, __m128i /* key_lo */, __m128i /* key_hi */, void* /* h */);
extern void          _mm_loadiwkey(unsigned int /* ctl */, __m128i /* intkey */, __m128i /* enkey_lo */, __m128i /* enkey_hi */);

// Protection key rights for user pages
extern unsigned int     __cdecl _rdpkru_u32(void);
extern void             __cdecl _wrpkru(unsigned int);

// Enqueue stores
extern int              __cdecl _enqcmd(void * /* dst */, const void * /* src */);
extern int              __cdecl _enqcmds(void * /* dst */, const void * /* src */);

/*
* Intel(R) Control-Flow Enforcement Technology (CET) shadow stack intrinsic functions
*/
extern void             __cdecl _incsspd (unsigned int);
extern unsigned int     __cdecl _rdsspd (void);
extern void             __cdecl _saveprevssp (void);
extern void             __cdecl _rstorssp (void *);
extern void             __cdecl _wrssd (unsigned int, void *);
extern void             __cdecl _wrussd (unsigned int, void *);
extern void             __cdecl _setssbsy (void);
extern void             __cdecl _clrssbsy (void *);
extern void *           __cdecl _switchssp(void *);

extern void             __cdecl _incsspq (unsigned __int64);
extern unsigned __int64 __cdecl _rdsspq (void);
extern void             __cdecl _wrssq (unsigned __int64, void *);
extern void             __cdecl _wrussq(unsigned __int64, void *);
#line 2171 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

/*
* Intrinsic functions for Short Vector Math Library (SVML)
*/

// vector integer divide and remainder
extern __m128i _mm_div_epi8(__m128i, __m128i);
extern __m128i _mm_div_epi16(__m128i, __m128i);
extern __m128i _mm_div_epi32(__m128i, __m128i);
extern __m128i _mm_div_epi64(__m128i, __m128i);
extern __m128i _mm_div_epu8(__m128i, __m128i);
extern __m128i _mm_div_epu16(__m128i, __m128i);
extern __m128i _mm_div_epu32(__m128i, __m128i);
extern __m128i _mm_div_epu64(__m128i, __m128i);
extern __m128i _mm_rem_epi8(__m128i, __m128i);
extern __m128i _mm_rem_epi16(__m128i, __m128i);
extern __m128i _mm_rem_epi32(__m128i, __m128i);
extern __m128i _mm_rem_epi64(__m128i, __m128i);
extern __m128i _mm_rem_epu8(__m128i, __m128i);
extern __m128i _mm_rem_epu16(__m128i, __m128i);
extern __m128i _mm_rem_epu32(__m128i, __m128i);
extern __m128i _mm_rem_epu64(__m128i, __m128i);
extern __m256i _mm256_div_epi8(__m256i, __m256i);
extern __m256i _mm256_div_epi16(__m256i, __m256i);
extern __m256i _mm256_div_epi32(__m256i, __m256i);
extern __m256i _mm256_div_epi64(__m256i, __m256i);
extern __m256i _mm256_div_epu8(__m256i, __m256i);
extern __m256i _mm256_div_epu16(__m256i, __m256i);
extern __m256i _mm256_div_epu32(__m256i, __m256i);
extern __m256i _mm256_div_epu64(__m256i, __m256i);
extern __m256i _mm256_rem_epi8(__m256i, __m256i);
extern __m256i _mm256_rem_epi16(__m256i, __m256i);
extern __m256i _mm256_rem_epi32(__m256i, __m256i);
extern __m256i _mm256_rem_epi64(__m256i, __m256i);
extern __m256i _mm256_rem_epu8(__m256i, __m256i);
extern __m256i _mm256_rem_epu16(__m256i, __m256i);
extern __m256i _mm256_rem_epu32(__m256i, __m256i);
extern __m256i _mm256_rem_epu64(__m256i, __m256i);










extern __m128i _mm_divrem_epi32(__m128i * /*rem_res*/, __m128i, __m128i);
extern __m128i _mm_divrem_epu32(__m128i * /*rem_res*/, __m128i, __m128i);
extern __m256i _mm256_divrem_epi32(__m256i * /*rem_res*/, __m256i, __m256i);
extern __m256i _mm256_divrem_epu32(__m256i * /*rem_res*/, __m256i, __m256i);






// Math functions
extern __m128  _mm_sin_ps(__m128);
extern __m128d _mm_sin_pd(__m128d);
extern __m256  _mm256_sin_ps(__m256);
extern __m256d _mm256_sin_pd(__m256d);
extern __m128  _mm_cos_ps(__m128);
extern __m128d _mm_cos_pd(__m128d);
extern __m256  _mm256_cos_ps(__m256);
extern __m256d _mm256_cos_pd(__m256d);
extern __m128  _mm_sincos_ps(__m128  * /*cos_res*/, __m128);
extern __m128d _mm_sincos_pd(__m128d * /*cos_res*/, __m128d);
extern __m256  _mm256_sincos_ps(__m256  * /*cos_res*/, __m256);
extern __m256d _mm256_sincos_pd(__m256d * /*cos_res*/, __m256d);
extern __m128  _mm_tan_ps(__m128);
extern __m128d _mm_tan_pd(__m128d);
extern __m256  _mm256_tan_ps(__m256);
extern __m256d _mm256_tan_pd(__m256d);
extern __m128  _mm_asin_ps(__m128);
extern __m128d _mm_asin_pd(__m128d);
extern __m256  _mm256_asin_ps(__m256);
extern __m256d _mm256_asin_pd(__m256d);
extern __m128  _mm_acos_ps(__m128);
extern __m128d _mm_acos_pd(__m128d);
extern __m256  _mm256_acos_ps(__m256);
extern __m256d _mm256_acos_pd(__m256d);
extern __m128  _mm_atan_ps(__m128);
extern __m128d _mm_atan_pd(__m128d);
extern __m256  _mm256_atan_ps(__m256);
extern __m256d _mm256_atan_pd(__m256d);
extern __m128  _mm_atan2_ps(__m128, __m128);
extern __m128d _mm_atan2_pd(__m128d, __m128d);
extern __m256  _mm256_atan2_ps(__m256, __m256);
extern __m256d _mm256_atan2_pd(__m256d, __m256d);
extern __m128  _mm_sind_ps(__m128);
extern __m128d _mm_sind_pd(__m128d);
extern __m256  _mm256_sind_ps(__m256);
extern __m256d _mm256_sind_pd(__m256d);
extern __m128  _mm_cosd_ps(__m128);
extern __m128d _mm_cosd_pd(__m128d);
extern __m256  _mm256_cosd_ps(__m256);
extern __m256d _mm256_cosd_pd(__m256d);
extern __m128  _mm_tand_ps(__m128);
extern __m128d _mm_tand_pd(__m128d);
extern __m256  _mm256_tand_ps(__m256);
extern __m256d _mm256_tand_pd(__m256d);
extern __m128  _mm_sinh_ps(__m128);
extern __m128d _mm_sinh_pd(__m128d);
extern __m256  _mm256_sinh_ps(__m256);
extern __m256d _mm256_sinh_pd(__m256d);
extern __m128  _mm_cosh_ps(__m128);
extern __m128d _mm_cosh_pd(__m128d);
extern __m256  _mm256_cosh_ps(__m256);
extern __m256d _mm256_cosh_pd(__m256d);
extern __m128  _mm_tanh_ps(__m128);
extern __m128d _mm_tanh_pd(__m128d);
extern __m256  _mm256_tanh_ps(__m256);
extern __m256d _mm256_tanh_pd(__m256d);
extern __m128  _mm_asinh_ps(__m128);
extern __m128d _mm_asinh_pd(__m128d);
extern __m256  _mm256_asinh_ps(__m256);
extern __m256d _mm256_asinh_pd(__m256d);
extern __m128  _mm_acosh_ps(__m128);
extern __m128d _mm_acosh_pd(__m128d);
extern __m256  _mm256_acosh_ps(__m256);
extern __m256d _mm256_acosh_pd(__m256d);
extern __m128  _mm_atanh_ps(__m128);
extern __m128d _mm_atanh_pd(__m128d);
extern __m256  _mm256_atanh_ps(__m256);
extern __m256d _mm256_atanh_pd(__m256d);
extern __m128  _mm_log_ps(__m128);
extern __m128d _mm_log_pd(__m128d);
extern __m256  _mm256_log_ps(__m256);
extern __m256d _mm256_log_pd(__m256d);
extern __m128  _mm_log1p_ps(__m128);
extern __m128d _mm_log1p_pd(__m128d);
extern __m256  _mm256_log1p_ps(__m256);
extern __m256d _mm256_log1p_pd(__m256d);
extern __m128  _mm_log10_ps(__m128);
extern __m128d _mm_log10_pd(__m128d);
extern __m256  _mm256_log10_ps(__m256);
extern __m256d _mm256_log10_pd(__m256d);
extern __m128  _mm_log2_ps(__m128);
extern __m128d _mm_log2_pd(__m128d);
extern __m256  _mm256_log2_ps(__m256);
extern __m256d _mm256_log2_pd(__m256d);
extern __m128  _mm_logb_ps(__m128);
extern __m128d _mm_logb_pd(__m128d);
extern __m256  _mm256_logb_ps(__m256);
extern __m256d _mm256_logb_pd(__m256d);
extern __m128  _mm_exp_ps(__m128);
extern __m128d _mm_exp_pd(__m128d);
extern __m256  _mm256_exp_ps(__m256);
extern __m256d _mm256_exp_pd(__m256d);
extern __m128  _mm_exp10_ps(__m128);
extern __m128d _mm_exp10_pd(__m128d);
extern __m256  _mm256_exp10_ps(__m256);
extern __m256d _mm256_exp10_pd(__m256d);
extern __m128  _mm_exp2_ps(__m128);
extern __m128d _mm_exp2_pd(__m128d);
extern __m256  _mm256_exp2_ps(__m256);
extern __m256d _mm256_exp2_pd(__m256d);
extern __m128  _mm_expm1_ps(__m128);
extern __m128d _mm_expm1_pd(__m128d);
extern __m256  _mm256_expm1_ps(__m256);
extern __m256d _mm256_expm1_pd(__m256d);
extern __m128  _mm_pow_ps(__m128, __m128);
extern __m128d _mm_pow_pd(__m128d, __m128d);
extern __m256  _mm256_pow_ps(__m256, __m256);
extern __m256d _mm256_pow_pd(__m256d, __m256d);
extern __m128  _mm_trunc_ps(__m128);
extern __m128d _mm_trunc_pd(__m128d);
extern __m256  _mm256_trunc_ps(__m256);
extern __m256d _mm256_trunc_pd(__m256d);
extern __m128  _mm_svml_floor_ps(__m128);
extern __m128d _mm_svml_floor_pd(__m128d);
extern __m256  _mm256_svml_floor_ps(__m256);
extern __m256d _mm256_svml_floor_pd(__m256d);
extern __m128  _mm_svml_ceil_ps(__m128);
extern __m128d _mm_svml_ceil_pd(__m128d);
extern __m256  _mm256_svml_ceil_ps(__m256);
extern __m256d _mm256_svml_ceil_pd(__m256d);
extern __m128  _mm_svml_round_ps(__m128);
extern __m128d _mm_svml_round_pd(__m128d);
extern __m256  _mm256_svml_round_ps(__m256);
extern __m256d _mm256_svml_round_pd(__m256d);
extern __m128  _mm_fmod_ps(__m128, __m128);
extern __m128d _mm_fmod_pd(__m128d, __m128d);
extern __m256  _mm256_fmod_ps(__m256, __m256);
extern __m256d _mm256_fmod_pd(__m256d, __m256d);
extern __m128  _mm_svml_sqrt_ps(__m128);
extern __m128d _mm_svml_sqrt_pd(__m128d);
extern __m256  _mm256_svml_sqrt_ps(__m256);
extern __m256d _mm256_svml_sqrt_pd(__m256d);
extern __m128  _mm_invsqrt_ps(__m128);
extern __m128d _mm_invsqrt_pd(__m128d);
extern __m256  _mm256_invsqrt_ps(__m256);
extern __m256d _mm256_invsqrt_pd(__m256d);
extern __m128  _mm_cbrt_ps(__m128);
extern __m128d _mm_cbrt_pd(__m128d);
extern __m256  _mm256_cbrt_ps(__m256);
extern __m256d _mm256_cbrt_pd(__m256d);
extern __m128  _mm_invcbrt_ps(__m128);
extern __m128d _mm_invcbrt_pd(__m128d);
extern __m256  _mm256_invcbrt_ps(__m256);
extern __m256d _mm256_invcbrt_pd(__m256d);
extern __m128  _mm_hypot_ps(__m128, __m128);
extern __m128d _mm_hypot_pd(__m128d, __m128d);
extern __m256  _mm256_hypot_ps(__m256, __m256);
extern __m256d _mm256_hypot_pd(__m256d, __m256d);
extern __m128  _mm_cdfnorm_ps(__m128);
extern __m128d _mm_cdfnorm_pd(__m128d);
extern __m256  _mm256_cdfnorm_ps(__m256);
extern __m256d _mm256_cdfnorm_pd(__m256d);
extern __m128  _mm_cdfnorminv_ps(__m128);
extern __m128d _mm_cdfnorminv_pd(__m128d);
extern __m256  _mm256_cdfnorminv_ps(__m256);
extern __m256d _mm256_cdfnorminv_pd(__m256d);
extern __m128  _mm_cexp_ps(__m128);
extern __m256  _mm256_cexp_ps(__m256);
extern __m128  _mm_clog_ps(__m128);
extern __m256  _mm256_clog_ps(__m256);
extern __m128  _mm_csqrt_ps(__m128);
extern __m256  _mm256_csqrt_ps(__m256);
extern __m128  _mm_erf_ps(__m128);
extern __m128d _mm_erf_pd(__m128d);
extern __m256  _mm256_erf_ps(__m256);
extern __m256d _mm256_erf_pd(__m256d);
extern __m128  _mm_erfc_ps(__m128);
extern __m128d _mm_erfc_pd(__m128d);
extern __m256  _mm256_erfc_ps(__m256);
extern __m256d _mm256_erfc_pd(__m256d);
extern __m128  _mm_erfcinv_ps(__m128);
extern __m128d _mm_erfcinv_pd(__m128d);
extern __m256  _mm256_erfcinv_ps(__m256);
extern __m256d _mm256_erfcinv_pd(__m256d);
extern __m128  _mm_erfinv_ps(__m128);
extern __m128d _mm_erfinv_pd(__m128d);
extern __m256  _mm256_erfinv_ps(__m256);
extern __m256d _mm256_erfinv_pd(__m256d);

/* Cache line demote */
extern void _mm_cldemote(void const *);


/* Direct stores */
extern void _directstoreu_u32(void *, unsigned int);

extern void _directstoreu_u64(void *, unsigned __int64);
#line 2418 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"
extern void _movdir64b(void *, void const *);

/* serialize and TSX load tracking */
extern void __cdecl _serialize(void);
extern void __cdecl _xsusldtrk(void);
extern void __cdecl _xresldtrk(void);

/* User wait */
extern void _umonitor(void *);
extern unsigned char _umwait(unsigned int, unsigned __int64);
extern unsigned char _tpause(unsigned int, unsigned __int64);

/* user interrupts */

extern void _clui(void);
extern void _stui(void);
extern unsigned char _testui(void);
extern void _senduipi(unsigned __int64);
#line 2437 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

/* Hreset */
extern void _hreset(unsigned __int32);

/* SVML conversions (no AVX-512 support needed) */
extern __m128 _mm_svml_cvtepu32_ps (__m128i);
extern __m256 _mm256_svml_cvtepu32_ps (__m256i);
extern __m128d _mm_svml_cvtepu32_pd (__m128i);
extern __m256d _mm256_svml_cvtepu32_pd (__m128i);
extern __m128d _mm_svml_cvtepi64_pd (__m128i);
extern __m256d _mm256_svml_cvtepi64_pd (__m256i);
extern __m128d _mm_svml_cvtepu64_pd (__m128i);
extern __m256d _mm256_svml_cvtepu64_pd (__m256i);


}; /* End "C" */
#line 2454 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\zmmintrin.h"
/*
 *  Copyright (C) 2007-2021 Intel Corporation.
 *
 *  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 */

/***
* zmmintrin.h - Meta Header file for Intel(R) Architecture intrinsic functions.
*
*******************************************************************************/









//
// Definitions and declarations for use with 512-bit compiler intrinsics.
//

//
// A word about intrinsic function naming conventions.  Most 512-bit
// vector instructions have names such as v<operation><type>.  For
// example "vaddps" is an addition operation (add) on packed single-
// precision (ps) values.  The corresponding intrinsic is usually
// (not always) named _mm512_<operation>_<type>. For example, the
// "_mm512_add_ps" function generates VADDPS.  The corresponding
// masked flavor adds "_mask" to the name, e.g. "_mm512_mask_add_ps".
//
// The types include:
//
//    ps    -- packed single precision
//    pd    -- packed double precision
//    epi32 -- packed 32-bit integers
//    epu32 -- packed 32-bit unsigned integers
//    epi64 -- packed 64-bit integers
//

typedef unsigned char       __mmask8;
typedef unsigned short      __mmask16;
typedef unsigned int        __mmask32;
typedef unsigned __int64    __mmask64;

typedef union __declspec(intrin_type) __declspec(align(64)) __m512 {
    float m512_f32[16];
} __m512;

typedef struct __declspec(intrin_type) __declspec(align(64)) __m512d {
    double m512d_f64[8];
} __m512d;

typedef union  __declspec(intrin_type) __declspec(align(64)) __m512i {
    __int8              m512i_i8[64];
    __int16             m512i_i16[32];
    __int32             m512i_i32[16];
    __int64             m512i_i64[8];
    unsigned __int8     m512i_u8[64];
    unsigned __int16    m512i_u16[32];
    unsigned __int32    m512i_u32[16];
    unsigned __int64    m512i_u64[8];
} __m512i;



extern "C" {
// Intrinsic functions use C name-mangling.
#line 71 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\zmmintrin.h"

/* Conversion from one type to another, no change in value. */
extern __m256  __cdecl _mm512_castps512_ps256(__m512);
extern __m512  __cdecl _mm512_castpd_ps(__m512d);
extern __m512  __cdecl _mm512_castps256_ps512(__m256);
extern __m512  __cdecl _mm512_castsi512_ps(__m512i);
extern __m512  __cdecl _mm512_castps128_ps512(__m128);

extern __m256d __cdecl _mm512_castpd512_pd256(__m512d);
extern __m512d __cdecl _mm512_castpd256_pd512(__m256d);
extern __m512d __cdecl _mm512_castps_pd(__m512);
extern __m512d __cdecl _mm512_castsi512_pd(__m512i);
extern __m512d __cdecl _mm512_castpd128_pd512(__m128d);

extern __m256i __cdecl _mm512_castsi512_si256(__m512i);
extern __m512i __cdecl _mm512_castpd_si512(__m512d);
extern __m512i __cdecl _mm512_castps_si512(__m512);
extern __m512i __cdecl _mm512_castsi256_si512(__m256i);

/* Constant for special read-only mask register 'k0'. */





/* Constants for broadcasts to vectors with 32-bit elements. */
typedef enum {
    _MM_BROADCAST32_NONE,   /* identity swizzle/convert */

    _MM_BROADCAST_1X16,     /* broadcast x 16 ( aaaa aaaa aaaa aaaa ) */
    _MM_BROADCAST_4X16      /* broadcast x 4  ( dcba dcba dcba dcba ) */
} _MM_BROADCAST32_ENUM;

/* Constants for broadcasts to vectors with 64-bit elements. */
typedef enum {
    _MM_BROADCAST64_NONE,   /* identity swizzle/convert */

    _MM_BROADCAST_1X8,      /* broadcast x 8 ( aaaa aaaa ) */
    _MM_BROADCAST_4X8       /* broadcast x 2 ( dcba dcba ) */
} _MM_BROADCAST64_ENUM;

/*
 * Constants for rounding mode.
 * These names beginning with "_MM_ROUND" are deprecated.
 * Use the names beginning with "_MM_FROUND" going forward.
 */
typedef enum {
    _MM_ROUND_MODE_NEAREST,             /* round to nearest (even) */
    _MM_ROUND_MODE_DOWN,                /* round toward negative infinity */
    _MM_ROUND_MODE_UP,                  /* round toward positive infinity */
    _MM_ROUND_MODE_TOWARD_ZERO,         /* round toward zero */
    _MM_ROUND_MODE_DEFAULT,             /* round mode from MXCSR */
    _MM_ROUND_MODE_NO_EXC = 8,          /* suppress all exceptions */
} _MM_ROUND_MODE_ENUM;

/* Constants for exponent adjustment. */
typedef enum {
    _MM_EXPADJ_NONE,               /* 2**0  (32.0 - no exp adjustment) */
    _MM_EXPADJ_4,                  /* 2**4  (28.4)  */
    _MM_EXPADJ_5,                  /* 2**5  (27.5)  */
    _MM_EXPADJ_8,                  /* 2**8  (24.8)  */
    _MM_EXPADJ_16,                 /* 2**16 (16.16) */
    _MM_EXPADJ_24,                 /* 2**24 (8.24)  */
    _MM_EXPADJ_31,                 /* 2**31 (1.31)  */
    _MM_EXPADJ_32                  /* 2**32 (0.32)  */
} _MM_EXP_ADJ_ENUM;

/* Constants for index scale (vgather/vscatter). */
typedef enum {
    _MM_SCALE_1 = 1,
    _MM_SCALE_2 = 2,
    _MM_SCALE_4 = 4,
    _MM_SCALE_8 = 8
} _MM_INDEX_SCALE_ENUM;

/* Constants for permute/shuffle. */
typedef enum {
    _MM_PERM_AAAA = 0x00, _MM_PERM_AAAB = 0x01, _MM_PERM_AAAC = 0x02,
    _MM_PERM_AAAD = 0x03, _MM_PERM_AABA = 0x04, _MM_PERM_AABB = 0x05,
    _MM_PERM_AABC = 0x06, _MM_PERM_AABD = 0x07, _MM_PERM_AACA = 0x08,
    _MM_PERM_AACB = 0x09, _MM_PERM_AACC = 0x0A, _MM_PERM_AACD = 0x0B,
    _MM_PERM_AADA = 0x0C, _MM_PERM_AADB = 0x0D, _MM_PERM_AADC = 0x0E,
    _MM_PERM_AADD = 0x0F, _MM_PERM_ABAA = 0x10, _MM_PERM_ABAB = 0x11,
    _MM_PERM_ABAC = 0x12, _MM_PERM_ABAD = 0x13, _MM_PERM_ABBA = 0x14,
    _MM_PERM_ABBB = 0x15, _MM_PERM_ABBC = 0x16, _MM_PERM_ABBD = 0x17,
    _MM_PERM_ABCA = 0x18, _MM_PERM_ABCB = 0x19, _MM_PERM_ABCC = 0x1A,
    _MM_PERM_ABCD = 0x1B, _MM_PERM_ABDA = 0x1C, _MM_PERM_ABDB = 0x1D,
    _MM_PERM_ABDC = 0x1E, _MM_PERM_ABDD = 0x1F, _MM_PERM_ACAA = 0x20,
    _MM_PERM_ACAB = 0x21, _MM_PERM_ACAC = 0x22, _MM_PERM_ACAD = 0x23,
    _MM_PERM_ACBA = 0x24, _MM_PERM_ACBB = 0x25, _MM_PERM_ACBC = 0x26,
    _MM_PERM_ACBD = 0x27, _MM_PERM_ACCA = 0x28, _MM_PERM_ACCB = 0x29,
    _MM_PERM_ACCC = 0x2A, _MM_PERM_ACCD = 0x2B, _MM_PERM_ACDA = 0x2C,
    _MM_PERM_ACDB = 0x2D, _MM_PERM_ACDC = 0x2E, _MM_PERM_ACDD = 0x2F,
    _MM_PERM_ADAA = 0x30, _MM_PERM_ADAB = 0x31, _MM_PERM_ADAC = 0x32,
    _MM_PERM_ADAD = 0x33, _MM_PERM_ADBA = 0x34, _MM_PERM_ADBB = 0x35,
    _MM_PERM_ADBC = 0x36, _MM_PERM_ADBD = 0x37, _MM_PERM_ADCA = 0x38,
    _MM_PERM_ADCB = 0x39, _MM_PERM_ADCC = 0x3A, _MM_PERM_ADCD = 0x3B,
    _MM_PERM_ADDA = 0x3C, _MM_PERM_ADDB = 0x3D, _MM_PERM_ADDC = 0x3E,
    _MM_PERM_ADDD = 0x3F, _MM_PERM_BAAA = 0x40, _MM_PERM_BAAB = 0x41,
    _MM_PERM_BAAC = 0x42, _MM_PERM_BAAD = 0x43, _MM_PERM_BABA = 0x44,
    _MM_PERM_BABB = 0x45, _MM_PERM_BABC = 0x46, _MM_PERM_BABD = 0x47,
    _MM_PERM_BACA = 0x48, _MM_PERM_BACB = 0x49, _MM_PERM_BACC = 0x4A,
    _MM_PERM_BACD = 0x4B, _MM_PERM_BADA = 0x4C, _MM_PERM_BADB = 0x4D,
    _MM_PERM_BADC = 0x4E, _MM_PERM_BADD = 0x4F, _MM_PERM_BBAA = 0x50,
    _MM_PERM_BBAB = 0x51, _MM_PERM_BBAC = 0x52, _MM_PERM_BBAD = 0x53,
    _MM_PERM_BBBA = 0x54, _MM_PERM_BBBB = 0x55, _MM_PERM_BBBC = 0x56,
    _MM_PERM_BBBD = 0x57, _MM_PERM_BBCA = 0x58, _MM_PERM_BBCB = 0x59,
    _MM_PERM_BBCC = 0x5A, _MM_PERM_BBCD = 0x5B, _MM_PERM_BBDA = 0x5C,
    _MM_PERM_BBDB = 0x5D, _MM_PERM_BBDC = 0x5E, _MM_PERM_BBDD = 0x5F,
    _MM_PERM_BCAA = 0x60, _MM_PERM_BCAB = 0x61, _MM_PERM_BCAC = 0x62,
    _MM_PERM_BCAD = 0x63, _MM_PERM_BCBA = 0x64, _MM_PERM_BCBB = 0x65,
    _MM_PERM_BCBC = 0x66, _MM_PERM_BCBD = 0x67, _MM_PERM_BCCA = 0x68,
    _MM_PERM_BCCB = 0x69, _MM_PERM_BCCC = 0x6A, _MM_PERM_BCCD = 0x6B,
    _MM_PERM_BCDA = 0x6C, _MM_PERM_BCDB = 0x6D, _MM_PERM_BCDC = 0x6E,
    _MM_PERM_BCDD = 0x6F, _MM_PERM_BDAA = 0x70, _MM_PERM_BDAB = 0x71,
    _MM_PERM_BDAC = 0x72, _MM_PERM_BDAD = 0x73, _MM_PERM_BDBA = 0x74,
    _MM_PERM_BDBB = 0x75, _MM_PERM_BDBC = 0x76, _MM_PERM_BDBD = 0x77,
    _MM_PERM_BDCA = 0x78, _MM_PERM_BDCB = 0x79, _MM_PERM_BDCC = 0x7A,
    _MM_PERM_BDCD = 0x7B, _MM_PERM_BDDA = 0x7C, _MM_PERM_BDDB = 0x7D,
    _MM_PERM_BDDC = 0x7E, _MM_PERM_BDDD = 0x7F, _MM_PERM_CAAA = 0x80,
    _MM_PERM_CAAB = 0x81, _MM_PERM_CAAC = 0x82, _MM_PERM_CAAD = 0x83,
    _MM_PERM_CABA = 0x84, _MM_PERM_CABB = 0x85, _MM_PERM_CABC = 0x86,
    _MM_PERM_CABD = 0x87, _MM_PERM_CACA = 0x88, _MM_PERM_CACB = 0x89,
    _MM_PERM_CACC = 0x8A, _MM_PERM_CACD = 0x8B, _MM_PERM_CADA = 0x8C,
    _MM_PERM_CADB = 0x8D, _MM_PERM_CADC = 0x8E, _MM_PERM_CADD = 0x8F,
    _MM_PERM_CBAA = 0x90, _MM_PERM_CBAB = 0x91, _MM_PERM_CBAC = 0x92,
    _MM_PERM_CBAD = 0x93, _MM_PERM_CBBA = 0x94, _MM_PERM_CBBB = 0x95,
    _MM_PERM_CBBC = 0x96, _MM_PERM_CBBD = 0x97, _MM_PERM_CBCA = 0x98,
    _MM_PERM_CBCB = 0x99, _MM_PERM_CBCC = 0x9A, _MM_PERM_CBCD = 0x9B,
    _MM_PERM_CBDA = 0x9C, _MM_PERM_CBDB = 0x9D, _MM_PERM_CBDC = 0x9E,
    _MM_PERM_CBDD = 0x9F, _MM_PERM_CCAA = 0xA0, _MM_PERM_CCAB = 0xA1,
    _MM_PERM_CCAC = 0xA2, _MM_PERM_CCAD = 0xA3, _MM_PERM_CCBA = 0xA4,
    _MM_PERM_CCBB = 0xA5, _MM_PERM_CCBC = 0xA6, _MM_PERM_CCBD = 0xA7,
    _MM_PERM_CCCA = 0xA8, _MM_PERM_CCCB = 0xA9, _MM_PERM_CCCC = 0xAA,
    _MM_PERM_CCCD = 0xAB, _MM_PERM_CCDA = 0xAC, _MM_PERM_CCDB = 0xAD,
    _MM_PERM_CCDC = 0xAE, _MM_PERM_CCDD = 0xAF, _MM_PERM_CDAA = 0xB0,
    _MM_PERM_CDAB = 0xB1, _MM_PERM_CDAC = 0xB2, _MM_PERM_CDAD = 0xB3,
    _MM_PERM_CDBA = 0xB4, _MM_PERM_CDBB = 0xB5, _MM_PERM_CDBC = 0xB6,
    _MM_PERM_CDBD = 0xB7, _MM_PERM_CDCA = 0xB8, _MM_PERM_CDCB = 0xB9,
    _MM_PERM_CDCC = 0xBA, _MM_PERM_CDCD = 0xBB, _MM_PERM_CDDA = 0xBC,
    _MM_PERM_CDDB = 0xBD, _MM_PERM_CDDC = 0xBE, _MM_PERM_CDDD = 0xBF,
    _MM_PERM_DAAA = 0xC0, _MM_PERM_DAAB = 0xC1, _MM_PERM_DAAC = 0xC2,
    _MM_PERM_DAAD = 0xC3, _MM_PERM_DABA = 0xC4, _MM_PERM_DABB = 0xC5,
    _MM_PERM_DABC = 0xC6, _MM_PERM_DABD = 0xC7, _MM_PERM_DACA = 0xC8,
    _MM_PERM_DACB = 0xC9, _MM_PERM_DACC = 0xCA, _MM_PERM_DACD = 0xCB,
    _MM_PERM_DADA = 0xCC, _MM_PERM_DADB = 0xCD, _MM_PERM_DADC = 0xCE,
    _MM_PERM_DADD = 0xCF, _MM_PERM_DBAA = 0xD0, _MM_PERM_DBAB = 0xD1,
    _MM_PERM_DBAC = 0xD2, _MM_PERM_DBAD = 0xD3, _MM_PERM_DBBA = 0xD4,
    _MM_PERM_DBBB = 0xD5, _MM_PERM_DBBC = 0xD6, _MM_PERM_DBBD = 0xD7,
    _MM_PERM_DBCA = 0xD8, _MM_PERM_DBCB = 0xD9, _MM_PERM_DBCC = 0xDA,
    _MM_PERM_DBCD = 0xDB, _MM_PERM_DBDA = 0xDC, _MM_PERM_DBDB = 0xDD,
    _MM_PERM_DBDC = 0xDE, _MM_PERM_DBDD = 0xDF, _MM_PERM_DCAA = 0xE0,
    _MM_PERM_DCAB = 0xE1, _MM_PERM_DCAC = 0xE2, _MM_PERM_DCAD = 0xE3,
    _MM_PERM_DCBA = 0xE4, _MM_PERM_DCBB = 0xE5, _MM_PERM_DCBC = 0xE6,
    _MM_PERM_DCBD = 0xE7, _MM_PERM_DCCA = 0xE8, _MM_PERM_DCCB = 0xE9,
    _MM_PERM_DCCC = 0xEA, _MM_PERM_DCCD = 0xEB, _MM_PERM_DCDA = 0xEC,
    _MM_PERM_DCDB = 0xED, _MM_PERM_DCDC = 0xEE, _MM_PERM_DCDD = 0xEF,
    _MM_PERM_DDAA = 0xF0, _MM_PERM_DDAB = 0xF1, _MM_PERM_DDAC = 0xF2,
    _MM_PERM_DDAD = 0xF3, _MM_PERM_DDBA = 0xF4, _MM_PERM_DDBB = 0xF5,
    _MM_PERM_DDBC = 0xF6, _MM_PERM_DDBD = 0xF7, _MM_PERM_DDCA = 0xF8,
    _MM_PERM_DDCB = 0xF9, _MM_PERM_DDCC = 0xFA, _MM_PERM_DDCD = 0xFB,
    _MM_PERM_DDDA = 0xFC, _MM_PERM_DDDB = 0xFD, _MM_PERM_DDDC = 0xFE,
    _MM_PERM_DDDD = 0xFF
} _MM_PERM_ENUM;

/*
 * Helper type and macro for computing the values of the immediate
 * used in mm512_fixup_ps.
 */
typedef enum {
    _MM_FIXUP_NO_CHANGE,
    _MM_FIXUP_NEG_INF,
    _MM_FIXUP_NEG_ZERO,
    _MM_FIXUP_POS_ZERO,
    _MM_FIXUP_POS_INF,
    _MM_FIXUP_NAN,
    _MM_FIXUP_MAX_FLOAT,
    _MM_FIXUP_MIN_FLOAT
} _MM_FIXUPRESULT_ENUM;
















/*
* Extract float32 or float64 normalized mantissas.
*/

/* Constants for mantissa extraction */
typedef enum {
    _MM_MANT_NORM_1_2,      /* interval [1, 2)      */
    _MM_MANT_NORM_p5_2,     /* interval [1.5, 2)    */
    _MM_MANT_NORM_p5_1,     /* interval [1.5, 1)    */
    _MM_MANT_NORM_p75_1p5   /* interval [0.75, 1.5) */
} _MM_MANTISSA_NORM_ENUM;

typedef enum {
    _MM_MANT_SIGN_src,      /* sign = sign(SRC)     */
    _MM_MANT_SIGN_zero,     /* sign = 0             */
    _MM_MANT_SIGN_nan       /* DEST = NaN if sign(SRC) = 1 */
} _MM_MANTISSA_SIGN_ENUM;

/*
* Compare float32, float64 or int32 vectors and set mask.
*/

/* Constants for integer comparison predicates */
typedef enum {
    _MM_CMPINT_EQ,      /* Equal */
    _MM_CMPINT_LT,      /* Less than */
    _MM_CMPINT_LE,      /* Less than or Equal */
    _MM_CMPINT_UNUSED,
    _MM_CMPINT_NE,      /* Not Equal */
    _MM_CMPINT_NLT,     /* Not Less than */

    _MM_CMPINT_NLE      /* Not Less than or Equal */

} _MM_CMPINT_ENUM;


/*
* Intel(R) AVX-512 intrinsic functions
*/
extern __m512  __cdecl _mm512_setzero_ps(void);
extern __m512d __cdecl _mm512_setzero_pd(void);

extern __m512  __cdecl _mm512_set_ps(float /* e15 */, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float /* e0 */);
extern __m512d __cdecl _mm512_set_pd(double /* e7 */, double, double, double, double, double, double, double /* e0 */);

extern __m512  __cdecl _mm512_setr_ps(float /* e0 */, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float /* e15 */);
extern __m512d __cdecl _mm512_setr_pd(double /* e0 */, double, double, double, double, double, double, double /* e7 */);

extern __m512  __cdecl _mm512_set1_ps(float);
extern __m512d __cdecl _mm512_set1_pd(double);

extern __m512  __cdecl _mm512_load_ps(void const*);
extern __m512d __cdecl _mm512_load_pd(void const*);
extern __m512  __cdecl _mm512_maskz_load_ps(__mmask16, void const*);
extern __m512d __cdecl _mm512_maskz_load_pd(__mmask8, void const*);
extern __m512  __cdecl _mm512_mask_load_ps(__m512, __mmask16, void const*);
extern __m512d __cdecl _mm512_mask_load_pd(__m512d, __mmask8, void const*);
extern __m512  __cdecl _mm512_loadu_ps(void const*);
extern __m512d __cdecl _mm512_loadu_pd(void const*);
extern __m512  __cdecl _mm512_maskz_loadu_ps(__mmask16, void const*);
extern __m512d __cdecl _mm512_maskz_loadu_pd(__mmask8, void const*);
extern __m512  __cdecl _mm512_mask_loadu_ps(__m512, __mmask16, void const*);
extern __m512d __cdecl _mm512_mask_loadu_pd(__m512d, __mmask8, void const*);

extern void    __cdecl _mm512_store_ps(void*, __m512);
extern void    __cdecl _mm512_store_pd(void*, __m512d);
extern void    __cdecl _mm512_storeu_ps(void*, __m512);
extern void    __cdecl _mm512_storeu_pd(void*, __m512d);
extern void    __cdecl _mm512_mask_store_ps(void*, __mmask16, __m512);
extern void    __cdecl _mm512_mask_store_pd(void*, __mmask8, __m512d);
extern void    __cdecl _mm512_mask_storeu_ps(void*, __mmask16, __m512);
extern void    __cdecl _mm512_mask_storeu_pd(void*, __mmask8, __m512d);

extern __m512  __cdecl _mm512_add_ps(__m512, __m512);
extern __m512  __cdecl _mm512_maskz_add_ps(__mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_mask_add_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_add_round_ps(__m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_maskz_add_round_ps(__mmask16, __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_mask_add_round_ps(__m512, __mmask16, __m512, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_add_pd(__m512d, __m512d);
extern __m512d __cdecl _mm512_maskz_add_pd(__mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_mask_add_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_add_round_pd(__m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_maskz_add_round_pd(__mmask8, __m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask_add_round_pd(__m512d, __mmask8, __m512d, __m512d, const int /* rounding */);

extern __m512  __cdecl _mm512_sub_ps(__m512, __m512);
extern __m512  __cdecl _mm512_maskz_sub_ps(__mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_mask_sub_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_sub_round_ps(__m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_maskz_sub_round_ps(__mmask16, __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_mask_sub_round_ps(__m512, __mmask16, __m512, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_sub_pd(__m512d, __m512d);
extern __m512d __cdecl _mm512_maskz_sub_pd(__mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_mask_sub_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_sub_round_pd(__m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_maskz_sub_round_pd(__mmask8, __m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask_sub_round_pd(__m512d, __mmask8, __m512d, __m512d, const int /* rounding */);

extern __m512  __cdecl _mm512_mul_ps(__m512, __m512);
extern __m512  __cdecl _mm512_maskz_mul_ps(__mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_mask_mul_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_mul_round_ps( __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_maskz_mul_round_ps(__mmask16, __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_mask_mul_round_ps(__m512, __mmask16, __m512, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_mul_pd(__m512d, __m512d);
extern __m512d __cdecl _mm512_maskz_mul_pd(__mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_mask_mul_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_mul_round_pd(__m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_maskz_mul_round_pd(__mmask8, __m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask_mul_round_pd(__m512d, __mmask8, __m512d, __m512d, const int /* rounding */);

extern __m512  __cdecl _mm512_div_ps(__m512, __m512);
extern __m512  __cdecl _mm512_maskz_div_ps(__mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_mask_div_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_div_round_ps(__m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_maskz_div_round_ps(__mmask16, __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_mask_div_round_ps(__m512, __mmask16, __m512, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_div_pd(__m512d, __m512d);
extern __m512d __cdecl _mm512_maskz_div_pd(__mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_mask_div_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_div_round_pd(__m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_maskz_div_round_pd(__mmask8, __m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask_div_round_pd(__m512d, __mmask8, __m512d, __m512d, const int /* rounding */);

extern __m512  __cdecl _mm512_fmadd_ps(__m512, __m512, __m512);
extern __m512  __cdecl _mm512_mask_fmadd_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_mask3_fmadd_ps(__m512, __m512, __m512, __mmask16);
extern __m512  __cdecl _mm512_maskz_fmadd_ps(__mmask16, __m512, __m512, __m512);
extern __m512  __cdecl _mm512_fmadd_round_ps(__m512, __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_mask_fmadd_round_ps(__m512, __mmask16, __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_mask3_fmadd_round_ps(__m512, __m512, __m512, __mmask16, const int /* rounding */);
extern __m512  __cdecl _mm512_maskz_fmadd_round_ps(__mmask16, __m512, __m512, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_fmadd_pd(__m512d, __m512d, __m512d);
extern __m512d __cdecl _mm512_mask_fmadd_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_mask3_fmadd_pd(__m512d, __m512d, __m512d, __mmask8);
extern __m512d __cdecl _mm512_maskz_fmadd_pd(__mmask8, __m512d, __m512d, __m512d);
extern __m512d __cdecl _mm512_fmadd_round_pd(__m512d, __m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask_fmadd_round_pd(__m512d, __mmask8, __m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask3_fmadd_round_pd(__m512d, __m512d, __m512d, __mmask8, const int /* rounding */);
extern __m512d __cdecl _mm512_maskz_fmadd_round_pd(__mmask8, __m512d, __m512d, __m512d, const int /* rounding */);

extern __m512  __cdecl _mm512_fmsub_ps(__m512, __m512, __m512);
extern __m512  __cdecl _mm512_mask_fmsub_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_mask3_fmsub_ps(__m512, __m512, __m512, __mmask16);
extern __m512  __cdecl _mm512_maskz_fmsub_ps(__mmask16, __m512, __m512, __m512);
extern __m512  __cdecl _mm512_fmsub_round_ps(__m512, __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_mask_fmsub_round_ps(__m512, __mmask16, __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_mask3_fmsub_round_ps(__m512, __m512, __m512, __mmask16, const int /* rounding */);
extern __m512  __cdecl _mm512_maskz_fmsub_round_ps(__mmask16, __m512, __m512, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_fmsub_pd(__m512d, __m512d, __m512d);
extern __m512d __cdecl _mm512_mask_fmsub_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_mask3_fmsub_pd(__m512d, __m512d, __m512d, __mmask8);
extern __m512d __cdecl _mm512_maskz_fmsub_pd(__mmask8, __m512d, __m512d, __m512d);
extern __m512d __cdecl _mm512_fmsub_round_pd(__m512d, __m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask_fmsub_round_pd(__m512d, __mmask8, __m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask3_fmsub_round_pd(__m512d, __m512d, __m512d, __mmask8, const int /* rounding */);
extern __m512d __cdecl _mm512_maskz_fmsub_round_pd(__mmask8, __m512d, __m512d, __m512d, const int /* rounding */);

extern __m512  __cdecl _mm512_fmaddsub_ps(__m512, __m512, __m512);
extern __m512  __cdecl _mm512_mask_fmaddsub_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_mask3_fmaddsub_ps(__m512, __m512, __m512, __mmask16);
extern __m512  __cdecl _mm512_maskz_fmaddsub_ps(__mmask16, __m512, __m512, __m512);
extern __m512  __cdecl _mm512_fmaddsub_round_ps(__m512, __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_mask_fmaddsub_round_ps(__m512, __mmask16, __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_mask3_fmaddsub_round_ps(__m512, __m512, __m512, __mmask16, const int /* rounding */);
extern __m512  __cdecl _mm512_maskz_fmaddsub_round_ps(__mmask16, __m512, __m512, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_fmaddsub_pd(__m512d, __m512d, __m512d);
extern __m512d __cdecl _mm512_mask_fmaddsub_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_mask3_fmaddsub_pd(__m512d, __m512d, __m512d, __mmask8);
extern __m512d __cdecl _mm512_maskz_fmaddsub_pd(__mmask8, __m512d, __m512d, __m512d);
extern __m512d __cdecl _mm512_fmaddsub_round_pd(__m512d, __m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask_fmaddsub_round_pd(__m512d, __mmask8, __m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask3_fmaddsub_round_pd(__m512d, __m512d, __m512d, __mmask8, const int /* rounding */);
extern __m512d __cdecl _mm512_maskz_fmaddsub_round_pd(__mmask8, __m512d, __m512d, __m512d, const int /* rounding */);

extern __m512  __cdecl _mm512_fmsubadd_ps(__m512, __m512, __m512);
extern __m512  __cdecl _mm512_mask_fmsubadd_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_mask3_fmsubadd_ps(__m512, __m512, __m512, __mmask16);
extern __m512  __cdecl _mm512_maskz_fmsubadd_ps(__mmask16, __m512, __m512, __m512);
extern __m512  __cdecl _mm512_fmsubadd_round_ps(__m512, __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_mask_fmsubadd_round_ps(__m512, __mmask16, __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_mask3_fmsubadd_round_ps(__m512, __m512, __m512, __mmask16, const int /* rounding */);
extern __m512  __cdecl _mm512_maskz_fmsubadd_round_ps(__mmask16, __m512, __m512, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_fmsubadd_pd(__m512d, __m512d, __m512d);
extern __m512d __cdecl _mm512_mask_fmsubadd_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_mask3_fmsubadd_pd(__m512d, __m512d, __m512d, __mmask8);
extern __m512d __cdecl _mm512_maskz_fmsubadd_pd(__mmask8, __m512d, __m512d, __m512d);
extern __m512d __cdecl _mm512_fmsubadd_round_pd(__m512d, __m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask_fmsubadd_round_pd(__m512d, __mmask8, __m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask3_fmsubadd_round_pd(__m512d, __m512d, __m512d, __mmask8, const int /* rounding */);
extern __m512d __cdecl _mm512_maskz_fmsubadd_round_pd(__mmask8, __m512d, __m512d, __m512d, const int /* rounding */);

extern __m512  __cdecl _mm512_fnmadd_ps(__m512, __m512, __m512);
extern __m512  __cdecl _mm512_mask_fnmadd_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_mask3_fnmadd_ps(__m512, __m512, __m512, __mmask16);
extern __m512  __cdecl _mm512_maskz_fnmadd_ps(__mmask16, __m512, __m512, __m512);
extern __m512  __cdecl _mm512_fnmadd_round_ps(__m512, __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_mask_fnmadd_round_ps(__m512, __mmask16, __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_mask3_fnmadd_round_ps(__m512, __m512, __m512, __mmask16, const int /* rounding */);
extern __m512  __cdecl _mm512_maskz_fnmadd_round_ps(__mmask16, __m512, __m512, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_fnmadd_pd(__m512d, __m512d, __m512d);
extern __m512d __cdecl _mm512_mask_fnmadd_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_mask3_fnmadd_pd(__m512d, __m512d, __m512d, __mmask8);
extern __m512d __cdecl _mm512_maskz_fnmadd_pd(__mmask8, __m512d, __m512d, __m512d);
extern __m512d __cdecl _mm512_fnmadd_round_pd(__m512d, __m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask_fnmadd_round_pd(__m512d, __mmask8, __m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask3_fnmadd_round_pd(__m512d, __m512d, __m512d, __mmask8, const int /* rounding */);
extern __m512d __cdecl _mm512_maskz_fnmadd_round_pd(__mmask8, __m512d, __m512d, __m512d, const int /* rounding */);

extern __m512  __cdecl _mm512_fnmsub_ps(__m512, __m512, __m512);
extern __m512  __cdecl _mm512_mask_fnmsub_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_mask3_fnmsub_ps(__m512, __m512, __m512, __mmask16);
extern __m512  __cdecl _mm512_maskz_fnmsub_ps(__mmask16, __m512, __m512, __m512);
extern __m512  __cdecl _mm512_fnmsub_round_ps(__m512, __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_mask_fnmsub_round_ps(__m512, __mmask16, __m512, __m512, const int /* rounding */);
extern __m512  __cdecl _mm512_mask3_fnmsub_round_ps(__m512, __m512, __m512, __mmask16, const int /* rounding */);
extern __m512  __cdecl _mm512_maskz_fnmsub_round_ps(__mmask16, __m512, __m512, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_fnmsub_pd(__m512d, __m512d, __m512d);
extern __m512d __cdecl _mm512_mask_fnmsub_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_mask3_fnmsub_pd(__m512d, __m512d, __m512d, __mmask8);
extern __m512d __cdecl _mm512_maskz_fnmsub_pd(__mmask8, __m512d, __m512d, __m512d);
extern __m512d __cdecl _mm512_fnmsub_round_pd(__m512d, __m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask_fnmsub_round_pd(__m512d, __mmask8, __m512d, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask3_fnmsub_round_pd(__m512d, __m512d, __m512d, __mmask8, const int /* rounding */);
extern __m512d __cdecl _mm512_maskz_fnmsub_round_pd(__mmask8, __m512d, __m512d, __m512d, const int /* rounding */);

extern __m512  __cdecl _mm512_sqrt_ps(__m512);
extern __m512d __cdecl _mm512_sqrt_pd(__m512d);
extern __m512  __cdecl _mm512_maskz_sqrt_ps(__mmask16, __m512);
extern __m512d __cdecl _mm512_maskz_sqrt_pd(__mmask8, __m512d);
extern __m512  __cdecl _mm512_mask_sqrt_ps(__m512, __mmask16, __m512);
extern __m512d __cdecl _mm512_mask_sqrt_pd(__m512d, __mmask8, __m512d);
extern __m512  __cdecl _mm512_sqrt_round_ps(__m512, const int /* rounding */);
extern __m512d __cdecl _mm512_sqrt_round_pd(__m512d, const int /* rounding */);
extern __m512  __cdecl _mm512_maskz_sqrt_round_ps(__mmask16, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_maskz_sqrt_round_pd(__mmask8, __m512d, const int /* rounding */);
extern __m512  __cdecl _mm512_mask_sqrt_round_ps(__m512, __mmask16, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_mask_sqrt_round_pd(__m512d, __mmask8, __m512d, const int /* rounding */);

extern __m512  __cdecl _mm512_abs_ps(__m512);
extern __m512  __cdecl _mm512_maskz_abs_ps(__mmask16, __m512);
extern __m512  __cdecl _mm512_mask_abs_ps(__m512, __mmask16, __m512);
extern __m512d __cdecl _mm512_abs_pd(__m512d);
extern __m512d __cdecl _mm512_maskz_abs_pd(__mmask8, __m512d);
extern __m512d __cdecl _mm512_mask_abs_pd(__m512d, __mmask8, __m512d);

extern __m512  __cdecl _mm512_max_ps(__m512, __m512);
extern __m512d __cdecl _mm512_max_pd(__m512d, __m512d);
extern __m512  __cdecl _mm512_maskz_max_ps(__mmask16, __m512, __m512);
extern __m512d __cdecl _mm512_maskz_max_pd(__mmask8, __m512d, __m512d);
extern __m512  __cdecl _mm512_mask_max_ps(__m512, __mmask16, __m512, __m512);
extern __m512d __cdecl _mm512_mask_max_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512  __cdecl _mm512_max_round_ps(__m512, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_max_round_pd(__m512d, __m512d, const int /* rounding */);
extern __m512  __cdecl _mm512_maskz_max_round_ps(__mmask16, __m512, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_maskz_max_round_pd(__mmask8, __m512d, __m512d, const int /* rounding */);
extern __m512  __cdecl _mm512_mask_max_round_ps(__m512, __mmask16, __m512, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_mask_max_round_pd(__m512d, __mmask8, __m512d, __m512d, const int /* rounding */);

extern __m512  __cdecl _mm512_min_ps(__m512, __m512);
extern __m512d __cdecl _mm512_min_pd(__m512d, __m512d);
extern __m512  __cdecl _mm512_maskz_min_ps(__mmask16, __m512, __m512);
extern __m512d __cdecl _mm512_maskz_min_pd(__mmask8, __m512d, __m512d);
extern __m512  __cdecl _mm512_mask_min_ps(__m512, __mmask16, __m512, __m512);
extern __m512d __cdecl _mm512_mask_min_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512  __cdecl _mm512_min_round_ps(__m512, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_min_round_pd(__m512d, __m512d, const int /* rounding */);
extern __m512  __cdecl _mm512_maskz_min_round_ps(__mmask16, __m512, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_maskz_min_round_pd(__mmask8, __m512d, __m512d, const int /* rounding */);
extern __m512  __cdecl _mm512_mask_min_round_ps(__m512, __mmask16, __m512, __m512, const int /* rounding */);
extern __m512d __cdecl _mm512_mask_min_round_pd(__m512d, __mmask8, __m512d, __m512d, const int /* rounding */);

extern __m512  __cdecl _mm512_rcp14_ps(__m512);
extern __m512d __cdecl _mm512_rcp14_pd(__m512d);
extern __m512  __cdecl _mm512_maskz_rcp14_ps(__mmask16, __m512);
extern __m512d __cdecl _mm512_maskz_rcp14_pd(__mmask8, __m512d);
extern __m512  __cdecl _mm512_mask_rcp14_ps(__m512, __mmask16, __m512);
extern __m512d __cdecl _mm512_mask_rcp14_pd(__m512d, __mmask8, __m512d);

extern __m512  __cdecl _mm512_rsqrt14_ps(__m512);
extern __m512d __cdecl _mm512_rsqrt14_pd(__m512d);
extern __m512  __cdecl _mm512_maskz_rsqrt14_ps(__mmask16, __m512);
extern __m512d __cdecl _mm512_maskz_rsqrt14_pd(__mmask8, __m512d);
extern __m512  __cdecl _mm512_mask_rsqrt14_ps(__m512, __mmask16, __m512);
extern __m512d __cdecl _mm512_mask_rsqrt14_pd(__m512d, __mmask8, __m512d);

extern __m512d __cdecl _mm512_cvtps_pd(__m256);
extern __m256  __cdecl _mm512_cvtpd_ps(__m512d);
extern __m512d __cdecl _mm512_maskz_cvtps_pd(__mmask8, __m256);
extern __m256  __cdecl _mm512_maskz_cvtpd_ps(__mmask8, __m512d);
extern __m512d __cdecl _mm512_mask_cvtps_pd(__m512d, __mmask8, __m256);
extern __m256  __cdecl _mm512_mask_cvtpd_ps(__m256, __mmask8, __m512d);
extern __m512d __cdecl _mm512_cvt_roundps_pd(__m256, const int /* rounding */);
extern __m256  __cdecl _mm512_cvt_roundpd_ps(__m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_maskz_cvt_roundps_pd(__mmask8, __m256, const int /* rounding */);
extern __m256  __cdecl _mm512_maskz_cvt_roundpd_ps(__mmask8, __m512d, const int /* rounding */);
extern __m512d __cdecl _mm512_mask_cvt_roundps_pd(__m512d, __mmask8, __m256, const int /* rounding */);
extern __m256  __cdecl _mm512_mask_cvt_roundpd_ps(__m256, __mmask8, __m512d, const int /* rounding */);

extern __mmask16 __cdecl _mm512_cmp_ps_mask(__m512, __m512, const int);
extern __mmask16 __cdecl _mm512_mask_cmp_ps_mask(__mmask16, __m512, __m512, const int);
extern __mmask16 __cdecl _mm512_cmp_round_ps_mask(__m512, __m512, const int, const int /* rounding */);
extern __mmask16 __cdecl _mm512_mask_cmp_round_ps_mask(__mmask16, __m512, __m512, const int, const int /* rounding */);
extern __mmask8  __cdecl _mm512_cmp_pd_mask(__m512d, __m512d, const int);
extern __mmask8  __cdecl _mm512_mask_cmp_pd_mask(__mmask8, __m512d, __m512d, const int);
extern __mmask8  __cdecl _mm512_cmp_round_pd_mask(__m512d, __m512d, const int, const int /* rounding */);
extern __mmask8  __cdecl _mm512_mask_cmp_round_pd_mask(__mmask8, __m512d, __m512d, const int, const int /* rounding */);

extern __m512  __cdecl _mm512_broadcast_f32x2(__m128);
extern __m512  __cdecl _mm512_mask_broadcast_f32x2(__m512, __mmask16, __m128);
extern __m512  __cdecl _mm512_maskz_broadcast_f32x2(__mmask16, __m128);
extern __m512  __cdecl _mm512_broadcast_f32x4(__m128);
extern __m512  __cdecl _mm512_mask_broadcast_f32x4(__m512, __mmask16, __m128);
extern __m512  __cdecl _mm512_maskz_broadcast_f32x4(__mmask16, __m128);
extern __m512  __cdecl _mm512_broadcast_f32x8(__m256);
extern __m512  __cdecl _mm512_mask_broadcast_f32x8(__m512, __mmask16, __m256);
extern __m512  __cdecl _mm512_maskz_broadcast_f32x8(__mmask16, __m256);
extern __m512d __cdecl _mm512_broadcast_f64x2(__m128d);
extern __m512d __cdecl _mm512_mask_broadcast_f64x2(__m512d, __mmask8, __m128d);
extern __m512d __cdecl _mm512_maskz_broadcast_f64x2(__mmask8, __m128d);
extern __m512d __cdecl _mm512_broadcast_f64x4(__m256d);
extern __m512d __cdecl _mm512_mask_broadcast_f64x4(__m512d, __mmask8, __m256d);
extern __m512d __cdecl _mm512_maskz_broadcast_f64x4(__mmask8, __m256d);
extern __m512d __cdecl _mm512_broadcastsd_pd(__m128d);
extern __m512d __cdecl _mm512_mask_broadcastsd_pd(__m512d, __mmask8, __m128d);
extern __m512d __cdecl _mm512_maskz_broadcastsd_pd(__mmask8, __m128d);
extern __m512  __cdecl _mm512_broadcastss_ps(__m128);
extern __m512  __cdecl _mm512_mask_broadcastss_ps(__m512, __mmask16, __m128);
extern __m512  __cdecl _mm512_maskz_broadcastss_ps(__mmask16, __m128);

extern __m128  __cdecl _mm512_extractf32x4_ps(__m512, int);
extern __m128  __cdecl _mm512_mask_extractf32x4_ps(__m128, __mmask8, __m512, const int);
extern __m128  __cdecl _mm512_maskz_extractf32x4_ps(__mmask8, __m512, int);
extern __m256  __cdecl _mm512_extractf32x8_ps(__m512, int);
extern __m256  __cdecl _mm512_mask_extractf32x8_ps(__m256, __mmask8, __m512, const int);
extern __m256  __cdecl _mm512_maskz_extractf32x8_ps(__mmask8, __m512, int);
extern __m128d __cdecl _mm512_extractf64x2_pd(__m512d, int);
extern __m128d __cdecl _mm512_mask_extractf64x2_pd(__m128d, __mmask8, __m512d, const int);
extern __m128d __cdecl _mm512_maskz_extractf64x2_pd(__mmask8, __m512d, int);
extern __m256d __cdecl _mm512_extractf64x4_pd(__m512d, int);
extern __m256d __cdecl _mm512_mask_extractf64x4_pd(__m256d, __mmask8, __m512d, const int);
extern __m256d __cdecl _mm512_maskz_extractf64x4_pd(__mmask8, __m512d, int);

extern __m512  __cdecl _mm512_insertf32x4(__m512, __m128, int);
extern __m512  __cdecl _mm512_mask_insertf32x4(__m512, __mmask16, __m512, __m128, const int);
extern __m512  __cdecl _mm512_maskz_insertf32x4(__mmask16, __m512, __m128, int);
extern __m512  __cdecl _mm512_insertf32x8(__m512, __m256, int);
extern __m512  __cdecl _mm512_mask_insertf32x8(__m512, __mmask16, __m512, __m256, const int);
extern __m512  __cdecl _mm512_maskz_insertf32x8(__mmask16, __m512, __m256, int);
extern __m512d __cdecl _mm512_insertf64x2(__m512d, __m128d, int);
extern __m512d __cdecl _mm512_mask_insertf64x2(__m512d, __mmask8, __m512d, __m128d, const int);
extern __m512d __cdecl _mm512_maskz_insertf64x2(__mmask8, __m512d, __m128d, int);
extern __m512d __cdecl _mm512_insertf64x4(__m512d, __m256d, int);
extern __m512d __cdecl _mm512_mask_insertf64x4(__m512d, __mmask8, __m512d, __m256d, const int);
extern __m512d __cdecl _mm512_maskz_insertf64x4(__mmask8, __m512d, __m256d, int);

extern __m512  __cdecl _mm512_shuffle_f32x4(__m512, __m512, const int);
extern __m512  __cdecl _mm512_mask_shuffle_f32x4(__m512, __mmask16, __m512, __m512, const int);
extern __m512  __cdecl _mm512_maskz_shuffle_f32x4(__mmask16, __m512, __m512, const int);
extern __m512d __cdecl _mm512_shuffle_f64x2(__m512d, __m512d, const int);
extern __m512d __cdecl _mm512_mask_shuffle_f64x2(__m512d, __mmask8, __m512d, __m512d, const int);
extern __m512d __cdecl _mm512_maskz_shuffle_f64x2(__mmask8, __m512d, __m512d, const int);
extern __m512d __cdecl _mm512_shuffle_pd(__m512d, __m512d, const int);
extern __m512d __cdecl _mm512_mask_shuffle_pd(__m512d, __mmask8, __m512d, __m512d, const int);
extern __m512d __cdecl _mm512_maskz_shuffle_pd(__mmask8, __m512d, __m512d, const int);
extern __m512  __cdecl _mm512_shuffle_ps(__m512, __m512, const int);
extern __m512  __cdecl _mm512_mask_shuffle_ps(__m512, __mmask16, __m512, __m512, const int);
extern __m512  __cdecl _mm512_maskz_shuffle_ps(__mmask16, __m512, __m512, const int);

extern __mmask16 _mm512_cmpeq_ps_mask(__m512, __m512);
extern __mmask16 _mm512_cmple_ps_mask(__m512, __m512);
extern __mmask16 _mm512_cmplt_ps_mask(__m512, __m512);
extern __mmask16 _mm512_cmpneq_ps_mask(__m512, __m512);
extern __mmask16 _mm512_cmpnle_ps_mask(__m512, __m512);
extern __mmask16 _mm512_cmpnlt_ps_mask(__m512, __m512);
extern __mmask16 _mm512_cmpord_ps_mask(__m512, __m512);
extern __mmask16 _mm512_cmpunord_ps_mask(__m512, __m512);

extern __mmask16 _mm512_mask_cmpeq_ps_mask(__mmask16, __m512, __m512);
extern __mmask16 _mm512_mask_cmple_ps_mask(__mmask16, __m512, __m512);
extern __mmask16 _mm512_mask_cmplt_ps_mask(__mmask16, __m512, __m512);
extern __mmask16 _mm512_mask_cmpneq_ps_mask(__mmask16, __m512, __m512);
extern __mmask16 _mm512_mask_cmpnle_ps_mask(__mmask16, __m512, __m512);
extern __mmask16 _mm512_mask_cmpnlt_ps_mask(__mmask16, __m512, __m512);
extern __mmask16 _mm512_mask_cmpord_ps_mask(__mmask16, __m512, __m512);
extern __mmask16 _mm512_mask_cmpunord_ps_mask(__mmask16, __m512, __m512);

extern __mmask8 _mm512_cmpeq_pd_mask(__m512d, __m512d);
extern __mmask8 _mm512_cmple_pd_mask(__m512d, __m512d);
extern __mmask8 _mm512_cmplt_pd_mask(__m512d, __m512d);
extern __mmask8 _mm512_cmpneq_pd_mask(__m512d, __m512d);
extern __mmask8 _mm512_cmpnle_pd_mask(__m512d, __m512d);
extern __mmask8 _mm512_cmpnlt_pd_mask(__m512d, __m512d);
extern __mmask8 _mm512_cmpord_pd_mask(__m512d, __m512d);
extern __mmask8 _mm512_cmpunord_pd_mask(__m512d, __m512d);

extern __mmask8 _mm512_mask_cmpeq_pd_mask(__mmask8, __m512d, __m512d);
extern __mmask8 _mm512_mask_cmple_pd_mask(__mmask8, __m512d, __m512d);
extern __mmask8 _mm512_mask_cmplt_pd_mask(__mmask8, __m512d, __m512d);
extern __mmask8 _mm512_mask_cmpneq_pd_mask(__mmask8, __m512d, __m512d);
extern __mmask8 _mm512_mask_cmpnle_pd_mask(__mmask8, __m512d, __m512d);
extern __mmask8 _mm512_mask_cmpnlt_pd_mask(__mmask8, __m512d, __m512d);
extern __mmask8 _mm512_mask_cmpord_pd_mask(__mmask8, __m512d, __m512d);
extern __mmask8 _mm512_mask_cmpunord_pd_mask(__mmask8, __m512d, __m512d);





































extern __m512i __cdecl _mm512_setzero_si512(void);

extern __m512i __cdecl _mm512_set_epi8(char /* e63 */, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char /* e0 */);
extern __m512i __cdecl _mm512_set_epi16(short /* e31 */, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short /* e0 */);
extern __m512i __cdecl _mm512_set_epi32(int /* e15 */, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int /* e0 */);
extern __m512i __cdecl _mm512_set_epi64(__int64 /* e7 */, __int64, __int64, __int64, __int64, __int64, __int64, __int64 /* e0 */);

extern __m512i __cdecl _mm512_setr_epi8(char /* e0 */, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char /* e63 */);
extern __m512i __cdecl _mm512_setr_epi16(short /* e0 */, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short, short /* e31 */);
extern __m512i __cdecl _mm512_setr_epi32(int /* e0 */, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int /* e15 */);
extern __m512i __cdecl _mm512_setr_epi64(__int64 /* e0 */, __int64, __int64, __int64, __int64, __int64, __int64, __int64 /* e7 */);

extern __m512i __cdecl _mm512_set1_epi8(char);
extern __m512i __cdecl _mm512_mask_set1_epi8(__m512i, __mmask64, char);
extern __m512i __cdecl _mm512_maskz_set1_epi8(__mmask64, char);
extern __m512i __cdecl _mm512_set1_epi16(short);
extern __m512i __cdecl _mm512_mask_set1_epi16(__m512i, __mmask32, short);
extern __m512i __cdecl _mm512_maskz_set1_epi16(__mmask32, short);
extern __m512i __cdecl _mm512_set1_epi32(int);
extern __m512i __cdecl _mm512_mask_set1_epi32(__m512i, __mmask16, int);
extern __m512i __cdecl _mm512_maskz_set1_epi32(__mmask16, int);
extern __m512i __cdecl _mm512_set1_epi64(__int64);
extern __m512i __cdecl _mm512_mask_set1_epi64(__m512i, __mmask8, __int64);
extern __m512i __cdecl _mm512_maskz_set1_epi64(__mmask8, __int64);

extern __m512i __cdecl _mm512_add_epi8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_add_epi8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_add_epi8(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_add_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_add_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_add_epi16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_add_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_add_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_add_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_add_epi64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_add_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_add_epi64(__mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_adds_epi8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_adds_epi8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_adds_epi8(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_adds_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_adds_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_adds_epi16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_adds_epu8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_adds_epu8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_adds_epu8(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_adds_epu16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_adds_epu16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_adds_epu16(__mmask32, __m512i, __m512i);

extern __m512i __cdecl _mm512_abs_epi8(__m512i);
extern __m512i __cdecl _mm512_mask_abs_epi8(__m512i, __mmask64, __m512i);
extern __m512i __cdecl _mm512_maskz_abs_epi8(__mmask64, __m512i);
extern __m512i __cdecl _mm512_abs_epi16(__m512i);
extern __m512i __cdecl _mm512_mask_abs_epi16(__m512i, __mmask32, __m512i);
extern __m512i __cdecl _mm512_maskz_abs_epi16(__mmask32, __m512i);
extern __m512i __cdecl _mm512_abs_epi32(__m512i);
extern __m512i __cdecl _mm512_mask_abs_epi32(__m512i, __mmask16, __m512i);
extern __m512i __cdecl _mm512_maskz_abs_epi32(__mmask16, __m512i);
extern __m512i __cdecl _mm512_abs_epi64(__m512i);
extern __m512i __cdecl _mm512_mask_abs_epi64(__m512i, __mmask8, __m512i);
extern __m512i __cdecl _mm512_maskz_abs_epi64(__mmask8, __m512i);

extern __m512i  __cdecl _mm512_broadcast_i32x2(__m128i);
extern __m512i  __cdecl _mm512_mask_broadcast_i32x2(__m512i, __mmask16, __m128i);
extern __m512i  __cdecl _mm512_maskz_broadcast_i32x2(__mmask16, __m128i);
extern __m512i  __cdecl _mm512_broadcast_i32x4(__m128i);
extern __m512i  __cdecl _mm512_mask_broadcast_i32x4(__m512i, __mmask16, __m128i);
extern __m512i  __cdecl _mm512_maskz_broadcast_i32x4(__mmask16, __m128i);
extern __m512i  __cdecl _mm512_broadcast_i32x8(__m256i);
extern __m512i  __cdecl _mm512_mask_broadcast_i32x8(__m512i, __mmask16, __m256i);
extern __m512i  __cdecl _mm512_maskz_broadcast_i32x8(__mmask16, __m256i);
extern __m512i  __cdecl _mm512_broadcast_i64x2(__m128i);
extern __m512i  __cdecl _mm512_mask_broadcast_i64x2(__m512i, __mmask8, __m128i);
extern __m512i  __cdecl _mm512_maskz_broadcast_i64x2(__mmask8, __m128i);
extern __m512i  __cdecl _mm512_broadcast_i64x4(__m256i);
extern __m512i  __cdecl _mm512_mask_broadcast_i64x4(__m512i, __mmask8, __m256i);
extern __m512i  __cdecl _mm512_maskz_broadcast_i64x4(__mmask8, __m256i);
extern __m512i __cdecl _mm512_broadcastb_epi8(__m128i);
extern __m512i __cdecl _mm512_mask_broadcastb_epi8(__m512i, __mmask64, __m128i);
extern __m512i __cdecl _mm512_maskz_broadcastb_epi8(__mmask64, __m128i);
extern __m512i __cdecl _mm512_broadcastw_epi16(__m128i);
extern __m512i __cdecl _mm512_mask_broadcastw_epi16(__m512i, __mmask32, __m128i);
extern __m512i __cdecl _mm512_maskz_broadcastw_epi16(__mmask32, __m128i);
extern __m512i __cdecl _mm512_broadcastd_epi32(__m128i);
extern __m512i __cdecl _mm512_mask_broadcastd_epi32(__m512i, __mmask16, __m128i);
extern __m512i __cdecl _mm512_maskz_broadcastd_epi32(__mmask16, __m128i);
extern __m512i __cdecl _mm512_broadcastq_epi64(__m128i);
extern __m512i __cdecl _mm512_mask_broadcastq_epi64(__m512i, __mmask8, __m128i);
extern __m512i __cdecl _mm512_maskz_broadcastq_epi64(__mmask8, __m128i);
extern __m512i __cdecl _mm512_broadcastmw_epi32(__mmask16);
extern __m512i __cdecl _mm512_broadcastmb_epi64(__mmask8);

extern __m512i __cdecl _mm512_sub_epi8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_sub_epi8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_sub_epi8(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_sub_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_sub_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_sub_epi16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_sub_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_sub_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_sub_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_sub_epi64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_sub_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_sub_epi64(__mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_subs_epi8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_subs_epi8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_subs_epi8(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_subs_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_subs_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_subs_epi16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_subs_epu8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_subs_epu8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_subs_epu8(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_subs_epu16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_subs_epu16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_subs_epu16(__mmask32, __m512i, __m512i);

extern __m512i __cdecl _mm512_max_epi8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_max_epi8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_max_epi8(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_max_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_max_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_max_epi16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_max_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_max_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_max_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_max_epi64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_max_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_max_epi64(__mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_max_epu8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_max_epu8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_max_epu8(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_max_epu16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_max_epu16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_max_epu16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_max_epu32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_max_epu32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_max_epu32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_max_epu64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_max_epu64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_max_epu64(__mmask8, __m512i, __m512i);

extern __m512i __cdecl _mm512_min_epi8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_min_epi8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_min_epi8(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_min_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_min_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_min_epi16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_min_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_min_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_min_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_min_epi64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_min_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_min_epi64(__mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_min_epu8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_min_epu8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_min_epu8(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_min_epu16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_min_epu16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_min_epu16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_min_epu32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_min_epu32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_min_epu32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_min_epu64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_min_epu64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_min_epu64(__mmask8, __m512i, __m512i);

extern __m512i __cdecl _mm512_mul_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_mul_epi32(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_mul_epi32(__mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_mul_epu32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_mul_epu32(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_mul_epu32(__mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_mulhi_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_mulhi_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_mulhi_epi16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_mulhi_epu16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_mulhi_epu16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_mulhi_epu16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_mullo_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_mullo_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_mullo_epi16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_mullo_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_mullo_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_mullo_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_mullo_epi64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_mullo_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_mullo_epi64(__mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_mullox_epi64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_mullox_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_mulhrs_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_mulhrs_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_mulhrs_epi16(__mmask32, __m512i, __m512i);

extern __m512i __cdecl _mm512_load_epi32(void const*);
extern __m512i __cdecl _mm512_mask_load_epi32(__m512i, __mmask16, void const*);
extern __m512i __cdecl _mm512_maskz_load_epi32(__mmask16, void const*);
extern __m512i __cdecl _mm512_load_epi64(void const*);
extern __m512i __cdecl _mm512_mask_load_epi64(__m512i, __mmask8, void const*);
extern __m512i __cdecl _mm512_maskz_load_epi64(__mmask8, void const*);
extern __m512i __cdecl _mm512_loadu_epi8(void const*);
extern __m512i __cdecl _mm512_mask_loadu_epi8(__m512i, __mmask64, void const*);
extern __m512i __cdecl _mm512_maskz_loadu_epi8(__mmask64, void const*);
extern __m512i __cdecl _mm512_loadu_epi16(void const*);
extern __m512i __cdecl _mm512_mask_loadu_epi16(__m512i, __mmask32, void const*);
extern __m512i __cdecl _mm512_maskz_loadu_epi16(__mmask32, void const*);
extern __m512i __cdecl _mm512_loadu_epi32(void const*);
extern __m512i __cdecl _mm512_mask_loadu_epi32(__m512i, __mmask16, void const*);
extern __m512i __cdecl _mm512_maskz_loadu_epi32(__mmask16, void const*);
extern __m512i __cdecl _mm512_loadu_epi64(void const*);
extern __m512i __cdecl _mm512_mask_loadu_epi64(__m512i, __mmask8, void const*);
extern __m512i __cdecl _mm512_maskz_loadu_epi64(__mmask8, void const*);

extern void    __cdecl _mm512_store_epi32(void*, __m512i);
extern void    __cdecl _mm512_mask_store_epi32(void*, __mmask16, __m512i);
extern void    __cdecl _mm512_store_epi64(void*, __m512i);
extern void    __cdecl _mm512_mask_store_epi64(void*, __mmask8, __m512i);
extern void    __cdecl _mm512_storeu_epi8(void*, __m512i);
extern void    __cdecl _mm512_mask_storeu_epi8(void*, __mmask64, __m512i);
extern void    __cdecl _mm512_storeu_epi16(void*, __m512i);
extern void    __cdecl _mm512_mask_storeu_epi16(void*, __mmask32, __m512i);
extern void    __cdecl _mm512_storeu_epi32(void*, __m512i);
extern void    __cdecl _mm512_mask_storeu_epi32(void*, __mmask16, __m512i);
extern void    __cdecl _mm512_storeu_epi64(void*, __m512i);
extern void    __cdecl _mm512_mask_storeu_epi64(void*, __mmask8, __m512i);

extern __m128i __cdecl _mm512_extracti32x4_epi32(__m512i, int);
extern __m128i __cdecl _mm512_mask_extracti32x4_epi32(__m128i, __mmask8, __m512i, int);
extern __m128i __cdecl _mm512_maskz_extracti32x4_epi32(__mmask8, __m512i, int);
extern __m256i __cdecl _mm512_extracti32x8_epi32(__m512i, int);
extern __m256i __cdecl _mm512_mask_extracti32x8_epi32(__m256i, __mmask8, __m512i, int);
extern __m256i __cdecl _mm512_maskz_extracti32x8_epi32(__mmask8, __m512i, int);
extern __m128i __cdecl _mm512_extracti64x2_epi64(__m512i, int);
extern __m128i __cdecl _mm512_mask_extracti64x2_epi64(__m128i, __mmask8, __m512i, int);
extern __m128i __cdecl _mm512_maskz_extracti64x2_epi64(__mmask8, __m512i, int);
extern __m256i __cdecl _mm512_extracti64x4_epi64(__m512i, int);
extern __m256i __cdecl _mm512_mask_extracti64x4_epi64(__m256i, __mmask8, __m512i, int);
extern __m256i __cdecl _mm512_maskz_extracti64x4_epi64(__mmask8, __m512i, int);

extern __m512i __cdecl _mm512_inserti32x4(__m512i, __m128i, int);
extern __m512i __cdecl _mm512_mask_inserti32x4(__m512i, __mmask16, __m512i, __m128i, int);
extern __m512i __cdecl _mm512_maskz_inserti32x4(__mmask16, __m512i, __m128i, int);
extern __m512i __cdecl _mm512_inserti32x8(__m512i, __m256i, int);
extern __m512i __cdecl _mm512_mask_inserti32x8(__m512i, __mmask16, __m512i, __m256i, int);
extern __m512i __cdecl _mm512_maskz_inserti32x8(__mmask16, __m512i, __m256i, int);
extern __m512i __cdecl _mm512_inserti64x2(__m512i, __m128i, int);
extern __m512i __cdecl _mm512_mask_inserti64x2(__m512i, __mmask8, __m512i, __m128i, int);
extern __m512i __cdecl _mm512_maskz_inserti64x2(__mmask8, __m512i, __m128i, int);
extern __m512i __cdecl _mm512_inserti64x4(__m512i, __m256i, int);
extern __m512i __cdecl _mm512_mask_inserti64x4(__m512i, __mmask8, __m512i, __m256i, int);
extern __m512i __cdecl _mm512_maskz_inserti64x4(__mmask8, __m512i, __m256i, int);

extern __m512i __cdecl _mm512_shuffle_epi8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_shuffle_epi8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_shuffle_epi8(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_shuffle_epi32(__m512i, int);
extern __m512i __cdecl _mm512_mask_shuffle_epi32(__m512i, __mmask16, __m512i, int);
extern __m512i __cdecl _mm512_maskz_shuffle_epi32(__mmask16, __m512i, int);
extern __m512i __cdecl _mm512_shuffle_i32x4(__m512i, __m512i, const int);
extern __m512i __cdecl _mm512_mask_shuffle_i32x4(__m512i, __mmask16, __m512i, __m512i, const int);
extern __m512i __cdecl _mm512_maskz_shuffle_i32x4(__mmask16, __m512i, __m512i, const int);
extern __m512i __cdecl _mm512_shuffle_i64x2(__m512i, __m512i, const int);
extern __m512i __cdecl _mm512_mask_shuffle_i64x2(__m512i, __mmask8, __m512i, __m512i, const int);
extern __m512i __cdecl _mm512_maskz_shuffle_i64x2(__mmask8, __m512i, __m512i, const int);
extern __m512i __cdecl _mm512_shufflehi_epi16(__m512i, int);
extern __m512i __cdecl _mm512_mask_shufflehi_epi16(__m512i, __mmask32, __m512i, int);
extern __m512i __cdecl _mm512_maskz_shufflehi_epi16(__mmask32, __m512i, int);
extern __m512i __cdecl _mm512_shufflelo_epi16(__m512i, int);
extern __m512i __cdecl _mm512_mask_shufflelo_epi16(__m512i, __mmask32, __m512i, int);
extern __m512i __cdecl _mm512_maskz_shufflelo_epi16(__mmask32, __m512i, int);

extern __m512  __cdecl _mm512_mask_mov_ps(__m512, __mmask16, __m512);
extern __m512  __cdecl _mm512_maskz_mov_ps(__mmask16, __m512);
extern __m512d __cdecl _mm512_mask_mov_pd(__m512d, __mmask8, __m512d);
extern __m512d __cdecl _mm512_maskz_mov_pd(__mmask8, __m512d);
extern __m512i __cdecl _mm512_mask_mov_epi8(__m512i, __mmask64, __m512i);
extern __m512i __cdecl _mm512_maskz_mov_epi8(__mmask64, __m512i);
extern __m512i __cdecl _mm512_mask_mov_epi16(__m512i, __mmask32, __m512i);
extern __m512i __cdecl _mm512_maskz_mov_epi16(__mmask32, __m512i);
extern __m512i __cdecl _mm512_mask_mov_epi32(__m512i, __mmask16, __m512i);
extern __m512i __cdecl _mm512_maskz_mov_epi32(__mmask16, __m512i);
extern __m512i __cdecl _mm512_mask_mov_epi64(__m512i, __mmask8, __m512i);
extern __m512i __cdecl _mm512_maskz_mov_epi64(__mmask8, __m512i);
extern __m512d __cdecl _mm512_movedup_pd(__m512d);
extern __m512d __cdecl _mm512_mask_movedup_pd(__m512d, __mmask8, __m512d);
extern __m512d __cdecl _mm512_maskz_movedup_pd(__mmask8, __m512d);
extern __m512  __cdecl _mm512_movehdup_ps(__m512);
extern __m512  __cdecl _mm512_mask_movehdup_ps(__m512, __mmask16, __m512);
extern __m512  __cdecl _mm512_maskz_movehdup_ps(__mmask16, __m512);
extern __m512  __cdecl _mm512_moveldup_ps(__m512);
extern __m512  __cdecl _mm512_mask_moveldup_ps(__m512, __mmask16, __m512);
extern __m512  __cdecl _mm512_maskz_moveldup_ps(__mmask16, __m512);

extern __m512i __cdecl _mm512_movm_epi8(__mmask64);
extern __m512i __cdecl _mm512_movm_epi16(__mmask32);
extern __m512i __cdecl _mm512_movm_epi32(__mmask16);
extern __m512i __cdecl _mm512_movm_epi64(__mmask8);
extern __mmask64 __cdecl _mm512_movepi8_mask(__m512i);
extern __mmask32 __cdecl _mm512_movepi16_mask(__m512i);
extern __mmask16 __cdecl _mm512_movepi32_mask(__m512i);
extern __mmask8  __cdecl _mm512_movepi64_mask(__m512i);

extern __m512i __cdecl _mm512_alignr_epi8(__m512i, __m512i, const int);
extern __m512i __cdecl _mm512_mask_alignr_epi8(__m512i, __mmask64, __m512i, __m512i, const int);
extern __m512i __cdecl _mm512_maskz_alignr_epi8(__mmask64, __m512i, __m512i, const int);
extern __m512i __cdecl _mm512_alignr_epi32(__m512i, __m512i, const int);
extern __m512i __cdecl _mm512_mask_alignr_epi32(__m512i, __mmask16, __m512i, __m512i, const int /* count */);
extern __m512i __cdecl _mm512_maskz_alignr_epi32(__mmask16, __m512i, __m512i, const int);
extern __m512i __cdecl _mm512_alignr_epi64(__m512i, __m512i, const int);
extern __m512i __cdecl _mm512_mask_alignr_epi64(__m512i, __mmask8, __m512i, __m512i, const int);
extern __m512i __cdecl _mm512_maskz_alignr_epi64(__mmask8, __m512i, __m512i, const int);

extern __m512d __cdecl _mm512_and_pd(__m512d, __m512d);
extern __m512d __cdecl _mm512_mask_and_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_maskz_and_pd(__mmask8, __m512d, __m512d);
extern __m512  __cdecl _mm512_and_ps(__m512, __m512);
extern __m512  __cdecl _mm512_mask_and_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_maskz_and_ps(__mmask16, __m512, __m512);
extern __m512i __cdecl _mm512_and_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_and_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_and_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_and_epi64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_and_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_and_epi64(__mmask8, __m512i, __m512i);

extern __m512d __cdecl _mm512_andnot_pd(__m512d, __m512d);
extern __m512d __cdecl _mm512_mask_andnot_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_maskz_andnot_pd(__mmask8, __m512d, __m512d);
extern __m512  __cdecl _mm512_andnot_ps(__m512, __m512);
extern __m512  __cdecl _mm512_mask_andnot_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_maskz_andnot_ps(__mmask16, __m512, __m512);
extern __m512i __cdecl _mm512_andnot_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_andnot_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_andnot_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_andnot_epi64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_andnot_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_andnot_epi64(__mmask8, __m512i, __m512i);

extern __m512d __cdecl _mm512_or_pd(__m512d, __m512d);
extern __m512d __cdecl _mm512_mask_or_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_maskz_or_pd(__mmask8, __m512d, __m512d);
extern __m512  __cdecl _mm512_or_ps(__m512, __m512);
extern __m512  __cdecl _mm512_mask_or_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_maskz_or_ps(__mmask16, __m512, __m512);
extern __m512i __cdecl _mm512_or_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_or_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_or_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_or_epi64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_or_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_or_epi64(__mmask8, __m512i, __m512i);

extern __m512d __cdecl _mm512_xor_pd(__m512d, __m512d);
extern __m512d __cdecl _mm512_mask_xor_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_maskz_xor_pd(__mmask8, __m512d, __m512d);
extern __m512  __cdecl _mm512_xor_ps(__m512, __m512);
extern __m512  __cdecl _mm512_mask_xor_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_maskz_xor_ps(__mmask16, __m512, __m512);
extern __m512i __cdecl _mm512_xor_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_xor_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_xor_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_xor_epi64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_xor_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_xor_epi64(__mmask8, __m512i, __m512i);

extern __m512  __cdecl _mm512_mask_blend_ps(__mmask16, __m512, __m512);
extern __m512d __cdecl _mm512_mask_blend_pd(__mmask8, __m512d, __m512d);
extern __m512i __cdecl _mm512_mask_blend_epi8(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_blend_epi16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_blend_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_blend_epi64(__mmask8, __m512i, __m512i);

extern __m512i __cdecl _mm512_sll_epi16(__m512i, __m128i);
extern __m512i __cdecl _mm512_sll_epi32(__m512i, __m128i);
extern __m512i __cdecl _mm512_sll_epi64(__m512i, __m128i);
extern __m512i __cdecl _mm512_slli_epi16(__m512i, unsigned int);
extern __m512i __cdecl _mm512_slli_epi32(__m512i, unsigned int);
extern __m512i __cdecl _mm512_slli_epi64(__m512i, unsigned int);
extern __m512i __cdecl _mm512_sllv_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_sllv_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_sllv_epi64(__m512i, __m512i);

extern __m512i __cdecl _mm512_mask_sll_epi16(__m512i, __mmask32, __m512i, __m128i);
extern __m512i __cdecl _mm512_maskz_sll_epi16(__mmask32, __m512i, __m128i);
extern __m512i __cdecl _mm512_mask_sll_epi32(__m512i, __mmask16, __m512i, __m128i);
extern __m512i __cdecl _mm512_maskz_sll_epi32(__mmask16, __m512i, __m128i);
extern __m512i __cdecl _mm512_mask_sll_epi64(__m512i, __mmask8, __m512i, __m128i);
extern __m512i __cdecl _mm512_maskz_sll_epi64(__mmask8, __m512i, __m128i);
extern __m512i __cdecl _mm512_mask_slli_epi16(__m512i, __mmask32, __m512i, unsigned int);
extern __m512i __cdecl _mm512_maskz_slli_epi16(__mmask32, __m512i, unsigned int);
extern __m512i __cdecl _mm512_mask_slli_epi32(__m512i, __mmask16, __m512i, unsigned int);
extern __m512i __cdecl _mm512_maskz_slli_epi32(__mmask16, __m512i, unsigned int);
extern __m512i __cdecl _mm512_mask_slli_epi64(__m512i, __mmask8, __m512i, unsigned int);
extern __m512i __cdecl _mm512_maskz_slli_epi64(__mmask8, __m512i, unsigned int);
extern __m512i __cdecl _mm512_mask_sllv_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_sllv_epi16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_sllv_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_sllv_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_sllv_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_sllv_epi64(__mmask8, __m512i, __m512i);

extern __m512i __cdecl _mm512_srl_epi16(__m512i, __m128i);
extern __m512i __cdecl _mm512_srl_epi32(__m512i, __m128i);
extern __m512i __cdecl _mm512_srl_epi64(__m512i, __m128i);
extern __m512i __cdecl _mm512_srli_epi16(__m512i, int);
extern __m512i __cdecl _mm512_srli_epi32(__m512i, unsigned int);
extern __m512i __cdecl _mm512_srli_epi64(__m512i, unsigned int);
extern __m512i __cdecl _mm512_srlv_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_srlv_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_srlv_epi64(__m512i, __m512i);

extern __m512i __cdecl _mm512_mask_srl_epi16(__m512i, __mmask32, __m512i, __m128i);
extern __m512i __cdecl _mm512_maskz_srl_epi16(__mmask32, __m512i, __m128i);
extern __m512i __cdecl _mm512_mask_srl_epi32(__m512i, __mmask16, __m512i, __m128i);
extern __m512i __cdecl _mm512_maskz_srl_epi32(__mmask16, __m512i, __m128i);
extern __m512i __cdecl _mm512_mask_srl_epi64(__m512i, __mmask8, __m512i, __m128i);
extern __m512i __cdecl _mm512_maskz_srl_epi64(__mmask8, __m512i, __m128i);
extern __m512i __cdecl _mm512_mask_srli_epi16(__m512i, __mmask32, __m512i, unsigned int);
extern __m512i __cdecl _mm512_maskz_srli_epi16(__mmask32, __m512i, int);
extern __m512i __cdecl _mm512_mask_srli_epi32(__m512i, __mmask16, __m512i, unsigned int);
extern __m512i __cdecl _mm512_maskz_srli_epi32(__mmask16, __m512i, unsigned int);
extern __m512i __cdecl _mm512_mask_srli_epi64(__m512i, __mmask8, __m512i, unsigned int);
extern __m512i __cdecl _mm512_maskz_srli_epi64(__mmask8, __m512i, unsigned int);
extern __m512i __cdecl _mm512_mask_srlv_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_srlv_epi16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_srlv_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_srlv_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_srlv_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_srlv_epi64(__mmask8, __m512i, __m512i);

extern __m512i __cdecl _mm512_sra_epi16(__m512i, __m128i);
extern __m512i __cdecl _mm512_sra_epi32(__m512i, __m128i);
extern __m512i __cdecl _mm512_sra_epi64(__m512i, __m128i);
extern __m512i __cdecl _mm512_srai_epi16(__m512i, unsigned int);
extern __m512i __cdecl _mm512_srai_epi32(__m512i, unsigned int);
extern __m512i __cdecl _mm512_srai_epi64(__m512i, unsigned int);
extern __m512i __cdecl _mm512_srav_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_srav_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_srav_epi64(__m512i, __m512i);

extern __m512i __cdecl _mm512_mask_sra_epi16(__m512i, __mmask32, __m512i, __m128i);
extern __m512i __cdecl _mm512_maskz_sra_epi16(__mmask32, __m512i, __m128i);
extern __m512i __cdecl _mm512_mask_sra_epi32(__m512i, __mmask16, __m512i, __m128i);
extern __m512i __cdecl _mm512_maskz_sra_epi32(__mmask16, __m512i, __m128i);
extern __m512i __cdecl _mm512_mask_sra_epi64(__m512i, __mmask8, __m512i, __m128i);
extern __m512i __cdecl _mm512_maskz_sra_epi64(__mmask8, __m512i, __m128i);
extern __m512i __cdecl _mm512_mask_srai_epi16(__m512i, __mmask32, __m512i, unsigned int);
extern __m512i __cdecl _mm512_maskz_srai_epi16(__mmask32, __m512i, unsigned int);
extern __m512i __cdecl _mm512_mask_srai_epi32(__m512i, __mmask16, __m512i, unsigned int);
extern __m512i __cdecl _mm512_maskz_srai_epi32(__mmask16, __m512i, unsigned int);
extern __m512i __cdecl _mm512_mask_srai_epi64(__m512i, __mmask8, __m512i, unsigned int);
extern __m512i __cdecl _mm512_maskz_srai_epi64(__mmask8, __m512i, unsigned int);
extern __m512i __cdecl _mm512_mask_srav_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_srav_epi16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_srav_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_srav_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_srav_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_srav_epi64(__mmask8, __m512i, __m512i);

extern __m512i __cdecl _mm512_bslli_epi128(__m512i, int);
extern __m512i __cdecl _mm512_bsrli_epi128(__m512i, int);

extern __m512i __cdecl _mm512_rol_epi32(__m512i, const int);
extern __m512i __cdecl _mm512_mask_rol_epi32(__m512i, __mmask16, __m512i, const int);
extern __m512i __cdecl _mm512_maskz_rol_epi32(__mmask16, __m512i, const int);
extern __m512i __cdecl _mm512_rol_epi64(__m512i, const int);
extern __m512i __cdecl _mm512_mask_rol_epi64(__m512i, __mmask8, __m512i, const int);
extern __m512i __cdecl _mm512_maskz_rol_epi64(__mmask8, __m512i, const int);
extern __m512i __cdecl _mm512_rolv_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_rolv_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_rolv_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_rolv_epi64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_rolv_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_rolv_epi64(__mmask8, __m512i, __m512i);

extern __m512i __cdecl _mm512_ror_epi32(__m512i, int);
extern __m512i __cdecl _mm512_mask_ror_epi32(__m512i, __mmask16, __m512i, int);
extern __m512i __cdecl _mm512_maskz_ror_epi32(__mmask16, __m512i, int);
extern __m512i __cdecl _mm512_ror_epi64(__m512i, int);
extern __m512i __cdecl _mm512_mask_ror_epi64(__m512i, __mmask8, __m512i, int);
extern __m512i __cdecl _mm512_maskz_ror_epi64(__mmask8, __m512i, int);
extern __m512i __cdecl _mm512_rorv_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_rorv_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_rorv_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_rorv_epi64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_rorv_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_rorv_epi64(__mmask8, __m512i, __m512i);

extern __m512d __cdecl _mm512_unpackhi_pd(__m512d, __m512d);
extern __m512d __cdecl _mm512_mask_unpackhi_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_maskz_unpackhi_pd(__mmask8, __m512d, __m512d);
extern __m512  __cdecl _mm512_unpackhi_ps(__m512, __m512);
extern __m512  __cdecl _mm512_mask_unpackhi_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_maskz_unpackhi_ps(__mmask16, __m512, __m512);
extern __m512d __cdecl _mm512_unpacklo_pd(__m512d, __m512d);
extern __m512d __cdecl _mm512_mask_unpacklo_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_maskz_unpacklo_pd(__mmask8, __m512d, __m512d);
extern __m512  __cdecl _mm512_unpacklo_ps(__m512, __m512);
extern __m512  __cdecl _mm512_mask_unpacklo_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_maskz_unpacklo_ps(__mmask16, __m512, __m512);
extern __m512i __cdecl _mm512_unpackhi_epi8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_unpackhi_epi8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_unpackhi_epi8(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_unpackhi_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_unpackhi_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_unpackhi_epi16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_unpackhi_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_unpackhi_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_unpackhi_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_unpackhi_epi64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_unpackhi_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_unpackhi_epi64(__mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_unpacklo_epi8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_unpacklo_epi8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_unpacklo_epi8(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_unpacklo_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_unpacklo_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_unpacklo_epi16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_unpacklo_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_unpacklo_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_unpacklo_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_unpacklo_epi64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_unpacklo_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_unpacklo_epi64(__mmask8, __m512i, __m512i);

extern __m512  __cdecl _mm512_getexp_ps(__m512);
extern __m512  __cdecl _mm512_mask_getexp_ps(__m512, __mmask16, __m512);
extern __m512  __cdecl _mm512_maskz_getexp_ps(__mmask16, __m512);
extern __m512  __cdecl _mm512_getexp_round_ps(__m512, int);
extern __m512  __cdecl _mm512_mask_getexp_round_ps(__m512, __mmask16, __m512, int);
extern __m512  __cdecl _mm512_maskz_getexp_round_ps(__mmask16, __m512, int);
extern __m512d __cdecl _mm512_getexp_pd(__m512d);
extern __m512d __cdecl _mm512_mask_getexp_pd(__m512d, __mmask8, __m512d);
extern __m512d __cdecl _mm512_maskz_getexp_pd(__mmask8, __m512d);
extern __m512d __cdecl _mm512_getexp_round_pd(__m512d, int);
extern __m512d __cdecl _mm512_mask_getexp_round_pd(__m512d, __mmask8, __m512d, int);
extern __m512d __cdecl _mm512_maskz_getexp_round_pd(__mmask8, __m512d, int);

extern __m512  __cdecl _mm512_getmant_ps(__m512, int, int);
extern __m512  __cdecl _mm512_mask_getmant_ps(__m512, __mmask16, __m512, int, int);
extern __m512  __cdecl _mm512_maskz_getmant_ps(__mmask16, __m512, int, int);
extern __m512  __cdecl _mm512_getmant_round_ps(__m512, int, int, int);
extern __m512  __cdecl _mm512_mask_getmant_round_ps(__m512, __mmask16, __m512, int, int, int);
extern __m512  __cdecl _mm512_maskz_getmant_round_ps(__mmask16, __m512, int, int, int);
extern __m512d __cdecl _mm512_getmant_pd(__m512d, int, int);
extern __m512d __cdecl _mm512_mask_getmant_pd(__m512d, __mmask8, __m512d, int, int);
extern __m512d __cdecl _mm512_maskz_getmant_pd(__mmask8, __m512d, int, int);
extern __m512d __cdecl _mm512_getmant_round_pd(__m512d, int, int, int);
extern __m512d __cdecl _mm512_mask_getmant_round_pd(__m512d, __mmask8, __m512d, int, int, int);
extern __m512d __cdecl _mm512_maskz_getmant_round_pd(__mmask8, __m512d, int, int, int);

extern __m512d __cdecl _mm512_permute_pd(__m512d, const int);
extern __m512d __cdecl _mm512_mask_permute_pd(__m512d, __mmask8, __m512d, const int);
extern __m512d __cdecl _mm512_maskz_permute_pd(__mmask8, __m512d, const int);
extern __m512  __cdecl _mm512_permute_ps(__m512, const int);
extern __m512  __cdecl _mm512_mask_permute_ps(__m512, __mmask16, __m512, const int);
extern __m512  __cdecl _mm512_maskz_permute_ps(__mmask16, __m512, const int);
extern __m512d __cdecl _mm512_permutevar_pd(__m512d, __m512i);
extern __m512d __cdecl _mm512_mask_permutevar_pd(__m512d, __mmask8, __m512d, __m512i);
extern __m512d __cdecl _mm512_maskz_permutevar_pd(__mmask8, __m512d, __m512i);
extern __m512  __cdecl _mm512_permutevar_ps(__m512, __m512i);
extern __m512  __cdecl _mm512_mask_permutevar_ps(__m512, __mmask16, __m512, __m512i);
extern __m512  __cdecl _mm512_maskz_permutevar_ps(__mmask16, __m512, __m512i);
extern __m512i __cdecl _mm512_permutevar_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_permutevar_epi32(__m512i, __mmask16, __m512i, __m512i);

extern __m512d __cdecl _mm512_permutex_pd(__m512d, const int);
extern __m512d __cdecl _mm512_mask_permutex_pd(__m512d, __mmask8, __m512d, const int);
extern __m512d __cdecl _mm512_maskz_permutex_pd(__mmask8, __m512d, const int);
extern __m512i __cdecl _mm512_permutex_epi64(__m512i, const int);
extern __m512i __cdecl _mm512_mask_permutex_epi64(__m512i, __mmask8, __m512i, const int);
extern __m512i __cdecl _mm512_maskz_permutex_epi64(__mmask8, __m512i, const int);
extern __m512d __cdecl _mm512_permutexvar_pd(__m512i, __m512d);
extern __m512d __cdecl _mm512_mask_permutexvar_pd(__m512d, __mmask8, __m512i, __m512d);
extern __m512d __cdecl _mm512_maskz_permutexvar_pd(__mmask8, __m512i, __m512d);
extern __m512  __cdecl _mm512_permutexvar_ps(__m512i, __m512);
extern __m512  __cdecl _mm512_mask_permutexvar_ps(__m512, __mmask16, __m512i, __m512);
extern __m512  __cdecl _mm512_maskz_permutexvar_ps(__mmask16, __m512i, __m512);
extern __m512i __cdecl _mm512_permutexvar_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_permutexvar_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_permutexvar_epi16(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_permutexvar_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_permutexvar_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_permutexvar_epi32(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_permutexvar_epi64(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_permutexvar_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_permutexvar_epi64(__mmask8, __m512i, __m512i);

extern __m512d __cdecl _mm512_permutex2var_pd(__m512d, __m512i /* index */, __m512d);
extern __m512d __cdecl _mm512_mask_permutex2var_pd(__m512d, __mmask8, __m512i /* index */, __m512d);
extern __m512d __cdecl _mm512_mask2_permutex2var_pd(__m512d, __m512i /* index */, __mmask8, __m512d);
extern __m512d __cdecl _mm512_maskz_permutex2var_pd(__mmask8, __m512d, __m512i /* index */, __m512d);
extern __m512  __cdecl _mm512_permutex2var_ps(__m512, __m512i /* index */, __m512);
extern __m512  __cdecl _mm512_mask_permutex2var_ps(__m512, __mmask16, __m512i /* index */, __m512);
extern __m512  __cdecl _mm512_mask2_permutex2var_ps(__m512, __m512i /* index */, __mmask16, __m512);
extern __m512  __cdecl _mm512_maskz_permutex2var_ps(__mmask16, __m512, __m512i /* index */, __m512);
extern __m512i __cdecl _mm512_permutex2var_epi16(__m512i, __m512i /* idx */, __m512i);
extern __m512i __cdecl _mm512_mask_permutex2var_epi16(__m512i, __mmask32, __m512i /* idx */, __m512i);
extern __m512i __cdecl _mm512_mask2_permutex2var_epi16(__m512i, __m512i /* idx */, __mmask32, __m512i);
extern __m512i __cdecl _mm512_maskz_permutex2var_epi16(__mmask32, __m512i, __m512i /* idx */, __m512i);
extern __m512i __cdecl _mm512_permutex2var_epi32(__m512i, __m512i /* idx */, __m512i);
extern __m512i __cdecl _mm512_mask_permutex2var_epi32(__m512i, __mmask16, __m512i /* idx */, __m512i);
extern __m512i __cdecl _mm512_mask2_permutex2var_epi32(__m512i, __m512i /* idx */, __mmask16, __m512i);
extern __m512i __cdecl _mm512_maskz_permutex2var_epi32(__mmask16, __m512i, __m512i /* idx */, __m512i);
extern __m512i __cdecl _mm512_permutex2var_epi64(__m512i, __m512i /* idx */, __m512i);
extern __m512i __cdecl _mm512_mask_permutex2var_epi64(__m512i, __mmask8, __m512i /* idx */, __m512i);
extern __m512i __cdecl _mm512_mask2_permutex2var_epi64(__m512i, __m512i /* idx */, __mmask8, __m512i);
extern __m512i __cdecl _mm512_maskz_permutex2var_epi64(__mmask8, __m512i, __m512i /* idx */, __m512i);

extern __m512d __cdecl _mm512_mask_compress_pd(__m512d, __mmask8, __m512d);
extern __m512d __cdecl _mm512_maskz_compress_pd(__mmask8, __m512d);
extern __m512  __cdecl _mm512_mask_compress_ps(__m512, __mmask16, __m512);
extern __m512  __cdecl _mm512_maskz_compress_ps(__mmask16, __m512);
extern __m512i __cdecl _mm512_mask_compress_epi8(__m512i, __mmask64, __m512i);
extern __m512i __cdecl _mm512_maskz_compress_epi8(__mmask64, __m512i);
extern __m512i __cdecl _mm512_mask_compress_epi16(__m512i, __mmask32, __m512i);
extern __m512i __cdecl _mm512_maskz_compress_epi16(__mmask32, __m512i);
extern __m512i __cdecl _mm512_mask_compress_epi32(__m512i, __mmask16, __m512i);
extern __m512i __cdecl _mm512_maskz_compress_epi32(__mmask16, __m512i);
extern __m512i __cdecl _mm512_mask_compress_epi64(__m512i, __mmask8, __m512i);
extern __m512i __cdecl _mm512_maskz_compress_epi64(__mmask8, __m512i);

extern void    __cdecl _mm512_mask_compressstoreu_pd(void*, __mmask8, __m512d);
extern void    __cdecl _mm512_mask_compressstoreu_ps(void*, __mmask16, __m512);
extern void    __cdecl _mm512_mask_compressstoreu_epi8(void*, __mmask64, __m512i);
extern void    __cdecl _mm512_mask_compressstoreu_epi16(void*, __mmask32, __m512i);
extern void    __cdecl _mm512_mask_compressstoreu_epi32(void*, __mmask16, __m512i);
extern void    __cdecl _mm512_mask_compressstoreu_epi64(void*, __mmask8, __m512i);

extern __m512d __cdecl _mm512_mask_expand_pd(__m512d, __mmask8, __m512d);
extern __m512d __cdecl _mm512_maskz_expand_pd(__mmask8, __m512d);
extern __m512  __cdecl _mm512_mask_expand_ps(__m512, __mmask16, __m512);
extern __m512  __cdecl _mm512_maskz_expand_ps(__mmask16, __m512);
extern __m512i __cdecl _mm512_mask_expand_epi8(__m512i, __mmask64, __m512i);
extern __m512i __cdecl _mm512_maskz_expand_epi8(__mmask64, __m512i);
extern __m512i __cdecl _mm512_mask_expand_epi16(__m512i, __mmask32, __m512i);
extern __m512i __cdecl _mm512_maskz_expand_epi16(__mmask32, __m512i);
extern __m512i __cdecl _mm512_mask_expand_epi32(__m512i, __mmask16, __m512i);
extern __m512i __cdecl _mm512_maskz_expand_epi32(__mmask16, __m512i);
extern __m512i __cdecl _mm512_mask_expand_epi64(__m512i, __mmask8, __m512i);
extern __m512i __cdecl _mm512_maskz_expand_epi64(__mmask8, __m512i);
extern __m512d __cdecl _mm512_mask_expandloadu_pd(__m512d, __mmask8, void const*);
extern __m512d __cdecl _mm512_maskz_expandloadu_pd(__mmask8, void const*);
extern __m512  __cdecl _mm512_mask_expandloadu_ps(__m512, __mmask16, void const*);
extern __m512  __cdecl _mm512_maskz_expandloadu_ps(__mmask16, void const*);
extern __m512i __cdecl _mm512_mask_expandloadu_epi8(__m512i, __mmask64, const void*);
extern __m512i __cdecl _mm512_maskz_expandloadu_epi8(__mmask64, const void*);
extern __m512i __cdecl _mm512_mask_expandloadu_epi16(__m512i, __mmask32, const void*);
extern __m512i __cdecl _mm512_maskz_expandloadu_epi16(__mmask32, const void*);
extern __m512i __cdecl _mm512_mask_expandloadu_epi32(__m512i, __mmask16, void const*);
extern __m512i __cdecl _mm512_maskz_expandloadu_epi32(__mmask16, void const*);
extern __m512i __cdecl _mm512_mask_expandloadu_epi64(__m512i, __mmask8, void const*);
extern __m512i __cdecl _mm512_maskz_expandloadu_epi64(__mmask8, void const*);

extern __m512i __cdecl _mm512_ternarylogic_epi32(__m512i, __m512i, __m512i, int);
extern __m512i __cdecl _mm512_mask_ternarylogic_epi32(__m512i, __mmask16, __m512i, __m512i, int);
extern __m512i __cdecl _mm512_maskz_ternarylogic_epi32(__mmask16, __m512i, __m512i, __m512i, int);
extern __m512i __cdecl _mm512_ternarylogic_epi64(__m512i, __m512i, __m512i, int);
extern __m512i __cdecl _mm512_mask_ternarylogic_epi64(__m512i, __mmask8, __m512i, __m512i, int);
extern __m512i __cdecl _mm512_maskz_ternarylogic_epi64(__mmask8, __m512i, __m512i, __m512i, int);

extern __m512i __cdecl _mm512_conflict_epi32(__m512i);
extern __m512i __cdecl _mm512_mask_conflict_epi32(__m512i, __mmask16, __m512i);
extern __m512i __cdecl _mm512_maskz_conflict_epi32(__mmask16, __m512i);
extern __m512i __cdecl _mm512_conflict_epi64(__m512i);
extern __m512i __cdecl _mm512_mask_conflict_epi64(__m512i, __mmask8, __m512i);
extern __m512i __cdecl _mm512_maskz_conflict_epi64(__mmask8, __m512i);

extern __m512i __cdecl _mm512_lzcnt_epi32(__m512i);
extern __m512i __cdecl _mm512_mask_lzcnt_epi32(__m512i, __mmask16, __m512i);
extern __m512i __cdecl _mm512_maskz_lzcnt_epi32(__mmask16, __m512i);
extern __m512i __cdecl _mm512_lzcnt_epi64(__m512i);
extern __m512i __cdecl _mm512_mask_lzcnt_epi64(__m512i, __mmask8, __m512i);
extern __m512i __cdecl _mm512_maskz_lzcnt_epi64(__mmask8, __m512i);

extern __m512i __cdecl _mm512_avg_epu8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_avg_epu8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_avg_epu8(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_avg_epu16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_avg_epu16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_avg_epu16(__mmask32, __m512i, __m512i);

extern __m512i __cdecl _mm512_sad_epu8(__m512i, __m512i);
extern __m512i __cdecl _mm512_dbsad_epu8(__m512i, __m512i, int);
extern __m512i __cdecl _mm512_mask_dbsad_epu8(__m512i, __mmask32, __m512i, __m512i, int);
extern __m512i __cdecl _mm512_maskz_dbsad_epu8(__mmask32, __m512i, __m512i, int);

extern float   __cdecl _mm512_reduce_add_ps(__m512);
extern float   __cdecl _mm512_mask_reduce_add_ps(__mmask16, __m512);
extern double  __cdecl _mm512_reduce_add_pd(__m512d);
extern double  __cdecl _mm512_mask_reduce_add_pd(__mmask8, __m512d);
extern int     __cdecl _mm512_reduce_add_epi8(__m512i);
extern int     __cdecl _mm512_mask_reduce_add_epi8(__mmask64, __m512i);
extern int     __cdecl _mm512_reduce_add_epi16(__m512i);
extern int     __cdecl _mm512_mask_reduce_add_epi16(__mmask32, __m512i);
extern int     __cdecl _mm512_reduce_add_epi32(__m512i);
extern int     __cdecl _mm512_mask_reduce_add_epi32(__mmask16, __m512i);
extern __int64 __cdecl _mm512_reduce_add_epi64(__m512i);
extern __int64 __cdecl _mm512_mask_reduce_add_epi64(__mmask8, __m512i);
extern int     __cdecl _mm512_reduce_add_epu8(__m512i);
extern int     __cdecl _mm512_mask_reduce_add_epu8(__mmask64, __m512i);
extern int     __cdecl _mm512_reduce_add_epu16(__m512i);
extern int     __cdecl _mm512_mask_reduce_add_epu16(__mmask32, __m512i);

extern float   __cdecl _mm512_reduce_mul_ps(__m512);
extern float   __cdecl _mm512_mask_reduce_mul_ps(__mmask16, __m512);
extern double  __cdecl _mm512_reduce_mul_pd(__m512d);
extern double  __cdecl _mm512_mask_reduce_mul_pd(__mmask8, __m512d);
extern int     __cdecl _mm512_reduce_mul_epi32(__m512i);
extern int     __cdecl _mm512_mask_reduce_mul_epi32(__mmask16, __m512i);
extern __int64 __cdecl _mm512_reduce_mul_epi64(__m512i);
extern __int64 __cdecl _mm512_mask_reduce_mul_epi64(__mmask8, __m512i);

extern float   __cdecl _mm512_reduce_min_ps(__m512);
extern float   __cdecl _mm512_mask_reduce_min_ps(__mmask16, __m512);
extern double  __cdecl _mm512_reduce_min_pd(__m512d);
extern double  __cdecl _mm512_mask_reduce_min_pd(__mmask8, __m512d);
extern int     __cdecl _mm512_reduce_min_epi32(__m512i);
extern int     __cdecl _mm512_mask_reduce_min_epi32(__mmask16, __m512i);
extern __int64 __cdecl _mm512_reduce_min_epi64(__m512i);
extern __int64 __cdecl _mm512_mask_reduce_min_epi64(__mmask8, __m512i);
extern unsigned int     __cdecl _mm512_reduce_min_epu32(__m512i);
extern unsigned int     __cdecl _mm512_mask_reduce_min_epu32(__mmask16, __m512i);
extern unsigned __int64 __cdecl _mm512_reduce_min_epu64(__m512i);
extern unsigned __int64 __cdecl _mm512_mask_reduce_min_epu64(__mmask8, __m512i);

extern float   __cdecl _mm512_reduce_max_ps(__m512);
extern float   __cdecl _mm512_mask_reduce_max_ps(__mmask16, __m512);
extern double  __cdecl _mm512_reduce_max_pd(__m512d);
extern double  __cdecl _mm512_mask_reduce_max_pd(__mmask8, __m512d);
extern int     __cdecl _mm512_reduce_max_epi32(__m512i);
extern int     __cdecl _mm512_mask_reduce_max_epi32(__mmask16, __m512i);
extern __int64 __cdecl _mm512_reduce_max_epi64(__m512i);
extern __int64 __cdecl _mm512_mask_reduce_max_epi64(__mmask8, __m512i);
extern unsigned int     __cdecl _mm512_reduce_max_epu32(__m512i);
extern unsigned int     __cdecl _mm512_mask_reduce_max_epu32(__mmask16, __m512i);
extern unsigned __int64 __cdecl _mm512_reduce_max_epu64(__m512i);
extern unsigned __int64 __cdecl _mm512_mask_reduce_max_epu64(__mmask8, __m512i);

extern int     __cdecl _mm512_reduce_and_epi32(__m512i);
extern int     __cdecl _mm512_mask_reduce_and_epi32(__mmask16, __m512i);
extern __int64 __cdecl _mm512_reduce_and_epi64(__m512i);
extern __int64 __cdecl _mm512_mask_reduce_and_epi64(__mmask8, __m512i);

extern int     __cdecl _mm512_reduce_or_epi32(__m512i);
extern int     __cdecl _mm512_mask_reduce_or_epi32(__mmask16, __m512i);
extern __int64 __cdecl _mm512_reduce_or_epi64(__m512i);
extern __int64 __cdecl _mm512_mask_reduce_or_epi64(__mmask8, __m512i);

extern int     __cdecl _mm512_reduce_xor_epi32(__m512i);
extern int     __cdecl _mm512_mask_reduce_xor_epi32(__mmask16, __m512i);
extern __int64 __cdecl _mm512_reduce_xor_epi64(__m512i);
extern __int64 __cdecl _mm512_mask_reduce_xor_epi64(__mmask8, __m512i);

extern __m512d __cdecl _mm512_reduce_pd(__m512d, int);
extern __m512d __cdecl _mm512_mask_reduce_pd(__m512d, __mmask8, __m512d, int);
extern __m512d __cdecl _mm512_maskz_reduce_pd(__mmask8, __m512d, int);
extern __m512d __cdecl _mm512_reduce_round_pd(__m512d, int, int);
extern __m512d __cdecl _mm512_mask_reduce_round_pd(__m512d, __mmask8, __m512d, int, int);
extern __m512d __cdecl _mm512_maskz_reduce_round_pd(__mmask8, __m512d, int, int);
extern __m512  __cdecl _mm512_reduce_ps(__m512, int);
extern __m512  __cdecl _mm512_mask_reduce_ps(__m512, __mmask16, __m512, int);
extern __m512  __cdecl _mm512_maskz_reduce_ps(__mmask16, __m512, int);
extern __m512  __cdecl _mm512_reduce_round_ps(__m512, int, int);
extern __m512  __cdecl _mm512_mask_reduce_round_ps(__m512, __mmask16, __m512, int, int);
extern __m512  __cdecl _mm512_maskz_reduce_round_ps(__mmask16, __m512, int, int);

extern __m512d __cdecl _mm512_roundscale_pd(__m512d, int);
extern __m512d __cdecl _mm512_mask_roundscale_pd(__m512d, __mmask8, __m512d, int);
extern __m512d __cdecl _mm512_maskz_roundscale_pd(__mmask8, __m512d, int);
extern __m512d __cdecl _mm512_roundscale_round_pd(__m512d, int, int);
extern __m512d __cdecl _mm512_mask_roundscale_round_pd(__m512d, __mmask8, __m512d, int, int);
extern __m512d __cdecl _mm512_maskz_roundscale_round_pd(__mmask8, __m512d, int, int);
extern __m512  __cdecl _mm512_roundscale_ps(__m512, int);
extern __m512  __cdecl _mm512_mask_roundscale_ps(__m512, __mmask16, __m512, int);
extern __m512  __cdecl _mm512_maskz_roundscale_ps(__mmask16, __m512, int);
extern __m512  __cdecl _mm512_roundscale_round_ps(__m512, int, int);
extern __m512  __cdecl _mm512_mask_roundscale_round_ps(__m512, __mmask16, __m512, int, int);
extern __m512  __cdecl _mm512_maskz_roundscale_round_ps(__mmask16, __m512, int, int);

extern __m512d __cdecl _mm512_scalef_pd(__m512d, __m512d);
extern __m512d __cdecl _mm512_mask_scalef_pd(__m512d, __mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_maskz_scalef_pd(__mmask8, __m512d, __m512d);
extern __m512d __cdecl _mm512_scalef_round_pd(__m512d, __m512d, int);
extern __m512d __cdecl _mm512_mask_scalef_round_pd(__m512d, __mmask8, __m512d, __m512d, int);
extern __m512d __cdecl _mm512_maskz_scalef_round_pd(__mmask8, __m512d, __m512d, int);
extern __m512  __cdecl _mm512_scalef_ps(__m512, __m512);
extern __m512  __cdecl _mm512_mask_scalef_ps(__m512, __mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_maskz_scalef_ps(__mmask16, __m512, __m512);
extern __m512  __cdecl _mm512_scalef_round_ps(__m512, __m512, int);
extern __m512  __cdecl _mm512_mask_scalef_round_ps(__m512, __mmask16, __m512, __m512, int);
extern __m512  __cdecl _mm512_maskz_scalef_round_ps(__mmask16, __m512, __m512, int);

extern __m512d __cdecl _mm512_fixupimm_pd(__m512d, __m512d, __m512i, const int);
extern __m512d __cdecl _mm512_mask_fixupimm_pd(__m512d, __mmask8, __m512d, __m512i, const int);
extern __m512d __cdecl _mm512_maskz_fixupimm_pd(__mmask8, __m512d, __m512d, __m512i, const int);
extern __m512d __cdecl _mm512_fixupimm_round_pd(__m512d, __m512d, __m512i, const int, const int);
extern __m512d __cdecl _mm512_mask_fixupimm_round_pd(__m512d, __mmask8, __m512d, __m512i, const int, const int);
extern __m512d __cdecl _mm512_maskz_fixupimm_round_pd(__mmask8, __m512d, __m512d, __m512i, const int, const int);
extern __m512  __cdecl _mm512_fixupimm_ps(__m512, __m512, __m512i, const int);
extern __m512  __cdecl _mm512_mask_fixupimm_ps(__m512, __mmask16, __m512, __m512i, const int);
extern __m512  __cdecl _mm512_maskz_fixupimm_ps(__mmask16, __m512, __m512, __m512i, const int);
extern __m512  __cdecl _mm512_fixupimm_round_ps(__m512, __m512, __m512i, const int, const int);
extern __m512  __cdecl _mm512_mask_fixupimm_round_ps(__m512, __mmask16, __m512, __m512i, const int, const int);
extern __m512  __cdecl _mm512_maskz_fixupimm_round_ps(__mmask16, __m512, __m512, __m512i, const int, const int);

extern void    __cdecl _mm512_stream_pd(void*, __m512d);
extern void    __cdecl _mm512_stream_ps(void*, __m512);
extern void    __cdecl _mm512_stream_si512(void*, __m512i);
extern __m512i __cdecl _mm512_stream_load_si512(void const*);

extern __m128d __cdecl _mm512_castpd512_pd128(__m512d);
extern __m128  __cdecl _mm512_castps512_ps128(__m512);
extern __m128i __cdecl _mm512_castsi512_si128(__m512i);
extern __m512i __cdecl _mm512_castsi128_si512(__m128i);

extern __mmask16 __cdecl _mm512_fpclass_ps_mask(__m512, int);
extern __mmask16 __cdecl _mm512_mask_fpclass_ps_mask(__mmask16, __m512, int);
extern __mmask8  __cdecl _mm512_fpclass_pd_mask(__m512d, int);
extern __mmask8  __cdecl _mm512_mask_fpclass_pd_mask(__mmask8, __m512d, int);

extern __m512d __cdecl _mm512_range_pd(__m512d, __m512d, int);
extern __m512d __cdecl _mm512_mask_range_pd(__m512d, __mmask8, __m512d, __m512d, int);
extern __m512d __cdecl _mm512_maskz_range_pd(__mmask8, __m512d, __m512d, int);
extern __m512d __cdecl _mm512_range_round_pd(__m512d, __m512d, int, int);
extern __m512d __cdecl _mm512_mask_range_round_pd(__m512d, __mmask8, __m512d, __m512d, int, int);
extern __m512d __cdecl _mm512_maskz_range_round_pd(__mmask8, __m512d, __m512d, int, int);
extern __m512  __cdecl _mm512_range_ps(__m512, __m512, int);
extern __m512  __cdecl _mm512_mask_range_ps(__m512, __mmask16, __m512, __m512, int);
extern __m512  __cdecl _mm512_maskz_range_ps(__mmask16, __m512, __m512, int);
extern __m512  __cdecl _mm512_range_round_ps(__m512, __m512, int, int);
extern __m512  __cdecl _mm512_mask_range_round_ps(__m512, __mmask16, __m512, __m512, int, int);
extern __m512  __cdecl _mm512_maskz_range_round_ps(__mmask16, __m512, __m512, int, int);

extern __m512i __cdecl _mm512_madd_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_madd_epi16(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_madd_epi16(__mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maddubs_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_maddubs_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_maddubs_epi16(__mmask32, __m512i, __m512i);

extern __m512i __cdecl _mm512_packs_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_packs_epi16(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_packs_epi16(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_packs_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_packs_epi32(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_packs_epi32(__mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_packus_epi16(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_packus_epi16(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_packus_epi16(__mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_packus_epi32(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_packus_epi32(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_packus_epi32(__mmask32, __m512i, __m512i);

extern __mmask64 __cdecl _mm512_cmp_epi8_mask(__m512i, __m512i, const int);
extern __mmask64 __cdecl _mm512_mask_cmp_epi8_mask(__mmask64, __m512i, __m512i, const int);
extern __mmask32 __cdecl _mm512_cmp_epi16_mask(__m512i, __m512i, const int);
extern __mmask32 __cdecl _mm512_mask_cmp_epi16_mask(__mmask32, __m512i, __m512i, const int);
extern __mmask16 __cdecl _mm512_cmp_epi32_mask(__m512i, __m512i, const int);
extern __mmask16 __cdecl _mm512_mask_cmp_epi32_mask(__mmask16, __m512i, __m512i, const int);
extern __mmask8  __cdecl _mm512_cmp_epi64_mask(__m512i, __m512i, const int);
extern __mmask8  __cdecl _mm512_mask_cmp_epi64_mask(__mmask8, __m512i, __m512i, const int);
extern __mmask64 __cdecl _mm512_cmp_epu8_mask(__m512i, __m512i, const int);
extern __mmask64 __cdecl _mm512_mask_cmp_epu8_mask(__mmask64, __m512i, __m512i, const int);
extern __mmask32 __cdecl _mm512_cmp_epu16_mask(__m512i, __m512i, const int);
extern __mmask32 __cdecl _mm512_mask_cmp_epu16_mask(__mmask32, __m512i, __m512i, const int);
extern __mmask16 __cdecl _mm512_cmp_epu32_mask(__m512i, __m512i, const int);
extern __mmask16 __cdecl _mm512_mask_cmp_epu32_mask(__mmask16, __m512i, __m512i, const int);
extern __mmask8  __cdecl _mm512_cmp_epu64_mask(__m512i, __m512i, const int);
extern __mmask8  __cdecl _mm512_mask_cmp_epu64_mask(__mmask8, __m512i, __m512i, const int);

extern __mmask64 __cdecl _mm512_test_epi8_mask(__m512i, __m512i);
extern __mmask64 __cdecl _mm512_mask_test_epi8_mask(__mmask64, __m512i, __m512i);
extern __mmask32 __cdecl _mm512_test_epi16_mask(__m512i, __m512i);
extern __mmask32 __cdecl _mm512_mask_test_epi16_mask(__mmask32, __m512i, __m512i);
extern __mmask64 __cdecl _mm512_testn_epi8_mask(__m512i, __m512i);
extern __mmask64 __cdecl _mm512_mask_testn_epi8_mask(__mmask64, __m512i, __m512i);
extern __mmask32 __cdecl _mm512_testn_epi16_mask(__m512i, __m512i);
extern __mmask32 __cdecl _mm512_mask_testn_epi16_mask(__mmask32, __m512i, __m512i);
extern __mmask16 __cdecl _mm512_test_epi32_mask(__m512i, __m512i);
extern __mmask16 __cdecl _mm512_mask_test_epi32_mask(__mmask16, __m512i, __m512i);
extern __mmask8  __cdecl _mm512_test_epi64_mask(__m512i, __m512i);
extern __mmask8  __cdecl _mm512_mask_test_epi64_mask(__mmask8, __m512i, __m512i);
extern __mmask16 __cdecl _mm512_testn_epi32_mask(__m512i, __m512i);
extern __mmask16 __cdecl _mm512_mask_testn_epi32_mask(__mmask16, __m512i, __m512i);
extern __mmask8  __cdecl _mm512_testn_epi64_mask(__m512i, __m512i);
extern __mmask8  __cdecl _mm512_mask_testn_epi64_mask(__mmask8, __m512i, __m512i);

extern __mmask16 __cdecl _mm512_kunpackb(__mmask16, __mmask16);
extern __mmask32 __cdecl _mm512_kunpackw(__mmask32, __mmask32);
extern __mmask64 __cdecl _mm512_kunpackd(__mmask64, __mmask64);

extern unsigned char __cdecl _mm512_testz_and_mask8(__mmask8, __mmask8);
extern unsigned char __cdecl _mm512_testz_and_mask16(__mmask16, __mmask16);
extern unsigned char __cdecl _mm512_testz_and_mask32(__mmask32, __mmask32);
extern unsigned char __cdecl _mm512_testz_and_mask64(__mmask64, __mmask64);
extern unsigned char __cdecl _mm512_testz_andn_mask8(__mmask8, __mmask8);
extern unsigned char __cdecl _mm512_testz_andn_mask16(__mmask16, __mmask16);
extern unsigned char __cdecl _mm512_testz_andn_mask32(__mmask32, __mmask32);
extern unsigned char __cdecl _mm512_testz_andn_mask64(__mmask64, __mmask64);
extern unsigned char __cdecl _mm512_testz_or_mask8(__mmask8, __mmask8);
extern unsigned char __cdecl _mm512_testz_or_mask16(__mmask16, __mmask16);
extern unsigned char __cdecl _mm512_testz_or_mask32(__mmask32, __mmask32);
extern unsigned char __cdecl _mm512_testz_or_mask64(__mmask64, __mmask64);
extern unsigned char __cdecl _mm512_testz_nor_mask8(__mmask8, __mmask8);
extern unsigned char __cdecl _mm512_testz_nor_mask16(__mmask16, __mmask16);
extern unsigned char __cdecl _mm512_testz_nor_mask32(__mmask32, __mmask32);
extern unsigned char __cdecl _mm512_testz_nor_mask64(__mmask64, __mmask64);

extern __m512  __cdecl _mm512_i32gather_ps(__m512i, void const*, int);
extern __m512  __cdecl _mm512_mask_i32gather_ps(__m512, __mmask16, __m512i, void const*, int);
extern void    __cdecl _mm512_i32scatter_ps(void*, __m512i, __m512, int);
extern void    __cdecl _mm512_mask_i32scatter_ps(void*, __mmask16, __m512i, __m512, int);
extern __m512d __cdecl _mm512_i64gather_pd(__m512i, void const*, int);
extern __m512d __cdecl _mm512_mask_i64gather_pd(__m512d, __mmask8, __m512i, void const*, int);
extern void    __cdecl _mm512_i64scatter_pd(void*, __m512i, __m512d, int);
extern void    __cdecl _mm512_mask_i64scatter_pd(void*, __mmask8, __m512i, __m512d, int);
extern __m512d __cdecl _mm512_i32gather_pd(__m256i, void const*, int);
extern __m512d __cdecl _mm512_mask_i32gather_pd(__m512d, __mmask8, __m256i, void const*, int);
extern void    __cdecl _mm512_i32scatter_pd(void*, __m256i, __m512d, int);
extern void    __cdecl _mm512_mask_i32scatter_pd(void*, __mmask8, __m256i, __m512d, int);
extern __m512i __cdecl _mm512_i32gather_epi32(__m512i, void const*, int);
extern __m512i __cdecl _mm512_mask_i32gather_epi32(__m512i, __mmask16, __m512i, void const*, int);
extern void    __cdecl _mm512_i32scatter_epi32(void*, __m512i, __m512i, int);
extern void    __cdecl _mm512_mask_i32scatter_epi32(void*, __mmask16, __m512i, __m512i, int);
extern __m512i __cdecl _mm512_i32gather_epi64(__m256i, void const*, int);
extern __m512i __cdecl _mm512_mask_i32gather_epi64(__m512i, __mmask8, __m256i, void const*, int);
extern __m512i __cdecl _mm512_i64gather_epi64(__m512i, void const*, int);
extern __m512i __cdecl _mm512_mask_i64gather_epi64(__m512i, __mmask8, __m512i, void const*, int);
extern void    __cdecl _mm512_i32scatter_epi64(void*, __m256i, __m512i, int);
extern void    __cdecl _mm512_mask_i32scatter_epi64(void*, __mmask8, __m256i, __m512i, int);
extern void    __cdecl _mm512_i64scatter_epi64(void*, __m512i, __m512i, int);
extern void    __cdecl _mm512_mask_i64scatter_epi64(void*, __mmask8, __m512i, __m512i, int);
extern __m256  __cdecl _mm512_i64gather_ps(__m512i, void const*, int);
extern __m256  __cdecl _mm512_mask_i64gather_ps(__m256, __mmask8, __m512i, void const*, int);
extern void    __cdecl _mm512_i64scatter_ps(void*, __m512i, __m256, int);
extern void    __cdecl _mm512_mask_i64scatter_ps(void*, __mmask8, __m512i, __m256, int);
extern __m256i __cdecl _mm512_i64gather_epi32(__m512i, void const*, int);
extern __m256i __cdecl _mm512_mask_i64gather_epi32(__m256i, __mmask8, __m512i, void const*, int);
extern void    __cdecl _mm512_i64scatter_epi32(void*, __m512i, __m256i, int);
extern void    __cdecl _mm512_mask_i64scatter_epi32(void*, __mmask8, __m512i, __m256i, int);

extern __m512d __cdecl _mm512_cvtpslo_pd(__m512);
extern __m512d __cdecl _mm512_mask_cvtpslo_pd(__m512d, __mmask8, __m512);
extern __m512d __cdecl _mm512_cvtepi32lo_pd(__m512i);
extern __m512d __cdecl _mm512_mask_cvtepi32lo_pd(__m512d, __mmask8, __m512i);
extern __m512d __cdecl _mm512_cvtepu32lo_pd(__m512i);
extern __m512d __cdecl _mm512_mask_cvtepu32lo_pd(__m512d, __mmask8, __m512i);
extern __m512d __cdecl _mm512_cvtepi32_pd(__m256i);
extern __m512d __cdecl _mm512_mask_cvtepi32_pd(__m512d, __mmask8, __m256i);
extern __m512d __cdecl _mm512_maskz_cvtepi32_pd(__mmask8, __m256i);
extern __m512d __cdecl _mm512_cvtepu32_pd(__m256i);
extern __m512d __cdecl _mm512_mask_cvtepu32_pd(__m512d, __mmask8, __m256i);
extern __m512d __cdecl _mm512_maskz_cvtepu32_pd(__mmask8, __m256i);

extern __m512  __cdecl _mm512_cvtepi32_ps( __m512i);
extern __m512  __cdecl _mm512_mask_cvtepi32_ps(__m512, __mmask16, __m512i);
extern __m512  __cdecl _mm512_maskz_cvtepi32_ps(__mmask16, __m512i);
extern __m512  __cdecl _mm512_cvt_roundepi32_ps(__m512i, int);
extern __m512  __cdecl _mm512_mask_cvt_roundepi32_ps(__m512, __mmask16, __m512i, int);
extern __m512  __cdecl _mm512_maskz_cvt_roundepi32_ps(__mmask16, __m512i, int);
extern __m512  __cdecl _mm512_cvtepu32_ps( __m512i);
extern __m512  __cdecl _mm512_mask_cvtepu32_ps(__m512, __mmask16, __m512i);
extern __m512  __cdecl _mm512_maskz_cvtepu32_ps(__mmask16, __m512i);
extern __m512  __cdecl _mm512_cvt_roundepu32_ps(__m512i, int);
extern __m512  __cdecl _mm512_mask_cvt_roundepu32_ps(__m512, __mmask16, __m512i, int);
extern __m512  __cdecl _mm512_maskz_cvt_roundepu32_ps(__mmask16, __m512i, int);
extern __m512  __cdecl _mm512_cvtph_ps(__m256i);
extern __m512  __cdecl _mm512_mask_cvtph_ps(__m512, __mmask16, __m256i);
extern __m512  __cdecl _mm512_maskz_cvtph_ps(__mmask16, __m256i);
extern __m512  __cdecl _mm512_cvt_roundph_ps(__m256i, int);
extern __m512  __cdecl _mm512_mask_cvt_roundph_ps(__m512, __mmask16, __m256i, int);
extern __m512  __cdecl _mm512_maskz_cvt_roundph_ps(__mmask16, __m256i, int);
extern __m256i __cdecl _mm512_cvtps_ph(__m512, int);
extern __m256i __cdecl _mm512_mask_cvtps_ph(__m256i, __mmask16, __m512, int);
extern __m256i __cdecl _mm512_maskz_cvtps_ph(__mmask16, __m512, int);
extern __m256i __cdecl _mm512_cvt_roundps_ph(__m512, int);
extern __m256i __cdecl _mm512_mask_cvt_roundps_ph(__m256i, __mmask16, __m512, int);
extern __m256i __cdecl _mm512_maskz_cvt_roundps_ph(__mmask16, __m512, int);
extern __m256  __cdecl _mm512_cvtepi64_ps(__m512i);
extern __m256  __cdecl _mm512_mask_cvtepi64_ps(__m256, __mmask8, __m512i);
extern __m256  __cdecl _mm512_maskz_cvtepi64_ps(__mmask8, __m512i);
extern __m256  __cdecl _mm512_cvt_roundepi64_ps(__m512i, int);
extern __m256  __cdecl _mm512_mask_cvt_roundepi64_ps(__m256, __mmask8, __m512i, int);
extern __m256  __cdecl _mm512_maskz_cvt_roundepi64_ps(__mmask8, __m512i, int);
extern __m256  __cdecl _mm512_cvtepu64_ps(__m512i);
extern __m256  __cdecl _mm512_mask_cvtepu64_ps(__m256, __mmask8, __m512i);
extern __m256  __cdecl _mm512_maskz_cvtepu64_ps(__mmask8, __m512i);
extern __m256  __cdecl _mm512_cvt_roundepu64_ps(__m512i, int);
extern __m256  __cdecl _mm512_mask_cvt_roundepu64_ps(__m256, __mmask8, __m512i, int);
extern __m256  __cdecl _mm512_maskz_cvt_roundepu64_ps(__mmask8, __m512i, int);

extern __m512i __cdecl _mm512_cvtepi8_epi32(__m128i);
extern __m512i __cdecl _mm512_mask_cvtepi8_epi32(__m512i, __mmask16, __m128i);
extern __m512i __cdecl _mm512_maskz_cvtepi8_epi32(__mmask16, __m128i);
extern __m512i __cdecl _mm512_cvtepi8_epi64(__m128i);
extern __m512i __cdecl _mm512_mask_cvtepi8_epi64(__m512i, __mmask8, __m128i);
extern __m512i __cdecl _mm512_maskz_cvtepi8_epi64(__mmask8, __m128i);
extern __m512i __cdecl _mm512_cvtepi16_epi32(__m256i);
extern __m512i __cdecl _mm512_mask_cvtepi16_epi32(__m512i, __mmask16, __m256i);
extern __m512i __cdecl _mm512_maskz_cvtepi16_epi32(__mmask16, __m256i);
extern __m512i __cdecl _mm512_cvtepi16_epi64(__m128i);
extern __m512i __cdecl _mm512_mask_cvtepi16_epi64(__m512i, __mmask8, __m128i);
extern __m512i __cdecl _mm512_maskz_cvtepi16_epi64(__mmask8, __m128i);
extern __m128i __cdecl _mm512_cvtepi32_epi8(__m512i);
extern __m128i __cdecl _mm512_mask_cvtepi32_epi8(__m128i, __mmask16, __m512i);
extern __m128i __cdecl _mm512_maskz_cvtepi32_epi8(__mmask16, __m512i);
extern void    __cdecl _mm512_mask_cvtepi32_storeu_epi8(void*, __mmask16, __m512i);
extern __m128i __cdecl _mm512_cvtsepi32_epi8(__m512i);
extern __m128i __cdecl _mm512_mask_cvtsepi32_epi8(__m128i, __mmask16, __m512i);
extern __m128i __cdecl _mm512_maskz_cvtsepi32_epi8(__mmask16, __m512i);
extern void    __cdecl _mm512_mask_cvtsepi32_storeu_epi8(void*, __mmask16, __m512i);
extern __m128i __cdecl _mm512_cvtusepi32_epi8(__m512i);
extern __m128i __cdecl _mm512_mask_cvtusepi32_epi8(__m128i, __mmask16, __m512i);
extern __m128i __cdecl _mm512_maskz_cvtusepi32_epi8(__mmask16, __m512i);
extern void    __cdecl _mm512_mask_cvtusepi32_storeu_epi8(void*, __mmask16, __m512i);
extern __m256i __cdecl _mm512_cvtepi32_epi16(__m512i);
extern __m256i __cdecl _mm512_mask_cvtepi32_epi16(__m256i, __mmask16, __m512i);
extern __m256i __cdecl _mm512_maskz_cvtepi32_epi16(__mmask16, __m512i);
extern void    __cdecl _mm512_mask_cvtepi32_storeu_epi16(void*, __mmask16, __m512i);
extern __m256i __cdecl _mm512_cvtsepi32_epi16(__m512i);
extern __m256i __cdecl _mm512_mask_cvtsepi32_epi16(__m256i, __mmask16, __m512i);
extern __m256i __cdecl _mm512_maskz_cvtsepi32_epi16(__mmask16, __m512i);
extern void    __cdecl _mm512_mask_cvtsepi32_storeu_epi16(void*, __mmask16, __m512i);
extern __m256i __cdecl _mm512_cvtusepi32_epi16(__m512i);
extern __m256i __cdecl _mm512_mask_cvtusepi32_epi16(__m256i, __mmask16, __m512i);
extern __m256i __cdecl _mm512_maskz_cvtusepi32_epi16(__mmask16, __m512i);
extern void    __cdecl _mm512_mask_cvtusepi32_storeu_epi16(void*, __mmask16, __m512i);
extern __m512i __cdecl _mm512_cvtepi32_epi64(__m256i);
extern __m512i __cdecl _mm512_mask_cvtepi32_epi64(__m512i, __mmask8, __m256i);
extern __m512i __cdecl _mm512_maskz_cvtepi32_epi64(__mmask8, __m256i);
extern __m128i __cdecl _mm512_cvtepi64_epi8(__m512i);
extern __m128i __cdecl _mm512_mask_cvtepi64_epi8(__m128i, __mmask8, __m512i);
extern __m128i __cdecl _mm512_maskz_cvtepi64_epi8(__mmask8, __m512i);
extern void    __cdecl _mm512_mask_cvtepi64_storeu_epi8(void*, __mmask8, __m512i);
extern __m128i __cdecl _mm512_cvtsepi64_epi8(__m512i);
extern __m128i __cdecl _mm512_mask_cvtsepi64_epi8(__m128i, __mmask8, __m512i);
extern __m128i __cdecl _mm512_maskz_cvtsepi64_epi8(__mmask8, __m512i);
extern void    __cdecl _mm512_mask_cvtsepi64_storeu_epi8(void*, __mmask8, __m512i);
extern __m128i __cdecl _mm512_cvtusepi64_epi8(__m512i);
extern __m128i __cdecl _mm512_mask_cvtusepi64_epi8(__m128i, __mmask8, __m512i);
extern __m128i __cdecl _mm512_maskz_cvtusepi64_epi8(__mmask8, __m512i);
extern void    __cdecl _mm512_mask_cvtusepi64_storeu_epi8(void*, __mmask8, __m512i);
extern __m128i __cdecl _mm512_cvtepi64_epi16(__m512i);
extern __m128i __cdecl _mm512_mask_cvtepi64_epi16(__m128i, __mmask8, __m512i);
extern __m128i __cdecl _mm512_maskz_cvtepi64_epi16(__mmask8, __m512i);
extern void    __cdecl _mm512_mask_cvtepi64_storeu_epi16(void*, __mmask8, __m512i);
extern __m128i __cdecl _mm512_cvtsepi64_epi16(__m512i);
extern __m128i __cdecl _mm512_mask_cvtsepi64_epi16(__m128i, __mmask8, __m512i);
extern __m128i __cdecl _mm512_maskz_cvtsepi64_epi16(__mmask8, __m512i);
extern void    __cdecl _mm512_mask_cvtsepi64_storeu_epi16(void*, __mmask8, __m512i);
extern __m128i __cdecl _mm512_cvtusepi64_epi16(__m512i);
extern __m128i __cdecl _mm512_mask_cvtusepi64_epi16(__m128i, __mmask8, __m512i);
extern __m128i __cdecl _mm512_maskz_cvtusepi64_epi16(__mmask8, __m512i);
extern void    __cdecl _mm512_mask_cvtusepi64_storeu_epi16(void*, __mmask8, __m512i);
extern __m256i __cdecl _mm512_cvtepi64_epi32(__m512i);
extern __m256i __cdecl _mm512_mask_cvtepi64_epi32(__m256i, __mmask8, __m512i);
extern __m256i __cdecl _mm512_maskz_cvtepi64_epi32(__mmask8, __m512i);
extern void    __cdecl _mm512_mask_cvtepi64_storeu_epi32(void*, __mmask8, __m512i);
extern __m256i __cdecl _mm512_cvtsepi64_epi32(__m512i);
extern __m256i __cdecl _mm512_mask_cvtsepi64_epi32(__m256i, __mmask8, __m512i);
extern __m256i __cdecl _mm512_maskz_cvtsepi64_epi32(__mmask8, __m512i);
extern void    __cdecl _mm512_mask_cvtsepi64_storeu_epi32(void*, __mmask8, __m512i);
extern __m256i __cdecl _mm512_cvtusepi64_epi32(__m512i);
extern __m256i __cdecl _mm512_mask_cvtusepi64_epi32(__m256i, __mmask8, __m512i);
extern __m256i __cdecl _mm512_maskz_cvtusepi64_epi32(__mmask8, __m512i);
extern void    __cdecl _mm512_mask_cvtusepi64_storeu_epi32(void*, __mmask8, __m512i);
extern __m512i __cdecl _mm512_cvtepu8_epi32(__m128i);
extern __m512i __cdecl _mm512_mask_cvtepu8_epi32(__m512i, __mmask16, __m128i);
extern __m512i __cdecl _mm512_maskz_cvtepu8_epi32(__mmask16, __m128i);
extern __m512i __cdecl _mm512_cvtepu8_epi64(__m128i);
extern __m512i __cdecl _mm512_mask_cvtepu8_epi64(__m512i, __mmask8, __m128i);
extern __m512i __cdecl _mm512_maskz_cvtepu8_epi64(__mmask8, __m128i);
extern __m512i __cdecl _mm512_cvtepu16_epi32(__m256i);
extern __m512i __cdecl _mm512_mask_cvtepu16_epi32(__m512i, __mmask16, __m256i);
extern __m512i __cdecl _mm512_maskz_cvtepu16_epi32(__mmask16, __m256i);
extern __m512i __cdecl _mm512_cvtepu16_epi64(__m128i);
extern __m512i __cdecl _mm512_mask_cvtepu16_epi64(__m512i, __mmask8, __m128i);
extern __m512i __cdecl _mm512_maskz_cvtepu16_epi64(__mmask8, __m128i);
extern __m512i __cdecl _mm512_cvtepu32_epi64(__m256i);
extern __m512i __cdecl _mm512_mask_cvtepu32_epi64(__m512i, __mmask8, __m256i);
extern __m512i __cdecl _mm512_maskz_cvtepu32_epi64(__mmask8, __m256i);

extern __m512i __cdecl _mm512_cvtps_epi32(__m512);
extern __m512i __cdecl _mm512_mask_cvtps_epi32(__m512i, __mmask16, __m512);
extern __m512i __cdecl _mm512_maskz_cvtps_epi32(__mmask16, __m512);
extern __m512i __cdecl _mm512_cvt_roundps_epi32(__m512, int);
extern __m512i __cdecl _mm512_mask_cvt_roundps_epi32(__m512i, __mmask16, __m512, int);
extern __m512i __cdecl _mm512_maskz_cvt_roundps_epi32(__mmask16, __m512, int);
extern __m512i __cdecl _mm512_cvttps_epi32(__m512);
extern __m512i __cdecl _mm512_mask_cvttps_epi32(__m512i, __mmask16, __m512);
extern __m512i __cdecl _mm512_maskz_cvttps_epi32(__mmask16, __m512);
extern __m512i __cdecl _mm512_cvtt_roundps_epi32(__m512, int);
extern __m512i __cdecl _mm512_mask_cvtt_roundps_epi32(__m512i, __mmask16, __m512, int);
extern __m512i __cdecl _mm512_maskz_cvtt_roundps_epi32(__mmask16, __m512, int);
extern __m512i __cdecl _mm512_cvtps_epu32(__m512);
extern __m512i __cdecl _mm512_mask_cvtps_epu32(__m512i, __mmask16, __m512);
extern __m512i __cdecl _mm512_maskz_cvtps_epu32(__mmask16, __m512);
extern __m512i __cdecl _mm512_cvt_roundps_epu32(__m512, int);
extern __m512i __cdecl _mm512_mask_cvt_roundps_epu32(__m512i, __mmask16, __m512, int);
extern __m512i __cdecl _mm512_maskz_cvt_roundps_epu32(__mmask16, __m512, int);
extern __m512i __cdecl _mm512_cvttps_epu32(__m512);
extern __m512i __cdecl _mm512_mask_cvttps_epu32(__m512i, __mmask16, __m512);
extern __m512i __cdecl _mm512_maskz_cvttps_epu32(__mmask16, __m512);
extern __m512i __cdecl _mm512_cvtt_roundps_epu32(__m512, int);
extern __m512i __cdecl _mm512_mask_cvtt_roundps_epu32(__m512i, __mmask16, __m512, int);
extern __m512i __cdecl _mm512_maskz_cvtt_roundps_epu32(__mmask16, __m512, int);
extern __m256i __cdecl _mm512_cvtpd_epi32(__m512d);
extern __m256i __cdecl _mm512_mask_cvtpd_epi32(__m256i, __mmask8, __m512d);
extern __m256i __cdecl _mm512_maskz_cvtpd_epi32(__mmask8, __m512d);
extern __m256i __cdecl _mm512_cvt_roundpd_epi32(__m512d, int);
extern __m256i __cdecl _mm512_mask_cvt_roundpd_epi32(__m256i, __mmask8, __m512d, int);
extern __m256i __cdecl _mm512_maskz_cvt_roundpd_epi32(__mmask8, __m512d, int);
extern __m256i __cdecl _mm512_cvttpd_epi32(__m512d);
extern __m256i __cdecl _mm512_mask_cvttpd_epi32(__m256i, __mmask8, __m512d);
extern __m256i __cdecl _mm512_maskz_cvttpd_epi32(__mmask8, __m512d);
extern __m256i __cdecl _mm512_cvtt_roundpd_epi32(__m512d, int);
extern __m256i __cdecl _mm512_mask_cvtt_roundpd_epi32(__m256i, __mmask8, __m512d, int);
extern __m256i __cdecl _mm512_maskz_cvtt_roundpd_epi32(__mmask8, __m512d, int);
extern __m256i __cdecl _mm512_cvtpd_epu32(__m512d);
extern __m256i __cdecl _mm512_mask_cvtpd_epu32(__m256i, __mmask8, __m512d);
extern __m256i __cdecl _mm512_maskz_cvtpd_epu32(__mmask8, __m512d);
extern __m256i __cdecl _mm512_cvt_roundpd_epu32(__m512d, int);
extern __m256i __cdecl _mm512_mask_cvt_roundpd_epu32(__m256i, __mmask8, __m512d, int);
extern __m256i __cdecl _mm512_maskz_cvt_roundpd_epu32(__mmask8, __m512d, int);
extern __m256i __cdecl _mm512_cvttpd_epu32(__m512d);
extern __m256i __cdecl _mm512_mask_cvttpd_epu32(__m256i, __mmask8, __m512d);
extern __m256i __cdecl _mm512_maskz_cvttpd_epu32(__mmask8, __m512d);
extern __m256i __cdecl _mm512_cvtt_roundpd_epu32(__m512d, int);
extern __m256i __cdecl _mm512_mask_cvtt_roundpd_epu32(__m256i, __mmask8, __m512d, int);
extern __m256i __cdecl _mm512_maskz_cvtt_roundpd_epu32(__mmask8, __m512d, int);

extern __m512i __cdecl _mm512_cvtepi8_epi16(__m256i);
extern __m512i __cdecl _mm512_mask_cvtepi8_epi16(__m512i, __mmask32, __m256i);
extern __m512i __cdecl _mm512_maskz_cvtepi8_epi16(__mmask32, __m256i);
extern __m512i __cdecl _mm512_cvtepu8_epi16(__m256i);
extern __m512i __cdecl _mm512_mask_cvtepu8_epi16(__m512i, __mmask32, __m256i);
extern __m512i __cdecl _mm512_maskz_cvtepu8_epi16(__mmask32, __m256i);
extern __m256i __cdecl _mm512_cvtepi16_epi8(__m512i);
extern __m256i __cdecl _mm512_mask_cvtepi16_epi8(__m256i, __mmask32, __m512i);
extern __m256i __cdecl _mm512_maskz_cvtepi16_epi8(__mmask32, __m512i);
extern void    __cdecl _mm512_mask_cvtepi16_storeu_epi8(void*, __mmask32, __m512i);
extern __m256i __cdecl _mm512_cvtsepi16_epi8(__m512i);
extern __m256i __cdecl _mm512_mask_cvtsepi16_epi8(__m256i, __mmask32, __m512i);
extern __m256i __cdecl _mm512_maskz_cvtsepi16_epi8(__mmask32, __m512i);
extern void    __cdecl _mm512_mask_cvtsepi16_storeu_epi8(void*, __mmask32, __m512i);
extern __m256i __cdecl _mm512_cvtusepi16_epi8(__m512i);
extern __m256i __cdecl _mm512_mask_cvtusepi16_epi8(__m256i, __mmask32, __m512i);
extern __m256i __cdecl _mm512_maskz_cvtusepi16_epi8(__mmask32, __m512i);
extern void    __cdecl _mm512_mask_cvtusepi16_storeu_epi8(void*, __mmask32, __m512i);

extern __m512d __cdecl _mm512_cvtepi64_pd(__m512i);
extern __m512d __cdecl _mm512_mask_cvtepi64_pd(__m512d, __mmask8, __m512i);
extern __m512d __cdecl _mm512_maskz_cvtepi64_pd(__mmask8, __m512i);
extern __m512d __cdecl _mm512_cvt_roundepi64_pd(__m512i, int);
extern __m512d __cdecl _mm512_mask_cvt_roundepi64_pd(__m512d, __mmask8, __m512i, int);
extern __m512d __cdecl _mm512_maskz_cvt_roundepi64_pd(__mmask8, __m512i, int);
extern __m512d __cdecl _mm512_cvtepu64_pd(__m512i);
extern __m512d __cdecl _mm512_mask_cvtepu64_pd(__m512d, __mmask8, __m512i);
extern __m512d __cdecl _mm512_maskz_cvtepu64_pd(__mmask8, __m512i);
extern __m512d __cdecl _mm512_cvt_roundepu64_pd(__m512i, int);
extern __m512d __cdecl _mm512_mask_cvt_roundepu64_pd(__m512d, __mmask8, __m512i, int);
extern __m512d __cdecl _mm512_maskz_cvt_roundepu64_pd(__mmask8, __m512i, int);
extern __m512i __cdecl _mm512_cvtpd_epi64(__m512d);
extern __m512i __cdecl _mm512_mask_cvtpd_epi64(__m512i, __mmask8, __m512d);
extern __m512i __cdecl _mm512_maskz_cvtpd_epi64(__mmask8, __m512d);
extern __m512i __cdecl _mm512_cvt_roundpd_epi64(__m512d, int);
extern __m512i __cdecl _mm512_mask_cvt_roundpd_epi64(__m512i, __mmask8, __m512d, int);
extern __m512i __cdecl _mm512_maskz_cvt_roundpd_epi64(__mmask8, __m512d, int);
extern __m512i __cdecl _mm512_cvtpd_epu64(__m512d);
extern __m512i __cdecl _mm512_mask_cvtpd_epu64(__m512i, __mmask8, __m512d);
extern __m512i __cdecl _mm512_maskz_cvtpd_epu64(__mmask8, __m512d);
extern __m512i __cdecl _mm512_cvt_roundpd_epu64(__m512d, int);
extern __m512i __cdecl _mm512_mask_cvt_roundpd_epu64(__m512i, __mmask8, __m512d, int);
extern __m512i __cdecl _mm512_maskz_cvt_roundpd_epu64(__mmask8, __m512d, int);
extern __m512i __cdecl _mm512_cvttpd_epi64(__m512d);
extern __m512i __cdecl _mm512_mask_cvttpd_epi64(__m512i, __mmask8, __m512d);
extern __m512i __cdecl _mm512_maskz_cvttpd_epi64(__mmask8, __m512d);
extern __m512i __cdecl _mm512_cvtt_roundpd_epi64(__m512d, int);
extern __m512i __cdecl _mm512_mask_cvtt_roundpd_epi64(__m512i, __mmask8, __m512d, int);
extern __m512i __cdecl _mm512_maskz_cvtt_roundpd_epi64(__mmask8, __m512d, int);
extern __m512i __cdecl _mm512_cvttpd_epu64(__m512d);
extern __m512i __cdecl _mm512_mask_cvttpd_epu64(__m512i, __mmask8, __m512d);
extern __m512i __cdecl _mm512_maskz_cvttpd_epu64(__mmask8, __m512d);
extern __m512i __cdecl _mm512_cvtt_roundpd_epu64(__m512d, int);
extern __m512i __cdecl _mm512_mask_cvtt_roundpd_epu64(__m512i, __mmask8, __m512d, int);
extern __m512i __cdecl _mm512_maskz_cvtt_roundpd_epu64(__mmask8, __m512d, int);
extern __m512i __cdecl _mm512_cvtps_epi64(__m256);
extern __m512i __cdecl _mm512_mask_cvtps_epi64(__m512i, __mmask8, __m256);
extern __m512i __cdecl _mm512_maskz_cvtps_epi64(__mmask8, __m256);
extern __m512i __cdecl _mm512_cvt_roundps_epi64(__m256, int);
extern __m512i __cdecl _mm512_mask_cvt_roundps_epi64(__m512i, __mmask8, __m256, int);
extern __m512i __cdecl _mm512_maskz_cvt_roundps_epi64(__mmask8, __m256, int);
extern __m512i __cdecl _mm512_cvtps_epu64(__m256);
extern __m512i __cdecl _mm512_mask_cvtps_epu64(__m512i, __mmask8, __m256);
extern __m512i __cdecl _mm512_maskz_cvtps_epu64(__mmask8, __m256);
extern __m512i __cdecl _mm512_cvt_roundps_epu64(__m256, int);
extern __m512i __cdecl _mm512_mask_cvt_roundps_epu64(__m512i, __mmask8, __m256, int);
extern __m512i __cdecl _mm512_maskz_cvt_roundps_epu64(__mmask8, __m256, int);
extern __m512i __cdecl _mm512_cvttps_epi64(__m256);
extern __m512i __cdecl _mm512_mask_cvttps_epi64(__m512i, __mmask8, __m256);
extern __m512i __cdecl _mm512_maskz_cvttps_epi64(__mmask8, __m256);
extern __m512i __cdecl _mm512_cvtt_roundps_epi64(__m256, int);
extern __m512i __cdecl _mm512_mask_cvtt_roundps_epi64(__m512i, __mmask8, __m256, int);
extern __m512i __cdecl _mm512_maskz_cvtt_roundps_epi64(__mmask8, __m256, int);
extern __m512i __cdecl _mm512_cvttps_epu64(__m256);
extern __m512i __cdecl _mm512_mask_cvttps_epu64(__m512i, __mmask8, __m256);
extern __m512i __cdecl _mm512_maskz_cvttps_epu64(__mmask8, __m256);
extern __m512i __cdecl _mm512_cvtt_roundps_epu64(__m256, int);
extern __m512i __cdecl _mm512_mask_cvtt_roundps_epu64(__m512i, __mmask8, __m256, int);
extern __m512i __cdecl _mm512_maskz_cvtt_roundps_epu64(__mmask8, __m256, int);

extern __mmask64  __cdecl _mm512_cmpeq_epi8_mask(__m512i, __m512i);
extern __mmask64  __cdecl _mm512_cmpge_epi8_mask(__m512i, __m512i);
extern __mmask64  __cdecl _mm512_cmpgt_epi8_mask(__m512i, __m512i);
extern __mmask64  __cdecl _mm512_cmple_epi8_mask(__m512i, __m512i);
extern __mmask64  __cdecl _mm512_cmplt_epi8_mask(__m512i, __m512i);
extern __mmask64  __cdecl _mm512_cmpneq_epi8_mask(__m512i, __m512i);
extern __mmask64  __cdecl _mm512_cmpeq_epu8_mask(__m512i, __m512i);
extern __mmask64  __cdecl _mm512_cmpge_epu8_mask(__m512i, __m512i);
extern __mmask64  __cdecl _mm512_cmpgt_epu8_mask(__m512i, __m512i);
extern __mmask64  __cdecl _mm512_cmple_epu8_mask(__m512i, __m512i);
extern __mmask64  __cdecl _mm512_cmplt_epu8_mask(__m512i, __m512i);
extern __mmask64  __cdecl _mm512_cmpneq_epu8_mask(__m512i, __m512i);

extern __mmask64  __cdecl _mm512_mask_cmpeq_epi8_mask(__mmask64, __m512i, __m512i);
extern __mmask64  __cdecl _mm512_mask_cmpge_epi8_mask(__mmask64, __m512i, __m512i);
extern __mmask64  __cdecl _mm512_mask_cmpgt_epi8_mask(__mmask64, __m512i, __m512i);
extern __mmask64  __cdecl _mm512_mask_cmple_epi8_mask(__mmask64, __m512i, __m512i);
extern __mmask64  __cdecl _mm512_mask_cmplt_epi8_mask(__mmask64, __m512i, __m512i);
extern __mmask64  __cdecl _mm512_mask_cmpneq_epi8_mask(__mmask64, __m512i, __m512i);
extern __mmask64  __cdecl _mm512_mask_cmpeq_epu8_mask(__mmask64, __m512i, __m512i);
extern __mmask64  __cdecl _mm512_mask_cmpge_epu8_mask(__mmask64, __m512i, __m512i);
extern __mmask64  __cdecl _mm512_mask_cmpgt_epu8_mask(__mmask64, __m512i, __m512i);
extern __mmask64  __cdecl _mm512_mask_cmple_epu8_mask(__mmask64, __m512i, __m512i);
extern __mmask64  __cdecl _mm512_mask_cmplt_epu8_mask(__mmask64, __m512i, __m512i);
extern __mmask64  __cdecl _mm512_mask_cmpneq_epu8_mask(__mmask64, __m512i, __m512i);

extern __mmask32  __cdecl _mm512_cmpeq_epi16_mask(__m512i, __m512i);
extern __mmask32  __cdecl _mm512_cmpge_epi16_mask(__m512i, __m512i);
extern __mmask32  __cdecl _mm512_cmpgt_epi16_mask(__m512i, __m512i);
extern __mmask32  __cdecl _mm512_cmple_epi16_mask(__m512i, __m512i);
extern __mmask32  __cdecl _mm512_cmplt_epi16_mask(__m512i, __m512i);
extern __mmask32  __cdecl _mm512_cmpneq_epi16_mask(__m512i, __m512i);
extern __mmask32  __cdecl _mm512_cmpeq_epu16_mask(__m512i, __m512i);
extern __mmask32  __cdecl _mm512_cmpge_epu16_mask(__m512i, __m512i);
extern __mmask32  __cdecl _mm512_cmpgt_epu16_mask(__m512i, __m512i);
extern __mmask32  __cdecl _mm512_cmple_epu16_mask(__m512i, __m512i);
extern __mmask32  __cdecl _mm512_cmplt_epu16_mask(__m512i, __m512i);
extern __mmask32  __cdecl _mm512_cmpneq_epu16_mask(__m512i, __m512i);

extern __mmask32  __cdecl _mm512_mask_cmpeq_epi16_mask(__mmask32, __m512i, __m512i);
extern __mmask32  __cdecl _mm512_mask_cmpge_epi16_mask(__mmask32, __m512i, __m512i);
extern __mmask32  __cdecl _mm512_mask_cmpgt_epi16_mask(__mmask32, __m512i, __m512i);
extern __mmask32  __cdecl _mm512_mask_cmple_epi16_mask(__mmask32, __m512i, __m512i);
extern __mmask32  __cdecl _mm512_mask_cmplt_epi16_mask(__mmask32, __m512i, __m512i);
extern __mmask32  __cdecl _mm512_mask_cmpneq_epi16_mask(__mmask32, __m512i, __m512i);
extern __mmask32  __cdecl _mm512_mask_cmpeq_epu16_mask(__mmask32, __m512i, __m512i);
extern __mmask32  __cdecl _mm512_mask_cmpge_epu16_mask(__mmask32, __m512i, __m512i);
extern __mmask32  __cdecl _mm512_mask_cmpgt_epu16_mask(__mmask32, __m512i, __m512i);
extern __mmask32  __cdecl _mm512_mask_cmple_epu16_mask(__mmask32, __m512i, __m512i);
extern __mmask32  __cdecl _mm512_mask_cmplt_epu16_mask(__mmask32, __m512i, __m512i);
extern __mmask32  __cdecl _mm512_mask_cmpneq_epu16_mask(__mmask32, __m512i, __m512i);

extern __mmask16  __cdecl _mm512_cmpeq_epi32_mask(__m512i, __m512i);
extern __mmask16  __cdecl _mm512_cmpge_epi32_mask(__m512i, __m512i);
extern __mmask16  __cdecl _mm512_cmpgt_epi32_mask(__m512i, __m512i);
extern __mmask16  __cdecl _mm512_cmple_epi32_mask(__m512i, __m512i);
extern __mmask16  __cdecl _mm512_cmplt_epi32_mask(__m512i, __m512i);
extern __mmask16  __cdecl _mm512_cmpneq_epi32_mask(__m512i, __m512i);
extern __mmask16  __cdecl _mm512_cmpeq_epu32_mask(__m512i, __m512i);
extern __mmask16  __cdecl _mm512_cmpge_epu32_mask(__m512i, __m512i);
extern __mmask16  __cdecl _mm512_cmpgt_epu32_mask(__m512i, __m512i);
extern __mmask16  __cdecl _mm512_cmple_epu32_mask(__m512i, __m512i);
extern __mmask16  __cdecl _mm512_cmplt_epu32_mask(__m512i, __m512i);
extern __mmask16  __cdecl _mm512_cmpneq_epu32_mask(__m512i, __m512i);

extern __mmask16  __cdecl _mm512_mask_cmpeq_epi32_mask(__mmask16, __m512i, __m512i);
extern __mmask16  __cdecl _mm512_mask_cmpge_epi32_mask(__mmask16, __m512i, __m512i);
extern __mmask16  __cdecl _mm512_mask_cmpgt_epi32_mask(__mmask16, __m512i, __m512i);
extern __mmask16  __cdecl _mm512_mask_cmple_epi32_mask(__mmask16, __m512i, __m512i);
extern __mmask16  __cdecl _mm512_mask_cmplt_epi32_mask(__mmask16, __m512i, __m512i);
extern __mmask16  __cdecl _mm512_mask_cmpneq_epi32_mask(__mmask16, __m512i, __m512i);
extern __mmask16  __cdecl _mm512_mask_cmpeq_epu32_mask(__mmask16, __m512i, __m512i);
extern __mmask16  __cdecl _mm512_mask_cmpge_epu32_mask(__mmask16, __m512i, __m512i);
extern __mmask16  __cdecl _mm512_mask_cmpgt_epu32_mask(__mmask16, __m512i, __m512i);
extern __mmask16  __cdecl _mm512_mask_cmple_epu32_mask(__mmask16, __m512i, __m512i);
extern __mmask16  __cdecl _mm512_mask_cmplt_epu32_mask(__mmask16, __m512i, __m512i);
extern __mmask16  __cdecl _mm512_mask_cmpneq_epu32_mask(__mmask16, __m512i, __m512i);

extern __mmask8  __cdecl _mm512_cmpeq_epi64_mask(__m512i, __m512i);
extern __mmask8  __cdecl _mm512_cmpge_epi64_mask(__m512i, __m512i);
extern __mmask8  __cdecl _mm512_cmpgt_epi64_mask(__m512i, __m512i);
extern __mmask8  __cdecl _mm512_cmple_epi64_mask(__m512i, __m512i);
extern __mmask8  __cdecl _mm512_cmplt_epi64_mask(__m512i, __m512i);
extern __mmask8  __cdecl _mm512_cmpneq_epi64_mask(__m512i, __m512i);
extern __mmask8  __cdecl _mm512_cmpeq_epu64_mask(__m512i, __m512i);
extern __mmask8  __cdecl _mm512_cmpge_epu64_mask(__m512i, __m512i);
extern __mmask8  __cdecl _mm512_cmpgt_epu64_mask(__m512i, __m512i);
extern __mmask8  __cdecl _mm512_cmple_epu64_mask(__m512i, __m512i);
extern __mmask8  __cdecl _mm512_cmplt_epu64_mask(__m512i, __m512i);
extern __mmask8  __cdecl _mm512_cmpneq_epu64_mask(__m512i, __m512i);

extern __mmask8  __cdecl _mm512_mask_cmpeq_epi64_mask(__mmask8, __m512i, __m512i);
extern __mmask8  __cdecl _mm512_mask_cmpge_epi64_mask(__mmask8, __m512i, __m512i);
extern __mmask8  __cdecl _mm512_mask_cmpgt_epi64_mask(__mmask8, __m512i, __m512i);
extern __mmask8  __cdecl _mm512_mask_cmple_epi64_mask(__mmask8, __m512i, __m512i);
extern __mmask8  __cdecl _mm512_mask_cmplt_epi64_mask(__mmask8, __m512i, __m512i);
extern __mmask8  __cdecl _mm512_mask_cmpneq_epi64_mask(__mmask8, __m512i, __m512i);
extern __mmask8  __cdecl _mm512_mask_cmpeq_epu64_mask(__mmask8, __m512i, __m512i);
extern __mmask8  __cdecl _mm512_mask_cmpge_epu64_mask(__mmask8, __m512i, __m512i);
extern __mmask8  __cdecl _mm512_mask_cmpgt_epu64_mask(__mmask8, __m512i, __m512i);
extern __mmask8  __cdecl _mm512_mask_cmple_epu64_mask(__mmask8, __m512i, __m512i);
extern __mmask8  __cdecl _mm512_mask_cmplt_epu64_mask(__mmask8, __m512i, __m512i);
extern __mmask8  __cdecl _mm512_mask_cmpneq_epu64_mask(__mmask8, __m512i, __m512i);






























































// AVX512VL functions
extern __m128i   __cdecl _mm_mask_abs_epi16(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_abs_epi16(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_abs_epi16(__m256i, __mmask16, __m256i);
extern __m256i   __cdecl _mm256_maskz_abs_epi16(__mmask16, __m256i);
extern __m128i   __cdecl _mm_mask_abs_epi32(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_abs_epi32(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_abs_epi32(__m256i, __mmask8, __m256i);
extern __m256i   __cdecl _mm256_maskz_abs_epi32(__mmask8, __m256i);
extern __m128i   __cdecl _mm_abs_epi64(__m128i);
extern __m128i   __cdecl _mm_mask_abs_epi64(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_abs_epi64(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_abs_epi64(__m256i);
extern __m256i   __cdecl _mm256_mask_abs_epi64(__m256i, __mmask8, __m256i);
extern __m256i   __cdecl _mm256_maskz_abs_epi64(__mmask8, __m256i);
extern __m128i   __cdecl _mm_mask_abs_epi8(__m128i, __mmask16, __m128i);
extern __m128i   __cdecl _mm_maskz_abs_epi8(__mmask16, __m128i);
extern __m256i   __cdecl _mm256_mask_abs_epi8(__m256i, __mmask32, __m256i);
extern __m256i   __cdecl _mm256_maskz_abs_epi8(__mmask32, __m256i);
extern __m128i   __cdecl _mm_mask_add_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_add_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_add_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_add_epi16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_add_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_add_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_add_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_add_epi32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_add_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_add_epi64(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_add_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_add_epi64(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_add_epi8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_add_epi8(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_add_epi8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_add_epi8(__mmask32, __m256i, __m256i);
extern __m128d   __cdecl _mm_mask_add_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_add_pd(__mmask8, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_add_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_maskz_add_pd(__mmask8, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_add_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_add_ps(__mmask8, __m128, __m128);
extern __m256    __cdecl _mm256_mask_add_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_maskz_add_ps(__mmask8, __m256, __m256);
extern __m128i   __cdecl _mm_mask_adds_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_adds_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_adds_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_adds_epi16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_adds_epi8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_adds_epi8(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_adds_epi8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_adds_epi8(__mmask32, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_adds_epu16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_adds_epu16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_adds_epu16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_adds_epu16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_adds_epu8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_adds_epu8(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_adds_epu8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_adds_epu8(__mmask32, __m256i, __m256i);
extern __m128i   __cdecl _mm_alignr_epi32(__m128i, __m128i, const int);
extern __m128i   __cdecl _mm_mask_alignr_epi32(__m128i, __mmask8, __m128i, __m128i, const int);
extern __m128i   __cdecl _mm_maskz_alignr_epi32(__mmask8, __m128i, __m128i, const int);
extern __m256i   __cdecl _mm256_alignr_epi32(__m256i, __m256i, const int);
extern __m256i   __cdecl _mm256_mask_alignr_epi32(__m256i, __mmask8, __m256i, __m256i, const int);
extern __m256i   __cdecl _mm256_maskz_alignr_epi32(__mmask8, __m256i, __m256i, const int);
extern __m128i   __cdecl _mm_alignr_epi64(__m128i, __m128i, const int);
extern __m128i   __cdecl _mm_mask_alignr_epi64(__m128i, __mmask8, __m128i, __m128i, const int);
extern __m128i   __cdecl _mm_maskz_alignr_epi64(__mmask8, __m128i, __m128i, const int);
extern __m256i   __cdecl _mm256_alignr_epi64(__m256i, __m256i, const int);
extern __m256i   __cdecl _mm256_mask_alignr_epi64(__m256i, __mmask8, __m256i, __m256i, const int);
extern __m256i   __cdecl _mm256_maskz_alignr_epi64(__mmask8, __m256i, __m256i, const int);
extern __m128i   __cdecl _mm_mask_alignr_epi8(__m128i, __mmask16, __m128i, __m128i, const int);
extern __m128i   __cdecl _mm_maskz_alignr_epi8(__mmask16, __m128i, __m128i, const int);
extern __m256i   __cdecl _mm256_mask_alignr_epi8(__m256i, __mmask32, __m256i, __m256i, const int);
extern __m256i   __cdecl _mm256_maskz_alignr_epi8(__mmask32, __m256i, __m256i, const int);
extern __m128i   __cdecl _mm_and_epi32(__m128i, __m128i);
extern __m128i   __cdecl _mm_mask_and_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_and_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_and_epi32(__m256i, __m256i);
extern __m256i   __cdecl _mm256_mask_and_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_and_epi32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_and_epi64(__m128i, __m128i);
extern __m128i   __cdecl _mm_mask_and_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_and_epi64(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_and_epi64(__m256i, __m256i);
extern __m256i   __cdecl _mm256_mask_and_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_and_epi64(__mmask8, __m256i, __m256i);
extern __m128d   __cdecl _mm_mask_and_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_and_pd(__mmask8, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_and_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_maskz_and_pd(__mmask8, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_and_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_and_ps(__mmask8, __m128, __m128);
extern __m256    __cdecl _mm256_mask_and_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_maskz_and_ps(__mmask8, __m256, __m256);
extern __m128i   __cdecl _mm_andnot_epi32(__m128i, __m128i);
extern __m128i   __cdecl _mm_mask_andnot_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_andnot_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_andnot_epi32(__m256i, __m256i);
extern __m256i   __cdecl _mm256_mask_andnot_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_andnot_epi32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_andnot_epi64(__m128i, __m128i);
extern __m128i   __cdecl _mm_mask_andnot_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_andnot_epi64(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_andnot_epi64(__m256i, __m256i);
extern __m256i   __cdecl _mm256_mask_andnot_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_andnot_epi64(__mmask8, __m256i, __m256i);
extern __m128d   __cdecl _mm_mask_andnot_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_andnot_pd(__mmask8, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_andnot_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_maskz_andnot_pd(__mmask8, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_andnot_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_andnot_ps(__mmask8, __m128, __m128);
extern __m256    __cdecl _mm256_mask_andnot_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_maskz_andnot_ps(__mmask8, __m256, __m256);
extern __m128i   __cdecl _mm_mask_avg_epu16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_avg_epu16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_avg_epu16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_avg_epu16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_avg_epu8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_avg_epu8(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_avg_epu8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_avg_epu8(__mmask32, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_blend_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_blend_epi16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_blend_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_blend_epi32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_blend_epi64(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_blend_epi64(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_blend_epi8(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_blend_epi8(__mmask32, __m256i, __m256i);
extern __m128d   __cdecl _mm_mask_blend_pd(__mmask8, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_blend_pd(__mmask8, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_blend_ps(__mmask8, __m128, __m128);
extern __m256    __cdecl _mm256_mask_blend_ps(__mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_broadcast_f32x2(__m128);
extern __m256    __cdecl _mm256_mask_broadcast_f32x2(__m256, __mmask8, __m128);
extern __m256    __cdecl _mm256_maskz_broadcast_f32x2(__mmask8, __m128);
extern __m256    __cdecl _mm256_broadcast_f32x4(__m128);
extern __m256    __cdecl _mm256_mask_broadcast_f32x4(__m256, __mmask8, __m128);
extern __m256    __cdecl _mm256_maskz_broadcast_f32x4(__mmask8, __m128);
extern __m256d   __cdecl _mm256_broadcast_f64x2(__m128d);
extern __m256d   __cdecl _mm256_mask_broadcast_f64x2(__m256d, __mmask8, __m128d);
extern __m256d   __cdecl _mm256_maskz_broadcast_f64x2(__mmask8, __m128d);
extern __m128i   __cdecl _mm_broadcast_i32x2(__m128i);
extern __m128i   __cdecl _mm_mask_broadcast_i32x2(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_broadcast_i32x2(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_broadcast_i32x2(__m128i);
extern __m256i   __cdecl _mm256_mask_broadcast_i32x2(__m256i, __mmask8, __m128i);
extern __m256i   __cdecl _mm256_maskz_broadcast_i32x2(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_broadcast_i32x4(__m128i);
extern __m256i   __cdecl _mm256_mask_broadcast_i32x4(__m256i, __mmask8, __m128i);
extern __m256i   __cdecl _mm256_maskz_broadcast_i32x4(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_broadcast_i64x2(__m128i);
extern __m256i   __cdecl _mm256_mask_broadcast_i64x2(__m256i, __mmask8, __m128i);
extern __m256i   __cdecl _mm256_maskz_broadcast_i64x2(__mmask8, __m128i);
extern __m128i   __cdecl _mm_mask_broadcastb_epi8(__m128i, __mmask16, __m128i);
extern __m128i   __cdecl _mm_maskz_broadcastb_epi8(__mmask16, __m128i);
extern __m256i   __cdecl _mm256_mask_broadcastb_epi8(__m256i, __mmask32, __m128i);
extern __m256i   __cdecl _mm256_maskz_broadcastb_epi8(__mmask32, __m128i);
extern __m128i   __cdecl _mm_mask_broadcastd_epi32(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_broadcastd_epi32(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_broadcastd_epi32(__m256i, __mmask8, __m128i);
extern __m256i   __cdecl _mm256_maskz_broadcastd_epi32(__mmask8, __m128i);
extern __m128i   __cdecl _mm_broadcastmb_epi64(__mmask8);
extern __m256i   __cdecl _mm256_broadcastmb_epi64(__mmask8);
extern __m128i   __cdecl _mm_broadcastmw_epi32(__mmask16);
extern __m256i   __cdecl _mm256_broadcastmw_epi32(__mmask16);
extern __m128i   __cdecl _mm_mask_broadcastq_epi64(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_broadcastq_epi64(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_broadcastq_epi64(__m256i, __mmask8, __m128i);
extern __m256i   __cdecl _mm256_maskz_broadcastq_epi64(__mmask8, __m128i);
extern __m256d   __cdecl _mm256_mask_broadcastsd_pd(__m256d, __mmask8, __m128d);
extern __m256d   __cdecl _mm256_maskz_broadcastsd_pd(__mmask8, __m128d);
extern __m128    __cdecl _mm_mask_broadcastss_ps(__m128, __mmask8, __m128);
extern __m128    __cdecl _mm_maskz_broadcastss_ps(__mmask8, __m128);
extern __m256    __cdecl _mm256_mask_broadcastss_ps(__m256, __mmask8, __m128);
extern __m256    __cdecl _mm256_maskz_broadcastss_ps(__mmask8, __m128);
extern __m128i   __cdecl _mm_mask_broadcastw_epi16(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_broadcastw_epi16(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_broadcastw_epi16(__m256i, __mmask16, __m128i);
extern __m256i   __cdecl _mm256_maskz_broadcastw_epi16(__mmask16, __m128i);
extern __mmask8  __cdecl _mm_cmp_epi16_mask(__m128i, __m128i, const int);
extern __mmask8  __cdecl _mm_mask_cmp_epi16_mask(__mmask8, __m128i, __m128i, const int);
extern __mmask16 __cdecl _mm256_cmp_epi16_mask(__m256i, __m256i, const int);
extern __mmask16 __cdecl _mm256_mask_cmp_epi16_mask(__mmask16, __m256i, __m256i, const int);
extern __mmask8  __cdecl _mm_cmp_epi32_mask(__m128i, __m128i, int);
extern __mmask8  __cdecl _mm_mask_cmp_epi32_mask(__mmask8, __m128i, __m128i, int);
extern __mmask8  __cdecl _mm256_cmp_epi32_mask(__m256i, __m256i, int);
extern __mmask8  __cdecl _mm256_mask_cmp_epi32_mask(__mmask8, __m256i, __m256i, int);
extern __mmask8  __cdecl _mm_cmp_epi64_mask(__m128i, __m128i, int);
extern __mmask8  __cdecl _mm_mask_cmp_epi64_mask(__mmask8, __m128i, __m128i, int);
extern __mmask8  __cdecl _mm256_cmp_epi64_mask(__m256i, __m256i, int);
extern __mmask8  __cdecl _mm256_mask_cmp_epi64_mask(__mmask8, __m256i, __m256i, int);
extern __mmask16 __cdecl _mm_cmp_epi8_mask(__m128i, __m128i, const int);
extern __mmask16 __cdecl _mm_mask_cmp_epi8_mask(__mmask16, __m128i, __m128i, const int);
extern __mmask32 __cdecl _mm256_cmp_epi8_mask(__m256i, __m256i, const int);
extern __mmask32 __cdecl _mm256_mask_cmp_epi8_mask(__mmask32, __m256i, __m256i, const int);
extern __mmask8  __cdecl _mm_cmp_epu16_mask(__m128i, __m128i, const int);
extern __mmask8  __cdecl _mm_mask_cmp_epu16_mask(__mmask8, __m128i, __m128i, const int);
extern __mmask16 __cdecl _mm256_cmp_epu16_mask(__m256i, __m256i, const int);
extern __mmask16 __cdecl _mm256_mask_cmp_epu16_mask(__mmask16, __m256i, __m256i, const int);
extern __mmask8  __cdecl _mm_cmp_epu32_mask(__m128i, __m128i, int);
extern __mmask8  __cdecl _mm_mask_cmp_epu32_mask(__mmask8, __m128i, __m128i, int);
extern __mmask8  __cdecl _mm256_cmp_epu32_mask(__m256i, __m256i, int);
extern __mmask8  __cdecl _mm256_mask_cmp_epu32_mask(__mmask8, __m256i, __m256i, int);
extern __mmask8  __cdecl _mm_cmp_epu64_mask(__m128i, __m128i, int);
extern __mmask8  __cdecl _mm_mask_cmp_epu64_mask(__mmask8, __m128i, __m128i, int);
extern __mmask8  __cdecl _mm256_cmp_epu64_mask(__m256i, __m256i, int);
extern __mmask8  __cdecl _mm256_mask_cmp_epu64_mask(__mmask8, __m256i, __m256i, int);
extern __mmask16 __cdecl _mm_cmp_epu8_mask(__m128i, __m128i, const int);
extern __mmask16 __cdecl _mm_mask_cmp_epu8_mask(__mmask16, __m128i, __m128i, const int);
extern __mmask32 __cdecl _mm256_cmp_epu8_mask(__m256i, __m256i, const int);
extern __mmask32 __cdecl _mm256_mask_cmp_epu8_mask(__mmask32, __m256i, __m256i, const int);
extern __mmask8  __cdecl _mm_cmp_pd_mask(__m128d, __m128d, const int);
extern __mmask8  __cdecl _mm_mask_cmp_pd_mask(__mmask8, __m128d, __m128d, const int);
extern __mmask8  __cdecl _mm256_cmp_pd_mask(__m256d, __m256d, const int);
extern __mmask8  __cdecl _mm256_mask_cmp_pd_mask(__mmask8, __m256d, __m256d, const int);
extern __mmask8  __cdecl _mm_cmp_ps_mask(__m128, __m128, const int);
extern __mmask8  __cdecl _mm_mask_cmp_ps_mask(__mmask8, __m128, __m128, const int);
extern __mmask8  __cdecl _mm256_cmp_ps_mask(__m256, __m256, const int);
extern __mmask8  __cdecl _mm256_mask_cmp_ps_mask(__mmask8, __m256, __m256, const int);
extern __m128i   __cdecl _mm_mask_compress_epi8(__m128i, __mmask16, __m128i);
extern __m128i   __cdecl _mm_maskz_compress_epi8(__mmask16, __m128i);
extern __m256i   __cdecl _mm256_mask_compress_epi8(__m256i, __mmask32, __m256i);
extern __m256i   __cdecl _mm256_maskz_compress_epi8(__mmask32, __m256i);
extern __m128i   __cdecl _mm_mask_compress_epi16(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_compress_epi16(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_compress_epi16(__m256i, __mmask16, __m256i);
extern __m256i   __cdecl _mm256_maskz_compress_epi16(__mmask16, __m256i);
extern __m128i   __cdecl _mm_mask_compress_epi32(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_compress_epi32(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_compress_epi32(__m256i, __mmask8, __m256i);
extern __m256i   __cdecl _mm256_maskz_compress_epi32(__mmask8, __m256i);
extern __m128i   __cdecl _mm_mask_compress_epi64(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_compress_epi64(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_compress_epi64(__m256i, __mmask8, __m256i);
extern __m256i   __cdecl _mm256_maskz_compress_epi64(__mmask8, __m256i);
extern __m128d   __cdecl _mm_mask_compress_pd(__m128d, __mmask8, __m128d);
extern __m128d   __cdecl _mm_maskz_compress_pd(__mmask8, __m128d);
extern __m256d   __cdecl _mm256_mask_compress_pd(__m256d, __mmask8, __m256d);
extern __m256d   __cdecl _mm256_maskz_compress_pd(__mmask8, __m256d);
extern __m128    __cdecl _mm_mask_compress_ps(__m128, __mmask8, __m128);
extern __m128    __cdecl _mm_maskz_compress_ps(__mmask8, __m128);
extern __m256    __cdecl _mm256_mask_compress_ps(__m256, __mmask8, __m256);
extern __m256    __cdecl _mm256_maskz_compress_ps(__mmask8, __m256);
extern void      __cdecl _mm_mask_compressstoreu_epi8(void*, __mmask16, __m128i);
extern void      __cdecl _mm256_mask_compressstoreu_epi8(void*, __mmask32, __m256i);
extern void      __cdecl _mm_mask_compressstoreu_epi16(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_compressstoreu_epi16(void*, __mmask16, __m256i);
extern void      __cdecl _mm_mask_compressstoreu_epi32(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_compressstoreu_epi32(void*, __mmask8, __m256i);
extern void      __cdecl _mm_mask_compressstoreu_epi64(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_compressstoreu_epi64(void*, __mmask8, __m256i);
extern void      __cdecl _mm_mask_compressstoreu_pd(void*, __mmask8, __m128d);
extern void      __cdecl _mm256_mask_compressstoreu_pd(void*, __mmask8, __m256d);
extern void      __cdecl _mm_mask_compressstoreu_ps(void*, __mmask8, __m128);
extern void      __cdecl _mm256_mask_compressstoreu_ps(void*, __mmask8, __m256);
extern __m128i   __cdecl _mm_conflict_epi32(__m128i);
extern __m128i   __cdecl _mm_mask_conflict_epi32(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_conflict_epi32(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_conflict_epi32(__m256i);
extern __m256i   __cdecl _mm256_mask_conflict_epi32(__m256i, __mmask8, __m256i);
extern __m256i   __cdecl _mm256_maskz_conflict_epi32(__mmask8, __m256i);
extern __m128i   __cdecl _mm_conflict_epi64(__m128i);
extern __m128i   __cdecl _mm_mask_conflict_epi64(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_conflict_epi64(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_conflict_epi64(__m256i);
extern __m256i   __cdecl _mm256_mask_conflict_epi64(__m256i, __mmask8, __m256i);
extern __m256i   __cdecl _mm256_maskz_conflict_epi64(__mmask8, __m256i);
extern __m128i   __cdecl _mm_mask_cvtps_ph(__m128i, __mmask8, __m128, int);
extern __m128i   __cdecl _mm_maskz_cvtps_ph(__mmask8, __m128, int);
extern __m128i   __cdecl _mm_mask_cvt_roundps_ph(__m128i, __mmask8, __m128, int);
extern __m128i   __cdecl _mm_maskz_cvt_roundps_ph(__mmask8, __m128, int);
extern __m128i   __cdecl _mm256_mask_cvtps_ph(__m128i, __mmask8, __m256, int);
extern __m128i   __cdecl _mm256_maskz_cvtps_ph(__mmask8, __m256, int);
extern __m128i   __cdecl _mm256_mask_cvt_roundps_ph(__m128i, __mmask8, __m256, int);
extern __m128i   __cdecl _mm256_maskz_cvt_roundps_ph(__mmask8, __m256, int);
extern __m128i   __cdecl _mm_mask_cvtepi16_epi32(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepi16_epi32(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_cvtepi16_epi32(__m256i, __mmask8, __m128i);
extern __m256i   __cdecl _mm256_maskz_cvtepi16_epi32(__mmask8, __m128i);
extern __m128i   __cdecl _mm_mask_cvtepi16_epi64(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepi16_epi64(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_cvtepi16_epi64(__m256i, __mmask8, __m128i);
extern __m256i   __cdecl _mm256_maskz_cvtepi16_epi64(__mmask8, __m128i);
extern __m128i   __cdecl _mm_cvtepi16_epi8(__m128i);
extern __m128i   __cdecl _mm_mask_cvtepi16_epi8(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepi16_epi8(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtepi16_epi8(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtepi16_epi8(__m128i, __mmask16, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtepi16_epi8(__mmask16, __m256i);
extern void      __cdecl _mm_mask_cvtepi16_storeu_epi8(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtepi16_storeu_epi8(void*, __mmask16, __m256i);
extern __m128i   __cdecl _mm_cvtepi32_epi16(__m128i);
extern __m128i   __cdecl _mm_mask_cvtepi32_epi16(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepi32_epi16(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtepi32_epi16(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtepi32_epi16(__m128i, __mmask8, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtepi32_epi16(__mmask8, __m256i);
extern __m128i   __cdecl _mm_mask_cvtepi32_epi64(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepi32_epi64(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_cvtepi32_epi64(__m256i, __mmask8, __m128i);
extern __m256i   __cdecl _mm256_maskz_cvtepi32_epi64(__mmask8, __m128i);
extern __m128i   __cdecl _mm_cvtepi32_epi8(__m128i);
extern __m128i   __cdecl _mm_mask_cvtepi32_epi8(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepi32_epi8(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtepi32_epi8(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtepi32_epi8(__m128i, __mmask8, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtepi32_epi8(__mmask8, __m256i);
extern __m128d   __cdecl _mm_mask_cvtepi32_pd(__m128d, __mmask8, __m128i);
extern __m128d   __cdecl _mm_maskz_cvtepi32_pd(__mmask8, __m128i);
extern __m256d   __cdecl _mm256_mask_cvtepi32_pd(__m256d, __mmask8, __m128i);
extern __m256d   __cdecl _mm256_maskz_cvtepi32_pd(__mmask8, __m128i);
extern __m128    __cdecl _mm_mask_cvtepi32_ps(__m128, __mmask8, __m128i);
extern __m128    __cdecl _mm_maskz_cvtepi32_ps(__mmask8, __m128i);
extern __m256    __cdecl _mm256_mask_cvtepi32_ps(__m256, __mmask8, __m256i);
extern __m256    __cdecl _mm256_maskz_cvtepi32_ps(__mmask8, __m256i);
extern void      __cdecl _mm_mask_cvtepi32_storeu_epi16(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtepi32_storeu_epi16(void*, __mmask8, __m256i);
extern void      __cdecl _mm_mask_cvtepi32_storeu_epi8(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtepi32_storeu_epi8(void*, __mmask8, __m256i);
extern __m128i   __cdecl _mm_cvtepi64_epi16(__m128i);
extern __m128i   __cdecl _mm_mask_cvtepi64_epi16(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepi64_epi16(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtepi64_epi16(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtepi64_epi16(__m128i, __mmask8, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtepi64_epi16(__mmask8, __m256i);
extern __m128i   __cdecl _mm_cvtepi64_epi32(__m128i);
extern __m128i   __cdecl _mm_mask_cvtepi64_epi32(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepi64_epi32(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtepi64_epi32(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtepi64_epi32(__m128i, __mmask8, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtepi64_epi32(__mmask8, __m256i);
extern __m128i   __cdecl _mm_cvtepi64_epi8(__m128i);
extern __m128i   __cdecl _mm_mask_cvtepi64_epi8(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepi64_epi8(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtepi64_epi8(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtepi64_epi8(__m128i, __mmask8, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtepi64_epi8(__mmask8, __m256i);
extern __m128d   __cdecl _mm_cvtepi64_pd(__m128i);
extern __m128d   __cdecl _mm_mask_cvtepi64_pd(__m128d, __mmask8, __m128i);
extern __m128d   __cdecl _mm_maskz_cvtepi64_pd(__mmask8, __m128i);
extern __m256d   __cdecl _mm256_cvtepi64_pd(__m256i);
extern __m256d   __cdecl _mm256_mask_cvtepi64_pd(__m256d, __mmask8, __m256i);
extern __m256d   __cdecl _mm256_maskz_cvtepi64_pd(__mmask8, __m256i);
extern __m128    __cdecl _mm_cvtepi64_ps(__m128i);
extern __m128    __cdecl _mm_mask_cvtepi64_ps(__m128, __mmask8, __m128i);
extern __m128    __cdecl _mm_maskz_cvtepi64_ps(__mmask8, __m128i);
extern __m128    __cdecl _mm256_cvtepi64_ps(__m256i);
extern __m128    __cdecl _mm256_mask_cvtepi64_ps(__m128, __mmask8, __m256i);
extern __m128    __cdecl _mm256_maskz_cvtepi64_ps(__mmask8, __m256i);
extern void      __cdecl _mm_mask_cvtepi64_storeu_epi16(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtepi64_storeu_epi16(void*, __mmask8, __m256i);
extern void      __cdecl _mm_mask_cvtepi64_storeu_epi32(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtepi64_storeu_epi32(void*, __mmask8, __m256i);
extern void      __cdecl _mm_mask_cvtepi64_storeu_epi8(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtepi64_storeu_epi8(void*, __mmask8, __m256i);
extern __m128i   __cdecl _mm_mask_cvtepi8_epi16(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepi8_epi16(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_cvtepi8_epi16(__m256i, __mmask16, __m128i);
extern __m256i   __cdecl _mm256_maskz_cvtepi8_epi16(__mmask16, __m128i);
extern __m128i   __cdecl _mm_mask_cvtepi8_epi32(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepi8_epi32(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_cvtepi8_epi32(__m256i, __mmask8, __m128i);
extern __m256i   __cdecl _mm256_maskz_cvtepi8_epi32(__mmask8, __m128i);
extern __m128i   __cdecl _mm_mask_cvtepi8_epi64(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepi8_epi64(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_cvtepi8_epi64(__m256i, __mmask8, __m128i);
extern __m256i   __cdecl _mm256_maskz_cvtepi8_epi64(__mmask8, __m128i);
extern __m128i   __cdecl _mm_mask_cvtepu16_epi32(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepu16_epi32(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_cvtepu16_epi32(__m256i, __mmask8, __m128i);
extern __m256i   __cdecl _mm256_maskz_cvtepu16_epi32(__mmask8, __m128i);
extern __m128i   __cdecl _mm_mask_cvtepu16_epi64(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepu16_epi64(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_cvtepu16_epi64(__m256i, __mmask8, __m128i);
extern __m256i   __cdecl _mm256_maskz_cvtepu16_epi64(__mmask8, __m128i);
extern __m128i   __cdecl _mm_mask_cvtepu32_epi64(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepu32_epi64(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_cvtepu32_epi64(__m256i, __mmask8, __m128i);
extern __m256i   __cdecl _mm256_maskz_cvtepu32_epi64(__mmask8, __m128i);
extern __m128    __cdecl _mm_cvtepu32_ps(__m128i);
extern __m128    __cdecl _mm_mask_cvtepu32_ps(__m128, __mmask8, __m128i);
extern __m128    __cdecl _mm_maskz_cvtepu32_ps(__mmask8, __m128i);
extern __m256    __cdecl _mm256_cvtepu32_ps(__m256i);
extern __m256    __cdecl _mm256_mask_cvtepu32_ps(__m256, __mmask8, __m256i);
extern __m256    __cdecl _mm256_maskz_cvtepu32_ps(__mmask8, __m256i);
extern __m128d   __cdecl _mm_cvtepu32_pd(__m128i);
extern __m128d   __cdecl _mm_mask_cvtepu32_pd(__m128d, __mmask8, __m128i);
extern __m128d   __cdecl _mm_maskz_cvtepu32_pd(__mmask8, __m128i);
extern __m256d   __cdecl _mm256_cvtepu32_pd(__m128i);
extern __m256d   __cdecl _mm256_mask_cvtepu32_pd(__m256d, __mmask8, __m128i);
extern __m256d   __cdecl _mm256_maskz_cvtepu32_pd(__mmask8, __m128i);
extern __m128d   __cdecl _mm_cvtepu64_pd(__m128i);
extern __m128d   __cdecl _mm_mask_cvtepu64_pd(__m128d, __mmask8, __m128i);
extern __m128d   __cdecl _mm_maskz_cvtepu64_pd(__mmask8, __m128i);
extern __m256d   __cdecl _mm256_cvtepu64_pd(__m256i);
extern __m256d   __cdecl _mm256_mask_cvtepu64_pd(__m256d, __mmask8, __m256i);
extern __m256d   __cdecl _mm256_maskz_cvtepu64_pd(__mmask8, __m256i);
extern __m128    __cdecl _mm_cvtepu64_ps(__m128i);
extern __m128    __cdecl _mm_mask_cvtepu64_ps(__m128, __mmask8, __m128i);
extern __m128    __cdecl _mm_maskz_cvtepu64_ps(__mmask8, __m128i);
extern __m128    __cdecl _mm256_cvtepu64_ps(__m256i);
extern __m128    __cdecl _mm256_mask_cvtepu64_ps(__m128, __mmask8, __m256i);
extern __m128    __cdecl _mm256_maskz_cvtepu64_ps(__mmask8, __m256i);
extern __m128i   __cdecl _mm_mask_cvtepu8_epi16(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepu8_epi16(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_cvtepu8_epi16(__m256i, __mmask16, __m128i);
extern __m256i   __cdecl _mm256_maskz_cvtepu8_epi16(__mmask16, __m128i);
extern __m128i   __cdecl _mm_mask_cvtepu8_epi32(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepu8_epi32(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_cvtepu8_epi32(__m256i, __mmask8, __m128i);
extern __m256i   __cdecl _mm256_maskz_cvtepu8_epi32(__mmask8, __m128i);
extern __m128i   __cdecl _mm_mask_cvtepu8_epi64(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtepu8_epi64(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_cvtepu8_epi64(__m256i, __mmask8, __m128i);
extern __m256i   __cdecl _mm256_maskz_cvtepu8_epi64(__mmask8, __m128i);
extern __m128i   __cdecl _mm_mask_cvtpd_epi32(__m128i, __mmask8, __m128d);
extern __m128i   __cdecl _mm_maskz_cvtpd_epi32(__mmask8, __m128d);
extern __m128i   __cdecl _mm256_mask_cvtpd_epi32(__m128i, __mmask8, __m256d);
extern __m128i   __cdecl _mm256_maskz_cvtpd_epi32(__mmask8, __m256d);
extern __m128i   __cdecl _mm_cvtpd_epi64(__m128d);
extern __m128i   __cdecl _mm_mask_cvtpd_epi64(__m128i, __mmask8, __m128d);
extern __m128i   __cdecl _mm_maskz_cvtpd_epi64(__mmask8, __m128d);
extern __m256i   __cdecl _mm256_cvtpd_epi64(__m256d);
extern __m256i   __cdecl _mm256_mask_cvtpd_epi64(__m256i, __mmask8, __m256d);
extern __m256i   __cdecl _mm256_maskz_cvtpd_epi64(__mmask8, __m256d);
extern __m128i   __cdecl _mm_cvtpd_epu32(__m128d);
extern __m128i   __cdecl _mm_mask_cvtpd_epu32(__m128i, __mmask8, __m128d);
extern __m128i   __cdecl _mm_maskz_cvtpd_epu32(__mmask8, __m128d);
extern __m128i   __cdecl _mm256_cvtpd_epu32(__m256d);
extern __m128i   __cdecl _mm256_mask_cvtpd_epu32(__m128i, __mmask8, __m256d);
extern __m128i   __cdecl _mm256_maskz_cvtpd_epu32(__mmask8, __m256d);
extern __m128i   __cdecl _mm_cvtpd_epu64(__m128d);
extern __m128i   __cdecl _mm_mask_cvtpd_epu64(__m128i, __mmask8, __m128d);
extern __m128i   __cdecl _mm_maskz_cvtpd_epu64(__mmask8, __m128d);
extern __m256i   __cdecl _mm256_cvtpd_epu64(__m256d);
extern __m256i   __cdecl _mm256_mask_cvtpd_epu64(__m256i, __mmask8, __m256d);
extern __m256i   __cdecl _mm256_maskz_cvtpd_epu64(__mmask8, __m256d);
extern __m128    __cdecl _mm_mask_cvtpd_ps(__m128, __mmask8, __m128d);
extern __m128    __cdecl _mm_maskz_cvtpd_ps(__mmask8, __m128d);
extern __m128    __cdecl _mm256_mask_cvtpd_ps(__m128, __mmask8, __m256d);
extern __m128    __cdecl _mm256_maskz_cvtpd_ps(__mmask8, __m256d);
extern __m128    __cdecl _mm_mask_cvtph_ps(__m128, __mmask8, __m128i);
extern __m128    __cdecl _mm_maskz_cvtph_ps(__mmask8, __m128i);
extern __m256    __cdecl _mm256_mask_cvtph_ps(__m256, __mmask8, __m128i);
extern __m256    __cdecl _mm256_maskz_cvtph_ps(__mmask8, __m128i);
extern __m128i   __cdecl _mm_mask_cvtps_epi32(__m128i, __mmask8, __m128);
extern __m128i   __cdecl _mm_maskz_cvtps_epi32(__mmask8, __m128);
extern __m256i   __cdecl _mm256_mask_cvtps_epi32(__m256i, __mmask8, __m256);
extern __m256i   __cdecl _mm256_maskz_cvtps_epi32(__mmask8, __m256);
extern __m128i   __cdecl _mm_cvtps_epi64(__m128);
extern __m128i   __cdecl _mm_mask_cvtps_epi64(__m128i, __mmask8, __m128);
extern __m128i   __cdecl _mm_maskz_cvtps_epi64(__mmask8, __m128);
extern __m256i   __cdecl _mm256_cvtps_epi64(__m128);
extern __m256i   __cdecl _mm256_mask_cvtps_epi64(__m256i, __mmask8, __m128);
extern __m256i   __cdecl _mm256_maskz_cvtps_epi64(__mmask8, __m128);
extern __m128i   __cdecl _mm_cvtps_epu32(__m128);
extern __m128i   __cdecl _mm_mask_cvtps_epu32(__m128i, __mmask8, __m128);
extern __m128i   __cdecl _mm_maskz_cvtps_epu32(__mmask8, __m128);
extern __m256i   __cdecl _mm256_cvtps_epu32(__m256);
extern __m256i   __cdecl _mm256_mask_cvtps_epu32(__m256i, __mmask8, __m256);
extern __m256i   __cdecl _mm256_maskz_cvtps_epu32(__mmask8, __m256);
extern __m128i   __cdecl _mm_cvtps_epu64(__m128);
extern __m128i   __cdecl _mm_mask_cvtps_epu64(__m128i, __mmask8, __m128);
extern __m128i   __cdecl _mm_maskz_cvtps_epu64(__mmask8, __m128);
extern __m256i   __cdecl _mm256_cvtps_epu64(__m128);
extern __m256i   __cdecl _mm256_mask_cvtps_epu64(__m256i, __mmask8, __m128);
extern __m256i   __cdecl _mm256_maskz_cvtps_epu64(__mmask8, __m128);
extern __m128d   __cdecl _mm_mask_cvtps_pd(__m128d, __mmask8, __m128);
extern __m128d   __cdecl _mm_maskz_cvtps_pd(__mmask8, __m128);
extern __m256d   __cdecl _mm256_mask_cvtps_pd(__m256d, __mmask8, __m128);
extern __m256d   __cdecl _mm256_maskz_cvtps_pd(__mmask8, __m128);
extern __m128i   __cdecl _mm_cvtsepi16_epi8(__m128i);
extern __m128i   __cdecl _mm_mask_cvtsepi16_epi8(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtsepi16_epi8(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtsepi16_epi8(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtsepi16_epi8(__m128i, __mmask16, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtsepi16_epi8(__mmask16, __m256i);
extern void      __cdecl _mm_mask_cvtsepi16_storeu_epi8(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtsepi16_storeu_epi8(void*, __mmask16, __m256i);
extern __m128i   __cdecl _mm_cvtsepi32_epi16(__m128i);
extern __m128i   __cdecl _mm_mask_cvtsepi32_epi16(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtsepi32_epi16(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtsepi32_epi16(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtsepi32_epi16(__m128i, __mmask8, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtsepi32_epi16(__mmask8, __m256i);
extern __m128i   __cdecl _mm_cvtsepi32_epi8(__m128i);
extern __m128i   __cdecl _mm_mask_cvtsepi32_epi8(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtsepi32_epi8(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtsepi32_epi8(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtsepi32_epi8(__m128i, __mmask8, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtsepi32_epi8(__mmask8, __m256i);
extern void      __cdecl _mm_mask_cvtsepi32_storeu_epi16(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtsepi32_storeu_epi16(void*, __mmask8, __m256i);
extern void      __cdecl _mm_mask_cvtsepi32_storeu_epi8(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtsepi32_storeu_epi8(void*, __mmask8, __m256i);
extern __m128i   __cdecl _mm_cvtsepi64_epi16(__m128i);
extern __m128i   __cdecl _mm_mask_cvtsepi64_epi16(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtsepi64_epi16(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtsepi64_epi16(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtsepi64_epi16(__m128i, __mmask8, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtsepi64_epi16(__mmask8, __m256i);
extern __m128i   __cdecl _mm_cvtsepi64_epi32(__m128i);
extern __m128i   __cdecl _mm_mask_cvtsepi64_epi32(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtsepi64_epi32(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtsepi64_epi32(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtsepi64_epi32(__m128i, __mmask8, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtsepi64_epi32(__mmask8, __m256i);
extern __m128i   __cdecl _mm_cvtsepi64_epi8(__m128i);
extern __m128i   __cdecl _mm_mask_cvtsepi64_epi8(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtsepi64_epi8(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtsepi64_epi8(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtsepi64_epi8(__m128i, __mmask8, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtsepi64_epi8(__mmask8, __m256i);
extern void      __cdecl _mm_mask_cvtsepi64_storeu_epi16(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtsepi64_storeu_epi16(void*, __mmask8, __m256i);
extern void      __cdecl _mm_mask_cvtsepi64_storeu_epi32(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtsepi64_storeu_epi32(void*, __mmask8, __m256i);
extern void      __cdecl _mm_mask_cvtsepi64_storeu_epi8(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtsepi64_storeu_epi8(void*, __mmask8, __m256i);
extern __m128i   __cdecl _mm_mask_cvttpd_epi32(__m128i, __mmask8, __m128d);
extern __m128i   __cdecl _mm_maskz_cvttpd_epi32(__mmask8, __m128d);
extern __m128i   __cdecl _mm256_mask_cvttpd_epi32(__m128i, __mmask8, __m256d);
extern __m128i   __cdecl _mm256_maskz_cvttpd_epi32(__mmask8, __m256d);
extern __m128i   __cdecl _mm_cvttpd_epi64(__m128d);
extern __m128i   __cdecl _mm_mask_cvttpd_epi64(__m128i, __mmask8, __m128d);
extern __m128i   __cdecl _mm_maskz_cvttpd_epi64(__mmask8, __m128d);
extern __m256i   __cdecl _mm256_cvttpd_epi64(__m256d);
extern __m256i   __cdecl _mm256_mask_cvttpd_epi64(__m256i, __mmask8, __m256d);
extern __m256i   __cdecl _mm256_maskz_cvttpd_epi64(__mmask8, __m256d);
extern __m128i   __cdecl _mm_cvttpd_epu32(__m128d);
extern __m128i   __cdecl _mm_mask_cvttpd_epu32(__m128i, __mmask8, __m128d);
extern __m128i   __cdecl _mm_maskz_cvttpd_epu32(__mmask8, __m128d);
extern __m128i   __cdecl _mm256_cvttpd_epu32(__m256d);
extern __m128i   __cdecl _mm256_mask_cvttpd_epu32(__m128i, __mmask8, __m256d);
extern __m128i   __cdecl _mm256_maskz_cvttpd_epu32(__mmask8, __m256d);
extern __m128i   __cdecl _mm_cvttpd_epu64(__m128d);
extern __m128i   __cdecl _mm_mask_cvttpd_epu64(__m128i, __mmask8, __m128d);
extern __m128i   __cdecl _mm_maskz_cvttpd_epu64(__mmask8, __m128d);
extern __m256i   __cdecl _mm256_cvttpd_epu64(__m256d);
extern __m256i   __cdecl _mm256_mask_cvttpd_epu64(__m256i, __mmask8, __m256d);
extern __m256i   __cdecl _mm256_maskz_cvttpd_epu64(__mmask8, __m256d);
extern __m128i   __cdecl _mm_mask_cvttps_epi32(__m128i, __mmask8, __m128);
extern __m128i   __cdecl _mm_maskz_cvttps_epi32(__mmask8, __m128);
extern __m256i   __cdecl _mm256_mask_cvttps_epi32(__m256i, __mmask8, __m256);
extern __m256i   __cdecl _mm256_maskz_cvttps_epi32(__mmask8, __m256);
extern __m128i   __cdecl _mm_cvttps_epi64(__m128);
extern __m128i   __cdecl _mm_mask_cvttps_epi64(__m128i, __mmask8, __m128);
extern __m128i   __cdecl _mm_maskz_cvttps_epi64(__mmask8, __m128);
extern __m256i   __cdecl _mm256_cvttps_epi64(__m128);
extern __m256i   __cdecl _mm256_mask_cvttps_epi64(__m256i, __mmask8, __m128);
extern __m256i   __cdecl _mm256_maskz_cvttps_epi64(__mmask8, __m128);
extern __m128i   __cdecl _mm_cvttps_epu32(__m128);
extern __m128i   __cdecl _mm_mask_cvttps_epu32(__m128i, __mmask8, __m128);
extern __m128i   __cdecl _mm_maskz_cvttps_epu32(__mmask8, __m128);
extern __m256i   __cdecl _mm256_cvttps_epu32(__m256);
extern __m256i   __cdecl _mm256_mask_cvttps_epu32(__m256i, __mmask8, __m256);
extern __m256i   __cdecl _mm256_maskz_cvttps_epu32(__mmask8, __m256);
extern __m128i   __cdecl _mm_cvttps_epu64(__m128);
extern __m128i   __cdecl _mm_mask_cvttps_epu64(__m128i, __mmask8, __m128);
extern __m128i   __cdecl _mm_maskz_cvttps_epu64(__mmask8, __m128);
extern __m256i   __cdecl _mm256_cvttps_epu64(__m128);
extern __m256i   __cdecl _mm256_mask_cvttps_epu64(__m256i, __mmask8, __m128);
extern __m256i   __cdecl _mm256_maskz_cvttps_epu64(__mmask8, __m128);
extern __m128i   __cdecl _mm_cvtusepi16_epi8(__m128i);
extern __m128i   __cdecl _mm_mask_cvtusepi16_epi8(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtusepi16_epi8(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtusepi16_epi8(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtusepi16_epi8(__m128i, __mmask16, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtusepi16_epi8(__mmask16, __m256i);
extern void      __cdecl _mm_mask_cvtusepi16_storeu_epi8(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtusepi16_storeu_epi8(void*, __mmask16, __m256i);
extern __m128i   __cdecl _mm_cvtusepi32_epi16(__m128i);
extern __m128i   __cdecl _mm_mask_cvtusepi32_epi16(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtusepi32_epi16(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtusepi32_epi16(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtusepi32_epi16(__m128i, __mmask8, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtusepi32_epi16(__mmask8, __m256i);
extern __m128i   __cdecl _mm_cvtusepi32_epi8(__m128i);
extern __m128i   __cdecl _mm_mask_cvtusepi32_epi8(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtusepi32_epi8(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtusepi32_epi8(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtusepi32_epi8(__m128i, __mmask8, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtusepi32_epi8(__mmask8, __m256i);
extern void      __cdecl _mm_mask_cvtusepi32_storeu_epi16(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtusepi32_storeu_epi16(void*, __mmask8, __m256i);
extern void      __cdecl _mm_mask_cvtusepi32_storeu_epi8(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtusepi32_storeu_epi8(void*, __mmask8, __m256i);
extern __m128i   __cdecl _mm_cvtusepi64_epi16(__m128i);
extern __m128i   __cdecl _mm_mask_cvtusepi64_epi16(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtusepi64_epi16(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtusepi64_epi16(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtusepi64_epi16(__m128i, __mmask8, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtusepi64_epi16(__mmask8, __m256i);
extern __m128i   __cdecl _mm_cvtusepi64_epi32(__m128i);
extern __m128i   __cdecl _mm_mask_cvtusepi64_epi32(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtusepi64_epi32(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtusepi64_epi32(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtusepi64_epi32(__m128i, __mmask8, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtusepi64_epi32(__mmask8, __m256i);
extern __m128i   __cdecl _mm_cvtusepi64_epi8(__m128i);
extern __m128i   __cdecl _mm_mask_cvtusepi64_epi8(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_cvtusepi64_epi8(__mmask8, __m128i);
extern __m128i   __cdecl _mm256_cvtusepi64_epi8(__m256i);
extern __m128i   __cdecl _mm256_mask_cvtusepi64_epi8(__m128i, __mmask8, __m256i);
extern __m128i   __cdecl _mm256_maskz_cvtusepi64_epi8(__mmask8, __m256i);
extern void      __cdecl _mm_mask_cvtusepi64_storeu_epi16(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtusepi64_storeu_epi16(void*, __mmask8, __m256i);
extern void      __cdecl _mm_mask_cvtusepi64_storeu_epi32(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtusepi64_storeu_epi32(void*, __mmask8, __m256i);
extern void      __cdecl _mm_mask_cvtusepi64_storeu_epi8(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_cvtusepi64_storeu_epi8(void*, __mmask8, __m256i);
extern __m128i   __cdecl _mm_dbsad_epu8(__m128i, __m128i, int);
extern __m128i   __cdecl _mm_mask_dbsad_epu8(__m128i, __mmask8, __m128i, __m128i, int);
extern __m128i   __cdecl _mm_maskz_dbsad_epu8(__mmask8, __m128i, __m128i, int);
extern __m256i   __cdecl _mm256_dbsad_epu8(__m256i, __m256i, int);
extern __m256i   __cdecl _mm256_mask_dbsad_epu8(__m256i, __mmask16, __m256i, __m256i, int);
extern __m256i   __cdecl _mm256_maskz_dbsad_epu8(__mmask16, __m256i, __m256i, int);
extern __m128d   __cdecl _mm_mask_div_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_div_pd(__mmask8, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_div_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_maskz_div_pd(__mmask8, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_div_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_div_ps(__mmask8, __m128, __m128);
extern __m256    __cdecl _mm256_mask_div_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_maskz_div_ps(__mmask8, __m256, __m256);
extern __m128i   __cdecl _mm_mask_expand_epi8(__m128i, __mmask16, __m128i);
extern __m128i   __cdecl _mm_maskz_expand_epi8(__mmask16, __m128i);
extern __m256i   __cdecl _mm256_mask_expand_epi8(__m256i, __mmask32, __m256i);
extern __m256i   __cdecl _mm256_maskz_expand_epi8(__mmask32, __m256i);
extern __m128i   __cdecl _mm_mask_expand_epi16(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_expand_epi16(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_expand_epi16(__m256i, __mmask16, __m256i);
extern __m256i   __cdecl _mm256_maskz_expand_epi16(__mmask16, __m256i);
extern __m128i   __cdecl _mm_mask_expand_epi32(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_expand_epi32(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_expand_epi32(__m256i, __mmask8, __m256i);
extern __m256i   __cdecl _mm256_maskz_expand_epi32(__mmask8, __m256i);
extern __m128i   __cdecl _mm_mask_expand_epi64(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_expand_epi64(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_expand_epi64(__m256i, __mmask8, __m256i);
extern __m256i   __cdecl _mm256_maskz_expand_epi64(__mmask8, __m256i);
extern __m128d   __cdecl _mm_mask_expand_pd(__m128d, __mmask8, __m128d);
extern __m128d   __cdecl _mm_maskz_expand_pd(__mmask8, __m128d);
extern __m256d   __cdecl _mm256_mask_expand_pd(__m256d, __mmask8, __m256d);
extern __m256d   __cdecl _mm256_maskz_expand_pd(__mmask8, __m256d);
extern __m128    __cdecl _mm_mask_expand_ps(__m128, __mmask8, __m128);
extern __m128    __cdecl _mm_maskz_expand_ps(__mmask8, __m128);
extern __m256    __cdecl _mm256_mask_expand_ps(__m256, __mmask8, __m256);
extern __m256    __cdecl _mm256_maskz_expand_ps(__mmask8, __m256);
extern __m128i   __cdecl _mm_mask_expandloadu_epi8(__m128i, __mmask16, const void*);
extern __m128i   __cdecl _mm_maskz_expandloadu_epi8(__mmask16, const void*);
extern __m256i   __cdecl _mm256_mask_expandloadu_epi8(__m256i, __mmask32, const void*);
extern __m256i   __cdecl _mm256_maskz_expandloadu_epi8(__mmask32, const void*);
extern __m128i   __cdecl _mm_mask_expandloadu_epi16(__m128i, __mmask8, const void*);
extern __m128i   __cdecl _mm_maskz_expandloadu_epi16(__mmask8, const void*);
extern __m256i   __cdecl _mm256_mask_expandloadu_epi16(__m256i, __mmask16, const void*);
extern __m256i   __cdecl _mm256_maskz_expandloadu_epi16(__mmask16, const void*);
extern __m128i   __cdecl _mm_mask_expandloadu_epi32(__m128i, __mmask8, void const*);
extern __m128i   __cdecl _mm_maskz_expandloadu_epi32(__mmask8, void const*);
extern __m256i   __cdecl _mm256_mask_expandloadu_epi32(__m256i, __mmask8, void const*);
extern __m256i   __cdecl _mm256_maskz_expandloadu_epi32(__mmask8, void const*);
extern __m128i   __cdecl _mm_mask_expandloadu_epi64(__m128i, __mmask8, void const*);
extern __m128i   __cdecl _mm_maskz_expandloadu_epi64(__mmask8, void const*);
extern __m256i   __cdecl _mm256_mask_expandloadu_epi64(__m256i, __mmask8, void const*);
extern __m256i   __cdecl _mm256_maskz_expandloadu_epi64(__mmask8, void const*);
extern __m128d   __cdecl _mm_mask_expandloadu_pd(__m128d, __mmask8, void const*);
extern __m128d   __cdecl _mm_maskz_expandloadu_pd(__mmask8, void const*);
extern __m256d   __cdecl _mm256_mask_expandloadu_pd(__m256d, __mmask8, void const*);
extern __m256d   __cdecl _mm256_maskz_expandloadu_pd(__mmask8, void const*);
extern __m128    __cdecl _mm_mask_expandloadu_ps(__m128, __mmask8, void const*);
extern __m128    __cdecl _mm_maskz_expandloadu_ps(__mmask8, void const*);
extern __m256    __cdecl _mm256_mask_expandloadu_ps(__m256, __mmask8, void const*);
extern __m256    __cdecl _mm256_maskz_expandloadu_ps(__mmask8, void const*);
extern __m128    __cdecl _mm256_extractf32x4_ps(__m256, int);
extern __m128    __cdecl _mm256_mask_extractf32x4_ps(__m128, __mmask8, __m256, int);
extern __m128    __cdecl _mm256_maskz_extractf32x4_ps(__mmask8, __m256, int);
extern __m128d   __cdecl _mm256_extractf64x2_pd(__m256d, int);
extern __m128d   __cdecl _mm256_mask_extractf64x2_pd(__m128d, __mmask8, __m256d, int);
extern __m128d   __cdecl _mm256_maskz_extractf64x2_pd(__mmask8, __m256d, int);
extern __m128i   __cdecl _mm256_extracti32x4_epi32(__m256i, int);
extern __m128i   __cdecl _mm256_mask_extracti32x4_epi32(__m128i, __mmask8, __m256i, int);
extern __m128i   __cdecl _mm256_maskz_extracti32x4_epi32(__mmask8, __m256i, int);
extern __m128i   __cdecl _mm256_extracti64x2_epi64(__m256i, int);
extern __m128i   __cdecl _mm256_mask_extracti64x2_epi64(__m128i, __mmask8, __m256i, int);
extern __m128i   __cdecl _mm256_maskz_extracti64x2_epi64(__mmask8, __m256i, int);
extern __m128d   __cdecl _mm_fixupimm_pd(__m128d, __m128d, __m128i, const int);
extern __m128d   __cdecl _mm_mask_fixupimm_pd(__m128d, __mmask8, __m128d, __m128i, const int);
extern __m128d   __cdecl _mm_maskz_fixupimm_pd(__mmask8, __m128d, __m128d, __m128i, const int);
extern __m256d   __cdecl _mm256_fixupimm_pd(__m256d, __m256d, __m256i, const int);
extern __m256d   __cdecl _mm256_mask_fixupimm_pd(__m256d, __mmask8, __m256d, __m256i, const int);
extern __m256d   __cdecl _mm256_maskz_fixupimm_pd(__mmask8, __m256d, __m256d, __m256i, const int);
extern __m128    __cdecl _mm_fixupimm_ps(__m128, __m128, __m128i, const int);
extern __m128    __cdecl _mm_mask_fixupimm_ps(__m128, __mmask8, __m128, __m128i, const int);
extern __m128    __cdecl _mm_maskz_fixupimm_ps(__mmask8, __m128, __m128, __m128i, const int);
extern __m256    __cdecl _mm256_fixupimm_ps(__m256, __m256, __m256i, const int);
extern __m256    __cdecl _mm256_mask_fixupimm_ps(__m256, __mmask8, __m256, __m256i, const int);
extern __m256    __cdecl _mm256_maskz_fixupimm_ps(__mmask8, __m256, __m256, __m256i, const int);
extern __m128d   __cdecl _mm_mask_fmadd_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_mask3_fmadd_pd(__m128d, __m128d, __m128d, __mmask8);
extern __m128d   __cdecl _mm_maskz_fmadd_pd(__mmask8, __m128d, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_fmadd_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_mask3_fmadd_pd(__m256d, __m256d, __m256d, __mmask8);
extern __m256d   __cdecl _mm256_maskz_fmadd_pd(__mmask8, __m256d, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_fmadd_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_mask3_fmadd_ps(__m128, __m128, __m128, __mmask8);
extern __m128    __cdecl _mm_maskz_fmadd_ps(__mmask8, __m128, __m128, __m128);
extern __m256    __cdecl _mm256_mask_fmadd_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_mask3_fmadd_ps(__m256, __m256, __m256, __mmask8);
extern __m256    __cdecl _mm256_maskz_fmadd_ps(__mmask8, __m256, __m256, __m256);
extern __m128d   __cdecl _mm_mask_fmaddsub_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_mask3_fmaddsub_pd(__m128d, __m128d, __m128d, __mmask8);
extern __m128d   __cdecl _mm_maskz_fmaddsub_pd(__mmask8, __m128d, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_fmaddsub_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_mask3_fmaddsub_pd(__m256d, __m256d, __m256d, __mmask8);
extern __m256d   __cdecl _mm256_maskz_fmaddsub_pd(__mmask8, __m256d, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_fmaddsub_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_mask3_fmaddsub_ps(__m128, __m128, __m128, __mmask8);
extern __m128    __cdecl _mm_maskz_fmaddsub_ps(__mmask8, __m128, __m128, __m128);
extern __m256    __cdecl _mm256_mask_fmaddsub_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_mask3_fmaddsub_ps(__m256, __m256, __m256, __mmask8);
extern __m256    __cdecl _mm256_maskz_fmaddsub_ps(__mmask8, __m256, __m256, __m256);
extern __m128d   __cdecl _mm_mask_fmsub_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_mask3_fmsub_pd(__m128d, __m128d, __m128d, __mmask8);
extern __m128d   __cdecl _mm_maskz_fmsub_pd(__mmask8, __m128d, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_fmsub_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_mask3_fmsub_pd(__m256d, __m256d, __m256d, __mmask8);
extern __m256d   __cdecl _mm256_maskz_fmsub_pd(__mmask8, __m256d, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_fmsub_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_mask3_fmsub_ps(__m128, __m128, __m128, __mmask8);
extern __m128    __cdecl _mm_maskz_fmsub_ps(__mmask8, __m128, __m128, __m128);
extern __m256    __cdecl _mm256_mask_fmsub_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_mask3_fmsub_ps(__m256, __m256, __m256, __mmask8);
extern __m256    __cdecl _mm256_maskz_fmsub_ps(__mmask8, __m256, __m256, __m256);
extern __m128d   __cdecl _mm_mask_fmsubadd_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_mask3_fmsubadd_pd(__m128d, __m128d, __m128d, __mmask8);
extern __m128d   __cdecl _mm_maskz_fmsubadd_pd(__mmask8, __m128d, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_fmsubadd_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_mask3_fmsubadd_pd(__m256d, __m256d, __m256d, __mmask8);
extern __m256d   __cdecl _mm256_maskz_fmsubadd_pd(__mmask8, __m256d, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_fmsubadd_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_mask3_fmsubadd_ps(__m128, __m128, __m128, __mmask8);
extern __m128    __cdecl _mm_maskz_fmsubadd_ps(__mmask8, __m128, __m128, __m128);
extern __m256    __cdecl _mm256_mask_fmsubadd_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_mask3_fmsubadd_ps(__m256, __m256, __m256, __mmask8);
extern __m256    __cdecl _mm256_maskz_fmsubadd_ps(__mmask8, __m256, __m256, __m256);
extern __m128d   __cdecl _mm_mask_fnmadd_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_mask3_fnmadd_pd(__m128d, __m128d, __m128d, __mmask8);
extern __m128d   __cdecl _mm_maskz_fnmadd_pd(__mmask8, __m128d, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_fnmadd_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_mask3_fnmadd_pd(__m256d, __m256d, __m256d, __mmask8);
extern __m256d   __cdecl _mm256_maskz_fnmadd_pd(__mmask8, __m256d, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_fnmadd_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_mask3_fnmadd_ps(__m128, __m128, __m128, __mmask8);
extern __m128    __cdecl _mm_maskz_fnmadd_ps(__mmask8, __m128, __m128, __m128);
extern __m256    __cdecl _mm256_mask_fnmadd_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_mask3_fnmadd_ps(__m256, __m256, __m256, __mmask8);
extern __m256    __cdecl _mm256_maskz_fnmadd_ps(__mmask8, __m256, __m256, __m256);
extern __m128d   __cdecl _mm_mask_fnmsub_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_mask3_fnmsub_pd(__m128d, __m128d, __m128d, __mmask8);
extern __m128d   __cdecl _mm_maskz_fnmsub_pd(__mmask8, __m128d, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_fnmsub_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_mask3_fnmsub_pd(__m256d, __m256d, __m256d, __mmask8);
extern __m256d   __cdecl _mm256_maskz_fnmsub_pd(__mmask8, __m256d, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_fnmsub_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_mask3_fnmsub_ps(__m128, __m128, __m128, __mmask8);
extern __m128    __cdecl _mm_maskz_fnmsub_ps(__mmask8, __m128, __m128, __m128);
extern __m256    __cdecl _mm256_mask_fnmsub_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_mask3_fnmsub_ps(__m256, __m256, __m256, __mmask8);
extern __m256    __cdecl _mm256_maskz_fnmsub_ps(__mmask8, __m256, __m256, __m256);
extern __mmask8  __cdecl _mm_fpclass_pd_mask(__m128d, int);
extern __mmask8  __cdecl _mm_mask_fpclass_pd_mask(__mmask8, __m128d, int);
extern __mmask8  __cdecl _mm256_fpclass_pd_mask(__m256d, int);
extern __mmask8  __cdecl _mm256_mask_fpclass_pd_mask(__mmask8, __m256d, int);
extern __mmask8  __cdecl _mm_fpclass_ps_mask(__m128, int);
extern __mmask8  __cdecl _mm_mask_fpclass_ps_mask(__mmask8, __m128, int);
extern __mmask8  __cdecl _mm256_fpclass_ps_mask(__m256, int);
extern __mmask8  __cdecl _mm256_mask_fpclass_ps_mask(__mmask8, __m256, int);
extern __m128d   __cdecl _mm_getexp_pd(__m128d);
extern __m128d   __cdecl _mm_mask_getexp_pd(__m128d, __mmask8, __m128d);
extern __m128d   __cdecl _mm_maskz_getexp_pd(__mmask8, __m128d);
extern __m256d   __cdecl _mm256_getexp_pd(__m256d);
extern __m256d   __cdecl _mm256_mask_getexp_pd(__m256d, __mmask8, __m256d);
extern __m256d   __cdecl _mm256_maskz_getexp_pd(__mmask8, __m256d);
extern __m128    __cdecl _mm_getexp_ps(__m128);
extern __m128    __cdecl _mm_mask_getexp_ps(__m128, __mmask8, __m128);
extern __m128    __cdecl _mm_maskz_getexp_ps(__mmask8, __m128);
extern __m256    __cdecl _mm256_getexp_ps(__m256);
extern __m256    __cdecl _mm256_mask_getexp_ps(__m256, __mmask8, __m256);
extern __m256    __cdecl _mm256_maskz_getexp_ps(__mmask8, __m256);
extern __m128d   __cdecl _mm_getmant_pd(__m128d, int, int);
extern __m128d   __cdecl _mm_mask_getmant_pd(__m128d, __mmask8, __m128d, int, int);
extern __m128d   __cdecl _mm_maskz_getmant_pd(__mmask8, __m128d, int, int);
extern __m256d   __cdecl _mm256_getmant_pd(__m256d, int, int);
extern __m256d   __cdecl _mm256_mask_getmant_pd(__m256d, __mmask8, __m256d, int, int);
extern __m256d   __cdecl _mm256_maskz_getmant_pd(__mmask8, __m256d, int, int);
extern __m128    __cdecl _mm_getmant_ps(__m128, int, int);
extern __m128    __cdecl _mm_mask_getmant_ps(__m128, __mmask8, __m128, int, int);
extern __m128    __cdecl _mm_maskz_getmant_ps(__mmask8, __m128, int, int);
extern __m256    __cdecl _mm256_getmant_ps(__m256, int, int);
extern __m256    __cdecl _mm256_mask_getmant_ps(__m256, __mmask8, __m256, int, int);
extern __m256    __cdecl _mm256_maskz_getmant_ps(__mmask8, __m256, int, int);
extern __m128i   __cdecl _mm_mmask_i32gather_epi32(__m128i, __mmask8, __m128i, void const*, const int);
extern __m256i   __cdecl _mm256_mmask_i32gather_epi32(__m256i, __mmask8, __m256i, void const*, const int);
extern __m128i   __cdecl _mm_mmask_i32gather_epi64(__m128i, __mmask8, __m128i, void const*, const int);
extern __m256i   __cdecl _mm256_mmask_i32gather_epi64(__m256i, __mmask8, __m128i, void const*, const int);
extern __m128d   __cdecl _mm_mmask_i32gather_pd(__m128d, __mmask8, __m128i, void const*, const int);
extern __m256d   __cdecl _mm256_mmask_i32gather_pd(__m256d, __mmask8, __m128i, void const*, const int);
extern __m128    __cdecl _mm_mmask_i32gather_ps(__m128, __mmask8, __m128i, void const*, const int);
extern __m256    __cdecl _mm256_mmask_i32gather_ps(__m256, __mmask8, __m256i, void const*, const int);
extern void      __cdecl _mm_i32scatter_epi32(void*, __m128i, __m128i, const int);
extern void      __cdecl _mm_mask_i32scatter_epi32(void*, __mmask8, __m128i, __m128i, const int);
extern void      __cdecl _mm256_i32scatter_epi32(void*, __m256i, __m256i, const int);
extern void      __cdecl _mm256_mask_i32scatter_epi32(void*, __mmask8, __m256i, __m256i, const int);
extern void      __cdecl _mm_i32scatter_epi64(void*, __m128i, __m128i, const int);
extern void      __cdecl _mm_mask_i32scatter_epi64(void*, __mmask8, __m128i, __m128i, const int);
extern void      __cdecl _mm256_i32scatter_epi64(void*, __m128i, __m256i, const int);
extern void      __cdecl _mm256_mask_i32scatter_epi64(void*, __mmask8, __m128i, __m256i, const int);
extern void      __cdecl _mm_i32scatter_pd(void*, __m128i, __m128d, const int);
extern void      __cdecl _mm_mask_i32scatter_pd(void*, __mmask8, __m128i, __m128d, const int);
extern void      __cdecl _mm256_i32scatter_pd(void*, __m128i, __m256d, const int);
extern void      __cdecl _mm256_mask_i32scatter_pd(void*, __mmask8, __m128i, __m256d, const int);
extern void      __cdecl _mm_i32scatter_ps(void*, __m128i, __m128, const int);
extern void      __cdecl _mm_mask_i32scatter_ps(void*, __mmask8, __m128i, __m128, const int);
extern void      __cdecl _mm256_i32scatter_ps(void*, __m256i, __m256, const int);
extern void      __cdecl _mm256_mask_i32scatter_ps(void*, __mmask8, __m256i, __m256, const int);
extern __m128i   __cdecl _mm_mmask_i64gather_epi32(__m128i, __mmask8, __m128i, void const*, const int);
extern __m128i   __cdecl _mm256_mmask_i64gather_epi32(__m128i, __mmask8, __m256i, void const*, const int);
extern __m128i   __cdecl _mm_mmask_i64gather_epi64(__m128i, __mmask8, __m128i, void const*, const int);
extern __m256i   __cdecl _mm256_mmask_i64gather_epi64(__m256i, __mmask8, __m256i, void const*, const int);
extern __m128d   __cdecl _mm_mmask_i64gather_pd(__m128d, __mmask8, __m128i, void const*, const int);
extern __m256d   __cdecl _mm256_mmask_i64gather_pd(__m256d, __mmask8, __m256i, void const*, const int);
extern __m128    __cdecl _mm_mmask_i64gather_ps(__m128, __mmask8, __m128i, void const*, const int);
extern __m128    __cdecl _mm256_mmask_i64gather_ps(__m128, __mmask8, __m256i, void const*, const int);
extern void      __cdecl _mm_i64scatter_epi32(void*, __m128i, __m128i, const int);
extern void      __cdecl _mm_mask_i64scatter_epi32(void*, __mmask8, __m128i, __m128i, const int);
extern void      __cdecl _mm256_i64scatter_epi32(void*, __m256i, __m128i, const int);
extern void      __cdecl _mm256_mask_i64scatter_epi32(void*, __mmask8, __m256i, __m128i, const int);
extern void      __cdecl _mm_i64scatter_epi64(void*, __m128i, __m128i, const int);
extern void      __cdecl _mm_mask_i64scatter_epi64(void*, __mmask8, __m128i, __m128i, const int);
extern void      __cdecl _mm256_i64scatter_epi64(void*, __m256i, __m256i, const int);
extern void      __cdecl _mm256_mask_i64scatter_epi64(void*, __mmask8, __m256i, __m256i, const int);
extern void      __cdecl _mm_i64scatter_pd(void*, __m128i, __m128d, const int);
extern void      __cdecl _mm_mask_i64scatter_pd(void*, __mmask8, __m128i, __m128d, const int);
extern void      __cdecl _mm256_i64scatter_pd(void*, __m256i, __m256d, const int);
extern void      __cdecl _mm256_mask_i64scatter_pd(void*, __mmask8, __m256i, __m256d, const int);
extern void      __cdecl _mm_i64scatter_ps(void*, __m128i, __m128, const int);
extern void      __cdecl _mm_mask_i64scatter_ps(void*, __mmask8, __m128i, __m128, const int);
extern void      __cdecl _mm256_i64scatter_ps(void*, __m256i, __m128, const int);
extern void      __cdecl _mm256_mask_i64scatter_ps(void*, __mmask8, __m256i, __m128, const int);
extern __m256    __cdecl _mm256_insertf32x4(__m256, __m128, int);
extern __m256    __cdecl _mm256_mask_insertf32x4(__m256, __mmask8, __m256, __m128, int);
extern __m256    __cdecl _mm256_maskz_insertf32x4(__mmask8, __m256, __m128, int);
extern __m256d   __cdecl _mm256_insertf64x2(__m256d, __m128d, int);
extern __m256d   __cdecl _mm256_mask_insertf64x2(__m256d, __mmask8, __m256d, __m128d, int);
extern __m256d   __cdecl _mm256_maskz_insertf64x2(__mmask8, __m256d, __m128d, int);
extern __m256i   __cdecl _mm256_inserti32x4(__m256i, __m128i, int);
extern __m256i   __cdecl _mm256_mask_inserti32x4(__m256i, __mmask8, __m256i, __m128i, int);
extern __m256i   __cdecl _mm256_maskz_inserti32x4(__mmask8, __m256i, __m128i, int);
extern __m256i   __cdecl _mm256_inserti64x2(__m256i, __m128i, int);
extern __m256i   __cdecl _mm256_mask_inserti64x2(__m256i, __mmask8, __m256i, __m128i, int);
extern __m256i   __cdecl _mm256_maskz_inserti64x2(__mmask8, __m256i, __m128i, int);
extern __m128i   __cdecl _mm_mask_load_epi32(__m128i, __mmask8, void const*);
extern __m128i   __cdecl _mm_maskz_load_epi32(__mmask8, void const*);
extern __m256i   __cdecl _mm256_mask_load_epi32(__m256i, __mmask8, void const*);
extern __m256i   __cdecl _mm256_maskz_load_epi32(__mmask8, void const*);
extern __m128i   __cdecl _mm_mask_load_epi64(__m128i, __mmask8, void const*);
extern __m128i   __cdecl _mm_maskz_load_epi64(__mmask8, void const*);
extern __m256i   __cdecl _mm256_mask_load_epi64(__m256i, __mmask8, void const*);
extern __m256i   __cdecl _mm256_maskz_load_epi64(__mmask8, void const*);
extern __m128d   __cdecl _mm_mask_load_pd(__m128d, __mmask8, void const*);
extern __m128d   __cdecl _mm_maskz_load_pd(__mmask8, void const*);
extern __m256d   __cdecl _mm256_mask_load_pd(__m256d, __mmask8, void const*);
extern __m256d   __cdecl _mm256_maskz_load_pd(__mmask8, void const*);
extern __m128    __cdecl _mm_mask_load_ps(__m128, __mmask8, void const*);
extern __m128    __cdecl _mm_maskz_load_ps(__mmask8, void const*);
extern __m256    __cdecl _mm256_mask_load_ps(__m256, __mmask8, void const*);
extern __m256    __cdecl _mm256_maskz_load_ps(__mmask8, void const*);
extern __m128i   __cdecl _mm_loadu_epi16(void const*);
extern __m128i   __cdecl _mm_mask_loadu_epi16(__m128i, __mmask8, void const*);
extern __m128i   __cdecl _mm_maskz_loadu_epi16(__mmask8, void const*);
extern __m256i   __cdecl _mm256_loadu_epi16(void const*);
extern __m256i   __cdecl _mm256_mask_loadu_epi16(__m256i, __mmask16, void const*);
extern __m256i   __cdecl _mm256_maskz_loadu_epi16(__mmask16, void const*);
extern __m128i   __cdecl _mm_loadu_epi32(void const*);
extern __m128i   __cdecl _mm_mask_loadu_epi32(__m128i, __mmask8, void const*);
extern __m128i   __cdecl _mm_maskz_loadu_epi32(__mmask8, void const*);
extern __m256i   __cdecl _mm256_loadu_epi32(void const*);
extern __m256i   __cdecl _mm256_mask_loadu_epi32(__m256i, __mmask8, void const*);
extern __m256i   __cdecl _mm256_maskz_loadu_epi32(__mmask8, void const*);
extern __m128i   __cdecl _mm_loadu_epi64(void const*);
extern __m128i   __cdecl _mm_mask_loadu_epi64(__m128i, __mmask8, void const*);
extern __m128i   __cdecl _mm_maskz_loadu_epi64(__mmask8, void const*);
extern __m256i   __cdecl _mm256_loadu_epi64(void const*);
extern __m256i   __cdecl _mm256_mask_loadu_epi64(__m256i, __mmask8, void const*);
extern __m256i   __cdecl _mm256_maskz_loadu_epi64(__mmask8, void const*);
extern __m128i   __cdecl _mm_loadu_epi8(void const*);
extern __m128i   __cdecl _mm_mask_loadu_epi8(__m128i, __mmask16, void const*);
extern __m128i   __cdecl _mm_maskz_loadu_epi8(__mmask16, void const*);
extern __m256i   __cdecl _mm256_loadu_epi8(void const*);
extern __m256i   __cdecl _mm256_mask_loadu_epi8(__m256i, __mmask32, void const*);
extern __m256i   __cdecl _mm256_maskz_loadu_epi8(__mmask32, void const*);
extern __m128d   __cdecl _mm_mask_loadu_pd(__m128d, __mmask8, void const*);
extern __m128d   __cdecl _mm_maskz_loadu_pd(__mmask8, void const*);
extern __m256d   __cdecl _mm256_mask_loadu_pd(__m256d, __mmask8, void const*);
extern __m256d   __cdecl _mm256_maskz_loadu_pd(__mmask8, void const*);
extern __m128    __cdecl _mm_mask_loadu_ps(__m128, __mmask8, void const*);
extern __m128    __cdecl _mm_maskz_loadu_ps(__mmask8, void const*);
extern __m256    __cdecl _mm256_mask_loadu_ps(__m256, __mmask8, void const*);
extern __m256    __cdecl _mm256_maskz_loadu_ps(__mmask8, void const*);
extern __m128i   __cdecl _mm_lzcnt_epi32(__m128i);
extern __m128i   __cdecl _mm_mask_lzcnt_epi32(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_lzcnt_epi32(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_lzcnt_epi32(__m256i);
extern __m256i   __cdecl _mm256_mask_lzcnt_epi32(__m256i, __mmask8, __m256i);
extern __m256i   __cdecl _mm256_maskz_lzcnt_epi32(__mmask8, __m256i);
extern __m128i   __cdecl _mm_lzcnt_epi64(__m128i);
extern __m128i   __cdecl _mm_mask_lzcnt_epi64(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_lzcnt_epi64(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_lzcnt_epi64(__m256i);
extern __m256i   __cdecl _mm256_mask_lzcnt_epi64(__m256i, __mmask8, __m256i);
extern __m256i   __cdecl _mm256_maskz_lzcnt_epi64(__mmask8, __m256i);
extern __m128i   __cdecl _mm_mask_madd_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_madd_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_madd_epi16(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_madd_epi16(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_maddubs_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_maddubs_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_maddubs_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_maddubs_epi16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_max_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_max_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_max_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_max_epi16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_max_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_max_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_max_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_max_epi32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_max_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_max_epi64(__mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_max_epi64(__m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_max_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_max_epi64(__mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_max_epi64(__m256i, __m256i);
extern __m128i   __cdecl _mm_mask_max_epi8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_max_epi8(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_max_epi8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_max_epi8(__mmask32, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_max_epu16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_max_epu16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_max_epu16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_max_epu16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_max_epu32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_max_epu32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_max_epu32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_max_epu32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_max_epu64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_max_epu64(__mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_max_epu64(__m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_max_epu64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_max_epu64(__mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_max_epu64(__m256i, __m256i);
extern __m128i   __cdecl _mm_mask_max_epu8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_max_epu8(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_max_epu8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_max_epu8(__mmask32, __m256i, __m256i);
extern __m128d   __cdecl _mm_mask_max_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_max_pd(__mmask8, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_max_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_maskz_max_pd(__mmask8, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_max_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_max_ps(__mmask8, __m128, __m128);
extern __m256    __cdecl _mm256_mask_max_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_maskz_max_ps(__mmask8, __m256, __m256);
extern __m128i   __cdecl _mm_mask_min_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_min_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_min_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_min_epi16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_min_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_min_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_min_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_min_epi32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_min_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_min_epi64(__mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_min_epi64(__m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_min_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_min_epi64(__mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_min_epi64(__m256i, __m256i);
extern __m128i   __cdecl _mm_mask_min_epi8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_min_epi8(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_min_epi8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_min_epi8(__mmask32, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_min_epu16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_min_epu16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_min_epu16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_min_epu16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_min_epu32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_min_epu32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_min_epu32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_min_epu32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_min_epu64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_min_epu64(__mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_min_epu64(__m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_min_epu64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_min_epu64(__mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_min_epu64(__m256i, __m256i);
extern __m128i   __cdecl _mm_mask_min_epu8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_min_epu8(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_min_epu8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_min_epu8(__mmask32, __m256i, __m256i);
extern __m128d   __cdecl _mm_mask_min_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_min_pd(__mmask8, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_min_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_maskz_min_pd(__mmask8, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_min_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_min_ps(__mmask8, __m128, __m128);
extern __m256    __cdecl _mm256_mask_min_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_maskz_min_ps(__mmask8, __m256, __m256);
extern __m128i   __cdecl _mm_mask_mov_epi16(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_mov_epi16(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_mov_epi16(__m256i, __mmask16, __m256i);
extern __m256i   __cdecl _mm256_maskz_mov_epi16(__mmask16, __m256i);
extern __m128i   __cdecl _mm_mask_mov_epi32(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_mov_epi32(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_mov_epi32(__m256i, __mmask8, __m256i);
extern __m256i   __cdecl _mm256_maskz_mov_epi32(__mmask8, __m256i);
extern __m128i   __cdecl _mm_mask_mov_epi64(__m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_mov_epi64(__mmask8, __m128i);
extern __m256i   __cdecl _mm256_mask_mov_epi64(__m256i, __mmask8, __m256i);
extern __m256i   __cdecl _mm256_maskz_mov_epi64(__mmask8, __m256i);
extern __m128i   __cdecl _mm_mask_mov_epi8(__m128i, __mmask16, __m128i);
extern __m128i   __cdecl _mm_maskz_mov_epi8(__mmask16, __m128i);
extern __m256i   __cdecl _mm256_mask_mov_epi8(__m256i, __mmask32, __m256i);
extern __m256i   __cdecl _mm256_maskz_mov_epi8(__mmask32, __m256i);
extern __m128d   __cdecl _mm_mask_mov_pd(__m128d, __mmask8, __m128d);
extern __m128d   __cdecl _mm_maskz_mov_pd(__mmask8, __m128d);
extern __m256d   __cdecl _mm256_mask_mov_pd(__m256d, __mmask8, __m256d);
extern __m256d   __cdecl _mm256_maskz_mov_pd(__mmask8, __m256d);
extern __m128    __cdecl _mm_mask_mov_ps(__m128, __mmask8, __m128);
extern __m128    __cdecl _mm_maskz_mov_ps(__mmask8, __m128);
extern __m256    __cdecl _mm256_mask_mov_ps(__m256, __mmask8, __m256);
extern __m256    __cdecl _mm256_maskz_mov_ps(__mmask8, __m256);
extern __m128d   __cdecl _mm_mask_movedup_pd(__m128d, __mmask8, __m128d);
extern __m128d   __cdecl _mm_maskz_movedup_pd(__mmask8, __m128d);
extern __m256d   __cdecl _mm256_mask_movedup_pd(__m256d, __mmask8, __m256d);
extern __m256d   __cdecl _mm256_maskz_movedup_pd(__mmask8, __m256d);
extern __m128    __cdecl _mm_mask_movehdup_ps(__m128, __mmask8, __m128);
extern __m128    __cdecl _mm_maskz_movehdup_ps(__mmask8, __m128);
extern __m256    __cdecl _mm256_mask_movehdup_ps(__m256, __mmask8, __m256);
extern __m256    __cdecl _mm256_maskz_movehdup_ps(__mmask8, __m256);
extern __m128    __cdecl _mm_mask_moveldup_ps(__m128, __mmask8, __m128);
extern __m128    __cdecl _mm_maskz_moveldup_ps(__mmask8, __m128);
extern __m256    __cdecl _mm256_mask_moveldup_ps(__m256, __mmask8, __m256);
extern __m256    __cdecl _mm256_maskz_moveldup_ps(__mmask8, __m256);
extern __mmask8  __cdecl _mm_movepi16_mask(__m128i);
extern __mmask16 __cdecl _mm256_movepi16_mask(__m256i);
extern __mmask8  __cdecl _mm_movepi32_mask(__m128i);
extern __mmask8  __cdecl _mm256_movepi32_mask(__m256i);
extern __mmask8  __cdecl _mm_movepi64_mask(__m128i);
extern __mmask8  __cdecl _mm256_movepi64_mask(__m256i);
extern __mmask16 __cdecl _mm_movepi8_mask(__m128i);
extern __mmask32 __cdecl _mm256_movepi8_mask(__m256i);
extern __m128i   __cdecl _mm_movm_epi16(__mmask8);
extern __m256i   __cdecl _mm256_movm_epi16(__mmask16);
extern __m128i   __cdecl _mm_movm_epi32(__mmask8);
extern __m256i   __cdecl _mm256_movm_epi32(__mmask8);
extern __m128i   __cdecl _mm_movm_epi64(__mmask8);
extern __m256i   __cdecl _mm256_movm_epi64(__mmask8);
extern __m128i   __cdecl _mm_movm_epi8(__mmask16);
extern __m256i   __cdecl _mm256_movm_epi8(__mmask32);
extern __m128i   __cdecl _mm_mask_mul_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_mul_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_mul_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_mul_epi32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_mul_epu32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_mul_epu32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_mul_epu32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_mul_epu32(__mmask8, __m256i, __m256i);
extern __m128d   __cdecl _mm_mask_mul_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_mul_pd(__mmask8, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_mul_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_maskz_mul_pd(__mmask8, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_mul_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_mul_ps(__mmask8, __m128, __m128);
extern __m256    __cdecl _mm256_mask_mul_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_maskz_mul_ps(__mmask8, __m256, __m256);
extern __m128i   __cdecl _mm_mask_mulhi_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_mulhi_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_mulhi_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_mulhi_epi16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_mulhi_epu16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_mulhi_epu16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_mulhi_epu16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_mulhi_epu16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_mulhrs_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_mulhrs_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_mulhrs_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_mulhrs_epi16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_mullo_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_mullo_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_mullo_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_mullo_epi16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_mullo_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_mullo_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_mullo_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_mullo_epi32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_mullo_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_mullo_epi64(__mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_mullo_epi64(__m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_mullo_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_mullo_epi64(__mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_mullo_epi64(__m256i, __m256i);
extern __m128i   __cdecl _mm_or_epi32(__m128i, __m128i);
extern __m128i   __cdecl _mm_mask_or_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_or_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_or_epi32(__m256i, __m256i);
extern __m256i   __cdecl _mm256_mask_or_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_or_epi32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_or_epi64(__m128i, __m128i);
extern __m128i   __cdecl _mm_mask_or_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_or_epi64(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_or_epi64(__m256i, __m256i);
extern __m256i   __cdecl _mm256_mask_or_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_or_epi64(__mmask8, __m256i, __m256i);
extern __m128d   __cdecl _mm_mask_or_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_or_pd(__mmask8, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_or_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_maskz_or_pd(__mmask8, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_or_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_or_ps(__mmask8, __m128, __m128);
extern __m256    __cdecl _mm256_mask_or_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_maskz_or_ps(__mmask8, __m256, __m256);
extern __m128i   __cdecl _mm_mask_packs_epi16(__m128i, __mmask16, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_packs_epi16(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_packs_epi16(__m256i, __mmask32, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_packs_epi16(__mmask32, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_packs_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_packs_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_packs_epi32(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_packs_epi32(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_packus_epi16(__m128i, __mmask16, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_packus_epi16(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_packus_epi16(__m256i, __mmask32, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_packus_epi16(__mmask32, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_packus_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_packus_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_packus_epi32(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_packus_epi32(__mmask16, __m256i, __m256i);
extern __m128d   __cdecl _mm_mask_permute_pd(__m128d, __mmask8, __m128d, const int);
extern __m128d   __cdecl _mm_maskz_permute_pd(__mmask8, __m128d, const int);
extern __m256d   __cdecl _mm256_mask_permute_pd(__m256d, __mmask8, __m256d, const int);
extern __m256d   __cdecl _mm256_maskz_permute_pd(__mmask8, __m256d, const int);
extern __m128    __cdecl _mm_mask_permute_ps(__m128, __mmask8, __m128, const int);
extern __m128    __cdecl _mm_maskz_permute_ps(__mmask8, __m128, const int);
extern __m256    __cdecl _mm256_mask_permute_ps(__m256, __mmask8, __m256, const int);
extern __m256    __cdecl _mm256_maskz_permute_ps(__mmask8, __m256, const int);
extern __m128d   __cdecl _mm_mask_permutevar_pd(__m128d, __mmask8, __m128d, __m128i);
extern __m128d   __cdecl _mm_maskz_permutevar_pd(__mmask8, __m128d, __m128i);
extern __m256d   __cdecl _mm256_mask_permutevar_pd(__m256d, __mmask8, __m256d, __m256i);
extern __m256d   __cdecl _mm256_maskz_permutevar_pd(__mmask8, __m256d, __m256i);
extern __m128    __cdecl _mm_mask_permutevar_ps(__m128, __mmask8, __m128, __m128i);
extern __m128    __cdecl _mm_maskz_permutevar_ps(__mmask8, __m128, __m128i);
extern __m256    __cdecl _mm256_mask_permutevar_ps(__m256, __mmask8, __m256, __m256i);
extern __m256    __cdecl _mm256_maskz_permutevar_ps(__mmask8, __m256, __m256i);
extern __m256i   __cdecl _mm256_mask_permutex_epi64(__m256i, __mmask8, __m256i, const int);
extern __m256i   __cdecl _mm256_maskz_permutex_epi64(__mmask8, __m256i, const int);
extern __m256i   __cdecl _mm256_permutex_epi64(__m256i, const int);
extern __m256d   __cdecl _mm256_mask_permutex_pd(__m256d, __mmask8, __m256d, const int);
extern __m256d   __cdecl _mm256_maskz_permutex_pd(__mmask8, __m256d, const int);
extern __m256d   __cdecl _mm256_permutex_pd(__m256d, const int);
extern __m128i   __cdecl _mm_mask_permutex2var_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_mask2_permutex2var_epi16(__m128i, __m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_permutex2var_epi16(__mmask8, __m128i, __m128i, __m128i);
extern __m128i   __cdecl _mm_permutex2var_epi16(__m128i, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_permutex2var_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_mask2_permutex2var_epi16(__m256i, __m256i, __mmask16, __m256i);
extern __m256i   __cdecl _mm256_maskz_permutex2var_epi16(__mmask16, __m256i, __m256i, __m256i);
extern __m256i   __cdecl _mm256_permutex2var_epi16(__m256i, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_permutex2var_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_mask2_permutex2var_epi32(__m128i, __m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_permutex2var_epi32(__mmask8, __m128i, __m128i, __m128i);
extern __m128i   __cdecl _mm_permutex2var_epi32(__m128i, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_permutex2var_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_mask2_permutex2var_epi32(__m256i, __m256i, __mmask8, __m256i);
extern __m256i   __cdecl _mm256_maskz_permutex2var_epi32(__mmask8, __m256i, __m256i, __m256i);
extern __m256i   __cdecl _mm256_permutex2var_epi32(__m256i, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_permutex2var_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_mask2_permutex2var_epi64(__m128i, __m128i, __mmask8, __m128i);
extern __m128i   __cdecl _mm_maskz_permutex2var_epi64(__mmask8, __m128i, __m128i, __m128i);
extern __m128i   __cdecl _mm_permutex2var_epi64(__m128i, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_permutex2var_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_mask2_permutex2var_epi64(__m256i, __m256i, __mmask8, __m256i);
extern __m256i   __cdecl _mm256_maskz_permutex2var_epi64(__mmask8, __m256i, __m256i, __m256i);
extern __m256i   __cdecl _mm256_permutex2var_epi64(__m256i, __m256i, __m256i);
extern __m128d   __cdecl _mm_mask_permutex2var_pd(__m128d, __mmask8, __m128i, __m128d);
extern __m128d   __cdecl _mm_mask2_permutex2var_pd(__m128d, __m128i, __mmask8, __m128d);
extern __m128d   __cdecl _mm_maskz_permutex2var_pd(__mmask8, __m128d, __m128i, __m128d);
extern __m128d   __cdecl _mm_permutex2var_pd(__m128d, __m128i, __m128d);
extern __m256d   __cdecl _mm256_mask_permutex2var_pd(__m256d, __mmask8, __m256i, __m256d);
extern __m256d   __cdecl _mm256_mask2_permutex2var_pd(__m256d, __m256i, __mmask8, __m256d);
extern __m256d   __cdecl _mm256_maskz_permutex2var_pd(__mmask8, __m256d, __m256i, __m256d);
extern __m256d   __cdecl _mm256_permutex2var_pd(__m256d, __m256i, __m256d);
extern __m128    __cdecl _mm_mask_permutex2var_ps(__m128, __mmask8, __m128i, __m128);
extern __m128    __cdecl _mm_mask2_permutex2var_ps(__m128, __m128i, __mmask8, __m128);
extern __m128    __cdecl _mm_maskz_permutex2var_ps(__mmask8, __m128, __m128i, __m128);
extern __m128    __cdecl _mm_permutex2var_ps(__m128, __m128i, __m128);
extern __m256    __cdecl _mm256_mask_permutex2var_ps(__m256, __mmask8, __m256i, __m256);
extern __m256    __cdecl _mm256_mask2_permutex2var_ps(__m256, __m256i, __mmask8, __m256);
extern __m256    __cdecl _mm256_maskz_permutex2var_ps(__mmask8, __m256, __m256i, __m256);
extern __m256    __cdecl _mm256_permutex2var_ps(__m256, __m256i, __m256);
extern __m128i   __cdecl _mm_mask_permutexvar_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_permutexvar_epi16(__mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_permutexvar_epi16(__m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_permutexvar_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_permutexvar_epi16(__mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_permutexvar_epi16(__m256i, __m256i);
extern __m256i   __cdecl _mm256_mask_permutexvar_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_permutexvar_epi32(__mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_permutexvar_epi32(__m256i, __m256i);
extern __m256i   __cdecl _mm256_mask_permutexvar_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_permutexvar_epi64(__mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_permutexvar_epi64(__m256i, __m256i);
extern __m256d   __cdecl _mm256_mask_permutexvar_pd(__m256d, __mmask8, __m256i, __m256d);
extern __m256d   __cdecl _mm256_maskz_permutexvar_pd(__mmask8, __m256i, __m256d);
extern __m256d   __cdecl _mm256_permutexvar_pd(__m256i, __m256d);
extern __m256    __cdecl _mm256_mask_permutexvar_ps(__m256, __mmask8, __m256i, __m256);
extern __m256    __cdecl _mm256_maskz_permutexvar_ps(__mmask8, __m256i, __m256);
extern __m256    __cdecl _mm256_permutexvar_ps(__m256i, __m256);
extern __m128d   __cdecl _mm_mask_range_pd(__m128d, __mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_maskz_range_pd(__mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_range_pd(__m128d, __m128d, int);
extern __m256d   __cdecl _mm256_mask_range_pd(__m256d, __mmask8, __m256d, __m256d, int);
extern __m256d   __cdecl _mm256_maskz_range_pd(__mmask8, __m256d, __m256d, int);
extern __m256d   __cdecl _mm256_range_pd(__m256d, __m256d, int);
extern __m128    __cdecl _mm_mask_range_ps(__m128, __mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_maskz_range_ps(__mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_range_ps(__m128, __m128, int);
extern __m256    __cdecl _mm256_mask_range_ps(__m256, __mmask8, __m256, __m256, int);
extern __m256    __cdecl _mm256_maskz_range_ps(__mmask8, __m256, __m256, int);
extern __m256    __cdecl _mm256_range_ps(__m256, __m256, int);
extern __m128d   __cdecl _mm_mask_rcp14_pd(__m128d, __mmask8, __m128d);
extern __m128d   __cdecl _mm_maskz_rcp14_pd(__mmask8, __m128d);
extern __m128d   __cdecl _mm_rcp14_pd(__m128d);
extern __m256d   __cdecl _mm256_mask_rcp14_pd(__m256d, __mmask8, __m256d);
extern __m256d   __cdecl _mm256_maskz_rcp14_pd(__mmask8, __m256d);
extern __m256d   __cdecl _mm256_rcp14_pd(__m256d);
extern __m128    __cdecl _mm_mask_rcp14_ps(__m128, __mmask8, __m128);
extern __m128    __cdecl _mm_maskz_rcp14_ps(__mmask8, __m128);
extern __m128    __cdecl _mm_rcp14_ps(__m128);
extern __m256    __cdecl _mm256_mask_rcp14_ps(__m256, __mmask8, __m256);
extern __m256    __cdecl _mm256_maskz_rcp14_ps(__mmask8, __m256);
extern __m256    __cdecl _mm256_rcp14_ps(__m256);
extern __m128d   __cdecl _mm_mask_reduce_pd(__m128d, __mmask8, __m128d, int);
extern __m128d   __cdecl _mm_maskz_reduce_pd(__mmask8, __m128d, int);
extern __m128d   __cdecl _mm_reduce_pd(__m128d, int);
extern __m256d   __cdecl _mm256_mask_reduce_pd(__m256d, __mmask8, __m256d, int);
extern __m256d   __cdecl _mm256_maskz_reduce_pd(__mmask8, __m256d, int);
extern __m256d   __cdecl _mm256_reduce_pd(__m256d, int);
extern __m128    __cdecl _mm_mask_reduce_ps(__m128, __mmask8, __m128, int);
extern __m128    __cdecl _mm_maskz_reduce_ps(__mmask8, __m128, int);
extern __m128    __cdecl _mm_reduce_ps(__m128, int);
extern __m256    __cdecl _mm256_mask_reduce_ps(__m256, __mmask8, __m256, int);
extern __m256    __cdecl _mm256_maskz_reduce_ps(__mmask8, __m256, int);
extern __m256    __cdecl _mm256_reduce_ps(__m256, int);
extern __m128i   __cdecl _mm_mask_rol_epi32(__m128i, __mmask8, __m128i, const int);
extern __m128i   __cdecl _mm_maskz_rol_epi32(__mmask8, __m128i, const int);
extern __m128i   __cdecl _mm_rol_epi32(__m128i, int);
extern __m256i   __cdecl _mm256_mask_rol_epi32(__m256i, __mmask8, __m256i, const int);
extern __m256i   __cdecl _mm256_maskz_rol_epi32(__mmask8, __m256i, const int);
extern __m256i   __cdecl _mm256_rol_epi32(__m256i, const int);
extern __m128i   __cdecl _mm_mask_rol_epi64(__m128i, __mmask8, __m128i, const int);
extern __m128i   __cdecl _mm_maskz_rol_epi64(__mmask8, __m128i, const int);
extern __m128i   __cdecl _mm_rol_epi64(__m128i, const int);
extern __m256i   __cdecl _mm256_mask_rol_epi64(__m256i, __mmask8, __m256i, const int);
extern __m256i   __cdecl _mm256_maskz_rol_epi64(__mmask8, __m256i, const int);
extern __m256i   __cdecl _mm256_rol_epi64(__m256i, const int);
extern __m128i   __cdecl _mm_mask_rolv_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_rolv_epi32(__mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_rolv_epi32(__m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_rolv_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_rolv_epi32(__mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_rolv_epi32(__m256i, __m256i);
extern __m128i   __cdecl _mm_mask_rolv_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_rolv_epi64(__mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_rolv_epi64(__m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_rolv_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_rolv_epi64(__mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_rolv_epi64(__m256i, __m256i);
extern __m128i   __cdecl _mm_mask_ror_epi32(__m128i, __mmask8, __m128i, const int);
extern __m128i   __cdecl _mm_maskz_ror_epi32(__mmask8, __m128i, const int);
extern __m128i   __cdecl _mm_ror_epi32(__m128i, const int);
extern __m256i   __cdecl _mm256_mask_ror_epi32(__m256i, __mmask8, __m256i, const int);
extern __m256i   __cdecl _mm256_maskz_ror_epi32(__mmask8, __m256i, const int);
extern __m256i   __cdecl _mm256_ror_epi32(__m256i, const int);
extern __m128i   __cdecl _mm_mask_ror_epi64(__m128i, __mmask8, __m128i, const int);
extern __m128i   __cdecl _mm_maskz_ror_epi64(__mmask8, __m128i, const int);
extern __m128i   __cdecl _mm_ror_epi64(__m128i, const int);
extern __m256i   __cdecl _mm256_mask_ror_epi64(__m256i, __mmask8, __m256i, const int);
extern __m256i   __cdecl _mm256_maskz_ror_epi64(__mmask8, __m256i, const int);
extern __m256i   __cdecl _mm256_ror_epi64(__m256i, const int);
extern __m128i   __cdecl _mm_mask_rorv_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_rorv_epi32(__mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_rorv_epi32(__m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_rorv_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_rorv_epi32(__mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_rorv_epi32(__m256i, __m256i);
extern __m128i   __cdecl _mm_mask_rorv_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_rorv_epi64(__mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_rorv_epi64(__m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_rorv_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_rorv_epi64(__mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_rorv_epi64(__m256i, __m256i);
extern __m128d   __cdecl _mm_mask_roundscale_pd(__m128d, __mmask8, __m128d, int);
extern __m128d   __cdecl _mm_maskz_roundscale_pd(__mmask8, __m128d, int);
extern __m128d   __cdecl _mm_roundscale_pd(__m128d, int);
extern __m256d   __cdecl _mm256_mask_roundscale_pd(__m256d, __mmask8, __m256d, int);
extern __m256d   __cdecl _mm256_maskz_roundscale_pd(__mmask8, __m256d, int);
extern __m256d   __cdecl _mm256_roundscale_pd(__m256d, int);
extern __m128    __cdecl _mm_mask_roundscale_ps(__m128, __mmask8, __m128, int);
extern __m128    __cdecl _mm_maskz_roundscale_ps(__mmask8, __m128, int);
extern __m128    __cdecl _mm_roundscale_ps(__m128, int);
extern __m256    __cdecl _mm256_mask_roundscale_ps(__m256, __mmask8, __m256, int);
extern __m256    __cdecl _mm256_maskz_roundscale_ps(__mmask8, __m256, int);
extern __m256    __cdecl _mm256_roundscale_ps(__m256, int);
extern __m128d   __cdecl _mm_mask_rsqrt14_pd(__m128d, __mmask8, __m128d);
extern __m128d   __cdecl _mm_maskz_rsqrt14_pd(__mmask8, __m128d);
extern __m256d   __cdecl _mm256_mask_rsqrt14_pd(__m256d, __mmask8, __m256d);
extern __m256d   __cdecl _mm256_maskz_rsqrt14_pd(__mmask8, __m256d);
extern __m128    __cdecl _mm_mask_rsqrt14_ps(__m128, __mmask8, __m128);
extern __m128    __cdecl _mm_maskz_rsqrt14_ps(__mmask8, __m128);
extern __m256    __cdecl _mm256_mask_rsqrt14_ps(__m256, __mmask8, __m256);
extern __m256    __cdecl _mm256_maskz_rsqrt14_ps(__mmask8, __m256);
extern __m128d   __cdecl _mm_mask_scalef_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_scalef_pd(__mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_scalef_pd(__m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_scalef_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_maskz_scalef_pd(__mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_scalef_pd(__m256d, __m256d);
extern __m128    __cdecl _mm_mask_scalef_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_scalef_ps(__mmask8, __m128, __m128);
extern __m128    __cdecl _mm_scalef_ps(__m128, __m128);
extern __m256    __cdecl _mm256_mask_scalef_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_maskz_scalef_ps(__mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_scalef_ps(__m256, __m256);
extern __m128i   __cdecl _mm_mask_set1_epi16(__m128i, __mmask8, short);
extern __m128i   __cdecl _mm_maskz_set1_epi16(__mmask8, short);
extern __m256i   __cdecl _mm256_mask_set1_epi16(__m256i, __mmask16, short);
extern __m256i   __cdecl _mm256_maskz_set1_epi16(__mmask16, short);
extern __m128i   __cdecl _mm_mask_set1_epi32(__m128i, __mmask8, int);
extern __m128i   __cdecl _mm_maskz_set1_epi32(__mmask8, int);
extern __m256i   __cdecl _mm256_mask_set1_epi32(__m256i, __mmask8, int);
extern __m256i   __cdecl _mm256_maskz_set1_epi32(__mmask8, int);
extern __m128i   __cdecl _mm_mask_set1_epi64(__m128i, __mmask8, __int64);
extern __m128i   __cdecl _mm_maskz_set1_epi64(__mmask8, __int64);
extern __m256i   __cdecl _mm256_mask_set1_epi64(__m256i, __mmask8, __int64);
extern __m256i   __cdecl _mm256_maskz_set1_epi64(__mmask8, __int64);
extern __m128i   __cdecl _mm_mask_set1_epi8(__m128i, __mmask16, char);
extern __m128i   __cdecl _mm_maskz_set1_epi8(__mmask16, char);
extern __m256i   __cdecl _mm256_mask_set1_epi8(__m256i, __mmask32, char);
extern __m256i   __cdecl _mm256_maskz_set1_epi8(__mmask32, char);
extern __m128i   __cdecl _mm_mask_shuffle_epi32(__m128i, __mmask8, __m128i, int);
extern __m128i   __cdecl _mm_maskz_shuffle_epi32(__mmask8, __m128i, int);
extern __m256i   __cdecl _mm256_mask_shuffle_epi32(__m256i, __mmask8, __m256i, int);
extern __m256i   __cdecl _mm256_maskz_shuffle_epi32(__mmask8, __m256i, int);
extern __m128i   __cdecl _mm_mask_shuffle_epi8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_shuffle_epi8(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_shuffle_epi8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_shuffle_epi8(__mmask32, __m256i, __m256i);
extern __m256    __cdecl _mm256_mask_shuffle_f32x4(__m256, __mmask8, __m256, __m256, const int);
extern __m256    __cdecl _mm256_maskz_shuffle_f32x4(__mmask8, __m256, __m256, const int);
extern __m256    __cdecl _mm256_shuffle_f32x4(__m256, __m256, const int);
extern __m256d   __cdecl _mm256_mask_shuffle_f64x2(__m256d, __mmask8, __m256d, __m256d, const int);
extern __m256d   __cdecl _mm256_maskz_shuffle_f64x2(__mmask8, __m256d, __m256d, const int);
extern __m256d   __cdecl _mm256_shuffle_f64x2(__m256d, __m256d, const int);
extern __m256i   __cdecl _mm256_mask_shuffle_i32x4(__m256i, __mmask8, __m256i, __m256i, const int);
extern __m256i   __cdecl _mm256_maskz_shuffle_i32x4(__mmask8, __m256i, __m256i, const int);
extern __m256i   __cdecl _mm256_shuffle_i32x4(__m256i, __m256i, const int);
extern __m256i   __cdecl _mm256_mask_shuffle_i64x2(__m256i, __mmask8, __m256i, __m256i, const int);
extern __m256i   __cdecl _mm256_maskz_shuffle_i64x2(__mmask8, __m256i, __m256i, const int);
extern __m256i   __cdecl _mm256_shuffle_i64x2(__m256i, __m256i, const int);
extern __m128d   __cdecl _mm_mask_shuffle_pd(__m128d, __mmask8, __m128d, __m128d, const int);
extern __m128d   __cdecl _mm_maskz_shuffle_pd(__mmask8, __m128d, __m128d, const int);
extern __m256d   __cdecl _mm256_mask_shuffle_pd(__m256d, __mmask8, __m256d, __m256d, const int);
extern __m256d   __cdecl _mm256_maskz_shuffle_pd(__mmask8, __m256d, __m256d, const int);
extern __m128    __cdecl _mm_mask_shuffle_ps(__m128, __mmask8, __m128, __m128, const int);
extern __m128    __cdecl _mm_maskz_shuffle_ps(__mmask8, __m128, __m128, const int);
extern __m256    __cdecl _mm256_mask_shuffle_ps(__m256, __mmask8, __m256, __m256, const int);
extern __m256    __cdecl _mm256_maskz_shuffle_ps(__mmask8, __m256, __m256, const int);
extern __m128i   __cdecl _mm_mask_shufflehi_epi16(__m128i, __mmask8, __m128i, int);
extern __m128i   __cdecl _mm_maskz_shufflehi_epi16(__mmask8, __m128i, int);
extern __m256i   __cdecl _mm256_mask_shufflehi_epi16(__m256i, __mmask16, __m256i, int);
extern __m256i   __cdecl _mm256_maskz_shufflehi_epi16(__mmask16, __m256i, int);
extern __m128i   __cdecl _mm_mask_shufflelo_epi16(__m128i, __mmask8, __m128i, int);
extern __m128i   __cdecl _mm_maskz_shufflelo_epi16(__mmask8, __m128i, int);
extern __m256i   __cdecl _mm256_mask_shufflelo_epi16(__m256i, __mmask16, __m256i, int);
extern __m256i   __cdecl _mm256_maskz_shufflelo_epi16(__mmask16, __m256i, int);
extern __m128i   __cdecl _mm_mask_sll_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_sll_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_sll_epi16(__m256i, __mmask16, __m256i, __m128i);
extern __m256i   __cdecl _mm256_maskz_sll_epi16(__mmask16, __m256i, __m128i);
extern __m128i   __cdecl _mm_mask_sll_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_sll_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_sll_epi32(__m256i, __mmask8, __m256i, __m128i);
extern __m256i   __cdecl _mm256_maskz_sll_epi32(__mmask8, __m256i, __m128i);
extern __m128i   __cdecl _mm_mask_sll_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_sll_epi64(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_sll_epi64(__m256i, __mmask8, __m256i, __m128i);
extern __m256i   __cdecl _mm256_maskz_sll_epi64(__mmask8, __m256i, __m128i);
extern __m128i   __cdecl _mm_mask_slli_epi16(__m128i, __mmask8, __m128i, unsigned int);
extern __m128i   __cdecl _mm_maskz_slli_epi16(__mmask8, __m128i, unsigned int);
extern __m256i   __cdecl _mm256_mask_slli_epi16(__m256i, __mmask16, __m256i, unsigned int);
extern __m256i   __cdecl _mm256_maskz_slli_epi16(__mmask16, __m256i, unsigned int);
extern __m128i   __cdecl _mm_mask_slli_epi32(__m128i, __mmask8, __m128i, unsigned int);
extern __m128i   __cdecl _mm_maskz_slli_epi32(__mmask8, __m128i, unsigned int);
extern __m256i   __cdecl _mm256_mask_slli_epi32(__m256i, __mmask8, __m256i, unsigned int);
extern __m256i   __cdecl _mm256_maskz_slli_epi32(__mmask8, __m256i, unsigned int);
extern __m128i   __cdecl _mm_mask_slli_epi64(__m128i, __mmask8, __m128i, unsigned int);
extern __m128i   __cdecl _mm_maskz_slli_epi64(__mmask8, __m128i, unsigned int);
extern __m256i   __cdecl _mm256_mask_slli_epi64(__m256i, __mmask8, __m256i, unsigned int);
extern __m256i   __cdecl _mm256_maskz_slli_epi64(__mmask8, __m256i, unsigned int);
extern __m128i   __cdecl _mm_mask_sllv_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_sllv_epi16(__mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_sllv_epi16(__m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_sllv_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_sllv_epi16(__mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_sllv_epi16(__m256i, __m256i);
extern __m128i   __cdecl _mm_mask_sllv_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_sllv_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_sllv_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_sllv_epi32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_sllv_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_sllv_epi64(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_sllv_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_sllv_epi64(__mmask8, __m256i, __m256i);
extern __m128d   __cdecl _mm_mask_sqrt_pd(__m128d, __mmask8, __m128d);
extern __m128d   __cdecl _mm_maskz_sqrt_pd(__mmask8, __m128d);
extern __m256d   __cdecl _mm256_mask_sqrt_pd(__m256d, __mmask8, __m256d);
extern __m256d   __cdecl _mm256_maskz_sqrt_pd(__mmask8, __m256d);
extern __m128    __cdecl _mm_mask_sqrt_ps(__m128, __mmask8, __m128);
extern __m128    __cdecl _mm_maskz_sqrt_ps(__mmask8, __m128);
extern __m256    __cdecl _mm256_mask_sqrt_ps(__m256, __mmask8, __m256);
extern __m256    __cdecl _mm256_maskz_sqrt_ps(__mmask8, __m256);
extern __m128i   __cdecl _mm_mask_sra_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_sra_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_sra_epi16(__m256i, __mmask16, __m256i, __m128i);
extern __m256i   __cdecl _mm256_maskz_sra_epi16(__mmask16, __m256i, __m128i);
extern __m128i   __cdecl _mm_mask_sra_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_sra_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_sra_epi32(__m256i, __mmask8, __m256i, __m128i);
extern __m256i   __cdecl _mm256_maskz_sra_epi32(__mmask8, __m256i, __m128i);
extern __m128i   __cdecl _mm_mask_sra_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_sra_epi64(__mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_sra_epi64(__m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_sra_epi64(__m256i, __mmask8, __m256i, __m128i);
extern __m256i   __cdecl _mm256_maskz_sra_epi64(__mmask8, __m256i, __m128i);
extern __m256i   __cdecl _mm256_sra_epi64(__m256i, __m128i);
extern __m128i   __cdecl _mm_mask_srai_epi16(__m128i, __mmask8, __m128i, unsigned int);
extern __m128i   __cdecl _mm_maskz_srai_epi16(__mmask8, __m128i, unsigned int);
extern __m256i   __cdecl _mm256_mask_srai_epi16(__m256i, __mmask16, __m256i, unsigned int);
extern __m256i   __cdecl _mm256_maskz_srai_epi16(__mmask16, __m256i, unsigned int);
extern __m128i   __cdecl _mm_mask_srai_epi32(__m128i, __mmask8, __m128i, unsigned int);
extern __m128i   __cdecl _mm_maskz_srai_epi32(__mmask8, __m128i, unsigned int);
extern __m256i   __cdecl _mm256_mask_srai_epi32(__m256i, __mmask8, __m256i, unsigned int);
extern __m256i   __cdecl _mm256_maskz_srai_epi32(__mmask8, __m256i, unsigned int);
extern __m128i   __cdecl _mm_mask_srai_epi64(__m128i, __mmask8, __m128i, unsigned int);
extern __m128i   __cdecl _mm_maskz_srai_epi64(__mmask8, __m128i, unsigned int);
extern __m128i   __cdecl _mm_srai_epi64(__m128i, unsigned int);
extern __m256i   __cdecl _mm256_mask_srai_epi64(__m256i, __mmask8, __m256i, unsigned int);
extern __m256i   __cdecl _mm256_maskz_srai_epi64(__mmask8, __m256i, unsigned int);
extern __m256i   __cdecl _mm256_srai_epi64(__m256i, unsigned int);
extern __m128i   __cdecl _mm_mask_srav_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_srav_epi16(__mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_srav_epi16(__m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_srav_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_srav_epi16(__mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_srav_epi16(__m256i, __m256i);
extern __m128i   __cdecl _mm_mask_srav_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_srav_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_srav_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_srav_epi32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_srav_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_srav_epi64(__mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_srav_epi64(__m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_srav_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_srav_epi64(__mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_srav_epi64(__m256i, __m256i);
extern __m128i   __cdecl _mm_mask_srl_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_srl_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_srl_epi16(__m256i, __mmask16, __m256i, __m128i);
extern __m256i   __cdecl _mm256_maskz_srl_epi16(__mmask16, __m256i, __m128i);
extern __m128i   __cdecl _mm_mask_srl_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_srl_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_srl_epi32(__m256i, __mmask8, __m256i, __m128i);
extern __m256i   __cdecl _mm256_maskz_srl_epi32(__mmask8, __m256i, __m128i);
extern __m128i   __cdecl _mm_mask_srl_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_srl_epi64(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_srl_epi64(__m256i, __mmask8, __m256i, __m128i);
extern __m256i   __cdecl _mm256_maskz_srl_epi64(__mmask8, __m256i, __m128i);
extern __m128i   __cdecl _mm_mask_srli_epi16(__m128i, __mmask8, __m128i, int);
extern __m128i   __cdecl _mm_maskz_srli_epi16(__mmask8, __m128i, int);
extern __m256i   __cdecl _mm256_mask_srli_epi16(__m256i, __mmask16, __m256i, int);
extern __m256i   __cdecl _mm256_maskz_srli_epi16(__mmask16, __m256i, int);
extern __m128i   __cdecl _mm_mask_srli_epi32(__m128i, __mmask8, __m128i, unsigned int);
extern __m128i   __cdecl _mm_maskz_srli_epi32(__mmask8, __m128i, unsigned int);
extern __m256i   __cdecl _mm256_mask_srli_epi32(__m256i, __mmask8, __m256i, unsigned int);
extern __m256i   __cdecl _mm256_maskz_srli_epi32(__mmask8, __m256i, unsigned int);
extern __m128i   __cdecl _mm_mask_srli_epi64(__m128i, __mmask8, __m128i, unsigned int);
extern __m128i   __cdecl _mm_maskz_srli_epi64(__mmask8, __m128i, unsigned int);
extern __m256i   __cdecl _mm256_mask_srli_epi64(__m256i, __mmask8, __m256i, unsigned int);
extern __m256i   __cdecl _mm256_maskz_srli_epi64(__mmask8, __m256i, unsigned int);
extern __m128i   __cdecl _mm_mask_srlv_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_srlv_epi16(__mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_srlv_epi16(__m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_srlv_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_srlv_epi16(__mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_srlv_epi16(__m256i, __m256i);
extern __m128i   __cdecl _mm_mask_srlv_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_srlv_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_srlv_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_srlv_epi32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_srlv_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_srlv_epi64(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_srlv_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_srlv_epi64(__mmask8, __m256i, __m256i);
extern void      __cdecl _mm_mask_store_epi32(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_store_epi32(void*, __mmask8, __m256i);
extern void      __cdecl _mm_mask_store_epi64(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_store_epi64(void*, __mmask8, __m256i);
extern void      __cdecl _mm_mask_store_pd(void*, __mmask8, __m128d);
extern void      __cdecl _mm256_mask_store_pd(void*, __mmask8, __m256d);
extern void      __cdecl _mm_mask_store_ps(void*, __mmask8, __m128);
extern void      __cdecl _mm256_mask_store_ps(void*, __mmask8, __m256);
extern void      __cdecl _mm_storeu_epi16(void*, __m128i);
extern void      __cdecl _mm256_storeu_epi16(void*, __m256i);
extern void      __cdecl _mm_mask_storeu_epi16(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_storeu_epi16(void*, __mmask16, __m256i);
extern void      __cdecl _mm_storeu_epi32(void*, __m128i);
extern void      __cdecl _mm256_storeu_epi32(void*, __m256i);
extern void      __cdecl _mm_mask_storeu_epi32(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_storeu_epi32(void*, __mmask8, __m256i);
extern void      __cdecl _mm_storeu_epi64(void*, __m128i);
extern void      __cdecl _mm256_storeu_epi64(void*, __m256i);
extern void      __cdecl _mm_mask_storeu_epi64(void*, __mmask8, __m128i);
extern void      __cdecl _mm256_mask_storeu_epi64(void*, __mmask8, __m256i);
extern void      __cdecl _mm_storeu_epi8(void*, __m128i);
extern void      __cdecl _mm256_storeu_epi8(void*, __m256i);
extern void      __cdecl _mm_mask_storeu_epi8(void*, __mmask16, __m128i);
extern void      __cdecl _mm256_mask_storeu_epi8(void*, __mmask32, __m256i);
extern void      __cdecl _mm_mask_storeu_pd(void*, __mmask8, __m128d);
extern void      __cdecl _mm256_mask_storeu_pd(void*, __mmask8, __m256d);
extern void      __cdecl _mm_mask_storeu_ps(void*, __mmask8, __m128);
extern void      __cdecl _mm256_mask_storeu_ps(void*, __mmask8, __m256);
extern __m128i   __cdecl _mm_mask_sub_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_sub_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_sub_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_sub_epi16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_sub_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_sub_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_sub_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_sub_epi32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_sub_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_sub_epi64(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_sub_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_sub_epi64(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_sub_epi8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_sub_epi8(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_sub_epi8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_sub_epi8(__mmask32, __m256i, __m256i);
extern __m128d   __cdecl _mm_mask_sub_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_sub_pd(__mmask8, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_sub_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_maskz_sub_pd(__mmask8, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_sub_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_sub_ps(__mmask8, __m128, __m128);
extern __m256    __cdecl _mm256_mask_sub_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_maskz_sub_ps(__mmask8, __m256, __m256);
extern __m128i   __cdecl _mm_mask_subs_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_subs_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_subs_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_subs_epi16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_subs_epi8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_subs_epi8(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_subs_epi8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_subs_epi8(__mmask32, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_subs_epu16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_subs_epu16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_subs_epu16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_subs_epu16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_subs_epu8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_subs_epu8(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_subs_epu8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_subs_epu8(__mmask32, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_ternarylogic_epi32(__m128i, __mmask8, __m128i, __m128i, int);
extern __m128i   __cdecl _mm_maskz_ternarylogic_epi32(__mmask8, __m128i, __m128i, __m128i, int);
extern __m128i   __cdecl _mm_ternarylogic_epi32(__m128i, __m128i, __m128i, int);
extern __m256i   __cdecl _mm256_mask_ternarylogic_epi32(__m256i, __mmask8, __m256i, __m256i, int);
extern __m256i   __cdecl _mm256_maskz_ternarylogic_epi32(__mmask8, __m256i, __m256i, __m256i, int);
extern __m256i   __cdecl _mm256_ternarylogic_epi32(__m256i, __m256i, __m256i, int);
extern __m128i   __cdecl _mm_mask_ternarylogic_epi64(__m128i, __mmask8, __m128i, __m128i, int);
extern __m128i   __cdecl _mm_maskz_ternarylogic_epi64(__mmask8, __m128i, __m128i, __m128i, int);
extern __m128i   __cdecl _mm_ternarylogic_epi64(__m128i, __m128i, __m128i, int);
extern __m256i   __cdecl _mm256_mask_ternarylogic_epi64(__m256i, __mmask8, __m256i, __m256i, int);
extern __m256i   __cdecl _mm256_maskz_ternarylogic_epi64(__mmask8, __m256i, __m256i, __m256i, int);
extern __m256i   __cdecl _mm256_ternarylogic_epi64(__m256i, __m256i, __m256i, int);
extern __mmask8  __cdecl _mm_mask_test_epi16_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_test_epi16_mask(__m128i, __m128i);
extern __mmask16 __cdecl _mm256_mask_test_epi16_mask(__mmask16, __m256i, __m256i);
extern __mmask16 __cdecl _mm256_test_epi16_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm_mask_test_epi32_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_test_epi32_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm256_mask_test_epi32_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_test_epi32_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm_mask_test_epi64_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_test_epi64_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm256_mask_test_epi64_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_test_epi64_mask(__m256i, __m256i);
extern __mmask16 __cdecl _mm_mask_test_epi8_mask(__mmask16, __m128i, __m128i);
extern __mmask16 __cdecl _mm_test_epi8_mask(__m128i, __m128i);
extern __mmask32 __cdecl _mm256_mask_test_epi8_mask(__mmask32, __m256i, __m256i);
extern __mmask32 __cdecl _mm256_test_epi8_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm_mask_testn_epi16_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_testn_epi16_mask(__m128i, __m128i);
extern __mmask16 __cdecl _mm256_mask_testn_epi16_mask(__mmask16, __m256i, __m256i);
extern __mmask16 __cdecl _mm256_testn_epi16_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm_mask_testn_epi32_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_testn_epi32_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm256_mask_testn_epi32_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_testn_epi32_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm_mask_testn_epi64_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_testn_epi64_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm256_mask_testn_epi64_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_testn_epi64_mask(__m256i, __m256i);
extern __mmask16 __cdecl _mm_mask_testn_epi8_mask(__mmask16, __m128i, __m128i);
extern __mmask16 __cdecl _mm_testn_epi8_mask(__m128i, __m128i);
extern __mmask32 __cdecl _mm256_mask_testn_epi8_mask(__mmask32, __m256i, __m256i);
extern __mmask32 __cdecl _mm256_testn_epi8_mask(__m256i, __m256i);
extern __m128i   __cdecl _mm_mask_unpackhi_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_unpackhi_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_unpackhi_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_unpackhi_epi16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_unpackhi_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_unpackhi_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_unpackhi_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_unpackhi_epi32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_unpackhi_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_unpackhi_epi64(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_unpackhi_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_unpackhi_epi64(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_unpackhi_epi8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_unpackhi_epi8(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_unpackhi_epi8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_unpackhi_epi8(__mmask32, __m256i, __m256i);
extern __m128d   __cdecl _mm_mask_unpackhi_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_unpackhi_pd(__mmask8, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_unpackhi_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_maskz_unpackhi_pd(__mmask8, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_unpackhi_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_unpackhi_ps(__mmask8, __m128, __m128);
extern __m256    __cdecl _mm256_mask_unpackhi_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_maskz_unpackhi_ps(__mmask8, __m256, __m256);
extern __m128i   __cdecl _mm_mask_unpacklo_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_unpacklo_epi16(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_unpacklo_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_unpacklo_epi16(__mmask16, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_unpacklo_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_unpacklo_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_unpacklo_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_unpacklo_epi32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_unpacklo_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_unpacklo_epi64(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_unpacklo_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_unpacklo_epi64(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_mask_unpacklo_epi8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_unpacklo_epi8(__mmask16, __m128i, __m128i);
extern __m256i   __cdecl _mm256_mask_unpacklo_epi8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_unpacklo_epi8(__mmask32, __m256i, __m256i);
extern __m128d   __cdecl _mm_mask_unpacklo_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_unpacklo_pd(__mmask8, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_unpacklo_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_maskz_unpacklo_pd(__mmask8, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_unpacklo_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_unpacklo_ps(__mmask8, __m128, __m128);
extern __m256    __cdecl _mm256_mask_unpacklo_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_maskz_unpacklo_ps(__mmask8, __m256, __m256);
extern __m128i   __cdecl _mm_xor_epi32(__m128i, __m128i);
extern __m128i   __cdecl _mm_mask_xor_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_xor_epi32(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_xor_epi32(__m256i, __m256i);
extern __m256i   __cdecl _mm256_mask_xor_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_xor_epi32(__mmask8, __m256i, __m256i);
extern __m128i   __cdecl _mm_xor_epi64(__m128i, __m128i);
extern __m128i   __cdecl _mm_mask_xor_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i   __cdecl _mm_maskz_xor_epi64(__mmask8, __m128i, __m128i);
extern __m256i   __cdecl _mm256_xor_epi64(__m256i, __m256i);
extern __m256i   __cdecl _mm256_mask_xor_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i   __cdecl _mm256_maskz_xor_epi64(__mmask8, __m256i, __m256i);
extern __m128d   __cdecl _mm_mask_xor_pd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_xor_pd(__mmask8, __m128d, __m128d);
extern __m256d   __cdecl _mm256_mask_xor_pd(__m256d, __mmask8, __m256d, __m256d);
extern __m256d   __cdecl _mm256_maskz_xor_pd(__mmask8, __m256d, __m256d);
extern __m128    __cdecl _mm_mask_xor_ps(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_xor_ps(__mmask8, __m128, __m128);
extern __m256    __cdecl _mm256_mask_xor_ps(__m256, __mmask8, __m256, __m256);
extern __m256    __cdecl _mm256_maskz_xor_ps(__mmask8, __m256, __m256);

extern __mmask16  __cdecl _mm_cmpeq_epi8_mask(__m128i, __m128i);
extern __mmask16  __cdecl _mm_cmpge_epi8_mask(__m128i, __m128i);
extern __mmask16  __cdecl _mm_cmpgt_epi8_mask(__m128i, __m128i);
extern __mmask16  __cdecl _mm_cmple_epi8_mask(__m128i, __m128i);
extern __mmask16  __cdecl _mm_cmplt_epi8_mask(__m128i, __m128i);
extern __mmask16  __cdecl _mm_cmpneq_epi8_mask(__m128i, __m128i);
extern __mmask16  __cdecl _mm_cmpeq_epu8_mask(__m128i, __m128i);
extern __mmask16  __cdecl _mm_cmpge_epu8_mask(__m128i, __m128i);
extern __mmask16  __cdecl _mm_cmpgt_epu8_mask(__m128i, __m128i);
extern __mmask16  __cdecl _mm_cmple_epu8_mask(__m128i, __m128i);
extern __mmask16  __cdecl _mm_cmplt_epu8_mask(__m128i, __m128i);
extern __mmask16  __cdecl _mm_cmpneq_epu8_mask(__m128i, __m128i);

extern __mmask16  __cdecl _mm_mask_cmpeq_epi8_mask(__mmask16, __m128i, __m128i);
extern __mmask16  __cdecl _mm_mask_cmpge_epi8_mask(__mmask16, __m128i, __m128i);
extern __mmask16  __cdecl _mm_mask_cmpgt_epi8_mask(__mmask16, __m128i, __m128i);
extern __mmask16  __cdecl _mm_mask_cmple_epi8_mask(__mmask16, __m128i, __m128i);
extern __mmask16  __cdecl _mm_mask_cmplt_epi8_mask(__mmask16, __m128i, __m128i);
extern __mmask16  __cdecl _mm_mask_cmpneq_epi8_mask(__mmask16, __m128i, __m128i);
extern __mmask16  __cdecl _mm_mask_cmpeq_epu8_mask(__mmask16, __m128i, __m128i);
extern __mmask16  __cdecl _mm_mask_cmpge_epu8_mask(__mmask16, __m128i, __m128i);
extern __mmask16  __cdecl _mm_mask_cmpgt_epu8_mask(__mmask16, __m128i, __m128i);
extern __mmask16  __cdecl _mm_mask_cmple_epu8_mask(__mmask16, __m128i, __m128i);
extern __mmask16  __cdecl _mm_mask_cmplt_epu8_mask(__mmask16, __m128i, __m128i);
extern __mmask16  __cdecl _mm_mask_cmpneq_epu8_mask(__mmask16, __m128i, __m128i);

extern __mmask8  __cdecl _mm_cmpeq_epi16_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpge_epi16_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpgt_epi16_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmple_epi16_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmplt_epi16_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpneq_epi16_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpeq_epu16_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpge_epu16_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpgt_epu16_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmple_epu16_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmplt_epu16_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpneq_epu16_mask(__m128i, __m128i);

extern __mmask8  __cdecl _mm_mask_cmpeq_epi16_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpge_epi16_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpgt_epi16_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmple_epi16_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmplt_epi16_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpneq_epi16_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpeq_epu16_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpge_epu16_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpgt_epu16_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmple_epu16_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmplt_epu16_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpneq_epu16_mask(__mmask8, __m128i, __m128i);

extern __mmask8  __cdecl _mm_cmpeq_epi32_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpge_epi32_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpgt_epi32_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmple_epi32_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmplt_epi32_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpneq_epi32_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpeq_epu32_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpge_epu32_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpgt_epu32_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmple_epu32_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmplt_epu32_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpneq_epu32_mask(__m128i, __m128i);

extern __mmask8  __cdecl _mm_mask_cmpeq_epi32_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpge_epi32_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpgt_epi32_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmple_epi32_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmplt_epi32_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpneq_epi32_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpeq_epu32_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpge_epu32_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpgt_epu32_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmple_epu32_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmplt_epu32_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpneq_epu32_mask(__mmask8, __m128i, __m128i);

extern __mmask8  __cdecl _mm_cmpeq_epi64_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpge_epi64_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpgt_epi64_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmple_epi64_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmplt_epi64_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpneq_epi64_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpeq_epu64_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpge_epu64_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpgt_epu64_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmple_epu64_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmplt_epu64_mask(__m128i, __m128i);
extern __mmask8  __cdecl _mm_cmpneq_epu64_mask(__m128i, __m128i);

extern __mmask8  __cdecl _mm_mask_cmpeq_epi64_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpge_epi64_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpgt_epi64_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmple_epi64_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmplt_epi64_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpneq_epi64_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpeq_epu64_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpge_epu64_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpgt_epu64_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmple_epu64_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmplt_epu64_mask(__mmask8, __m128i, __m128i);
extern __mmask8  __cdecl _mm_mask_cmpneq_epu64_mask(__mmask8, __m128i, __m128i);

extern __mmask32  __cdecl _mm256_cmpeq_epi8_mask(__m256i, __m256i);
extern __mmask32  __cdecl _mm256_cmpge_epi8_mask(__m256i, __m256i);
extern __mmask32  __cdecl _mm256_cmpgt_epi8_mask(__m256i, __m256i);
extern __mmask32  __cdecl _mm256_cmple_epi8_mask(__m256i, __m256i);
extern __mmask32  __cdecl _mm256_cmplt_epi8_mask(__m256i, __m256i);
extern __mmask32  __cdecl _mm256_cmpneq_epi8_mask(__m256i, __m256i);
extern __mmask32  __cdecl _mm256_cmpeq_epu8_mask(__m256i, __m256i);
extern __mmask32  __cdecl _mm256_cmpge_epu8_mask(__m256i, __m256i);
extern __mmask32  __cdecl _mm256_cmpgt_epu8_mask(__m256i, __m256i);
extern __mmask32  __cdecl _mm256_cmple_epu8_mask(__m256i, __m256i);
extern __mmask32  __cdecl _mm256_cmplt_epu8_mask(__m256i, __m256i);
extern __mmask32  __cdecl _mm256_cmpneq_epu8_mask(__m256i, __m256i);

extern __mmask32  __cdecl _mm256_mask_cmpeq_epi8_mask(__mmask32, __m256i, __m256i);
extern __mmask32  __cdecl _mm256_mask_cmpge_epi8_mask(__mmask32, __m256i, __m256i);
extern __mmask32  __cdecl _mm256_mask_cmpgt_epi8_mask(__mmask32, __m256i, __m256i);
extern __mmask32  __cdecl _mm256_mask_cmple_epi8_mask(__mmask32, __m256i, __m256i);
extern __mmask32  __cdecl _mm256_mask_cmplt_epi8_mask(__mmask32, __m256i, __m256i);
extern __mmask32  __cdecl _mm256_mask_cmpneq_epi8_mask(__mmask32, __m256i, __m256i);
extern __mmask32  __cdecl _mm256_mask_cmpeq_epu8_mask(__mmask32, __m256i, __m256i);
extern __mmask32  __cdecl _mm256_mask_cmpge_epu8_mask(__mmask32, __m256i, __m256i);
extern __mmask32  __cdecl _mm256_mask_cmpgt_epu8_mask(__mmask32, __m256i, __m256i);
extern __mmask32  __cdecl _mm256_mask_cmple_epu8_mask(__mmask32, __m256i, __m256i);
extern __mmask32  __cdecl _mm256_mask_cmplt_epu8_mask(__mmask32, __m256i, __m256i);
extern __mmask32  __cdecl _mm256_mask_cmpneq_epu8_mask(__mmask32, __m256i, __m256i);

extern __mmask16  __cdecl _mm256_cmpeq_epi16_mask(__m256i, __m256i);
extern __mmask16  __cdecl _mm256_cmpge_epi16_mask(__m256i, __m256i);
extern __mmask16  __cdecl _mm256_cmpgt_epi16_mask(__m256i, __m256i);
extern __mmask16  __cdecl _mm256_cmple_epi16_mask(__m256i, __m256i);
extern __mmask16  __cdecl _mm256_cmplt_epi16_mask(__m256i, __m256i);
extern __mmask16  __cdecl _mm256_cmpneq_epi16_mask(__m256i, __m256i);
extern __mmask16  __cdecl _mm256_cmpeq_epu16_mask(__m256i, __m256i);
extern __mmask16  __cdecl _mm256_cmpge_epu16_mask(__m256i, __m256i);
extern __mmask16  __cdecl _mm256_cmpgt_epu16_mask(__m256i, __m256i);
extern __mmask16  __cdecl _mm256_cmple_epu16_mask(__m256i, __m256i);
extern __mmask16  __cdecl _mm256_cmplt_epu16_mask(__m256i, __m256i);
extern __mmask16  __cdecl _mm256_cmpneq_epu16_mask(__m256i, __m256i);

extern __mmask16  __cdecl _mm256_mask_cmpeq_epi16_mask(__mmask16, __m256i, __m256i);
extern __mmask16  __cdecl _mm256_mask_cmpge_epi16_mask(__mmask16, __m256i, __m256i);
extern __mmask16  __cdecl _mm256_mask_cmpgt_epi16_mask(__mmask16, __m256i, __m256i);
extern __mmask16  __cdecl _mm256_mask_cmple_epi16_mask(__mmask16, __m256i, __m256i);
extern __mmask16  __cdecl _mm256_mask_cmplt_epi16_mask(__mmask16, __m256i, __m256i);
extern __mmask16  __cdecl _mm256_mask_cmpneq_epi16_mask(__mmask16, __m256i, __m256i);
extern __mmask16  __cdecl _mm256_mask_cmpeq_epu16_mask(__mmask16, __m256i, __m256i);
extern __mmask16  __cdecl _mm256_mask_cmpge_epu16_mask(__mmask16, __m256i, __m256i);
extern __mmask16  __cdecl _mm256_mask_cmpgt_epu16_mask(__mmask16, __m256i, __m256i);
extern __mmask16  __cdecl _mm256_mask_cmple_epu16_mask(__mmask16, __m256i, __m256i);
extern __mmask16  __cdecl _mm256_mask_cmplt_epu16_mask(__mmask16, __m256i, __m256i);
extern __mmask16  __cdecl _mm256_mask_cmpneq_epu16_mask(__mmask16, __m256i, __m256i);

extern __mmask8  __cdecl _mm256_cmpeq_epi32_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmpge_epi32_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmpgt_epi32_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmple_epi32_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmplt_epi32_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmpneq_epi32_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmpeq_epu32_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmpge_epu32_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmpgt_epu32_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmple_epu32_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmplt_epu32_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmpneq_epu32_mask(__m256i, __m256i);

extern __mmask8  __cdecl _mm256_mask_cmpeq_epi32_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmpge_epi32_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmpgt_epi32_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmple_epi32_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmplt_epi32_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmpneq_epi32_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmpeq_epu32_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmpge_epu32_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmpgt_epu32_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmple_epu32_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmplt_epu32_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmpneq_epu32_mask(__mmask8, __m256i, __m256i);

extern __mmask8  __cdecl _mm256_cmpeq_epi64_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmpge_epi64_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmpgt_epi64_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmple_epi64_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmplt_epi64_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmpneq_epi64_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmpeq_epu64_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmpge_epu64_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmpgt_epu64_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmple_epu64_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmplt_epu64_mask(__m256i, __m256i);
extern __mmask8  __cdecl _mm256_cmpneq_epu64_mask(__m256i, __m256i);

extern __mmask8  __cdecl _mm256_mask_cmpeq_epi64_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmpge_epi64_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmpgt_epi64_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmple_epi64_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmplt_epi64_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmpneq_epi64_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmpeq_epu64_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmpge_epu64_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmpgt_epu64_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmple_epu64_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmplt_epu64_mask(__mmask8, __m256i, __m256i);
extern __mmask8  __cdecl _mm256_mask_cmpneq_epu64_mask(__mmask8, __m256i, __m256i);

// AVX-512 scalar functions
extern __m128d   __cdecl _mm_add_round_sd(__m128d, __m128d, int);
extern __m128d   __cdecl _mm_mask_add_round_sd(__m128d, __mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_maskz_add_round_sd(__mmask8, __m128d, __m128d, int);
extern __m128    __cdecl _mm_add_round_ss(__m128, __m128, int);
extern __m128    __cdecl _mm_mask_add_round_ss(__m128, __mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_maskz_add_round_ss(__mmask8, __m128, __m128, int);
extern __m128d   __cdecl _mm_mask_add_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_add_sd(__mmask8, __m128d, __m128d);
extern __m128    __cdecl _mm_mask_add_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_add_ss(__mmask8, __m128, __m128);
extern __mmask8  __cdecl _mm_cmp_round_sd_mask(__m128d, __m128d, const int, const int);
extern __mmask8  __cdecl _mm_mask_cmp_round_sd_mask(__mmask8, __m128d, __m128d, const int, const int);
extern __mmask8  __cdecl _mm_cmp_round_ss_mask(__m128, __m128, const int, const int);
extern __mmask8  __cdecl _mm_mask_cmp_round_ss_mask(__mmask8, __m128, __m128, const int, const int);
extern __mmask8  __cdecl _mm_cmp_sd_mask(__m128d, __m128d, const int);
extern __mmask8  __cdecl _mm_mask_cmp_sd_mask(__mmask8, __m128d, __m128d, const int);
extern __mmask8  __cdecl _mm_cmp_ss_mask(__m128, __m128, const int);
extern __mmask8  __cdecl _mm_mask_cmp_ss_mask(__mmask8, __m128, __m128, const int);
extern int       __cdecl _mm_comi_round_sd(__m128d, __m128d, const int, const int);
extern int       __cdecl _mm_comi_round_ss(__m128, __m128, const int, const int);
extern __m128    __cdecl _mm_cvt_roundi32_ss(__m128, int, int);
extern int       __cdecl _mm_cvt_roundsd_i32(__m128d, int);
extern int       __cdecl _mm_cvt_roundsd_si32(__m128d, int);
extern __m128    __cdecl _mm_cvt_roundsd_ss(__m128, __m128d, int);
extern __m128    __cdecl _mm_mask_cvt_roundsd_ss(__m128, __mmask8, __m128, __m128d, int);
extern __m128    __cdecl _mm_maskz_cvt_roundsd_ss(__mmask8, __m128, __m128d, int);
extern unsigned int __cdecl _mm_cvt_roundsd_u32(__m128d, int);
extern __m128    __cdecl _mm_cvt_roundsi32_ss(__m128, int, int);
extern int       __cdecl _mm_cvt_roundss_i32(__m128, int);
extern __m128d   __cdecl _mm_cvt_roundss_sd(__m128d, __m128, int);
extern __m128d   __cdecl _mm_mask_cvt_roundss_sd(__m128d, __mmask8, __m128d, __m128, int);
extern __m128d   __cdecl _mm_maskz_cvt_roundss_sd(__mmask8, __m128d, __m128, int);
extern int       __cdecl _mm_cvt_roundss_si32(__m128, int);
extern unsigned int __cdecl _mm_cvt_roundss_u32(__m128, int);
extern __m128    __cdecl _mm_cvt_roundu32_ss(__m128, unsigned int, int);
extern __m128d   __cdecl _mm_cvti32_sd(__m128d, int);
extern __m128    __cdecl _mm_cvti32_ss(__m128, int);
extern int       __cdecl _mm_cvtsd_i32(__m128d);
extern __m128    __cdecl _mm_mask_cvtsd_ss(__m128, __mmask8, __m128, __m128d);
extern __m128    __cdecl _mm_maskz_cvtsd_ss(__mmask8, __m128, __m128d);
extern unsigned int __cdecl _mm_cvtsd_u32(__m128d);
extern int       __cdecl _mm_cvtss_i32(__m128);
extern __m128d   __cdecl _mm_mask_cvtss_sd(__m128d, __mmask8, __m128d, __m128);
extern __m128d   __cdecl _mm_maskz_cvtss_sd(__mmask8, __m128d, __m128);
extern unsigned int __cdecl _mm_cvtss_u32(__m128);
extern int       __cdecl _mm_cvtt_roundsd_i32(__m128d, int);
extern int       __cdecl _mm_cvtt_roundsd_si32(__m128d, int);
extern unsigned int __cdecl _mm_cvtt_roundsd_u32(__m128d, int);
extern int       __cdecl _mm_cvtt_roundss_i32(__m128, int);
extern int       __cdecl _mm_cvtt_roundss_si32(__m128, int);
extern unsigned int __cdecl _mm_cvtt_roundss_u32(__m128, int);
extern int       __cdecl _mm_cvttsd_i32(__m128d);
extern unsigned int __cdecl _mm_cvttsd_u32(__m128d);
extern int       __cdecl _mm_cvttss_i32(__m128);
extern unsigned int __cdecl _mm_cvttss_u32(__m128);
extern __m128d   __cdecl _mm_cvtu32_sd(__m128d, unsigned int);
extern __m128    __cdecl _mm_cvtu32_ss(__m128, unsigned int);
extern __m128d   __cdecl _mm_div_round_sd(__m128d, __m128d, int);
extern __m128d   __cdecl _mm_mask_div_round_sd(__m128d, __mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_maskz_div_round_sd(__mmask8, __m128d, __m128d, int);
extern __m128    __cdecl _mm_div_round_ss(__m128, __m128, int);
extern __m128    __cdecl _mm_mask_div_round_ss(__m128, __mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_maskz_div_round_ss(__mmask8, __m128, __m128, int);
extern __m128d   __cdecl _mm_mask_div_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_div_sd(__mmask8, __m128d, __m128d);
extern __m128    __cdecl _mm_mask_div_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_div_ss(__mmask8, __m128, __m128);
extern __m128d   __cdecl _mm_fixupimm_round_sd(__m128d, __m128d, __m128i, const int, int);
extern __m128d   __cdecl _mm_mask_fixupimm_round_sd(__m128d, __mmask8, __m128d, __m128i, const int, const int);
extern __m128d   __cdecl _mm_maskz_fixupimm_round_sd(__mmask8, __m128d, __m128d, __m128i, const int, const int);
extern __m128    __cdecl _mm_fixupimm_round_ss(__m128, __m128, __m128i, const int, const int);
extern __m128    __cdecl _mm_mask_fixupimm_round_ss(__m128, __mmask8, __m128, __m128i, const int, const int);
extern __m128    __cdecl _mm_maskz_fixupimm_round_ss(__mmask8, __m128, __m128, __m128i, const int, const int);
extern __m128d   __cdecl _mm_fixupimm_sd(__m128d, __m128d, __m128i, const int);
extern __m128d   __cdecl _mm_mask_fixupimm_sd(__m128d, __mmask8, __m128d, __m128i, const int);
extern __m128d   __cdecl _mm_maskz_fixupimm_sd(__mmask8, __m128d, __m128d, __m128i, const int);
extern __m128    __cdecl _mm_fixupimm_ss(__m128, __m128, __m128i, const int);
extern __m128    __cdecl _mm_mask_fixupimm_ss(__m128, __mmask8, __m128, __m128i, const int);
extern __m128    __cdecl _mm_maskz_fixupimm_ss(__mmask8, __m128, __m128, __m128i, const int);
extern __m128d   __cdecl _mm_fmadd_round_sd(__m128d, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_mask_fmadd_round_sd(__m128d, __mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_mask3_fmadd_round_sd(__m128d, __m128d, __m128d, __mmask8, int);
extern __m128d   __cdecl _mm_maskz_fmadd_round_sd(__mmask8, __m128d, __m128d, __m128d, int);
extern __m128    __cdecl _mm_fmadd_round_ss(__m128, __m128, __m128, int);
extern __m128    __cdecl _mm_mask_fmadd_round_ss(__m128, __mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_mask3_fmadd_round_ss(__m128, __m128, __m128, __mmask8, int);
extern __m128    __cdecl _mm_maskz_fmadd_round_ss(__mmask8, __m128, __m128, __m128, int);
extern __m128d   __cdecl _mm_mask_fmadd_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_mask3_fmadd_sd(__m128d, __m128d, __m128d, __mmask8);
extern __m128d   __cdecl _mm_maskz_fmadd_sd(__mmask8, __m128d, __m128d, __m128d);
extern __m128    __cdecl _mm_mask_fmadd_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_mask3_fmadd_ss(__m128, __m128, __m128, __mmask8);
extern __m128    __cdecl _mm_maskz_fmadd_ss(__mmask8, __m128, __m128, __m128);
extern __m128d   __cdecl _mm_fmsub_round_sd(__m128d, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_mask_fmsub_round_sd(__m128d, __mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_mask3_fmsub_round_sd(__m128d, __m128d, __m128d, __mmask8, int);
extern __m128d   __cdecl _mm_maskz_fmsub_round_sd(__mmask8, __m128d, __m128d, __m128d, int);
extern __m128    __cdecl _mm_fmsub_round_ss(__m128, __m128, __m128, int);
extern __m128    __cdecl _mm_mask_fmsub_round_ss(__m128, __mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_mask3_fmsub_round_ss(__m128, __m128, __m128, __mmask8, int);
extern __m128    __cdecl _mm_maskz_fmsub_round_ss(__mmask8, __m128, __m128, __m128, int);
extern __m128d   __cdecl _mm_mask_fmsub_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_mask3_fmsub_sd(__m128d, __m128d, __m128d, __mmask8);
extern __m128d   __cdecl _mm_maskz_fmsub_sd(__mmask8, __m128d, __m128d, __m128d);
extern __m128    __cdecl _mm_mask_fmsub_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_mask3_fmsub_ss(__m128, __m128, __m128, __mmask8);
extern __m128    __cdecl _mm_maskz_fmsub_ss(__mmask8, __m128, __m128, __m128);
extern __m128d   __cdecl _mm_fnmadd_round_sd(__m128d, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_mask_fnmadd_round_sd(__m128d, __mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_mask3_fnmadd_round_sd(__m128d, __m128d, __m128d, __mmask8, int);
extern __m128d   __cdecl _mm_maskz_fnmadd_round_sd(__mmask8, __m128d, __m128d, __m128d, int);
extern __m128    __cdecl _mm_fnmadd_round_ss(__m128, __m128, __m128, int);
extern __m128    __cdecl _mm_mask_fnmadd_round_ss(__m128, __mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_mask3_fnmadd_round_ss(__m128, __m128, __m128, __mmask8, int);
extern __m128    __cdecl _mm_maskz_fnmadd_round_ss(__mmask8, __m128, __m128, __m128, int);
extern __m128d   __cdecl _mm_mask_fnmadd_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_mask3_fnmadd_sd(__m128d, __m128d, __m128d, __mmask8);
extern __m128d   __cdecl _mm_maskz_fnmadd_sd(__mmask8, __m128d, __m128d, __m128d);
extern __m128    __cdecl _mm_mask_fnmadd_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_mask3_fnmadd_ss(__m128, __m128, __m128, __mmask8);
extern __m128    __cdecl _mm_maskz_fnmadd_ss(__mmask8, __m128, __m128, __m128);
extern __m128d   __cdecl _mm_fnmsub_round_sd(__m128d, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_mask_fnmsub_round_sd(__m128d, __mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_mask3_fnmsub_round_sd(__m128d, __m128d, __m128d, __mmask8, int);
extern __m128d   __cdecl _mm_maskz_fnmsub_round_sd(__mmask8, __m128d, __m128d, __m128d, int);
extern __m128    __cdecl _mm_fnmsub_round_ss(__m128, __m128, __m128, int);
extern __m128    __cdecl _mm_mask_fnmsub_round_ss(__m128, __mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_mask3_fnmsub_round_ss(__m128, __m128, __m128, __mmask8, int);
extern __m128    __cdecl _mm_maskz_fnmsub_round_ss(__mmask8, __m128, __m128, __m128, int);
extern __m128d   __cdecl _mm_mask_fnmsub_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_mask3_fnmsub_sd(__m128d, __m128d, __m128d, __mmask8);
extern __m128d   __cdecl _mm_maskz_fnmsub_sd(__mmask8, __m128d, __m128d, __m128d);
extern __m128    __cdecl _mm_mask_fnmsub_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_mask3_fnmsub_ss(__m128, __m128, __m128, __mmask8);
extern __m128    __cdecl _mm_maskz_fnmsub_ss(__mmask8, __m128, __m128, __m128);
extern __mmask8  __cdecl _mm_fpclass_sd_mask(__m128d, int);
extern __mmask8  __cdecl _mm_mask_fpclass_sd_mask(__mmask8, __m128d, int);
extern __mmask8  __cdecl _mm_fpclass_ss_mask(__m128, int);
extern __mmask8  __cdecl _mm_mask_fpclass_ss_mask(__mmask8, __m128, int);
extern __m128d   __cdecl _mm_getexp_round_sd(__m128d, __m128d, const int);
extern __m128d   __cdecl _mm_mask_getexp_round_sd(__m128d, __mmask8, __m128d, __m128d, const int);
extern __m128d   __cdecl _mm_maskz_getexp_round_sd(__mmask8, __m128d, __m128d, const int);
extern __m128    __cdecl _mm_getexp_round_ss(__m128, __m128, const int);
extern __m128    __cdecl _mm_mask_getexp_round_ss(__m128, __mmask8, __m128, __m128, const int);
extern __m128    __cdecl _mm_maskz_getexp_round_ss(__mmask8, __m128, __m128, const int);
extern __m128d   __cdecl _mm_getexp_sd(__m128d, __m128d);
extern __m128d   __cdecl _mm_mask_getexp_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_getexp_sd(__mmask8, __m128d, __m128d);
extern __m128    __cdecl _mm_getexp_ss(__m128, __m128);
extern __m128    __cdecl _mm_mask_getexp_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_getexp_ss(__mmask8, __m128, __m128);
extern __m128d   __cdecl _mm_getmant_round_sd(__m128d, __m128d, int, int, int);
extern __m128d   __cdecl _mm_mask_getmant_round_sd(__m128d, __mmask8, __m128d, __m128d, int, int, int);
extern __m128d   __cdecl _mm_maskz_getmant_round_sd(__mmask8, __m128d, __m128d, int, int, int);
extern __m128    __cdecl _mm_getmant_round_ss(__m128, __m128, int, int, int);
extern __m128    __cdecl _mm_mask_getmant_round_ss(__m128, __mmask8, __m128, __m128, int, int, int);
extern __m128    __cdecl _mm_maskz_getmant_round_ss(__mmask8, __m128, __m128, int, int, int);
extern __m128d   __cdecl _mm_getmant_sd(__m128d, __m128d, int, int);
extern __m128d   __cdecl _mm_mask_getmant_sd(__m128d, __mmask8, __m128d, __m128d, int, int);
extern __m128d   __cdecl _mm_maskz_getmant_sd(__mmask8, __m128d, __m128d, int, int);
extern __m128    __cdecl _mm_getmant_ss(__m128, __m128, int, int);
extern __m128    __cdecl _mm_mask_getmant_ss(__m128, __mmask8, __m128, __m128, int, int);
extern __m128    __cdecl _mm_maskz_getmant_ss(__mmask8, __m128, __m128, int, int);
extern __m128d   __cdecl _mm_mask_load_sd(__m128d, __mmask8, const double*);
extern __m128d   __cdecl _mm_maskz_load_sd(__mmask8, const double*);
extern __m128    __cdecl _mm_mask_load_ss(__m128, __mmask8, const float*);
extern __m128    __cdecl _mm_maskz_load_ss(__mmask8, const float*);
extern __m128d   __cdecl _mm_mask_max_round_sd(__m128d, __mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_maskz_max_round_sd(__mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_max_round_sd(__m128d, __m128d, int);
extern __m128    __cdecl _mm_mask_max_round_ss(__m128, __mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_maskz_max_round_ss(__mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_max_round_ss(__m128, __m128, int);
extern __m128d   __cdecl _mm_mask_max_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_max_sd(__mmask8, __m128d, __m128d);
extern __m128    __cdecl _mm_mask_max_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_max_ss(__mmask8, __m128, __m128);
extern __m128d   __cdecl _mm_mask_min_round_sd(__m128d, __mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_maskz_min_round_sd(__mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_min_round_sd(__m128d, __m128d, int);
extern __m128    __cdecl _mm_mask_min_round_ss(__m128, __mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_maskz_min_round_ss(__mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_min_round_ss(__m128, __m128, int);
extern __m128d   __cdecl _mm_mask_min_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_min_sd(__mmask8, __m128d, __m128d);
extern __m128    __cdecl _mm_mask_min_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_min_ss(__mmask8, __m128, __m128);
extern __m128d   __cdecl _mm_mask_move_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_move_sd(__mmask8, __m128d, __m128d);
extern __m128    __cdecl _mm_mask_move_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_move_ss(__mmask8, __m128, __m128);
extern __m128d   __cdecl _mm_mask_mul_round_sd(__m128d, __mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_maskz_mul_round_sd(__mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_mul_round_sd(__m128d, __m128d, int);
extern __m128    __cdecl _mm_mask_mul_round_ss(__m128, __mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_maskz_mul_round_ss(__mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_mul_round_ss(__m128, __m128, int);
extern __m128d   __cdecl _mm_mask_mul_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_mul_sd(__mmask8, __m128d, __m128d);
extern __m128    __cdecl _mm_mask_mul_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_mul_ss(__mmask8, __m128, __m128);
extern __m128d   __cdecl _mm_range_sd(__m128d, __m128d, const int);
extern __m128d   __cdecl _mm_mask_range_sd(__m128d, __mmask8, __m128d, __m128d, const int);
extern __m128d   __cdecl _mm_maskz_range_sd(__mmask8, __m128d, __m128d, const int);
extern __m128d   __cdecl _mm_range_round_sd(__m128d, __m128d, const int, int);
extern __m128d   __cdecl _mm_mask_range_round_sd(__m128d, __mmask8, __m128d, __m128d, const int, int);
extern __m128d   __cdecl _mm_maskz_range_round_sd(__mmask8, __m128d, __m128d, const int, int);
extern __m128    __cdecl _mm_range_ss(__m128, __m128, const int);
extern __m128    __cdecl _mm_mask_range_ss(__m128, __mmask8, __m128, __m128, const int);
extern __m128    __cdecl _mm_maskz_range_ss(__mmask8, __m128, __m128, const int);
extern __m128    __cdecl _mm_range_round_ss(__m128, __m128, const int, int);
extern __m128    __cdecl _mm_mask_range_round_ss(__m128, __mmask8, __m128, __m128, const int, int);
extern __m128    __cdecl _mm_maskz_range_round_ss(__mmask8, __m128, __m128, const int, int);
extern __m128d   __cdecl _mm_mask_rcp14_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_rcp14_sd(__mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_rcp14_sd(__m128d, __m128d);
extern __m128    __cdecl _mm_mask_rcp14_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_rcp14_ss(__mmask8, __m128, __m128);
extern __m128    __cdecl _mm_rcp14_ss(__m128, __m128);
extern __m128d   __cdecl _mm_mask_rcp28_round_sd(__m128d, __mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_maskz_rcp28_round_sd(__mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_rcp28_round_sd(__m128d, __m128d, int);
extern __m128    __cdecl _mm_mask_rcp28_round_ss(__m128, __mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_maskz_rcp28_round_ss(__mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_rcp28_round_ss(__m128, __m128, int);
extern __m128d   __cdecl _mm_mask_rcp28_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_rcp28_sd(__mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_rcp28_sd(__m128d, __m128d);
extern __m128    __cdecl _mm_mask_rcp28_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_rcp28_ss(__mmask8, __m128, __m128);
extern __m128    __cdecl _mm_rcp28_ss(__m128, __m128);
extern __m128d   __cdecl _mm_mask_reduce_round_sd(__m128d, __mmask8, __m128d, __m128d, const int, int);
extern __m128d   __cdecl _mm_maskz_reduce_round_sd(__mmask8, __m128d, __m128d, const int, int);
extern __m128d   __cdecl _mm_reduce_round_sd(__m128d, __m128d, const int, int);
extern __m128    __cdecl _mm_mask_reduce_round_ss(__m128, __mmask8, __m128, __m128, const int, int);
extern __m128    __cdecl _mm_maskz_reduce_round_ss(__mmask8, __m128, __m128, const int, int);
extern __m128    __cdecl _mm_reduce_round_ss(__m128, __m128, const int, int);
extern __m128d   __cdecl _mm_mask_reduce_sd(__m128d, __mmask8, __m128d, __m128d, const int);
extern __m128d   __cdecl _mm_maskz_reduce_sd(__mmask8, __m128d, __m128d, const int);
extern __m128d   __cdecl _mm_reduce_sd(__m128d, __m128d, const int);
extern __m128    __cdecl _mm_mask_reduce_ss(__m128, __mmask8, __m128, __m128, const int);
extern __m128    __cdecl _mm_maskz_reduce_ss(__mmask8, __m128, __m128, const int);
extern __m128    __cdecl _mm_reduce_ss(__m128, __m128, const int);
extern __m128d   __cdecl _mm_mask_roundscale_round_sd(__m128d, __mmask8, __m128d, __m128d, const int, const int);
extern __m128d   __cdecl _mm_maskz_roundscale_round_sd(__mmask8, __m128d, __m128d, const int, const int);
extern __m128d   __cdecl _mm_roundscale_round_sd(__m128d, __m128d, const int, const int);
extern __m128    __cdecl _mm_mask_roundscale_round_ss(__m128, __mmask8, __m128, __m128, const int, const int);
extern __m128    __cdecl _mm_maskz_roundscale_round_ss(__mmask8, __m128, __m128, const int, const int);
extern __m128    __cdecl _mm_roundscale_round_ss(__m128, __m128, const int, const int);
extern __m128d   __cdecl _mm_mask_roundscale_sd(__m128d, __mmask8, __m128d, __m128d, const int);
extern __m128d   __cdecl _mm_maskz_roundscale_sd(__mmask8, __m128d, __m128d, const int);
extern __m128d   __cdecl _mm_roundscale_sd(__m128d, __m128d, const int);
extern __m128    __cdecl _mm_mask_roundscale_ss(__m128, __mmask8, __m128, __m128, const int);
extern __m128    __cdecl _mm_maskz_roundscale_ss(__mmask8, __m128, __m128, const int);
extern __m128    __cdecl _mm_roundscale_ss(__m128, __m128, const int);
extern __m128d   __cdecl _mm_mask_rsqrt14_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_rsqrt14_sd(__mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_rsqrt14_sd(__m128d, __m128d);
extern __m128    __cdecl _mm_mask_rsqrt14_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_rsqrt14_ss(__mmask8, __m128, __m128);
extern __m128    __cdecl _mm_rsqrt14_ss(__m128, __m128);
extern __m128d   __cdecl _mm_mask_rsqrt28_round_sd(__m128d, __mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_maskz_rsqrt28_round_sd(__mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_rsqrt28_round_sd(__m128d, __m128d, int);
extern __m128    __cdecl _mm_mask_rsqrt28_round_ss(__m128, __mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_maskz_rsqrt28_round_ss(__mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_rsqrt28_round_ss(__m128, __m128, int);
extern __m128d   __cdecl _mm_mask_rsqrt28_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_rsqrt28_sd(__mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_rsqrt28_sd(__m128d, __m128d);
extern __m128    __cdecl _mm_mask_rsqrt28_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_rsqrt28_ss(__mmask8, __m128, __m128);
extern __m128    __cdecl _mm_rsqrt28_ss(__m128, __m128);
extern __m128d   __cdecl _mm_mask_scalef_round_sd(__m128d, __mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_maskz_scalef_round_sd(__mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_scalef_round_sd(__m128d, __m128d, int);
extern __m128    __cdecl _mm_mask_scalef_round_ss(__m128, __mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_maskz_scalef_round_ss(__mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_scalef_round_ss(__m128, __m128, int);
extern __m128d   __cdecl _mm_mask_scalef_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_scalef_sd(__mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_scalef_sd(__m128d, __m128d);
extern __m128    __cdecl _mm_mask_scalef_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_scalef_ss(__mmask8, __m128, __m128);
extern __m128    __cdecl _mm_scalef_ss(__m128, __m128);
extern __m128d   __cdecl _mm_mask_sqrt_round_sd(__m128d, __mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_maskz_sqrt_round_sd(__mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_sqrt_round_sd(__m128d, __m128d, int);
extern __m128    __cdecl _mm_mask_sqrt_round_ss(__m128, __mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_maskz_sqrt_round_ss(__mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_sqrt_round_ss(__m128, __m128, int);
extern __m128d   __cdecl _mm_mask_sqrt_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_sqrt_sd(__mmask8, __m128d, __m128d);
extern __m128    __cdecl _mm_mask_sqrt_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_sqrt_ss(__mmask8, __m128, __m128);
extern void      __cdecl _mm_mask_store_sd(double*, __mmask8, __m128d);
extern void      __cdecl _mm_mask_store_ss(float*, __mmask8, __m128);
extern __m128d   __cdecl _mm_mask_sub_round_sd(__m128d, __mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_maskz_sub_round_sd(__mmask8, __m128d, __m128d, int);
extern __m128d   __cdecl _mm_sub_round_sd(__m128d, __m128d, int);
extern __m128    __cdecl _mm_mask_sub_round_ss(__m128, __mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_maskz_sub_round_ss(__mmask8, __m128, __m128, int);
extern __m128    __cdecl _mm_sub_round_ss(__m128, __m128, int);
extern __m128d   __cdecl _mm_mask_sub_sd(__m128d, __mmask8, __m128d, __m128d);
extern __m128d   __cdecl _mm_maskz_sub_sd(__mmask8, __m128d, __m128d);
extern __m128    __cdecl _mm_mask_sub_ss(__m128, __mmask8, __m128, __m128);
extern __m128    __cdecl _mm_maskz_sub_ss(__mmask8, __m128, __m128);



extern unsigned __int64 __cdecl _mm_cvtsd_u64(__m128d);
extern unsigned __int64 __cdecl _mm_cvtss_u64(__m128);
extern unsigned __int64 __cdecl _mm_cvttsd_u64(__m128d);
extern unsigned __int64 __cdecl _mm_cvttss_u64(__m128);
extern unsigned __int64 __cdecl _mm_cvt_roundsd_u64(__m128d, int);
extern unsigned __int64 __cdecl _mm_cvt_roundss_u64(__m128, int);
extern unsigned __int64 __cdecl _mm_cvtt_roundsd_u64(__m128d, int);
extern unsigned __int64 __cdecl _mm_cvtt_roundss_u64(__m128, int);

extern __m128d   __cdecl _mm_cvti64_sd(__m128d, __int64);
extern __m128    __cdecl _mm_cvti64_ss(__m128, __int64);
extern __int64   __cdecl _mm_cvtsd_i64(__m128d);
extern __int64   __cdecl _mm_cvtss_i64(__m128);
extern __int64   __cdecl _mm_cvttsd_i64(__m128d);
extern __int64   __cdecl _mm_cvttss_i64(__m128);
extern __int64   __cdecl _mm_cvtt_roundsd_i64(__m128d, int);
extern __int64   __cdecl _mm_cvtt_roundsd_si64(__m128d, int);
extern __int64   __cdecl _mm_cvtt_roundss_i64(__m128, int);
extern __int64   __cdecl _mm_cvtt_roundss_si64(__m128, int);
extern __m128d   __cdecl _mm_cvtu64_sd(__m128d, unsigned __int64);
extern __m128    __cdecl _mm_cvtu64_ss(__m128, unsigned __int64);
extern __m128d   __cdecl _mm_cvt_roundi64_sd(__m128d, __int64, int);
extern __m128    __cdecl _mm_cvt_roundi64_ss(__m128, __int64, int);
extern __int64   __cdecl _mm_cvt_roundsd_i64(__m128d, int);
extern __int64   __cdecl _mm_cvt_roundsd_si64(__m128d, int);
extern __m128d   __cdecl _mm_cvt_roundsi64_sd(__m128d, __int64, int);
extern __m128    __cdecl _mm_cvt_roundsi64_ss(__m128, __int64, int);
extern __int64   __cdecl _mm_cvt_roundss_i64(__m128, int);
extern __int64   __cdecl _mm_cvt_roundss_si64(__m128, int);
extern __m128d   __cdecl _mm_cvt_roundu64_sd(__m128d, unsigned __int64, int);
extern __m128    __cdecl _mm_cvt_roundu64_ss(__m128, unsigned __int64, int);

#line 4344 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\zmmintrin.h"

// Zero-extended cast functions
extern __m512d   __cdecl _mm512_zextpd128_pd512(__m128d);
extern __m512d   __cdecl _mm512_zextpd256_pd512(__m256d);
extern __m512    __cdecl _mm512_zextps128_ps512(__m128);
extern __m512    __cdecl _mm512_zextps256_ps512(__m256);
extern __m512i   __cdecl _mm512_zextsi128_si512(__m128i);
extern __m512i   __cdecl _mm512_zextsi256_si512(__m256i);

/*
 * Convert Scalar Single-Precision Floating-point value in 512-bit vector to
 * equivalent C/C++ float type.
 */
// float _mm512_cvtss_f32 (__m512 a)


/*
 * Convert Scalar Double-Precision Floating-point value in 512-bit vector to
 * equivalent C/C++ double type.
 */
// double _mm512_cvtsd_f64 (__m512d a)


/*
 * Convert 32-bit Scalar integer in 512-bit vector to equivalent C/C++ int type.
 */
// int _mm512_cvtsi512_si32 (__m512i a)



/*
 * Convert 64-bit Scalar integer in 512-bit vector to equivalent C/C++ __int64 type.
 */
// __int64 _mm512_cvtsi512_si64 (__m512i a)

#line 4380 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\zmmintrin.h"

// AVX512_IFMA intrinsics
extern __m128i __cdecl _mm_madd52hi_epu64(__m128i, __m128i, __m128i);
extern __m128i __cdecl _mm_mask_madd52hi_epu64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i __cdecl _mm_maskz_madd52hi_epu64(__mmask8, __m128i, __m128i, __m128i);
extern __m256i __cdecl _mm256_madd52hi_epu64(__m256i, __m256i, __m256i);
extern __m256i __cdecl _mm256_mask_madd52hi_epu64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i __cdecl _mm256_maskz_madd52hi_epu64(__mmask8, __m256i, __m256i, __m256i);
extern __m512i __cdecl _mm512_madd52hi_epu64(__m512i, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_madd52hi_epu64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_madd52hi_epu64(__mmask8, __m512i, __m512i, __m512i);

extern __m128i __cdecl _mm_madd52lo_epu64(__m128i, __m128i, __m128i);
extern __m128i __cdecl _mm_mask_madd52lo_epu64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i __cdecl _mm_maskz_madd52lo_epu64(__mmask8, __m128i, __m128i, __m128i);
extern __m256i __cdecl _mm256_madd52lo_epu64(__m256i, __m256i, __m256i);
extern __m256i __cdecl _mm256_mask_madd52lo_epu64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i __cdecl _mm256_maskz_madd52lo_epu64(__mmask8, __m256i, __m256i, __m256i);
extern __m512i __cdecl _mm512_madd52lo_epu64(__m512i, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_madd52lo_epu64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_madd52lo_epu64(__mmask8, __m512i, __m512i, __m512i);

// AVX512_VBMI intrinsics
extern __m128i __cdecl _mm_permutexvar_epi8(__m128i, __m128i);
extern __m128i __cdecl _mm_mask_permutexvar_epi8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i __cdecl _mm_maskz_permutexvar_epi8(__mmask16, __m128i, __m128i);
extern __m256i __cdecl _mm256_permutexvar_epi8(__m256i, __m256i);
extern __m256i __cdecl _mm256_mask_permutexvar_epi8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i __cdecl _mm256_maskz_permutexvar_epi8(__mmask32, __m256i, __m256i);
extern __m512i __cdecl _mm512_permutexvar_epi8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_permutexvar_epi8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_permutexvar_epi8(__mmask64, __m512i, __m512i);

extern __m128i __cdecl _mm_permutex2var_epi8(__m128i, __m128i /* idx */, __m128i);
extern __m128i __cdecl _mm_mask_permutex2var_epi8(__m128i, __mmask16, __m128i /* idx */, __m128i);
extern __m128i __cdecl _mm_mask2_permutex2var_epi8(__m128i, __m128i /* idx */, __mmask16, __m128i);
extern __m128i __cdecl _mm_maskz_permutex2var_epi8(__mmask16, __m128i, __m128i /* idx */, __m128i);
extern __m256i __cdecl _mm256_permutex2var_epi8(__m256i, __m256i, __m256i);
extern __m256i __cdecl _mm256_mask_permutex2var_epi8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i __cdecl _mm256_mask2_permutex2var_epi8(__m256i, __m256i, __mmask32, __m256i);
extern __m256i __cdecl _mm256_maskz_permutex2var_epi8(__mmask32, __m256i, __m256i, __m256i);
extern __m512i __cdecl _mm512_permutex2var_epi8(__m512i, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_permutex2var_epi8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask2_permutex2var_epi8(__m512i, __m512i, __mmask64, __m512i);
extern __m512i __cdecl _mm512_maskz_permutex2var_epi8(__mmask64, __m512i, __m512i, __m512i);

extern __m128i __cdecl _mm_multishift_epi64_epi8(__m128i, __m128i);
extern __m128i __cdecl _mm_mask_multishift_epi64_epi8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i __cdecl _mm_maskz_multishift_epi64_epi8(__mmask16, __m128i, __m128i);
extern __m256i __cdecl _mm256_multishift_epi64_epi8(__m256i, __m256i);
extern __m256i __cdecl _mm256_mask_multishift_epi64_epi8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i __cdecl _mm256_maskz_multishift_epi64_epi8(__mmask32, __m256i, __m256i);
extern __m512i __cdecl _mm512_multishift_epi64_epi8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_multishift_epi64_epi8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_multishift_epi64_epi8(__mmask64, __m512i, __m512i);

// AVX512_VNNI intrinsics
extern __m128i __cdecl _mm_dpbusd_epi32(__m128i, __m128i, __m128i);
extern __m128i __cdecl _mm_mask_dpbusd_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i __cdecl _mm_maskz_dpbusd_epi32(__mmask8, __m128i, __m128i, __m128i);
extern __m256i __cdecl _mm256_dpbusd_epi32(__m256i, __m256i, __m256i);
extern __m256i __cdecl _mm256_mask_dpbusd_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i __cdecl _mm256_maskz_dpbusd_epi32(__mmask8, __m256i, __m256i, __m256i);
extern __m512i __cdecl _mm512_dpbusd_epi32(__m512i, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_dpbusd_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_dpbusd_epi32(__mmask16, __m512i, __m512i, __m512i);

extern __m128i __cdecl _mm_dpbusds_epi32(__m128i, __m128i, __m128i);
extern __m128i __cdecl _mm_mask_dpbusds_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i __cdecl _mm_maskz_dpbusds_epi32(__mmask8, __m128i, __m128i, __m128i);
extern __m256i __cdecl _mm256_dpbusds_epi32(__m256i, __m256i, __m256i);
extern __m256i __cdecl _mm256_mask_dpbusds_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i __cdecl _mm256_maskz_dpbusds_epi32(__mmask8, __m256i, __m256i, __m256i);
extern __m512i __cdecl _mm512_dpbusds_epi32(__m512i, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_dpbusds_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_dpbusds_epi32(__mmask16, __m512i, __m512i, __m512i);

extern __m128i __cdecl _mm_dpwssd_epi32(__m128i, __m128i, __m128i);
extern __m128i __cdecl _mm_mask_dpwssd_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i __cdecl _mm_maskz_dpwssd_epi32(__mmask8, __m128i, __m128i, __m128i);
extern __m256i __cdecl _mm256_dpwssd_epi32(__m256i, __m256i, __m256i);
extern __m256i __cdecl _mm256_mask_dpwssd_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i __cdecl _mm256_maskz_dpwssd_epi32(__mmask8, __m256i, __m256i, __m256i);
extern __m512i __cdecl _mm512_dpwssd_epi32(__m512i, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_dpwssd_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_dpwssd_epi32(__mmask16, __m512i, __m512i, __m512i);

extern __m128i __cdecl _mm_dpwssds_epi32(__m128i, __m128i, __m128i);
extern __m128i __cdecl _mm_mask_dpwssds_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i __cdecl _mm_maskz_dpwssds_epi32(__mmask8, __m128i, __m128i, __m128i);
extern __m256i __cdecl _mm256_dpwssds_epi32(__m256i, __m256i, __m256i);
extern __m256i __cdecl _mm256_mask_dpwssds_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i __cdecl _mm256_maskz_dpwssds_epi32(__mmask8, __m256i, __m256i, __m256i);
extern __m512i __cdecl _mm512_dpwssds_epi32(__m512i, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_dpwssds_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_dpwssds_epi32(__mmask16, __m512i, __m512i, __m512i);

// VAES
extern __m256i __cdecl _mm256_aesenc_epi128(__m256i, __m256i);
extern __m512i __cdecl _mm512_aesenc_epi128(__m512i, __m512i);
extern __m256i __cdecl _mm256_aesenclast_epi128(__m256i, __m256i);
extern __m512i __cdecl _mm512_aesenclast_epi128(__m512i, __m512i);
extern __m256i __cdecl _mm256_aesdec_epi128(__m256i, __m256i);
extern __m512i __cdecl _mm512_aesdec_epi128(__m512i, __m512i);
extern __m256i __cdecl _mm256_aesdeclast_epi128(__m256i, __m256i);
extern __m512i __cdecl _mm512_aesdeclast_epi128(__m512i, __m512i);

// VPCLMULQDQ
extern __m256i __cdecl _mm256_clmulepi64_epi128(__m256i, __m256i, const int);
extern __m512i __cdecl _mm512_clmulepi64_epi128(__m512i, __m512i, const int);

// AVX512_VPOPCNTDQ
extern __m128i __cdecl _mm_popcnt_epi32(__m128i);
extern __m128i __cdecl _mm_mask_popcnt_epi32(__m128i, __mmask8, __m128i);
extern __m128i __cdecl _mm_maskz_popcnt_epi32(__mmask8, __m128i);
extern __m256i __cdecl _mm256_popcnt_epi32(__m256i);
extern __m256i __cdecl _mm256_mask_popcnt_epi32(__m256i, __mmask8, __m256i);
extern __m256i __cdecl _mm256_maskz_popcnt_epi32(__mmask8, __m256i);
extern __m512i __cdecl _mm512_popcnt_epi32(__m512i);
extern __m512i __cdecl _mm512_mask_popcnt_epi32(__m512i, __mmask16, __m512i);
extern __m512i __cdecl _mm512_maskz_popcnt_epi32(__mmask16, __m512i);

extern __m128i __cdecl _mm_popcnt_epi64(__m128i);
extern __m128i __cdecl _mm_mask_popcnt_epi64(__m128i, __mmask8, __m128i);
extern __m128i __cdecl _mm_maskz_popcnt_epi64(__mmask8, __m128i);
extern __m256i __cdecl _mm256_popcnt_epi64(__m256i);
extern __m256i __cdecl _mm256_mask_popcnt_epi64(__m256i, __mmask8, __m256i);
extern __m256i __cdecl _mm256_maskz_popcnt_epi64(__mmask8, __m256i);
extern __m512i __cdecl _mm512_popcnt_epi64(__m512i);
extern __m512i __cdecl _mm512_mask_popcnt_epi64(__m512i, __mmask8, __m512i);
extern __m512i __cdecl _mm512_maskz_popcnt_epi64(__mmask8, __m512i);

// AVX512_BITALG
extern __m128i __cdecl _mm_popcnt_epi8(__m128i);
extern __m128i __cdecl _mm_mask_popcnt_epi8(__m128i, __mmask16, __m128i);
extern __m128i __cdecl _mm_maskz_popcnt_epi8(__mmask16, __m128i);
extern __m256i __cdecl _mm256_popcnt_epi8(__m256i);
extern __m256i __cdecl _mm256_mask_popcnt_epi8(__m256i, __mmask32, __m256i);
extern __m256i __cdecl _mm256_maskz_popcnt_epi8(__mmask32, __m256i);
extern __m512i __cdecl _mm512_popcnt_epi8(__m512i);
extern __m512i __cdecl _mm512_mask_popcnt_epi8(__m512i, __mmask64, __m512i);
extern __m512i __cdecl _mm512_maskz_popcnt_epi8(__mmask64, __m512i);

extern __m128i __cdecl _mm_popcnt_epi16(__m128i);
extern __m128i __cdecl _mm_mask_popcnt_epi16(__m128i, __mmask8, __m128i);
extern __m128i __cdecl _mm_maskz_popcnt_epi16(__mmask8, __m128i);
extern __m256i __cdecl _mm256_popcnt_epi16(__m256i);
extern __m256i __cdecl _mm256_mask_popcnt_epi16(__m256i, __mmask16, __m256i);
extern __m256i __cdecl _mm256_maskz_popcnt_epi16(__mmask16, __m256i);
extern __m512i __cdecl _mm512_popcnt_epi16(__m512i);
extern __m512i __cdecl _mm512_mask_popcnt_epi16(__m512i, __mmask32, __m512i);
extern __m512i __cdecl _mm512_maskz_popcnt_epi16(__mmask32, __m512i);

extern __mmask16 __cdecl _mm_bitshuffle_epi64_mask(__m128i, __m128i);
extern __mmask16 __cdecl _mm_mask_bitshuffle_epi64_mask(__mmask16, __m128i, __m128i);
extern __mmask32 __cdecl _mm256_bitshuffle_epi64_mask(__m256i, __m256i);
extern __mmask32 __cdecl _mm256_mask_bitshuffle_epi64_mask(__mmask32, __m256i, __m256i);
extern __mmask64 __cdecl _mm512_bitshuffle_epi64_mask(__m512i, __m512i);
extern __mmask64 __cdecl _mm512_mask_bitshuffle_epi64_mask(__mmask64, __m512i, __m512i);

// GFNI
extern __m128i __cdecl _mm_gf2p8affineinv_epi64_epi8(__m128i, __m128i, int);
extern __m128i __cdecl _mm_mask_gf2p8affineinv_epi64_epi8(__m128i, __mmask16, __m128i, __m128i, int);
extern __m128i __cdecl _mm_maskz_gf2p8affineinv_epi64_epi8(__mmask16, __m128i, __m128i, int);
extern __m256i __cdecl _mm256_gf2p8affineinv_epi64_epi8(__m256i, __m256i, int);
extern __m256i __cdecl _mm256_mask_gf2p8affineinv_epi64_epi8(__m256i, __mmask32, __m256i, __m256i, int);
extern __m256i __cdecl _mm256_maskz_gf2p8affineinv_epi64_epi8(__mmask32, __m256i, __m256i, int);
extern __m512i __cdecl _mm512_gf2p8affineinv_epi64_epi8(__m512i, __m512i, int);
extern __m512i __cdecl _mm512_mask_gf2p8affineinv_epi64_epi8(__m512i, __mmask64, __m512i, __m512i, int);
extern __m512i __cdecl _mm512_maskz_gf2p8affineinv_epi64_epi8(__mmask64, __m512i, __m512i, int);
extern __m128i __cdecl _mm_gf2p8affine_epi64_epi8(__m128i, __m128i, int);
extern __m128i __cdecl _mm_mask_gf2p8affine_epi64_epi8(__m128i, __mmask16, __m128i, __m128i, int);
extern __m128i __cdecl _mm_maskz_gf2p8affine_epi64_epi8(__mmask16, __m128i, __m128i, int);
extern __m256i __cdecl _mm256_gf2p8affine_epi64_epi8(__m256i, __m256i, int);
extern __m256i __cdecl _mm256_mask_gf2p8affine_epi64_epi8(__m256i, __mmask32, __m256i, __m256i, int);
extern __m256i __cdecl _mm256_maskz_gf2p8affine_epi64_epi8(__mmask32, __m256i, __m256i, int);
extern __m512i __cdecl _mm512_gf2p8affine_epi64_epi8(__m512i, __m512i, int);
extern __m512i __cdecl _mm512_mask_gf2p8affine_epi64_epi8(__m512i, __mmask64, __m512i, __m512i, int);
extern __m512i __cdecl _mm512_maskz_gf2p8affine_epi64_epi8(__mmask64, __m512i, __m512i, int);
extern __m128i __cdecl _mm_gf2p8mul_epi8(__m128i, __m128i);
extern __m128i __cdecl _mm_mask_gf2p8mul_epi8(__m128i, __mmask16, __m128i, __m128i);
extern __m128i __cdecl _mm_maskz_gf2p8mul_epi8(__mmask16, __m128i, __m128i);
extern __m256i __cdecl _mm256_gf2p8mul_epi8(__m256i, __m256i);
extern __m256i __cdecl _mm256_mask_gf2p8mul_epi8(__m256i, __mmask32, __m256i, __m256i);
extern __m256i __cdecl _mm256_maskz_gf2p8mul_epi8(__mmask32, __m256i, __m256i);
extern __m512i __cdecl _mm512_gf2p8mul_epi8(__m512i, __m512i);
extern __m512i __cdecl _mm512_mask_gf2p8mul_epi8(__m512i, __mmask64, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_gf2p8mul_epi8(__mmask64, __m512i, __m512i);

// VPSHLDW|D|Q, VPSHLDVW|D|Q
extern __m128i __cdecl _mm_shldi_epi16(__m128i, __m128i, int);
extern __m128i __cdecl _mm_mask_shldi_epi16(__m128i, __mmask8, __m128i, __m128i, int);
extern __m128i __cdecl _mm_maskz_shldi_epi16(__mmask8, __m128i, __m128i, int);
extern __m256i __cdecl _mm256_shldi_epi16(__m256i, __m256i, int);
extern __m256i __cdecl _mm256_mask_shldi_epi16(__m256i, __mmask16, __m256i, __m256i, int);
extern __m256i __cdecl _mm256_maskz_shldi_epi16(__mmask16, __m256i, __m256i, int);
extern __m512i __cdecl _mm512_shldi_epi16(__m512i, __m512i, int);
extern __m512i __cdecl _mm512_mask_shldi_epi16(__m512i, __mmask32, __m512i, __m512i, int);
extern __m512i __cdecl _mm512_maskz_shldi_epi16(__mmask32, __m512i, __m512i, int);
extern __m128i __cdecl _mm_shldi_epi32(__m128i, __m128i, int);
extern __m128i __cdecl _mm_mask_shldi_epi32(__m128i, __mmask8, __m128i, __m128i, int);
extern __m128i __cdecl _mm_maskz_shldi_epi32(__mmask8, __m128i, __m128i, int);
extern __m256i __cdecl _mm256_shldi_epi32(__m256i, __m256i, int);
extern __m256i __cdecl _mm256_mask_shldi_epi32(__m256i, __mmask8, __m256i, __m256i, int);
extern __m256i __cdecl _mm256_maskz_shldi_epi32(__mmask8, __m256i, __m256i, int);
extern __m512i __cdecl _mm512_shldi_epi32(__m512i, __m512i, int);
extern __m512i __cdecl _mm512_mask_shldi_epi32(__m512i, __mmask16, __m512i, __m512i, int);
extern __m512i __cdecl _mm512_maskz_shldi_epi32(__mmask16, __m512i, __m512i, int);
extern __m128i __cdecl _mm_shldi_epi64(__m128i, __m128i, int);
extern __m128i __cdecl _mm_mask_shldi_epi64(__m128i, __mmask8, __m128i, __m128i, int);
extern __m128i __cdecl _mm_maskz_shldi_epi64(__mmask8, __m128i, __m128i, int);
extern __m256i __cdecl _mm256_shldi_epi64(__m256i, __m256i, int);
extern __m256i __cdecl _mm256_mask_shldi_epi64(__m256i, __mmask8, __m256i, __m256i, int);
extern __m256i __cdecl _mm256_maskz_shldi_epi64(__mmask8, __m256i, __m256i, int);
extern __m512i __cdecl _mm512_shldi_epi64(__m512i, __m512i, int);
extern __m512i __cdecl _mm512_mask_shldi_epi64(__m512i, __mmask8, __m512i, __m512i, int);
extern __m512i __cdecl _mm512_maskz_shldi_epi64(__mmask8, __m512i, __m512i, int);
extern __m128i __cdecl _mm_shldv_epi16(__m128i, __m128i, __m128i);
extern __m128i __cdecl _mm_mask_shldv_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i __cdecl _mm_maskz_shldv_epi16(__mmask8, __m128i, __m128i, __m128i);
extern __m256i __cdecl _mm256_shldv_epi16(__m256i, __m256i, __m256i);
extern __m256i __cdecl _mm256_mask_shldv_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i __cdecl _mm256_maskz_shldv_epi16(__mmask16, __m256i, __m256i, __m256i);
extern __m512i __cdecl _mm512_shldv_epi16(__m512i, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_shldv_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_shldv_epi16(__mmask32, __m512i, __m512i, __m512i);
extern __m128i __cdecl _mm_shldv_epi32(__m128i, __m128i, __m128i);
extern __m128i __cdecl _mm_mask_shldv_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i __cdecl _mm_maskz_shldv_epi32(__mmask8, __m128i, __m128i, __m128i);
extern __m256i __cdecl _mm256_shldv_epi32(__m256i, __m256i, __m256i);
extern __m256i __cdecl _mm256_mask_shldv_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i __cdecl _mm256_maskz_shldv_epi32(__mmask8, __m256i, __m256i, __m256i);
extern __m512i __cdecl _mm512_shldv_epi32(__m512i, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_shldv_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_shldv_epi32(__mmask16, __m512i, __m512i, __m512i);
extern __m128i __cdecl _mm_shldv_epi64(__m128i, __m128i, __m128i);
extern __m128i __cdecl _mm_mask_shldv_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i __cdecl _mm_maskz_shldv_epi64(__mmask8, __m128i, __m128i, __m128i);
extern __m256i __cdecl _mm256_shldv_epi64(__m256i, __m256i, __m256i);
extern __m256i __cdecl _mm256_mask_shldv_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i __cdecl _mm256_maskz_shldv_epi64(__mmask8, __m256i, __m256i, __m256i);
extern __m512i __cdecl _mm512_shldv_epi64(__m512i, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_shldv_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_shldv_epi64(__mmask8, __m512i, __m512i, __m512i);

// VPSHRDW|D|Q, VPSHRDVW|D|Q
extern __m128i __cdecl _mm_shrdi_epi16(__m128i, __m128i, int);
extern __m128i __cdecl _mm_mask_shrdi_epi16(__m128i, __mmask8, __m128i, __m128i, int);
extern __m128i __cdecl _mm_maskz_shrdi_epi16(__mmask8, __m128i, __m128i, int);
extern __m256i __cdecl _mm256_shrdi_epi16(__m256i, __m256i, int);
extern __m256i __cdecl _mm256_mask_shrdi_epi16(__m256i, __mmask16, __m256i, __m256i, int);
extern __m256i __cdecl _mm256_maskz_shrdi_epi16(__mmask16, __m256i, __m256i, int);
extern __m512i __cdecl _mm512_shrdi_epi16(__m512i, __m512i, int);
extern __m512i __cdecl _mm512_mask_shrdi_epi16(__m512i, __mmask32, __m512i, __m512i, int);
extern __m512i __cdecl _mm512_maskz_shrdi_epi16(__mmask32, __m512i, __m512i, int);
extern __m128i __cdecl _mm_shrdi_epi32(__m128i, __m128i, int);
extern __m128i __cdecl _mm_mask_shrdi_epi32(__m128i, __mmask8, __m128i, __m128i, int);
extern __m128i __cdecl _mm_maskz_shrdi_epi32(__mmask8, __m128i, __m128i, int);
extern __m256i __cdecl _mm256_shrdi_epi32(__m256i, __m256i, int);
extern __m256i __cdecl _mm256_mask_shrdi_epi32(__m256i, __mmask8, __m256i, __m256i, int);
extern __m256i __cdecl _mm256_maskz_shrdi_epi32(__mmask8, __m256i, __m256i, int);
extern __m512i __cdecl _mm512_shrdi_epi32(__m512i, __m512i, int);
extern __m512i __cdecl _mm512_mask_shrdi_epi32(__m512i, __mmask16, __m512i, __m512i, int);
extern __m512i __cdecl _mm512_maskz_shrdi_epi32(__mmask16, __m512i, __m512i, int);
extern __m128i __cdecl _mm_shrdi_epi64(__m128i, __m128i, int);
extern __m128i __cdecl _mm_mask_shrdi_epi64(__m128i, __mmask8, __m128i, __m128i, int);
extern __m128i __cdecl _mm_maskz_shrdi_epi64(__mmask8, __m128i, __m128i, int);
extern __m256i __cdecl _mm256_shrdi_epi64(__m256i, __m256i, int);
extern __m256i __cdecl _mm256_mask_shrdi_epi64(__m256i, __mmask8, __m256i, __m256i, int);
extern __m256i __cdecl _mm256_maskz_shrdi_epi64(__mmask8, __m256i, __m256i, int);
extern __m512i __cdecl _mm512_shrdi_epi64(__m512i, __m512i, int);
extern __m512i __cdecl _mm512_mask_shrdi_epi64(__m512i, __mmask8, __m512i, __m512i, int);
extern __m512i __cdecl _mm512_maskz_shrdi_epi64(__mmask8, __m512i, __m512i, int);
extern __m128i __cdecl _mm_shrdv_epi16(__m128i, __m128i, __m128i);
extern __m128i __cdecl _mm_mask_shrdv_epi16(__m128i, __mmask8, __m128i, __m128i);
extern __m128i __cdecl _mm_maskz_shrdv_epi16(__mmask8, __m128i, __m128i, __m128i);
extern __m256i __cdecl _mm256_shrdv_epi16(__m256i, __m256i, __m256i);
extern __m256i __cdecl _mm256_mask_shrdv_epi16(__m256i, __mmask16, __m256i, __m256i);
extern __m256i __cdecl _mm256_maskz_shrdv_epi16(__mmask16, __m256i, __m256i, __m256i);
extern __m512i __cdecl _mm512_shrdv_epi16(__m512i, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_shrdv_epi16(__m512i, __mmask32, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_shrdv_epi16(__mmask32, __m512i, __m512i, __m512i);
extern __m128i __cdecl _mm_shrdv_epi32(__m128i, __m128i, __m128i);
extern __m128i __cdecl _mm_mask_shrdv_epi32(__m128i, __mmask8, __m128i, __m128i);
extern __m128i __cdecl _mm_maskz_shrdv_epi32(__mmask8, __m128i, __m128i, __m128i);
extern __m256i __cdecl _mm256_shrdv_epi32(__m256i, __m256i, __m256i);
extern __m256i __cdecl _mm256_mask_shrdv_epi32(__m256i, __mmask8, __m256i, __m256i);
extern __m256i __cdecl _mm256_maskz_shrdv_epi32(__mmask8, __m256i, __m256i, __m256i);
extern __m512i __cdecl _mm512_shrdv_epi32(__m512i, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_shrdv_epi32(__m512i, __mmask16, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_shrdv_epi32(__mmask16, __m512i, __m512i, __m512i);
extern __m128i __cdecl _mm_shrdv_epi64(__m128i, __m128i, __m128i);
extern __m128i __cdecl _mm_mask_shrdv_epi64(__m128i, __mmask8, __m128i, __m128i);
extern __m128i __cdecl _mm_maskz_shrdv_epi64(__mmask8, __m128i, __m128i, __m128i);
extern __m256i __cdecl _mm256_shrdv_epi64(__m256i, __m256i, __m256i);
extern __m256i __cdecl _mm256_mask_shrdv_epi64(__m256i, __mmask8, __m256i, __m256i);
extern __m256i __cdecl _mm256_maskz_shrdv_epi64(__mmask8, __m256i, __m256i, __m256i);
extern __m512i __cdecl _mm512_shrdv_epi64(__m512i, __m512i, __m512i);
extern __m512i __cdecl _mm512_mask_shrdv_epi64(__m512i, __mmask8, __m512i, __m512i);
extern __m512i __cdecl _mm512_maskz_shrdv_epi64(__mmask8, __m512i, __m512i, __m512i);

/*
* Intrinsic functions for Short Vector Math Library (SVML)
*/

// vector integer divide and remainder
extern __m512i _mm512_div_epi8(__m512i, __m512i);
extern __m512i _mm512_div_epi16(__m512i, __m512i);
extern __m512i _mm512_div_epi32(__m512i, __m512i);
extern __m512i _mm512_div_epi64(__m512i, __m512i);
extern __m512i _mm512_div_epu8(__m512i, __m512i);
extern __m512i _mm512_div_epu16(__m512i, __m512i);
extern __m512i _mm512_div_epu32(__m512i, __m512i);
extern __m512i _mm512_div_epu64(__m512i, __m512i);
extern __m512i _mm512_mask_div_epi32(__m512i /*src*/, __mmask16, __m512i, __m512i);
extern __m512i _mm512_mask_div_epu32(__m512i /*src*/, __mmask16, __m512i, __m512i);
extern __m512i _mm512_rem_epi8(__m512i, __m512i);
extern __m512i _mm512_rem_epi16(__m512i, __m512i);
extern __m512i _mm512_rem_epi32(__m512i, __m512i);
extern __m512i _mm512_rem_epi64(__m512i, __m512i);
extern __m512i _mm512_rem_epu8(__m512i, __m512i);
extern __m512i _mm512_rem_epu16(__m512i, __m512i);
extern __m512i _mm512_rem_epu32(__m512i, __m512i);
extern __m512i _mm512_rem_epu64(__m512i, __m512i);
extern __m512i _mm512_mask_rem_epi32(__m512i /*src*/, __mmask16, __m512i, __m512i);
extern __m512i _mm512_mask_rem_epu32(__m512i /*src*/, __mmask16, __m512i, __m512i);

// Math functions
extern __m512  _mm512_sin_ps(__m512);
extern __m512  _mm512_mask_sin_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_sin_pd(__m512d);
extern __m512d _mm512_mask_sin_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_cos_ps(__m512);
extern __m512  _mm512_mask_cos_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_cos_pd(__m512d);
extern __m512d _mm512_mask_cos_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_sincos_ps(__m512  * /*cos_res*/, __m512);
extern __m512  _mm512_mask_sincos_ps(__m512  * /*cos_res*/, __m512  /*sin_src*/, __m512  /*cos_src*/, __mmask16, __m512);
extern __m512d _mm512_sincos_pd(__m512d * /*cos_res*/, __m512d);
extern __m512d _mm512_mask_sincos_pd(__m512d * /*cos_res*/, __m512d /*sin_src*/, __m512d /*cos_src*/, __mmask8, __m512d);
extern __m512  _mm512_tan_ps(__m512);
extern __m512  _mm512_mask_tan_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_tan_pd(__m512d);
extern __m512d _mm512_mask_tan_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_asin_ps(__m512);
extern __m512  _mm512_mask_asin_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_asin_pd(__m512d);
extern __m512d _mm512_mask_asin_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_acos_ps(__m512);
extern __m512  _mm512_mask_acos_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_acos_pd(__m512d);
extern __m512d _mm512_mask_acos_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_atan_ps(__m512);
extern __m512  _mm512_mask_atan_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_atan_pd(__m512d);
extern __m512d _mm512_mask_atan_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_atan2_ps(__m512, __m512);
extern __m512  _mm512_mask_atan2_ps(__m512  /*src*/, __mmask16, __m512, __m512);
extern __m512d _mm512_atan2_pd(__m512d, __m512d);
extern __m512d _mm512_mask_atan2_pd(__m512d /*src*/, __mmask8, __m512d, __m512d);
extern __m512  _mm512_sind_ps(__m512);
extern __m512  _mm512_mask_sind_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_sind_pd(__m512d);
extern __m512d _mm512_mask_sind_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_cosd_ps(__m512);
extern __m512  _mm512_mask_cosd_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_cosd_pd(__m512d);
extern __m512d _mm512_mask_cosd_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_tand_ps(__m512);
extern __m512  _mm512_mask_tand_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_tand_pd(__m512d);
extern __m512d _mm512_mask_tand_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_sinh_ps(__m512);
extern __m512  _mm512_mask_sinh_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_sinh_pd(__m512d);
extern __m512d _mm512_mask_sinh_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_cosh_ps(__m512);
extern __m512  _mm512_mask_cosh_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_cosh_pd(__m512d);
extern __m512d _mm512_mask_cosh_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_tanh_ps(__m512);
extern __m512  _mm512_mask_tanh_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_tanh_pd(__m512d);
extern __m512d _mm512_mask_tanh_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_asinh_ps(__m512);
extern __m512  _mm512_mask_asinh_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_asinh_pd(__m512d);
extern __m512d _mm512_mask_asinh_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_acosh_ps(__m512);
extern __m512  _mm512_mask_acosh_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_acosh_pd(__m512d);
extern __m512d _mm512_mask_acosh_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_atanh_ps(__m512);
extern __m512  _mm512_mask_atanh_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_atanh_pd(__m512d);
extern __m512d _mm512_mask_atanh_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_log_ps(__m512);
extern __m512  _mm512_mask_log_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_log_pd(__m512d);
extern __m512d _mm512_mask_log_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_log1p_ps(__m512);
extern __m512  _mm512_mask_log1p_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_log1p_pd(__m512d);
extern __m512d _mm512_mask_log1p_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_log10_ps(__m512);
extern __m512  _mm512_mask_log10_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_log10_pd(__m512d);
extern __m512d _mm512_mask_log10_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_log2_ps(__m512);
extern __m512  _mm512_mask_log2_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_log2_pd(__m512d);
extern __m512d _mm512_mask_log2_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_logb_ps(__m512);
extern __m512  _mm512_mask_logb_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_logb_pd(__m512d);
extern __m512d _mm512_mask_logb_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_exp_ps(__m512);
extern __m512  _mm512_mask_exp_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_exp_pd(__m512d);
extern __m512d _mm512_mask_exp_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_exp10_ps(__m512);
extern __m512  _mm512_mask_exp10_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_exp10_pd(__m512d);
extern __m512d _mm512_mask_exp10_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_exp2_ps(__m512);
extern __m512  _mm512_mask_exp2_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_exp2_pd(__m512d);
extern __m512d _mm512_mask_exp2_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_expm1_ps(__m512);
extern __m512  _mm512_mask_expm1_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_expm1_pd(__m512d);
extern __m512d _mm512_mask_expm1_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_pow_ps(__m512, __m512);
extern __m512  _mm512_mask_pow_ps(__m512  /*src*/, __mmask16, __m512, __m512);
extern __m512d _mm512_pow_pd(__m512d, __m512d);
extern __m512d _mm512_mask_pow_pd(__m512d /*src*/, __mmask8, __m512d, __m512d);
extern __m512  _mm512_trunc_ps(__m512);
extern __m512  _mm512_mask_trunc_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_trunc_pd(__m512d);
extern __m512d _mm512_mask_trunc_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_floor_ps(__m512);
extern __m512  _mm512_mask_floor_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_floor_pd(__m512d);
extern __m512d _mm512_mask_floor_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_ceil_ps(__m512);
extern __m512  _mm512_mask_ceil_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_ceil_pd(__m512d);
extern __m512d _mm512_mask_ceil_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512 _mm512_svml_round_ps(__m512);
extern __m512 _mm512_mask_svml_round_ps(__m512 /*src*/, __mmask16, __m512);
extern __m512d _mm512_svml_round_pd(__m512d);
extern __m512d _mm512_mask_svml_round_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_fmod_ps(__m512, __m512);
extern __m512  _mm512_mask_fmod_ps(__m512  /*src*/, __mmask16, __m512, __m512);
extern __m512d _mm512_fmod_pd(__m512d, __m512d);
extern __m512d _mm512_mask_fmod_pd(__m512d /*src*/, __mmask8, __m512d, __m512d);
extern __m512  _mm512_rint_ps(__m512);
extern __m512  _mm512_mask_rint_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_rint_pd(__m512d);
extern __m512d _mm512_mask_rint_pd(__m512d /*src*/, __mmask8, __m512d);






extern __m512  _mm512_invsqrt_ps(__m512);
extern __m512  _mm512_mask_invsqrt_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_invsqrt_pd(__m512d);
extern __m512d _mm512_mask_invsqrt_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_cbrt_ps(__m512);
extern __m512  _mm512_mask_cbrt_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_cbrt_pd(__m512d);
extern __m512d _mm512_mask_cbrt_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_invcbrt_ps(__m512);
extern __m512  _mm512_mask_invcbrt_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_invcbrt_pd(__m512d);
extern __m512d _mm512_mask_invcbrt_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_hypot_ps(__m512, __m512);
extern __m512  _mm512_mask_hypot_ps(__m512  /*src*/, __mmask16, __m512, __m512);
extern __m512d _mm512_hypot_pd(__m512d, __m512d);
extern __m512d _mm512_mask_hypot_pd(__m512d /*src*/, __mmask8, __m512d, __m512d);
extern __m512  _mm512_cdfnorm_ps(__m512);
extern __m512  _mm512_mask_cdfnorm_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_cdfnorm_pd(__m512d);
extern __m512d _mm512_mask_cdfnorm_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_cdfnorminv_ps(__m512);
extern __m512  _mm512_mask_cdfnorminv_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_cdfnorminv_pd(__m512d);
extern __m512d _mm512_mask_cdfnorminv_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_erf_ps(__m512);
extern __m512  _mm512_mask_erf_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_erf_pd(__m512d);
extern __m512d _mm512_mask_erf_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_erfc_ps(__m512);
extern __m512  _mm512_mask_erfc_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_erfc_pd(__m512d);
extern __m512d _mm512_mask_erfc_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_erfcinv_ps(__m512);
extern __m512  _mm512_mask_erfcinv_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_erfcinv_pd(__m512d);
extern __m512d _mm512_mask_erfcinv_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_erfinv_ps(__m512);
extern __m512  _mm512_mask_erfinv_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_erfinv_pd(__m512d);
extern __m512d _mm512_mask_erfinv_pd(__m512d /*src*/, __mmask8, __m512d);
extern __m512  _mm512_nearbyint_ps(__m512);
extern __m512  _mm512_mask_nearbyint_ps(__m512  /*src*/, __mmask16, __m512);
extern __m512d _mm512_nearbyint_pd(__m512d);
extern __m512d _mm512_mask_nearbyint_pd(__m512d /*src*/, __mmask8, __m512d);

// BFLOAT16
typedef __m128i __m128bh;
typedef __m256i __m256bh;
typedef __m512i __m512bh;

extern __m128bh _mm_cvtneps_pbh(__m128);
extern __m128bh _mm_mask_cvtneps_pbh(__m128bh, __mmask8, __m128);
extern __m128bh _mm_maskz_cvtneps_pbh(__mmask8, __m128);
extern __m128bh _mm_cvtne2ps_pbh(__m128, __m128);
extern __m128bh _mm_mask_cvtne2ps_pbh(__m128bh, __mmask8, __m128, __m128);
extern __m128bh _mm_maskz_cvtne2ps_pbh(__mmask8, __m128, __m128);
extern __m128   _mm_dpbf16_ps(__m128, __m128bh, __m128bh);
extern __m128   _mm_mask_dpbf16_ps(__m128, __mmask8, __m128bh, __m128bh);
extern __m128   _mm_maskz_dpbf16_ps(__mmask8, __m128, __m128bh, __m128bh);
extern __m128bh _mm256_cvtneps_pbh(__m256);
extern __m128bh _mm256_mask_cvtneps_pbh(__m128bh, __mmask8, __m256);
extern __m128bh _mm256_maskz_cvtneps_pbh(__mmask8, __m256);
extern __m256bh _mm256_cvtne2ps_pbh(__m256, __m256);
extern __m256bh _mm256_mask_cvtne2ps_pbh(__m256bh, __mmask16, __m256, __m256);
extern __m256bh _mm256_maskz_cvtne2ps_pbh(__mmask16, __m256, __m256);
extern __m256   _mm256_dpbf16_ps(__m256, __m256bh, __m256bh);
extern __m256   _mm256_mask_dpbf16_ps(__m256, __mmask8, __m256bh, __m256bh);
extern __m256   _mm256_maskz_dpbf16_ps(__mmask8, __m256, __m256bh, __m256bh);
extern __m256bh _mm512_cvtneps_pbh(__m512);
extern __m256bh _mm512_mask_cvtneps_pbh(__m256bh, __mmask16, __m512);
extern __m256bh _mm512_maskz_cvtneps_pbh(__mmask16, __m512);
extern __m512bh _mm512_cvtne2ps_pbh(__m512, __m512);
extern __m512bh _mm512_mask_cvtne2ps_pbh(__m512bh, __mmask32, __m512, __m512);
extern __m512bh _mm512_maskz_cvtne2ps_pbh(__mmask32, __m512, __m512);
extern __m512   _mm512_dpbf16_ps(__m512, __m512bh, __m512bh);
extern __m512   _mm512_mask_dpbf16_ps(__m512, __mmask16, __m512bh, __m512bh);
extern __m512   _mm512_maskz_dpbf16_ps(__mmask16, __m512, __m512bh, __m512bh);




















extern __mmask8  __cdecl _kadd_mask8(__mmask8, __mmask8);
extern __mmask16 __cdecl _kadd_mask16(__mmask16, __mmask16);
extern __mmask32 __cdecl _kadd_mask32(__mmask32, __mmask32);
extern __mmask64 __cdecl _kadd_mask64(__mmask64, __mmask64);
extern __mmask8  __cdecl _kand_mask8(__mmask8, __mmask8);
extern __mmask16 __cdecl _kand_mask16(__mmask16, __mmask16);
extern __mmask32 __cdecl _kand_mask32(__mmask32, __mmask32);
extern __mmask64 __cdecl _kand_mask64(__mmask64, __mmask64);
extern __mmask8  __cdecl _kandn_mask8(__mmask8, __mmask8);
extern __mmask16 __cdecl _kandn_mask16(__mmask16, __mmask16);
extern __mmask32 __cdecl _kandn_mask32(__mmask32, __mmask32);
extern __mmask64 __cdecl _kandn_mask64(__mmask64, __mmask64);
extern __mmask8  __cdecl _knot_mask8(__mmask8);
extern __mmask16 __cdecl _knot_mask16(__mmask16);
extern __mmask32 __cdecl _knot_mask32(__mmask32);
extern __mmask64 __cdecl _knot_mask64(__mmask64);
extern __mmask8  __cdecl _kor_mask8(__mmask8, __mmask8);
extern __mmask16 __cdecl _kor_mask16(__mmask16, __mmask16);
extern __mmask32 __cdecl _kor_mask32(__mmask32, __mmask32);
extern __mmask64 __cdecl _kor_mask64(__mmask64, __mmask64);
extern __mmask8  __cdecl _kxnor_mask8(__mmask8, __mmask8);
extern __mmask16 __cdecl _kxnor_mask16(__mmask16, __mmask16);
extern __mmask32 __cdecl _kxnor_mask32(__mmask32, __mmask32);
extern __mmask64 __cdecl _kxnor_mask64(__mmask64, __mmask64);
extern __mmask8  __cdecl _kxor_mask8(__mmask8, __mmask8);
extern __mmask16 __cdecl _kxor_mask16(__mmask16, __mmask16);
extern __mmask32 __cdecl _kxor_mask32(__mmask32, __mmask32);
extern __mmask64 __cdecl _kxor_mask64(__mmask64, __mmask64);
extern __mmask8  __cdecl _kshiftli_mask8(__mmask8, unsigned int);
extern __mmask16 __cdecl _kshiftli_mask16(__mmask16, unsigned int);
extern __mmask32 __cdecl _kshiftli_mask32(__mmask32, unsigned int);
extern __mmask64 __cdecl _kshiftli_mask64(__mmask64, unsigned int);
extern __mmask8  __cdecl _kshiftri_mask8(__mmask8, unsigned int);
extern __mmask16 __cdecl _kshiftri_mask16(__mmask16, unsigned int);
extern __mmask32 __cdecl _kshiftri_mask32(__mmask32, unsigned int);
extern __mmask64 __cdecl _kshiftri_mask64(__mmask64, unsigned int);
extern __mmask8  __cdecl _load_mask8(__mmask8 *);
extern __mmask16 __cdecl _load_mask16(__mmask16 *);
extern __mmask32 __cdecl _load_mask32(__mmask32 *);
extern __mmask64 __cdecl _load_mask64(__mmask64 *);
extern void      __cdecl _store_mask8(__mmask8 *, __mmask8);
extern void      __cdecl _store_mask16(__mmask16 *, __mmask16);
extern void      __cdecl _store_mask32(__mmask32 *, __mmask32);
extern void      __cdecl _store_mask64(__mmask64 *, __mmask64);
extern unsigned int     __cdecl _cvtmask8_u32(__mmask8);
extern unsigned int     __cdecl _cvtmask16_u32(__mmask16);
extern unsigned int     __cdecl _cvtmask32_u32(__mmask32);
extern unsigned __int64 __cdecl _cvtmask64_u64(__mmask64);
extern __mmask8         __cdecl _cvtu32_mask8(unsigned int);
extern __mmask16        __cdecl _cvtu32_mask16(unsigned int);
extern __mmask32        __cdecl _cvtu32_mask32(unsigned int);
extern __mmask64        __cdecl _cvtu64_mask64(unsigned __int64);
extern __mmask16        __cdecl _mm512_kmov(__mmask16);
extern unsigned char __cdecl _kortest_mask8_u8(__mmask8, __mmask8, unsigned char *);
extern unsigned char __cdecl _kortest_mask16_u8(__mmask16, __mmask16, unsigned char *);
extern unsigned char __cdecl _kortest_mask32_u8(__mmask32, __mmask32, unsigned char *);
extern unsigned char __cdecl _kortest_mask64_u8(__mmask64, __mmask64, unsigned char *);
extern unsigned char __cdecl _ktest_mask8_u8(__mmask8, __mmask8, unsigned char *);
extern unsigned char __cdecl _ktest_mask16_u8(__mmask16, __mmask16, unsigned char *);
extern unsigned char __cdecl _ktest_mask32_u8(__mmask32, __mmask32, unsigned char *);
extern unsigned char __cdecl _ktest_mask64_u8(__mmask64, __mmask64, unsigned char *);


































// These functions are not supported with Windows.
// They are deprecated and will be removed in a future release

extern __m512  __cdecl _mm512_mask_exp2a23_round_ps(__m512, __mmask16, __m512, int);
extern __m512  __cdecl _mm512_maskz_exp2a23_round_ps(__mmask16, __m512, int);
extern __m512d __cdecl _mm512_mask_exp2a23_round_pd(__m512d, __mmask8, __m512d, int);
extern __m512d __cdecl _mm512_maskz_exp2a23_round_pd(__mmask8, __m512d, int);


















extern __m512  __cdecl _mm512_mask_rcp28_round_ps(__m512, __mmask16, __m512, const int);
extern __m512  __cdecl _mm512_maskz_rcp28_round_ps(__mmask16, __m512, const int);
extern __m512d __cdecl _mm512_mask_rcp28_round_pd(__m512d, __mmask8, __m512d, const int);
extern __m512d __cdecl _mm512_maskz_rcp28_round_pd(__mmask8, __m512d, const int);


















extern __m512  __cdecl _mm512_mask_rsqrt28_round_ps(__m512, __mmask16, __m512, const int);
extern __m512  __cdecl _mm512_maskz_rsqrt28_round_ps(__mmask16, __m512, const int);
extern __m512d __cdecl _mm512_mask_rsqrt28_round_pd(__m512d, __mmask8, __m512d, const int);
extern __m512d __cdecl _mm512_maskz_rsqrt28_round_pd(__mmask8, __m512d, const int);


















extern void __cdecl _mm512_prefetch_i32gather_pd(__m256i vindex, void const* base_addr, int scale, const int hint);
extern void __cdecl _mm512_prefetch_i32gather_ps(__m512i index, void const* mv, int scale, const int hint);
extern void __cdecl _mm512_prefetch_i32scatter_pd(void* base_addr, __m256i vindex, int scale, const int hint);
extern void __cdecl _mm512_prefetch_i32scatter_ps(void* mv, __m512i index, int scale, const int hint);
extern void __cdecl _mm512_prefetch_i64gather_pd(__m512i vindex, void const* base_addr, int scale, const int hint);
extern void __cdecl _mm512_prefetch_i64gather_ps(__m512i vindex, void const* base_addr, int scale, const int hint);
extern void __cdecl _mm512_prefetch_i64scatter_pd(void* base_addr, __m512i vindex, int scale, const int hint);
extern void __cdecl _mm512_prefetch_i64scatter_ps(void* base_addr, __m512i vindex, int scale, const int hint);
extern void __cdecl _mm512_mask_prefetch_i32gather_pd(__m256i vindex, __mmask8 mask, void const* base_addr, int scale, const int hint);
extern void __cdecl _mm512_mask_prefetch_i32gather_ps(__m512i vindex, __mmask16 mask, void const* base_addr, int scale, const int hint);
extern void __cdecl _mm512_mask_prefetch_i32scatter_pd(void* base_addr, __mmask8 mask, __m256i vinde, int scale, const int hint);
extern void __cdecl _mm512_mask_prefetch_i32scatter_ps(void* mv, __mmask16 k, __m512i index, int scale, const int hint);
extern void __cdecl _mm512_mask_prefetch_i64gather_pd(__m512i vindex, __mmask8 mask, void const* base_addr, int scale, const int hint);
extern void __cdecl _mm512_mask_prefetch_i64gather_ps(__m512i vindex, __mmask8 mask, void const* base_addr, int scale, const int hint);
extern void __cdecl _mm512_mask_prefetch_i64scatter_pd(void* base_addr, __mmask8 mask, __m512i vindex, int scale, const int hint);
extern void __cdecl _mm512_mask_prefetch_i64scatter_ps(void* base_addr, __mmask8 mask, __m512i vindex, int scale, const int hint);

// AVX512_VP2INTERSECT
extern void __cdecl _mm_2intersect_epi32(__m128i, __m128i, __mmask8 *, __mmask8 *);
extern void __cdecl _mm256_2intersect_epi32(__m256i, __m256i, __mmask8 *, __mmask8 *);
extern void __cdecl _mm512_2intersect_epi32(__m512i, __m512i, __mmask16 *, __mmask16 *);
extern void __cdecl _mm_2intersect_epi64(__m128i, __m128i, __mmask8 *, __mmask8 *);
extern void __cdecl _mm256_2intersect_epi64(__m256i, __m256i, __mmask8 *, __mmask8 *);
extern void __cdecl _mm512_2intersect_epi64(__m512i, __m512i, __mmask8 *, __mmask8 *);



// AMX
typedef int __tile;

extern void __cdecl _tile_loadconfig(const void *);
extern void __cdecl _tile_storeconfig(void *);
extern void __cdecl _tile_release(void);

extern void __cdecl _tile_loadd(__tile dst, const void *base, int stride);
extern void __cdecl _tile_stream_loadd(__tile dst, const void *base, int stride);
extern void __cdecl _tile_stored(__tile src, void *base, int stride);
extern void __cdecl _tile_zero(__tile dst);

extern void __cdecl _tile_dpbf16ps(__tile dst, __tile src1, __tile src2);
extern void __cdecl _tile_dpbssd(__tile dst, __tile src1, __tile src2);
extern void __cdecl _tile_dpbsud(__tile dst, __tile src1, __tile src2);
extern void __cdecl _tile_dpbusd(__tile dst, __tile src1, __tile src2);
extern void __cdecl _tile_dpbuud(__tile dst, __tile src1, __tile src2);

#line 5153 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\zmmintrin.h"

/* hfni */
typedef __m128i __m128h;
typedef __m256i __m256h;
typedef __m512i __m512h;

// VADDPH
extern __m128h __cdecl _mm_add_ph(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_add_ph(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_add_ph(__mmask8, __m128h, __m128h);
extern __m256h __cdecl _mm256_add_ph(__m256h, __m256h);
extern __m256h __cdecl _mm256_mask_add_ph(__m256h, __mmask16, __m256h, __m256h);
extern __m256h __cdecl _mm256_maskz_add_ph(__mmask16, __m256h, __m256h);
extern __m512h __cdecl _mm512_add_ph(__m512h, __m512h);
extern __m512h __cdecl _mm512_mask_add_ph(__m512h, __mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_maskz_add_ph(__mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_add_round_ph(__m512h, __m512h, int);
extern __m512h __cdecl _mm512_mask_add_round_ph(__m512h, __mmask32, __m512h, __m512h, int);
extern __m512h __cdecl _mm512_maskz_add_round_ph(__mmask32, __m512h, __m512h, int);

// VADDSH
extern __m128h __cdecl _mm_add_sh(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_add_sh(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_add_sh(__mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_add_round_sh(__m128h, __m128h, int);
extern __m128h __cdecl _mm_mask_add_round_sh(__m128h, __mmask8, __m128h, __m128h, int);
extern __m128h __cdecl _mm_maskz_add_round_sh(__mmask8, __m128h, __m128h, int);
/* Intrinsics for broadcasting an fp16 value to vector */
/*short float data type is not supported in utc*/
// extern __m128h _mm_set1_ph(short float);
// extern __m256h _mm256_set1_ph(short float);
// extern __m512h _mm512_set1_ph(short float);

// VCMPPH
extern __mmask8 __cdecl _mm_cmp_ph_mask(__m128h, __m128h, const int);
extern __mmask8 __cdecl _mm_mask_cmp_ph_mask(__mmask8, __m128h, __m128h, const int);
extern __mmask16 __cdecl _mm256_cmp_ph_mask(__m256h, __m256h, const int);
extern __mmask16 __cdecl _mm256_mask_cmp_ph_mask(__mmask16, __m256h, __m256h, const int);
extern __mmask32 __cdecl _mm512_cmp_ph_mask(__m512h, __m512h, const int);
extern __mmask32 __cdecl _mm512_mask_cmp_ph_mask(__mmask32, __m512h, __m512h, const int);
extern __mmask32 __cdecl _mm512_cmp_round_ph_mask(__m512h, __m512h, const int, const int);
extern __mmask32 __cdecl _mm512_mask_cmp_round_ph_mask(__mmask32, __m512h, __m512h, const int, const int);

// VCMPSH
extern __mmask8 __cdecl _mm_cmp_sh_mask(__m128h, __m128h, const int);
extern __mmask8 __cdecl _mm_mask_cmp_sh_mask(__mmask8, __m128h, __m128h, const int);
extern __mmask8 __cdecl _mm_cmp_round_sh_mask(__m128h, __m128h, const int, const int);
extern __mmask8 __cdecl _mm_mask_cmp_round_sh_mask(__mmask8, __m128h, __m128h, const int, const int);

// VCOMISH
extern int __cdecl _mm_comi_sh(__m128h, __m128h, const int);
extern int __cdecl _mm_comi_round_sh(__m128h, __m128h, const int, const int);

// VCVTDQ2PH
extern __m128h __cdecl _mm_cvtepi32_ph(__m128i);
extern __m128h __cdecl _mm_mask_cvtepi32_ph(__m128h, __mmask8, __m128i);
extern __m128h __cdecl _mm_maskz_cvtepi32_ph(__mmask8, __m128i);
extern __m128h __cdecl _mm256_cvtepi32_ph(__m256i);
extern __m128h __cdecl _mm256_mask_cvtepi32_ph(__m128h, __mmask8, __m256i);
extern __m128h __cdecl _mm256_maskz_cvtepi32_ph(__mmask8, __m256i);
extern __m256h __cdecl _mm512_cvtepi32_ph (__m512i);
extern __m256h __cdecl _mm512_mask_cvtepi32_ph (__m256h, __mmask16, __m512i);;
extern __m256h __cdecl _mm512_maskz_cvtepi32_ph (__mmask16, __m512i);
extern __m256h __cdecl _mm512_cvt_roundepi32_ph(__m512i, int);
extern __m256h __cdecl _mm512_mask_cvt_roundepi32_ph(__m256h, __mmask16, __m512i, int);
extern __m256h __cdecl _mm512_maskz_cvt_roundepi32_ph(__mmask16, __m512i, int);

//VCVTPD2PH
extern __m128h __cdecl _mm_cvtpd_ph(__m128d);
extern __m128h __cdecl _mm_mask_cvtpd_ph(__m128h, __mmask8, __m128d);
extern __m128h __cdecl _mm_maskz_cvtpd_ph(__mmask8, __m128d);
extern __m128h __cdecl _mm256_cvtpd_ph(__m256d);
extern __m128h __cdecl _mm256_mask_cvtpd_ph(__m128h, __mmask8, __m256d);
extern __m128h __cdecl _mm256_maskz_cvtpd_ph(__mmask8, __m256d);
extern __m128h __cdecl _mm512_cvtpd_ph (__m512d);
extern __m128h __cdecl _mm512_mask_cvtpd_ph (__m128h, __mmask8, __m512d);
extern __m128h __cdecl _mm512_maskz_cvtpd_ph (__mmask8, __m512d);
extern __m128h __cdecl _mm512_cvt_roundpd_ph(__m512d, int);
extern __m128h __cdecl _mm512_mask_cvt_roundpd_ph(__m128h, __mmask8, __m512d, int);
extern __m128h __cdecl _mm512_maskz_cvt_roundpd_ph(__mmask8, __m512d, int);

// VCVTPH2DQ
extern __m128i __cdecl _mm_cvtph_epi32(__m128h);
extern __m128i __cdecl _mm_mask_cvtph_epi32(__m128i, __mmask8, __m128h);
extern __m128i __cdecl _mm_maskz_cvtph_epi32(__mmask8, __m128h);
extern __m256i __cdecl _mm256_cvtph_epi32(__m128h);
extern __m256i __cdecl _mm256_mask_cvtph_epi32(__m256i, __mmask8, __m128h);
extern __m256i __cdecl _mm256_maskz_cvtph_epi32(__mmask8, __m128h);
extern __m512i __cdecl _mm512_cvtph_epi32 (__m256h);
extern __m512i __cdecl _mm512_mask_cvtph_epi32 (__m512i, __mmask16, __m256h);
extern __m512i __cdecl _mm512_maskz_cvtph_epi32 (__mmask16, __m256h);
extern __m512i __cdecl _mm512_cvt_roundph_epi32(__m256h, int);
extern __m512i __cdecl _mm512_mask_cvt_roundph_epi32(__m512i, __mmask16, __m256h, int);
extern __m512i __cdecl _mm512_maskz_cvt_roundph_epi32(__mmask16, __m256h, int);

// VCVTPH2PD
extern __m128d __cdecl _mm_cvtph_pd(__m128h);
extern __m128d __cdecl _mm_mask_cvtph_pd(__m128d, __mmask8, __m128h);
extern __m128d __cdecl _mm_maskz_cvtph_pd(__mmask8, __m128h);
extern __m256d __cdecl _mm256_cvtph_pd(__m128h);
extern __m256d __cdecl _mm256_mask_cvtph_pd(__m256d, __mmask8, __m128h);
extern __m256d __cdecl _mm256_maskz_cvtph_pd(__mmask8, __m128h);
extern __m512d __cdecl _mm512_cvtph_pd (__m128h);
extern __m512d __cdecl _mm512_mask_cvtph_pd (__m512d, __mmask8, __m128h);
extern __m512d __cdecl _mm512_maskz_cvtph_pd (__mmask8, __m128h);
extern __m512d __cdecl _mm512_cvt_roundph_pd(__m128h, int);
extern __m512d __cdecl _mm512_mask_cvt_roundph_pd(__m512d, __mmask8, __m128h, int);
extern __m512d __cdecl _mm512_maskz_cvt_roundph_pd(__mmask8, __m128h, int);

// VCVTPH2PSX
extern __m128 __cdecl _mm_cvtxph_ps(__m128h);
extern __m128 __cdecl _mm_mask_cvtxph_ps(__m128, __mmask8, __m128h);
extern __m128 __cdecl _mm_maskz_cvtxph_ps(__mmask8, __m128h);
extern __m256 __cdecl _mm256_cvtxph_ps(__m128h);
extern __m256 __cdecl _mm256_mask_cvtxph_ps(__m256, __mmask8, __m128h);
extern __m256 __cdecl _mm256_maskz_cvtxph_ps(__mmask8, __m128h);
extern __m512 __cdecl _mm512_cvtxph_ps(__m256h);
extern __m512 __cdecl _mm512_mask_cvtxph_ps(__m512, __mmask16, __m256h);
extern __m512 __cdecl _mm512_maskz_cvtxph_ps(__mmask16, __m256h);
extern __m512 __cdecl _mm512_cvtx_roundph_ps(__m256h, int);
extern __m512 __cdecl _mm512_mask_cvtx_roundph_ps(__m512, __mmask16, __m256h, int);
extern __m512 __cdecl _mm512_maskz_cvtx_roundph_ps(__mmask16, __m256h, int);

// VCVTPH2QQ
extern __m128i __cdecl _mm_cvtph_epi64(__m128h);
extern __m128i __cdecl _mm_mask_cvtph_epi64(__m128i, __mmask8, __m128h);
extern __m128i __cdecl _mm_maskz_cvtph_epi64(__mmask8, __m128h);
extern __m256i __cdecl _mm256_cvtph_epi64(__m128h);
extern __m256i __cdecl _mm256_mask_cvtph_epi64(__m256i, __mmask8, __m128h);
extern __m256i __cdecl _mm256_maskz_cvtph_epi64(__mmask8, __m128h);
extern __m512i __cdecl _mm512_cvtph_epi64 (__m128h);
extern __m512i __cdecl _mm512_mask_cvtph_epi64 (__m512i, __mmask8, __m128h);
extern __m512i __cdecl _mm512_maskz_cvtph_epi64 (__mmask8, __m128h);
extern __m512i __cdecl _mm512_cvt_roundph_epi64(__m128h, int);
extern __m512i __cdecl _mm512_mask_cvt_roundph_epi64(__m512i, __mmask8, __m128h, int);
extern __m512i __cdecl _mm512_maskz_cvt_roundph_epi64(__mmask8, __m128h, int);

// VCVTPH2UDQ
extern __m128i __cdecl _mm_cvtph_epu32(__m128h);
extern __m128i __cdecl _mm_mask_cvtph_epu32(__m128i, __mmask8, __m128h);
extern __m128i __cdecl _mm_maskz_cvtph_epu32(__mmask8, __m128h);
extern __m256i __cdecl _mm256_cvtph_epu32(__m128h);
extern __m256i __cdecl _mm256_mask_cvtph_epu32(__m256i, __mmask8, __m128h);
extern __m256i __cdecl _mm256_maskz_cvtph_epu32(__mmask8, __m128h);
extern __m512i __cdecl _mm512_cvtph_epu32 (__m256h);
extern __m512i __cdecl _mm512_mask_cvtph_epu32 (__m512i, __mmask16, __m256h);
extern __m512i __cdecl _mm512_maskz_cvtph_epu32 (__mmask16, __m256h);
extern __m512i __cdecl _mm512_cvt_roundph_epu32(__m256h, int);
extern __m512i __cdecl _mm512_mask_cvt_roundph_epu32(__m512i, __mmask16, __m256h, int);
extern __m512i __cdecl _mm512_maskz_cvt_roundph_epu32(__mmask16, __m256h, int);

// VCVTPH2UQQ
extern __m128i __cdecl _mm_cvtph_epu64(__m128h);
extern __m128i __cdecl _mm_mask_cvtph_epu64(__m128i, __mmask8, __m128h);
extern __m128i __cdecl _mm_maskz_cvtph_epu64(__mmask8, __m128h);
extern __m256i __cdecl _mm256_cvtph_epu64(__m128h);
extern __m256i __cdecl _mm256_mask_cvtph_epu64(__m256i, __mmask8, __m128h);
extern __m256i __cdecl _mm256_maskz_cvtph_epu64(__mmask8, __m128h);
extern __m512i __cdecl _mm512_cvtph_epu64 (__m128h);
extern __m512i __cdecl _mm512_mask_cvtph_epu64 (__m512i, __mmask8, __m128h);
extern __m512i __cdecl _mm512_maskz_cvtph_epu64 (__mmask8, __m128h);
extern __m512i __cdecl _mm512_cvt_roundph_epu64(__m128h, int);
extern __m512i __cdecl _mm512_mask_cvt_roundph_epu64(__m512i, __mmask8, __m128h, int);
extern __m512i __cdecl _mm512_maskz_cvt_roundph_epu64(__mmask8, __m128h, int);

// VCVTPH2UW
extern __m128i __cdecl _mm_cvtph_epu16(__m128h);
extern __m128i __cdecl _mm_mask_cvtph_epu16(__m128i, __mmask8, __m128h);
extern __m128i __cdecl _mm_maskz_cvtph_epu16(__mmask8, __m128h);
extern __m256i __cdecl _mm256_cvtph_epu16(__m256h);
extern __m256i __cdecl _mm256_mask_cvtph_epu16(__m256i, __mmask16, __m256h);
extern __m256i __cdecl _mm256_maskz_cvtph_epu16(__mmask16, __m256h);
extern __m512i __cdecl _mm512_cvtph_epu16 (__m512h);
extern __m512i __cdecl _mm512_mask_cvtph_epu16 (__m512i, __mmask32, __m512h);
extern __m512i __cdecl _mm512_maskz_cvtph_epu16 (__mmask32, __m512h);
extern __m512i __cdecl _mm512_cvt_roundph_epu16(__m512h, int);
extern __m512i __cdecl _mm512_mask_cvt_roundph_epu16(__m512i, __mmask32, __m512h, int);
extern __m512i __cdecl _mm512_maskz_cvt_roundph_epu16(__mmask32, __m512h, int);

// VCVTPH2W
extern __m128i __cdecl _mm_cvtph_epi16(__m128h);
extern __m128i __cdecl _mm_mask_cvtph_epi16(__m128i, __mmask8, __m128h);
extern __m128i __cdecl _mm_maskz_cvtph_epi16(__mmask8, __m128h);
extern __m256i __cdecl _mm256_cvtph_epi16(__m256h);
extern __m256i __cdecl _mm256_mask_cvtph_epi16(__m256i, __mmask16, __m256h);
extern __m256i __cdecl _mm256_maskz_cvtph_epi16(__mmask16, __m256h);
extern __m512i __cdecl _mm512_cvtph_epi16 (__m512h);
extern __m512i __cdecl _mm512_mask_cvtph_epi16 (__m512i, __mmask32, __m512h);
extern __m512i __cdecl _mm512_maskz_cvtph_epi16 (__mmask32, __m512h);
extern __m512i __cdecl _mm512_cvt_roundph_epi16(__m512h, int);
extern __m512i __cdecl _mm512_mask_cvt_roundph_epi16(__m512i, __mmask32, __m512h, int);
extern __m512i __cdecl _mm512_maskz_cvt_roundph_epi16(__mmask32, __m512h, int);

// VCVTPS2PHX
extern __m128h __cdecl _mm_cvtxps_ph(__m128);
extern __m128h __cdecl _mm_mask_cvtxps_ph(__m128h, __mmask8, __m128);
extern __m128h __cdecl _mm_maskz_cvtxps_ph(__mmask8, __m128);
extern __m128h __cdecl _mm256_cvtxps_ph(__m256);
extern __m128h __cdecl _mm256_mask_cvtxps_ph(__m128h, __mmask8, __m256);
extern __m128h __cdecl _mm256_maskz_cvtxps_ph(__mmask8, __m256);
extern __m256h __cdecl _mm512_cvtxps_ph(__m512);
extern __m256h __cdecl _mm512_mask_cvtxps_ph(__m256h, __mmask16, __m512);
extern __m256h __cdecl _mm512_maskz_cvtxps_ph(__mmask16, __m512);
extern __m256h __cdecl _mm512_cvtx_roundps_ph(__m512, int);
extern __m256h __cdecl _mm512_mask_cvtx_roundps_ph(__m256h, __mmask16, __m512, int);
extern __m256h __cdecl _mm512_maskz_cvtx_roundps_ph(__mmask16, __m512, int);

// VCVTQQ2PH
extern __m128h __cdecl _mm_cvtepi64_ph(__m128i);
extern __m128h __cdecl _mm_mask_cvtepi64_ph(__m128h, __mmask8, __m128i);
extern __m128h __cdecl _mm_maskz_cvtepi64_ph(__mmask8, __m128i);
extern __m128h __cdecl _mm256_cvtepi64_ph(__m256i);
extern __m128h __cdecl _mm256_mask_cvtepi64_ph(__m128h, __mmask8, __m256i);
extern __m128h __cdecl _mm256_maskz_cvtepi64_ph(__mmask8, __m256i);
extern __m128h __cdecl _mm512_cvtepi64_ph (__m512i);
extern __m128h __cdecl _mm512_mask_cvtepi64_ph (__m128h, __mmask8, __m512i);
extern __m128h __cdecl _mm512_maskz_cvtepi64_ph (__mmask8, __m512i);
extern __m128h __cdecl _mm512_cvt_roundepi64_ph(__m512i, int);
extern __m128h __cdecl _mm512_mask_cvt_roundepi64_ph(__m128h, __mmask8, __m512i, int);
extern __m128h __cdecl _mm512_maskz_cvt_roundepi64_ph(__mmask8, __m512i, int);

// VCVTSD2SH
extern __m128h __cdecl _mm_cvtsd_sh(__m128h, __m128d);
extern __m128h __cdecl _mm_mask_cvtsd_sh(__m128h, __mmask8, __m128h, __m128d);
extern __m128h __cdecl _mm_maskz_cvtsd_sh(__mmask8, __m128h, __m128d);
extern __m128h __cdecl _mm_cvt_roundsd_sh(__m128h, __m128d, const int);
extern __m128h __cdecl _mm_mask_cvt_roundsd_sh(__m128h, __mmask8, __m128h, __m128d, const int);
extern __m128h __cdecl _mm_maskz_cvt_roundsd_sh(__mmask8, __m128h, __m128d, const int);

// VCVTSH2SD
extern __m128d __cdecl _mm_cvtsh_sd(__m128d, __m128h);
extern __m128d __cdecl _mm_mask_cvtsh_sd(__m128d, __mmask8, __m128d, __m128h);
extern __m128d __cdecl _mm_maskz_cvtsh_sd(__mmask8, __m128d, __m128h);
extern __m128d __cdecl _mm_cvt_roundsh_sd(__m128d, __m128h, const int);
extern __m128d __cdecl _mm_mask_cvt_roundsh_sd(__m128d, __mmask8, __m128d, __m128h, const int);
extern __m128d __cdecl _mm_maskz_cvt_roundsh_sd(__mmask8, __m128d, __m128h, const int);

// VCVTSH2SI
extern int __cdecl _mm_cvtsh_i32(__m128h);
extern __int64 __cdecl _mm_cvtsh_i64(__m128h);
extern int __cdecl _mm_cvt_roundsh_i32(__m128h, int);
extern __int64 __cdecl _mm_cvt_roundsh_i64(__m128h, int);

// VCVTSH2SS
extern __m128 __cdecl _mm_cvtsh_ss(__m128, __m128h);
extern __m128 __cdecl _mm_mask_cvtsh_ss(__m128, __mmask8, __m128, __m128h);
extern __m128 __cdecl _mm_maskz_cvtsh_ss(__mmask8, __m128, __m128h);
extern __m128 __cdecl _mm_cvt_roundsh_ss(__m128, __m128h, const int);
extern __m128 __cdecl _mm_mask_cvt_roundsh_ss(__m128, __mmask8, __m128, __m128h, const int);
extern __m128 __cdecl _mm_maskz_cvt_roundsh_ss(__mmask8, __m128, __m128h, const int);

// VCVTSH2USI
extern unsigned int __cdecl _mm_cvtsh_u32(__m128h);
extern unsigned __int64 __cdecl _mm_cvtsh_u64(__m128h);
extern unsigned int __cdecl _mm_cvt_roundsh_u32(__m128h, int);
extern unsigned __int64 __cdecl _mm_cvt_roundsh_u64(__m128h, int);

// VCVTSI2SH
extern __m128h __cdecl _mm_cvti32_sh(__m128h, int);
extern __m128h __cdecl _mm_cvti64_sh(__m128h, __int64);
extern __m128h __cdecl _mm_cvt_roundi32_sh(__m128h, int, int);
extern __m128h __cdecl _mm_cvt_roundi64_sh(__m128h, __int64, int);

// VCVTSS2SH
extern __m128h __cdecl _mm_cvtss_sh(__m128h, __m128, const int);
extern __m128h __cdecl _mm_mask_cvtss_sh(__m128h, __mmask8, __m128h, __m128, const int);
extern __m128h __cdecl _mm_maskz_cvtss_sh(__mmask8, __m128h, __m128, const int);
extern __m128h __cdecl _mm_cvt_roundss_sh(__m128h, __m128, const int);
extern __m128h __cdecl _mm_mask_cvt_roundss_sh(__m128h, __mmask8, __m128h, __m128, const int);
extern __m128h __cdecl _mm_maskz_cvt_roundss_sh(__mmask8, __m128h, __m128, const int);

// VCVTTPH2DQ
extern __m128i __cdecl _mm_cvttph_epi32(__m128h);
extern __m128i __cdecl _mm_mask_cvttph_epi32(__m128i, __mmask8, __m128h);
extern __m128i __cdecl _mm_maskz_cvttph_epi32(__mmask8, __m128h);
extern __m256i __cdecl _mm256_cvttph_epi32(__m128h);
extern __m256i __cdecl _mm256_mask_cvttph_epi32(__m256i, __mmask8, __m128h);
extern __m256i __cdecl _mm256_maskz_cvttph_epi32(__mmask8, __m128h);
extern __m512i __cdecl _mm512_cvttph_epi32 (__m256h);
extern __m512i __cdecl _mm512_mask_cvttph_epi32 (__m512i, __mmask16, __m256h);
extern __m512i __cdecl _mm512_maskz_cvttph_epi32 (__mmask16, __m256h);
extern __m512i __cdecl _mm512_cvtt_roundph_epi32(__m256h, int);
extern __m512i __cdecl _mm512_mask_cvtt_roundph_epi32(__m512i, __mmask16, __m256h, int);
extern __m512i __cdecl _mm512_maskz_cvtt_roundph_epi32(__mmask16, __m256h, int);

// VCVTTPH2QQ
extern __m128i __cdecl _mm_cvttph_epi64(__m128h);
extern __m128i __cdecl _mm_mask_cvttph_epi64(__m128i, __mmask8, __m128h);
extern __m128i __cdecl _mm_maskz_cvttph_epi64(__mmask8, __m128h);
extern __m256i __cdecl _mm256_cvttph_epi64(__m128h);
extern __m256i __cdecl _mm256_mask_cvttph_epi64(__m256i, __mmask8, __m128h);
extern __m256i __cdecl _mm256_maskz_cvttph_epi64(__mmask8, __m128h);
extern __m512i __cdecl _mm512_cvttph_epi64 (__m128h);
extern __m512i __cdecl _mm512_mask_cvttph_epi64 (__m512i, __mmask8, __m128h);
extern __m512i __cdecl _mm512_maskz_cvttph_epi64 (__mmask8, __m128h);
extern __m512i __cdecl _mm512_cvtt_roundph_epi64(__m128h, int);
extern __m512i __cdecl _mm512_mask_cvtt_roundph_epi64(__m512i, __mmask8, __m128h, int);
extern __m512i __cdecl _mm512_maskz_cvtt_roundph_epi64(__mmask8, __m128h, int);

// VCVTTPH2UDQ
extern __m128i __cdecl _mm_cvttph_epu32(__m128h);
extern __m128i __cdecl _mm_mask_cvttph_epu32(__m128i, __mmask8, __m128h);
extern __m128i __cdecl _mm_maskz_cvttph_epu32(__mmask8, __m128h);
extern __m256i __cdecl _mm256_cvttph_epu32(__m128h);
extern __m256i __cdecl _mm256_mask_cvttph_epu32(__m256i, __mmask8, __m128h);
extern __m256i __cdecl _mm256_maskz_cvttph_epu32(__mmask8, __m128h);
extern __m512i __cdecl _mm512_cvttph_epu32 (__m256h);
extern __m512i __cdecl _mm512_mask_cvttph_epu32 (__m512i, __mmask16, __m256h);
extern __m512i __cdecl _mm512_maskz_cvttph_epu32 (__mmask16, __m256h);
extern __m512i __cdecl _mm512_cvtt_roundph_epu32(__m256h, int);
extern __m512i __cdecl _mm512_mask_cvtt_roundph_epu32(__m512i, __mmask16, __m256h, int);
extern __m512i __cdecl _mm512_maskz_cvtt_roundph_epu32(__mmask16, __m256h, int);

// VCVTTPH2UQQ
extern __m128i __cdecl _mm_cvttph_epu64(__m128h);
extern __m128i __cdecl _mm_mask_cvttph_epu64(__m128i, __mmask8, __m128h);
extern __m128i __cdecl _mm_maskz_cvttph_epu64(__mmask8, __m128h);
extern __m256i __cdecl _mm256_cvttph_epu64(__m128h);
extern __m256i __cdecl _mm256_mask_cvttph_epu64(__m256i, __mmask8, __m128h);
extern __m256i __cdecl _mm256_maskz_cvttph_epu64(__mmask8, __m128h);
extern __m512i __cdecl _mm512_cvttph_epu64 (__m128h);
extern __m512i __cdecl _mm512_mask_cvttph_epu64 (__m512i, __mmask8, __m128h);
extern __m512i __cdecl _mm512_maskz_cvttph_epu64 (__mmask8, __m128h);
extern __m512i __cdecl _mm512_cvtt_roundph_epu64(__m128h, int);
extern __m512i __cdecl _mm512_mask_cvtt_roundph_epu64(__m512i, __mmask8, __m128h, int);
extern __m512i __cdecl _mm512_maskz_cvtt_roundph_epu64(__mmask8, __m128h, int);

// VCVTTPH2UW
extern __m128i __cdecl _mm_cvttph_epu16(__m128h);
extern __m128i __cdecl _mm_mask_cvttph_epu16(__m128i, __mmask8, __m128h);
extern __m128i __cdecl _mm_maskz_cvttph_epu16(__mmask8, __m128h);
extern __m256i __cdecl _mm256_cvttph_epu16(__m256h);
extern __m256i __cdecl _mm256_mask_cvttph_epu16(__m256i, __mmask16, __m256h);
extern __m256i __cdecl _mm256_maskz_cvttph_epu16(__mmask16, __m256h);
extern __m512i __cdecl _mm512_cvttph_epu16 (__m512h);
extern __m512i __cdecl _mm512_mask_cvttph_epu16 (__m512i, __mmask32, __m512h);
extern __m512i __cdecl _mm512_maskz_cvttph_epu16 (__mmask32, __m512h);
extern __m512i __cdecl _mm512_cvtt_roundph_epu16(__m512h, int);
extern __m512i __cdecl _mm512_mask_cvtt_roundph_epu16(__m512i, __mmask32, __m512h, int);
extern __m512i __cdecl _mm512_maskz_cvtt_roundph_epu16(__mmask32, __m512h, int);

// VCVTTPH2W
extern __m128i __cdecl _mm_cvttph_epi16(__m128h);
extern __m128i __cdecl _mm_mask_cvttph_epi16(__m128i, __mmask8, __m128h);
extern __m128i __cdecl _mm_maskz_cvttph_epi16(__mmask8, __m128h);
extern __m256i __cdecl _mm256_cvttph_epi16(__m256h);
extern __m256i __cdecl _mm256_mask_cvttph_epi16(__m256i, __mmask16, __m256h);
extern __m256i __cdecl _mm256_maskz_cvttph_epi16(__mmask16, __m256h);
extern __m512i __cdecl _mm512_cvttph_epi16 (__m512h);
extern __m512i __cdecl _mm512_mask_cvttph_epi16 (__m512i, __mmask32, __m512h);
extern __m512i __cdecl _mm512_maskz_cvttph_epi16 (__mmask32, __m512h);
extern __m512i __cdecl _mm512_cvtt_roundph_epi16(__m512h, int);
extern __m512i __cdecl _mm512_mask_cvtt_roundph_epi16(__m512i, __mmask32, __m512h, int);
extern __m512i __cdecl _mm512_maskz_cvtt_roundph_epi16(__mmask32, __m512h, int);

// VCVTTSH2SI
extern int __cdecl _mm_cvttsh_i32(__m128h);
extern __int64 __cdecl _mm_cvttsh_i64(__m128h);
extern int __cdecl _mm_cvtt_roundsh_i32(__m128h, int);
extern __int64 __cdecl _mm_cvtt_roundsh_i64(__m128h, int);

// VCVTTSH2USI
extern unsigned int __cdecl _mm_cvttsh_u32(__m128h);
extern unsigned __int64 __cdecl _mm_cvttsh_u64(__m128h);
extern unsigned int __cdecl _mm_cvtt_roundsh_u32(__m128h, int);
extern unsigned __int64 __cdecl _mm_cvtt_roundsh_u64(__m128h, int);

//VCVTUDQ2PH
extern __m128h __cdecl _mm_cvtepu32_ph(__m128i);
extern __m128h __cdecl _mm_mask_cvtepu32_ph(__m128h, __mmask8, __m128i);
extern __m128h __cdecl _mm_maskz_cvtepu32_ph(__mmask8, __m128i);
extern __m128h __cdecl _mm256_cvtepu32_ph(__m256i);
extern __m128h __cdecl _mm256_mask_cvtepu32_ph(__m128h, __mmask8, __m256i);
extern __m128h __cdecl _mm256_maskz_cvtepu32_ph(__mmask8, __m256i);
extern __m256h __cdecl _mm512_cvtepu32_ph (__m512i);
extern __m256h __cdecl _mm512_mask_cvtepu32_ph (__m256h, __mmask16, __m512i);
extern __m256h __cdecl _mm512_maskz_cvtepu32_ph (__mmask16, __m512i);
extern __m256h __cdecl _mm512_cvt_roundepu32_ph(__m512i, int);
extern __m256h __cdecl _mm512_mask_cvt_roundepu32_ph(__m256h, __mmask16, __m512i, int);
extern __m256h __cdecl _mm512_maskz_cvt_roundepu32_ph(__mmask16, __m512i, int);

// VCVTUQQ2PH
extern __m128h __cdecl _mm_cvtepu64_ph(__m128i);
extern __m128h __cdecl _mm_mask_cvtepu64_ph(__m128h, __mmask8, __m128i);
extern __m128h __cdecl _mm_maskz_cvtepu64_ph(__mmask8, __m128i);
extern __m128h __cdecl _mm256_cvtepu64_ph(__m256i);
extern __m128h __cdecl _mm256_mask_cvtepu64_ph(__m128h, __mmask8, __m256i);
extern __m128h __cdecl _mm256_maskz_cvtepu64_ph(__mmask8, __m256i);
extern __m128h __cdecl _mm512_cvtepu64_ph (__m512i);
extern __m128h __cdecl _mm512_mask_cvtepu64_ph (__m128h, __mmask8, __m512i);
extern __m128h __cdecl _mm512_maskz_cvtepu64_ph (__mmask8, __m512i);
extern __m128h __cdecl _mm512_cvt_roundepu64_ph(__m512i, int);
extern __m128h __cdecl _mm512_mask_cvt_roundepu64_ph(__m128h, __mmask8, __m512i, int);
extern __m128h __cdecl _mm512_maskz_cvt_roundepu64_ph(__mmask8, __m512i, int);

// VCVTUSI2SH
extern __m128h __cdecl _mm_cvtu32_sh(__m128h, unsigned int);
extern __m128h __cdecl _mm_cvtu64_sh(__m128h, unsigned __int64);
extern __m128h __cdecl _mm_cvt_roundu32_sh(__m128h, unsigned int, int);
extern __m128h __cdecl _mm_cvt_roundu64_sh(__m128h, unsigned __int64, int);

// VCVTUW2PH
extern __m128h __cdecl _mm_cvtepu16_ph(__m128i);
extern __m128h __cdecl _mm_mask_cvtepu16_ph(__m128h, __mmask8, __m128i);
extern __m128h __cdecl _mm_maskz_cvtepu16_ph(__mmask8, __m128i);
extern __m256h __cdecl _mm256_cvtepu16_ph(__m256i);
extern __m256h __cdecl _mm256_mask_cvtepu16_ph(__m256h, __mmask16, __m256i);
extern __m256h __cdecl _mm256_maskz_cvtepu16_ph(__mmask16, __m256i);
extern __m512h __cdecl _mm512_cvtepu16_ph (__m512i);
extern __m512h __cdecl _mm512_mask_cvtepu16_ph (__m512h, __mmask32, __m512i);
extern __m512h __cdecl _mm512_maskz_cvtepu16_ph (__mmask32, __m512i);
extern __m512h __cdecl _mm512_cvt_roundepu16_ph(__m512i, int);
extern __m512h __cdecl _mm512_mask_cvt_roundepu16_ph(__m512h, __mmask32, __m512i, int);
extern __m512h __cdecl _mm512_maskz_cvt_roundepu16_ph(__mmask32, __m512i, int);

// VCVTW2PH
extern __m128h __cdecl _mm_cvtepi16_ph(__m128i);
extern __m128h __cdecl _mm_mask_cvtepi16_ph(__m128h, __mmask8, __m128i);
extern __m128h __cdecl _mm_maskz_cvtepi16_ph(__mmask8, __m128i);
extern __m256h __cdecl _mm256_cvtepi16_ph(__m256i);
extern __m256h __cdecl _mm256_mask_cvtepi16_ph(__m256h, __mmask16, __m256i);
extern __m256h __cdecl _mm256_maskz_cvtepi16_ph(__mmask16, __m256i);
extern __m512h __cdecl _mm512_cvtepi16_ph (__m512i);
extern __m512h __cdecl _mm512_mask_cvtepi16_ph (__m512h, __mmask32, __m512i);
extern __m512h __cdecl _mm512_maskz_cvtepi16_ph (__mmask32, __m512i);
extern __m512h __cdecl _mm512_cvt_roundepi16_ph(__m512i, int);
extern __m512h __cdecl _mm512_mask_cvt_roundepi16_ph(__m512h, __mmask32, __m512i, int);
extern __m512h __cdecl _mm512_maskz_cvt_roundepi16_ph(__mmask32, __m512i, int);

// VDIVPH
extern __m128h __cdecl _mm_div_ph(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_div_ph(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_div_ph(__mmask8, __m128h, __m128h);
extern __m256h __cdecl _mm256_div_ph(__m256h, __m256h);
extern __m256h __cdecl _mm256_mask_div_ph(__m256h, __mmask16, __m256h, __m256h);
extern __m256h __cdecl _mm256_maskz_div_ph(__mmask16, __m256h, __m256h);
extern __m512h __cdecl _mm512_div_ph(__m512h, __m512h);
extern __m512h __cdecl _mm512_mask_div_ph(__m512h, __mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_maskz_div_ph(__mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_div_round_ph(__m512h, __m512h, int);
extern __m512h __cdecl _mm512_mask_div_round_ph(__m512h, __mmask32, __m512h, __m512h, int);
extern __m512h __cdecl _mm512_maskz_div_round_ph(__mmask32, __m512h, __m512h, int);

// VDIVSH
extern __m128h __cdecl _mm_div_sh(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_div_sh(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_div_sh(__mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_div_round_sh(__m128h, __m128h, int);
extern __m128h __cdecl _mm_mask_div_round_sh(__m128h, __mmask8, __m128h, __m128h, int);
extern __m128h __cdecl _mm_maskz_div_round_sh(__mmask8, __m128h, __m128h, int);

// VFMADDSUB[132,213,231]PH
extern __m128h __cdecl _mm_fmaddsub_ph(__m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_mask_fmaddsub_ph(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_mask3_fmaddsub_ph(__m128h, __m128h, __m128h, __mmask8);
extern __m128h __cdecl _mm_maskz_fmaddsub_ph(__mmask8, __m128h, __m128h, __m128h);
extern __m256h __cdecl _mm256_fmaddsub_ph(__m256h, __m256h, __m256h);
extern __m256h __cdecl _mm256_mask_fmaddsub_ph(__m256h, __mmask16, __m256h, __m256h);
extern __m256h __cdecl _mm256_mask3_fmaddsub_ph(__m256h, __m256h, __m256h, __mmask16);
extern __m256h __cdecl _mm256_maskz_fmaddsub_ph(__mmask16, __m256h, __m256h, __m256h);
extern __m512h __cdecl _mm512_fmaddsub_ph(__m512h, __m512h, __m512h);
extern __m512h __cdecl _mm512_mask_fmaddsub_ph(__m512h, __mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_mask3_fmaddsub_ph(__m512h, __m512h, __m512h, __mmask32);
extern __m512h __cdecl _mm512_maskz_fmaddsub_ph(__mmask32, __m512h, __m512h, __m512h);
extern __m512h __cdecl _mm512_fmaddsub_round_ph(__m512h, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask_fmaddsub_round_ph(__m512h, __mmask32, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask3_fmaddsub_round_ph(__m512h, __m512h, __m512h, __mmask32, const int);
extern __m512h __cdecl _mm512_maskz_fmaddsub_round_ph(__mmask32, __m512h, __m512h, __m512h, const int);

// VFMSUBADD[132,213,231]PH
extern __m128h __cdecl _mm_fmsubadd_ph(__m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_mask_fmsubadd_ph(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_mask3_fmsubadd_ph(__m128h, __m128h, __m128h, __mmask8);
extern __m128h __cdecl _mm_maskz_fmsubadd_ph(__mmask8, __m128h, __m128h, __m128h);
extern __m256h __cdecl _mm256_fmsubadd_ph(__m256h, __m256h, __m256h);
extern __m256h __cdecl _mm256_mask_fmsubadd_ph(__m256h, __mmask16, __m256h, __m256h);
extern __m256h __cdecl _mm256_mask3_fmsubadd_ph(__m256h, __m256h, __m256h, __mmask16);
extern __m256h __cdecl _mm256_maskz_fmsubadd_ph(__mmask16, __m256h, __m256h, __m256h);
extern __m512h __cdecl _mm512_fmsubadd_ph(__m512h, __m512h, __m512h);
extern __m512h __cdecl _mm512_mask_fmsubadd_ph(__m512h, __mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_mask3_fmsubadd_ph(__m512h, __m512h, __m512h, __mmask32);
extern __m512h __cdecl _mm512_maskz_fmsubadd_ph(__mmask32, __m512h, __m512h, __m512h);
extern __m512h __cdecl _mm512_fmsubadd_round_ph(__m512h, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask_fmsubadd_round_ph(__m512h, __mmask32, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask3_fmsubadd_round_ph(__m512h, __m512h, __m512h, __mmask32, const int);
extern __m512h __cdecl _mm512_maskz_fmsubadd_round_ph(__mmask32, __m512h, __m512h, __m512h, const int);

// VFPCLASSPH
extern __mmask8 __cdecl _mm_fpclass_ph_mask(__m128h, int);
extern __mmask8 __cdecl _mm_mask_fpclass_ph_mask(__mmask8, __m128h, int);
extern __mmask16 __cdecl _mm256_fpclass_ph_mask(__m256h, int);
extern __mmask16 __cdecl _mm256_mask_fpclass_ph_mask(__mmask16, __m256h, int);
extern __mmask32 __cdecl _mm512_fpclass_ph_mask(__m512h, int);
extern __mmask32 __cdecl _mm512_mask_fpclass_ph_mask(__mmask32, __m512h, int);

// VFPCLASSSH
extern __mmask8 __cdecl _mm_fpclass_sh_mask(__m128h, int);
extern __mmask8 __cdecl _mm_mask_fpclass_sh_mask(__mmask8, __m128h, int);

// VF[,C]MADDCPH
extern __m128h __cdecl _mm_fmadd_pch(__m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_mask_fmadd_pch(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_mask3_fmadd_pch(__m128h, __m128h, __m128h, __mmask8);
extern __m128h __cdecl _mm_maskz_fmadd_pch(__mmask8, __m128h, __m128h, __m128h);
extern __m256h __cdecl _mm256_fmadd_pch(__m256h, __m256h, __m256h);
extern __m256h __cdecl _mm256_mask_fmadd_pch(__m256h, __mmask8, __m256h, __m256h);
extern __m256h __cdecl _mm256_mask3_fmadd_pch(__m256h, __m256h, __m256h, __mmask8);
extern __m256h __cdecl _mm256_maskz_fmadd_pch(__mmask8, __m256h, __m256h, __m256h);
extern __m512h __cdecl _mm512_fmadd_pch(__m512h, __m512h, __m512h);
extern __m512h __cdecl _mm512_mask_fmadd_pch(__m512h, __mmask16, __m512h, __m512h);
extern __m512h __cdecl _mm512_mask3_fmadd_pch(__m512h, __m512h, __m512h, __mmask16);
extern __m512h __cdecl _mm512_maskz_fmadd_pch(__mmask16, __m512h, __m512h, __m512h);
extern __m512h __cdecl _mm512_fmadd_round_pch(__m512h, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask_fmadd_round_pch(__m512h, __mmask16, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask3_fmadd_round_pch(__m512h, __m512h, __m512h, __mmask16, const int);
extern __m512h __cdecl _mm512_maskz_fmadd_round_pch(__mmask16, __m512h, __m512h, __m512h, const int);
extern __m128h __cdecl _mm_fcmadd_pch(__m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_mask_fcmadd_pch(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_mask3_fcmadd_pch(__m128h, __m128h, __m128h, __mmask8);
extern __m128h __cdecl _mm_maskz_fcmadd_pch(__mmask8, __m128h, __m128h, __m128h);
extern __m256h __cdecl _mm256_fcmadd_pch(__m256h, __m256h, __m256h);
extern __m256h __cdecl _mm256_mask_fcmadd_pch(__m256h, __mmask8, __m256h, __m256h);
extern __m256h __cdecl _mm256_mask3_fcmadd_pch(__m256h, __m256h, __m256h, __mmask8);
extern __m256h __cdecl _mm256_maskz_fcmadd_pch(__mmask8, __m256h, __m256h, __m256h);
extern __m512h __cdecl _mm512_fcmadd_pch(__m512h, __m512h, __m512h);
extern __m512h __cdecl _mm512_mask_fcmadd_pch(__m512h, __mmask16, __m512h, __m512h);
extern __m512h __cdecl _mm512_mask3_fcmadd_pch(__m512h, __m512h, __m512h, __mmask16);
extern __m512h __cdecl _mm512_maskz_fcmadd_pch(__mmask16, __m512h, __m512h, __m512h);
extern __m512h __cdecl _mm512_fcmadd_round_pch(__m512h, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask_fcmadd_round_pch(__m512h, __mmask16, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask3_fcmadd_round_pch(__m512h, __m512h, __m512h, __mmask16, const int);
extern __m512h __cdecl _mm512_maskz_fcmadd_round_pch(__mmask16, __m512h, __m512h, __m512h, const int);

// VF[,C]MADDCSH
extern __m128h __cdecl _mm_fcmadd_sch(__m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_mask_fcmadd_sch(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_mask3_fcmadd_sch(__m128h, __m128h, __m128h, __mmask8);
extern __m128h __cdecl _mm_maskz_fcmadd_sch(__mmask8, __m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_fcmadd_round_sch(__m128h, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_mask_fcmadd_round_sch(__m128h, __mmask8, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_mask3_fcmadd_round_sch(__m128h, __m128h, __m128h, __mmask8, const int);
extern __m128h __cdecl _mm_maskz_fcmadd_round_sch(__mmask8, __m128h, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_fmadd_sch(__m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_mask_fmadd_sch(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_fmadd_sch(__mmask8, __m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_fmadd_round_sch(__m128h, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_mask_fmadd_round_sch(__m128h, __mmask8, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_maskz_fmadd_round_sch(__mmask8, __m128h, __m128h, __m128h, const int);

// VF[,C]MULCPH
extern __m128h __cdecl _mm_fcmul_pch(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_fcmul_pch(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_fcmul_pch(__mmask8, __m128h, __m128h);
extern __m256h __cdecl _mm256_fcmul_pch(__m256h, __m256h);
extern __m256h __cdecl _mm256_mask_fcmul_pch(__m256h, __mmask8, __m256h, __m256h);
extern __m256h __cdecl _mm256_maskz_fcmul_pch(__mmask8, __m256h, __m256h);
extern __m512h __cdecl _mm512_fcmul_pch(__m512h, __m512h);
extern __m512h __cdecl _mm512_mask_fcmul_pch(__m512h, __mmask16, __m512h, __m512h);
extern __m512h __cdecl _mm512_maskz_fcmul_pch(__mmask16, __m512h, __m512h);
extern __m512h __cdecl _mm512_fcmul_round_pch(__m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask_fcmul_round_pch(__m512h, __mmask16, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_maskz_fcmul_round_pch(__mmask16, __m512h, __m512h, const int);
extern __m128h __cdecl _mm_fmul_pch(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_fmul_pch(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_fmul_pch(__mmask8, __m128h, __m128h);
extern __m256h __cdecl _mm256_fmul_pch(__m256h, __m256h);
extern __m256h __cdecl _mm256_mask_fmul_pch(__m256h, __mmask8, __m256h, __m256h);
extern __m256h __cdecl _mm256_maskz_fmul_pch(__mmask8, __m256h, __m256h);
extern __m512h __cdecl _mm512_fmul_pch(__m512h, __m512h);
extern __m512h __cdecl _mm512_mask_fmul_pch(__m512h, __mmask16, __m512h, __m512h);
extern __m512h __cdecl _mm512_maskz_fmul_pch(__mmask16, __m512h, __m512h);
extern __m512h __cdecl _mm512_fmul_round_pch(__m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask_fmul_round_pch(__m512h, __mmask16, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_maskz_fmul_round_pch(__mmask16, __m512h, __m512h, const int);

// VF[,C]MULCSH
extern __m128h __cdecl _mm_fcmul_sch(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_fcmul_sch(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_fcmul_sch(__mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_fcmul_round_sch(__m128h, __m128h, const int);
extern __m128h __cdecl _mm_mask_fcmul_round_sch(__m128h, __mmask8, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_maskz_fcmul_round_sch(__mmask8, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_fmul_sch(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_fmul_sch(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_fmul_sch(__mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_fmul_round_sch(__m128h, __m128h, const int);
extern __m128h __cdecl _mm_mask_fmul_round_sch(__m128h, __mmask8, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_maskz_fmul_round_sch(__mmask8, __m128h, __m128h, const int);

// VF[,N]MADD[132,213,231]PH
extern __m128h __cdecl _mm_fnmadd_ph(__m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_mask_fnmadd_ph(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_mask3_fnmadd_ph(__m128h, __m128h, __m128h, __mmask8);
extern __m128h __cdecl _mm_maskz_fnmadd_ph(__mmask8, __m128h, __m128h, __m128h);
extern __m256h __cdecl _mm256_fnmadd_ph(__m256h, __m256h, __m256h);
extern __m256h __cdecl _mm256_mask_fnmadd_ph(__m256h, __mmask16, __m256h, __m256h);
extern __m256h __cdecl _mm256_mask3_fnmadd_ph(__m256h, __m256h, __m256h, __mmask16);
extern __m256h __cdecl _mm256_maskz_fnmadd_ph(__mmask16, __m256h, __m256h, __m256h);
extern __m512h __cdecl _mm512_fnmadd_ph(__m512h, __m512h, __m512h);
extern __m512h __cdecl _mm512_mask_fnmadd_ph(__m512h, __mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_mask3_fnmadd_ph(__m512h, __m512h, __m512h, __mmask32);
extern __m512h __cdecl _mm512_maskz_fnmadd_ph(__mmask32, __m512h, __m512h, __m512h);
extern __m512h __cdecl _mm512_fnmadd_round_ph(__m512h, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask_fnmadd_round_ph(__m512h, __mmask32, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask3_fnmadd_round_ph(__m512h, __m512h, __m512h, __mmask32, const int);
extern __m512h __cdecl _mm512_maskz_fnmadd_round_ph(__mmask32, __m512h, __m512h, __m512h, const int);
extern __m128h __cdecl _mm_fmadd_ph(__m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_mask_fmadd_ph(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_mask3_fmadd_ph(__m128h, __m128h, __m128h, __mmask8);
extern __m128h __cdecl _mm_maskz_fmadd_ph(__mmask8, __m128h, __m128h, __m128h);
extern __m256h __cdecl _mm256_fmadd_ph(__m256h, __m256h, __m256h);
extern __m256h __cdecl _mm256_mask_fmadd_ph(__m256h, __mmask16, __m256h, __m256h);
extern __m256h __cdecl _mm256_mask3_fmadd_ph(__m256h, __m256h, __m256h, __mmask16);
extern __m256h __cdecl _mm256_maskz_fmadd_ph(__mmask16, __m256h, __m256h, __m256h);
extern __m512h __cdecl _mm512_fmadd_ph(__m512h, __m512h, __m512h);
extern __m512h __cdecl _mm512_mask_fmadd_ph(__m512h, __mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_mask3_fmadd_ph(__m512h, __m512h, __m512h, __mmask32);
extern __m512h __cdecl _mm512_maskz_fmadd_ph(__mmask32, __m512h, __m512h, __m512h);
extern __m512h __cdecl _mm512_fmadd_round_ph(__m512h, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask_fmadd_round_ph(__m512h, __mmask32, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask3_fmadd_round_ph(__m512h, __m512h, __m512h, __mmask32, const int);
extern __m512h __cdecl _mm512_maskz_fmadd_round_ph(__mmask32, __m512h, __m512h, __m512h, const int);

// VF[,N]MADD[132,213,231]SH
extern __m128h __cdecl _mm_fnmadd_sh(__m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_mask_fnmadd_sh(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_mask3_fnmadd_sh(__m128h, __m128h, __m128h, __mmask8);
extern __m128h __cdecl _mm_maskz_fnmadd_sh(__mmask8, __m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_fnmadd_round_sh(__m128h, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_mask_fnmadd_round_sh(__m128h, __mmask8, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_mask3_fnmadd_round_sh(__m128h, __m128h, __m128h, __mmask8, const int);
extern __m128h __cdecl _mm_maskz_fnmadd_round_sh(__mmask8, __m128h, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_fmadd_sh(__m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_mask_fmadd_sh(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_mask3_fmadd_sh(__m128h, __m128h, __m128h, __mmask8);
extern __m128h __cdecl _mm_maskz_fmadd_sh(__mmask8, __m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_fmadd_round_sh(__m128h, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_mask_fmadd_round_sh(__m128h, __mmask8, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_mask3_fmadd_round_sh(__m128h, __m128h, __m128h, __mmask8, const int);
extern __m128h __cdecl _mm_maskz_fmadd_round_sh(__mmask8, __m128h, __m128h, __m128h, const int);

// VF[,N]MSUB[132,213,231]PH
extern __m128h __cdecl _mm_fnmsub_ph(__m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_mask_fnmsub_ph(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_mask3_fnmsub_ph(__m128h, __m128h, __m128h, __mmask8);
extern __m128h __cdecl _mm_maskz_fnmsub_ph(__mmask8, __m128h, __m128h, __m128h);
extern __m256h __cdecl _mm256_fnmsub_ph(__m256h, __m256h, __m256h);
extern __m256h __cdecl _mm256_mask_fnmsub_ph(__m256h, __mmask16, __m256h, __m256h);
extern __m256h __cdecl _mm256_mask3_fnmsub_ph(__m256h, __m256h, __m256h, __mmask16);
extern __m256h __cdecl _mm256_maskz_fnmsub_ph(__mmask16, __m256h, __m256h, __m256h);
extern __m512h __cdecl _mm512_fnmsub_ph(__m512h, __m512h, __m512h);
extern __m512h __cdecl _mm512_mask_fnmsub_ph(__m512h, __mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_mask3_fnmsub_ph(__m512h, __m512h, __m512h, __mmask32);
extern __m512h __cdecl _mm512_maskz_fnmsub_ph(__mmask32, __m512h, __m512h, __m512h);
extern __m512h __cdecl _mm512_fnmsub_round_ph(__m512h, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask_fnmsub_round_ph(__m512h, __mmask32, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask3_fnmsub_round_ph(__m512h, __m512h, __m512h, __mmask32, const int);
extern __m512h __cdecl _mm512_maskz_fnmsub_round_ph(__mmask32, __m512h, __m512h, __m512h, const int);
extern __m128h __cdecl _mm_fmsub_ph(__m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_mask_fmsub_ph(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_mask3_fmsub_ph(__m128h, __m128h, __m128h, __mmask8);
extern __m128h __cdecl _mm_maskz_fmsub_ph(__mmask8, __m128h, __m128h, __m128h);
extern __m256h __cdecl _mm256_fmsub_ph(__m256h, __m256h, __m256h);
extern __m256h __cdecl _mm256_mask_fmsub_ph(__m256h, __mmask16, __m256h, __m256h);
extern __m256h __cdecl _mm256_mask3_fmsub_ph(__m256h, __m256h, __m256h, __mmask16);
extern __m256h __cdecl _mm256_maskz_fmsub_ph(__mmask16, __m256h, __m256h, __m256h);
extern __m512h __cdecl _mm512_fmsub_ph(__m512h, __m512h, __m512h);
extern __m512h __cdecl _mm512_mask_fmsub_ph(__m512h, __mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_mask3_fmsub_ph(__m512h, __m512h, __m512h, __mmask32);
extern __m512h __cdecl _mm512_maskz_fmsub_ph(__mmask32, __m512h, __m512h, __m512h);
extern __m512h __cdecl _mm512_fmsub_round_ph(__m512h, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask_fmsub_round_ph(__m512h, __mmask32, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask3_fmsub_round_ph(__m512h, __m512h, __m512h, __mmask32, const int);
extern __m512h __cdecl _mm512_maskz_fmsub_round_ph(__mmask32, __m512h, __m512h, __m512h, const int);

// VF[,N]MSUB[132,213,231]SH
extern __m128h __cdecl _mm_fnmsub_sh(__m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_mask_fnmsub_sh(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_mask3_fnmsub_sh(__m128h, __m128h, __m128h, __mmask8);
extern __m128h __cdecl _mm_maskz_fnmsub_sh(__mmask8, __m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_fnmsub_round_sh(__m128h, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_mask_fnmsub_round_sh(__m128h, __mmask8, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_mask3_fnmsub_round_sh(__m128h, __m128h, __m128h, __mmask8, const int);
extern __m128h __cdecl _mm_maskz_fnmsub_round_sh(__mmask8, __m128h, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_fmsub_sh(__m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_mask_fmsub_sh(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_mask3_fmsub_sh(__m128h, __m128h, __m128h, __mmask8);
extern __m128h __cdecl _mm_maskz_fmsub_sh(__mmask8, __m128h, __m128h, __m128h);
extern __m128h __cdecl _mm_fmsub_round_sh(__m128h, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_mask_fmsub_round_sh(__m128h, __mmask8, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_mask3_fmsub_round_sh(__m128h, __m128h, __m128h, __mmask8, const int);
extern __m128h __cdecl _mm_maskz_fmsub_round_sh(__mmask8, __m128h, __m128h, __m128h, const int);

// VGETEXPPH
extern __m128h __cdecl _mm_getexp_ph(__m128h);
extern __m128h __cdecl _mm_mask_getexp_ph(__m128h, __mmask8, __m128h);
extern __m128h __cdecl _mm_maskz_getexp_ph(__mmask8, __m128h);
extern __m256h __cdecl _mm256_getexp_ph(__m256h);
extern __m256h __cdecl _mm256_mask_getexp_ph(__m256h, __mmask16, __m256h);
extern __m256h __cdecl _mm256_maskz_getexp_ph(__mmask16, __m256h);
extern __m512h __cdecl _mm512_getexp_ph(__m512h);
extern __m512h __cdecl _mm512_mask_getexp_ph(__m512h, __mmask32, __m512h);
extern __m512h __cdecl _mm512_maskz_getexp_ph(__mmask32, __m512h);
extern __m512h __cdecl _mm512_getexp_round_ph(__m512h, const int);
extern __m512h __cdecl _mm512_mask_getexp_round_ph(__m512h, __mmask32, __m512h, const int);
extern __m512h __cdecl _mm512_maskz_getexp_round_ph(__mmask32, __m512h, const int);

// VGETEXPSH
extern __m128h __cdecl _mm_getexp_sh(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_getexp_sh(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_getexp_sh(__mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_getexp_round_sh(__m128h, __m128h, const int);
extern __m128h __cdecl _mm_mask_getexp_round_sh(__m128h, __mmask8, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_maskz_getexp_round_sh(__mmask8, __m128h, __m128h, const int);

// VGETMANTPH
extern __m128h __cdecl _mm_getmant_ph(__m128h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM);
extern __m128h __cdecl _mm_mask_getmant_ph(__m128h, __mmask8, __m128h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM);
extern __m128h __cdecl _mm_maskz_getmant_ph(__mmask8, __m128h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM);
extern __m256h __cdecl _mm256_getmant_ph(__m256h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM);
extern __m256h __cdecl _mm256_mask_getmant_ph(__m256h, __mmask16, __m256h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM);
extern __m256h __cdecl _mm256_maskz_getmant_ph(__mmask16, __m256h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM);
extern __m512h __cdecl _mm512_getmant_ph(__m512h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM);
extern __m512h __cdecl _mm512_mask_getmant_ph(__m512h, __mmask32, __m512h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM);
extern __m512h __cdecl _mm512_maskz_getmant_ph(__mmask32, __m512h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM);
extern __m512h __cdecl _mm512_getmant_round_ph(__m512h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM, const int);
extern __m512h __cdecl _mm512_mask_getmant_round_ph(__m512h, __mmask32, __m512h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM, const int);
extern __m512h __cdecl _mm512_maskz_getmant_round_ph(__mmask32, __m512h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM, const int);

// VGETMANTSH
extern __m128h __cdecl _mm_getmant_sh(__m128h, __m128h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM);
extern __m128h __cdecl _mm_mask_getmant_sh(__m128h, __mmask8, __m128h, __m128h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM);
extern __m128h __cdecl _mm_maskz_getmant_sh(__mmask8, __m128h, __m128h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM);
extern __m128h __cdecl _mm_getmant_round_sh(__m128h, __m128h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM, const int);
extern __m128h __cdecl _mm_mask_getmant_round_sh(__m128h, __mmask8, __m128h, __m128h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM, const int);
extern __m128h __cdecl _mm_maskz_getmant_round_sh(__mmask8, __m128h, __m128h, _MM_MANTISSA_NORM_ENUM, _MM_MANTISSA_SIGN_ENUM, const int);

// VMAXPH
extern __m128h __cdecl _mm_max_ph(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_max_ph(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_max_ph(__mmask8, __m128h, __m128h);
extern __m256h __cdecl _mm256_max_ph(__m256h, __m256h);
extern __m256h __cdecl _mm256_mask_max_ph(__m256h, __mmask16, __m256h, __m256h);
extern __m256h __cdecl _mm256_maskz_max_ph(__mmask16, __m256h, __m256h);
extern __m512h __cdecl _mm512_max_ph(__m512h, __m512h);
extern __m512h __cdecl _mm512_mask_max_ph(__m512h, __mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_maskz_max_ph(__mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_max_round_ph(__m512h, __m512h, int);
extern __m512h __cdecl _mm512_mask_max_round_ph(__m512h, __mmask32, __m512h, __m512h, int);
extern __m512h __cdecl _mm512_maskz_max_round_ph(__mmask32, __m512h, __m512h, int);

// VMAXSH
extern __m128h __cdecl _mm_max_sh(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_max_sh(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_max_sh(__mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_max_round_sh(__m128h, __m128h, int);
extern __m128h __cdecl _mm_mask_max_round_sh(__m128h, __mmask8, __m128h, __m128h, int);
extern __m128h __cdecl _mm_maskz_max_round_sh(__mmask8, __m128h, __m128h, int);

// VMINPH
extern __m128h __cdecl _mm_min_ph(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_min_ph(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_min_ph(__mmask8, __m128h, __m128h);
extern __m256h __cdecl _mm256_min_ph(__m256h, __m256h);
extern __m256h __cdecl _mm256_mask_min_ph(__m256h, __mmask16, __m256h, __m256h);
extern __m256h __cdecl _mm256_maskz_min_ph(__mmask16, __m256h, __m256h);
extern __m512h __cdecl _mm512_min_ph(__m512h, __m512h);
extern __m512h __cdecl _mm512_mask_min_ph(__m512h, __mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_maskz_min_ph(__mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_min_round_ph(__m512h, __m512h, int);
extern __m512h __cdecl _mm512_mask_min_round_ph(__m512h, __mmask32, __m512h, __m512h, int);
extern __m512h __cdecl _mm512_maskz_min_round_ph(__mmask32, __m512h, __m512h, int);

// VMINSH
extern __m128h __cdecl _mm_min_sh(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_min_sh(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_min_sh(__mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_min_round_sh(__m128h, __m128h, int);
extern __m128h __cdecl _mm_mask_min_round_sh(__m128h, __mmask8, __m128h, __m128h, int);
extern __m128h __cdecl _mm_maskz_min_round_sh(__mmask8, __m128h, __m128h, int);

// VMOVSH
extern __m128h __cdecl _mm_load_sh(void const*);
extern __m128h __cdecl _mm_mask_load_sh(__m128h, __mmask8, void const*);
extern __m128h __cdecl _mm_maskz_load_sh(__mmask8, void const*);
extern void __cdecl _mm_store_sh(void*, __m128h);
extern void __cdecl _mm_mask_store_sh(void*, __mmask8, __m128h);
extern __m128h __cdecl _mm_move_sh(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_move_sh(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_move_sh(__mmask8, __m128h, __m128h);

// VMOVW
extern __m128i __cdecl _mm_cvtsi16_si128(short);
extern short __cdecl _mm_cvtsi128_si16(__m128i);

// MULPH
extern __m128h __cdecl _mm_mul_ph(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_mul_ph(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_mul_ph(__mmask8, __m128h, __m128h);
extern __m256h __cdecl _mm256_mul_ph(__m256h, __m256h);
extern __m256h __cdecl _mm256_mask_mul_ph(__m256h, __mmask16, __m256h, __m256h);
extern __m256h __cdecl _mm256_maskz_mul_ph(__mmask16, __m256h, __m256h);
extern __m512h __cdecl _mm512_mul_ph(__m512h, __m512h);
extern __m512h __cdecl _mm512_mask_mul_ph(__m512h, __mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_maskz_mul_ph(__mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_mul_round_ph(__m512h, __m512h, int);
extern __m512h __cdecl _mm512_mask_mul_round_ph(__m512h, __mmask32, __m512h, __m512h, int);
extern __m512h __cdecl _mm512_maskz_mul_round_ph(__mmask32, __m512h, __m512h, int);

// VMULSH
extern __m128h __cdecl _mm_mul_sh(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_mul_sh(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_mul_sh(__mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_mul_round_sh(__m128h, __m128h, int);
extern __m128h __cdecl _mm_mask_mul_round_sh(__m128h, __mmask8, __m128h, __m128h, int);
extern __m128h __cdecl _mm_maskz_mul_round_sh(__mmask8, __m128h, __m128h, int);

// VRCPPH
extern __m128h __cdecl _mm_rcp_ph(__m128h);
extern __m128h __cdecl _mm_mask_rcp_ph(__m128h, __mmask8, __m128h);
extern __m128h __cdecl _mm_maskz_rcp_ph(__mmask8, __m128h);
extern __m256h __cdecl _mm256_rcp_ph(__m256h);
extern __m256h __cdecl _mm256_mask_rcp_ph(__m256h, __mmask16, __m256h);
extern __m256h __cdecl _mm256_maskz_rcp_ph(__mmask16, __m256h);
extern __m512h __cdecl _mm512_rcp_ph(__m512h);
extern __m512h __cdecl _mm512_mask_rcp_ph(__m512h, __mmask32, __m512h);
extern __m512h __cdecl _mm512_maskz_rcp_ph(__mmask32, __m512h);

// VRCPSH
extern __m128h __cdecl _mm_rcp_sh(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_rcp_sh(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_rcp_sh(__mmask8, __m128h, __m128h);

// VREDUCEPH
extern __m128h __cdecl _mm_reduce_ph(__m128h, int);
extern __m128h __cdecl _mm_mask_reduce_ph(__m128h, __mmask8, __m128h, int);
extern __m128h __cdecl _mm_maskz_reduce_ph(__mmask8, __m128h, int);
extern __m256h __cdecl _mm256_reduce_ph(__m256h, int);
extern __m256h __cdecl _mm256_mask_reduce_ph(__m256h, __mmask16, __m256h, int);
extern __m256h __cdecl _mm256_maskz_reduce_ph(__mmask16, __m256h, int);
extern __m512h __cdecl _mm512_reduce_ph(__m512h, int);
extern __m512h __cdecl _mm512_mask_reduce_ph(__m512h, __mmask32, __m512h, int);
extern __m512h __cdecl _mm512_maskz_reduce_ph(__mmask32, __m512h, int);
extern __m512h __cdecl _mm512_reduce_round_ph(__m512h, int, const int);
extern __m512h __cdecl _mm512_mask_reduce_round_ph(__m512h, __mmask32, __m512h, int, const int);
extern __m512h __cdecl _mm512_maskz_reduce_round_ph(__mmask32, __m512h, int, const int);

// VREDUCESH
extern __m128h __cdecl _mm_reduce_sh(__m128h, __m128h, int);
extern __m128h __cdecl _mm_mask_reduce_sh(__m128h, __mmask8, __m128h, __m128h, int);
extern __m128h __cdecl _mm_maskz_reduce_sh(__mmask8, __m128h, __m128h, int);
extern __m128h __cdecl _mm_reduce_round_sh(__m128h, __m128h, int, const int);
extern __m128h __cdecl _mm_mask_reduce_round_sh(__m128h, __mmask8, __m128h, __m128h, int, const int);
extern __m128h __cdecl _mm_maskz_reduce_round_sh(__mmask8, __m128h, __m128h, int, const int);

// VRNDSCALEPH
extern __m128h __cdecl _mm_roundscale_ph(__m128h, int);
extern __m128h __cdecl _mm_mask_roundscale_ph(__m128h, __mmask8, __m128h, int);
extern __m128h __cdecl _mm_maskz_roundscale_ph(__mmask8, __m128h, int);
extern __m256h __cdecl _mm256_roundscale_ph(__m256h, int);
extern __m256h __cdecl _mm256_mask_roundscale_ph(__m256h, __mmask16, __m256h, int);
extern __m256h __cdecl _mm256_maskz_roundscale_ph(__mmask16, __m256h, int);
extern __m512h __cdecl _mm512_roundscale_ph(__m512h, int);
extern __m512h __cdecl _mm512_mask_roundscale_ph(__m512h, __mmask32, __m512h, int);
extern __m512h __cdecl _mm512_maskz_roundscale_ph(__mmask32, __m512h, int);
extern __m512h __cdecl _mm512_roundscale_round_ph(__m512h, int, const int);
extern __m512h __cdecl _mm512_mask_roundscale_round_ph(__m512h, __mmask32, __m512h, int, const int);
extern __m512h __cdecl _mm512_maskz_roundscale_round_ph(__mmask32, __m512h, int, const int);

// VRNDSCALESH
extern __m128h __cdecl _mm_roundscale_sh(__m128h, __m128h, int);
extern __m128h __cdecl _mm_mask_roundscale_sh(__m128h, __mmask8, __m128h, __m128h, int);
extern __m128h __cdecl _mm_maskz_roundscale_sh(__mmask8, __m128h, __m128h, int);
extern __m128h __cdecl _mm_roundscale_round_sh(__m128h, __m128h, int, const int);
extern __m128h __cdecl _mm_mask_roundscale_round_sh(__m128h, __mmask8, __m128h, __m128h, int, const int);
extern __m128h __cdecl _mm_maskz_roundscale_round_sh(__mmask8, __m128h, __m128h, int, const int);

// VRSQRTPH
extern __m128h __cdecl _mm_rsqrt_ph(__m128h);
extern __m128h __cdecl _mm_mask_rsqrt_ph(__m128h, __mmask8, __m128h);
extern __m128h __cdecl _mm_maskz_rsqrt_ph(__mmask8, __m128h);
extern __m256h __cdecl _mm256_rsqrt_ph(__m256h);
extern __m256h __cdecl _mm256_mask_rsqrt_ph(__m256h, __mmask16, __m256h);
extern __m256h __cdecl _mm256_maskz_rsqrt_ph(__mmask16, __m256h);
extern __m512h __cdecl _mm512_rsqrt_ph(__m512h);
extern __m512h __cdecl _mm512_mask_rsqrt_ph(__m512h, __mmask32, __m512h);
extern __m512h __cdecl _mm512_maskz_rsqrt_ph(__mmask32, __m512h);

// VRSQRTSH
extern __m128h __cdecl _mm_rsqrt_sh(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_rsqrt_sh(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_rsqrt_sh(__mmask8, __m128h, __m128h);

// VSCALEFPH
extern __m128h __cdecl _mm_scalef_ph(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_scalef_ph(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_scalef_ph(__mmask8, __m128h, __m128h);
extern __m256h __cdecl _mm256_scalef_ph(__m256h, __m256h);
extern __m256h __cdecl _mm256_mask_scalef_ph(__m256h, __mmask16, __m256h, __m256h);
extern __m256h __cdecl _mm256_maskz_scalef_ph(__mmask16, __m256h, __m256h);
extern __m512h __cdecl _mm512_scalef_ph(__m512h, __m512h);
extern __m512h __cdecl _mm512_mask_scalef_ph(__m512h, __mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_maskz_scalef_ph(__mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_scalef_round_ph(__m512h, __m512h, const int);
extern __m512h __cdecl _mm512_mask_scalef_round_ph(__m512h, __mmask32, __m512h, __m512h, const int);
extern __m512h __cdecl _mm512_maskz_scalef_round_ph(__mmask32, __m512h, __m512h, const int);

// VSCALEFSH
extern __m128h __cdecl _mm_scalef_sh(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_scalef_sh(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_scalef_sh(__mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_scalef_round_sh(__m128h, __m128h, const int);
extern __m128h __cdecl _mm_mask_scalef_round_sh(__m128h, __mmask8, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_maskz_scalef_round_sh(__mmask8, __m128h, __m128h, const int);

// VSQRTPH
extern __m128h __cdecl _mm_sqrt_ph(__m128h);
extern __m128h __cdecl _mm_mask_sqrt_ph(__m128h, __mmask8, __m128h);
extern __m128h __cdecl _mm_maskz_sqrt_ph(__mmask8, __m128h);
extern __m256h __cdecl _mm256_sqrt_ph(__m256h);
extern __m256h __cdecl _mm256_mask_sqrt_ph(__m256h, __mmask16, __m256h);
extern __m256h __cdecl _mm256_maskz_sqrt_ph(__mmask16, __m256h);
extern __m512h __cdecl _mm512_sqrt_ph(__m512h);
extern __m512h __cdecl _mm512_mask_sqrt_ph(__m512h, __mmask32, __m512h);
extern __m512h __cdecl _mm512_maskz_sqrt_ph(__mmask32, __m512h);
extern __m512h __cdecl _mm512_sqrt_round_ph(__m512h, const int);
extern __m512h __cdecl _mm512_mask_sqrt_round_ph(__m512h, __mmask32, __m512h, const int);
extern __m512h __cdecl _mm512_maskz_sqrt_round_ph(__mmask32, __m512h, const int);

// VSQRTSH
extern __m128h __cdecl _mm_sqrt_sh(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_sqrt_sh(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_sqrt_sh(__mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_sqrt_round_sh(__m128h, __m128h, const int);
extern __m128h __cdecl _mm_mask_sqrt_round_sh(__m128h, __mmask8, __m128h, __m128h, const int);
extern __m128h __cdecl _mm_maskz_sqrt_round_sh(__mmask8, __m128h, __m128h, const int);

// VSUBPH
extern __m128h __cdecl _mm_sub_ph(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_sub_ph(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_sub_ph(__mmask8, __m128h, __m128h);
extern __m256h __cdecl _mm256_sub_ph(__m256h, __m256h);
extern __m256h __cdecl _mm256_mask_sub_ph(__m256h, __mmask16, __m256h, __m256h);
extern __m256h __cdecl _mm256_maskz_sub_ph(__mmask16, __m256h, __m256h);
extern __m512h __cdecl _mm512_sub_ph(__m512h, __m512h);
extern __m512h __cdecl _mm512_mask_sub_ph(__m512h, __mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_maskz_sub_ph(__mmask32, __m512h, __m512h);
extern __m512h __cdecl _mm512_sub_round_ph(__m512h, __m512h, int);
extern __m512h __cdecl _mm512_mask_sub_round_ph(__m512h, __mmask32, __m512h, __m512h, int);
extern __m512h __cdecl _mm512_maskz_sub_round_ph(__mmask32, __m512h, __m512h, int);

// VSUBSH
extern __m128h __cdecl _mm_sub_sh(__m128h, __m128h);
extern __m128h __cdecl _mm_mask_sub_sh(__m128h, __mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_maskz_sub_sh(__mmask8, __m128h, __m128h);
extern __m128h __cdecl _mm_sub_round_sh(__m128h, __m128h, int);
extern __m128h __cdecl _mm_mask_sub_round_sh(__m128h, __mmask8, __m128h, __m128h, int);
extern __m128h __cdecl _mm_maskz_sub_round_sh(__mmask8, __m128h, __m128h, int);

extern __m128h __cdecl _mm_mask_blend_ph (__mmask8, __m128h, __m128h);
extern __m256h __cdecl _mm256_mask_blend_ph (__mmask16, __m256h, __m256h);
extern __m512h __cdecl _mm512_mask_blend_ph (__mmask32, __m512h, __m512h);

// FP16 type conversion macros











































}
#line 6162 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\zmmintrin.h"


#line 6165 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\zmmintrin.h"
#pragma external_header(pop)
#line 2456 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"

#line 2458 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"
#line 2459 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"
#line 2460 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\immintrin.h"
#pragma external_header(pop)
#line 33 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"
        #pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\ammintrin.h"
/****
*  Copyright (C) 2007-2020 Advanced Micro Devices Inc.  All rights reserved.
*
*  Boost Software License - Version 1.0 - August 17th, 2003
*
*  Permission is hereby granted, free of charge, to any person or organization
*  obtaining a copy of the software and accompanying documentation covered by
*  this license (the "Software") to use, reproduce, display, distribute,
*  execute, and transmit the Software, and to prepare derivative works of the
*  Software, and to permit third-parties to whom the Software is furnished to
*  do so, all subject to the following:
*
*  The copyright notices in the Software and this entire statement, including
*  the above license grant, this restriction and the following disclaimer,
*  must be included in all copies of the Software, in whole or in part, and
*  all derivative works of the Software, unless such copies or derivative
*  works are solely in the form of machine-executable object code generated by
*  a source language processor.
*
*  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
*  FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
*  SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
*  FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
*  DEALINGS IN THE SOFTWARE.
*
*  ammintrin.h - Definitions for AMD-specific intrinsics
*
****/

#pragma once



#line 37 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\ammintrin.h"







#line 45 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\ammintrin.h"



#line 49 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\ammintrin.h"


extern "C" { /* Intrinsics use C name-mangling. */
#line 53 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\ammintrin.h"

/*
 * Vector integer comparison control macros
 */










/*
 * MACRO functions for vector integer comparisons
 */









































































/* SSE5 intrinsics */

/* Float/double multiply-accumulate */
__m128 _mm_macc_ps(__m128, __m128, __m128);
__m128d _mm_macc_pd(__m128d, __m128d, __m128d);
__m128 _mm_macc_ss(__m128, __m128, __m128);
__m128d _mm_macc_sd(__m128d, __m128d, __m128d);
__m128 _mm_maddsub_ps(__m128, __m128, __m128);
__m128d _mm_maddsub_pd(__m128d, __m128d, __m128d);
__m128 _mm_msubadd_ps(__m128, __m128, __m128);
__m128d _mm_msubadd_pd(__m128d, __m128d, __m128d);
__m128 _mm_msub_ps(__m128, __m128, __m128);
__m128d _mm_msub_pd(__m128d, __m128d, __m128d);
__m128 _mm_msub_ss(__m128, __m128, __m128);
__m128d _mm_msub_sd(__m128d, __m128d, __m128d);
__m128 _mm_nmacc_ps(__m128, __m128, __m128);
__m128d _mm_nmacc_pd(__m128d, __m128d, __m128d);
__m128 _mm_nmacc_ss(__m128, __m128, __m128);
__m128d _mm_nmacc_sd(__m128d, __m128d, __m128d);
__m128 _mm_nmsub_ps(__m128, __m128, __m128);
__m128d _mm_nmsub_pd(__m128d, __m128d, __m128d);
__m128 _mm_nmsub_ss(__m128, __m128, __m128);
__m128d _mm_nmsub_sd(__m128d, __m128d, __m128d);

/* Integer multiply-accumulate */
__m128i _mm_maccs_epi16(__m128i, __m128i, __m128i);
__m128i _mm_macc_epi16(__m128i, __m128i, __m128i);
__m128i _mm_maccsd_epi16(__m128i, __m128i, __m128i);
__m128i _mm_maccd_epi16(__m128i, __m128i, __m128i);
__m128i _mm_maccs_epi32(__m128i, __m128i, __m128i);
__m128i _mm_macc_epi32(__m128i, __m128i, __m128i);
__m128i _mm_maccslo_epi32(__m128i, __m128i, __m128i);
__m128i _mm_macclo_epi32(__m128i, __m128i, __m128i);
__m128i _mm_maccshi_epi32(__m128i, __m128i, __m128i);
__m128i _mm_macchi_epi32(__m128i, __m128i, __m128i);
__m128i _mm_maddsd_epi16(__m128i, __m128i, __m128i);
__m128i _mm_maddd_epi16(__m128i, __m128i, __m128i);

/* Horizontal add/subtract */
__m128i _mm_haddw_epi8(__m128i);
__m128i _mm_haddd_epi8(__m128i);
__m128i _mm_haddq_epi8(__m128i);
__m128i _mm_haddd_epi16(__m128i);
__m128i _mm_haddq_epi16(__m128i);
__m128i _mm_haddq_epi32(__m128i);
__m128i _mm_haddw_epu8(__m128i);
__m128i _mm_haddd_epu8(__m128i);
__m128i _mm_haddq_epu8(__m128i);
__m128i _mm_haddd_epu16(__m128i);
__m128i _mm_haddq_epu16(__m128i);
__m128i _mm_haddq_epu32(__m128i);
__m128i _mm_hsubw_epi8(__m128i);
__m128i _mm_hsubd_epi16(__m128i);
__m128i _mm_hsubq_epi32(__m128i);

/* Vector conditional moves */
__m128i _mm_cmov_si128(__m128i, __m128i, __m128i);
__m128i _mm_perm_epi8(__m128i, __m128i, __m128i);

/* Vector shifts and rotates */
__m128i _mm_rot_epi8(__m128i, __m128i);
__m128i _mm_rot_epi16(__m128i, __m128i);
__m128i _mm_rot_epi32(__m128i, __m128i);
__m128i _mm_rot_epi64(__m128i, __m128i);
__m128i _mm_roti_epi8(__m128i, int);
__m128i _mm_roti_epi16(__m128i, int);
__m128i _mm_roti_epi32(__m128i, int);
__m128i _mm_roti_epi64(__m128i, int);
__m128i _mm_shl_epi8(__m128i, __m128i);
__m128i _mm_shl_epi16(__m128i, __m128i);
__m128i _mm_shl_epi32(__m128i, __m128i);
__m128i _mm_shl_epi64(__m128i, __m128i);
__m128i _mm_sha_epi8(__m128i, __m128i);
__m128i _mm_sha_epi16(__m128i, __m128i);
__m128i _mm_sha_epi32(__m128i, __m128i);
__m128i _mm_sha_epi64(__m128i, __m128i);

/* Vector integer comparisons */

__m128i _mm_com_epu8(__m128i, __m128i, int);
__m128i _mm_com_epu16(__m128i, __m128i, int);
__m128i _mm_com_epu32(__m128i, __m128i, int);
__m128i _mm_com_epu64(__m128i, __m128i, int);
__m128i _mm_com_epi8(__m128i, __m128i, int);
__m128i _mm_com_epi16(__m128i, __m128i, int);
__m128i _mm_com_epi32(__m128i, __m128i, int);
__m128i _mm_com_epi64(__m128i, __m128i, int);

/* Precision control */

__m128 _mm_frcz_ps(__m128);
__m128d _mm_frcz_pd(__m128d);
__m128 _mm_frcz_ss(__m128, __m128);
__m128d _mm_frcz_sd(__m128d, __m128d);

/* Control values for permute2 intrinsics */

/* Note that using the constant 1 would have the same effect as 0 */



/* Permutation */
__m128 _mm_permute2_ps(__m128, __m128, __m128i, int);
__m128d _mm_permute2_pd(__m128d, __m128d, __m128i, int);


/* YMM versions */
__m256 _mm256_macc_ps(__m256, __m256, __m256);
__m256d _mm256_macc_pd(__m256d, __m256d, __m256d);
__m256 _mm256_maddsub_ps(__m256, __m256, __m256);
__m256d _mm256_maddsub_pd(__m256d, __m256d, __m256d);
__m256 _mm256_msubadd_ps(__m256, __m256, __m256);
__m256d _mm256_msubadd_pd(__m256d, __m256d, __m256d);
__m256 _mm256_msub_ps(__m256, __m256, __m256);
__m256d _mm256_msub_pd(__m256d, __m256d, __m256d);
__m256 _mm256_nmacc_ps(__m256, __m256, __m256);
__m256d _mm256_nmacc_pd(__m256d, __m256d, __m256d);
__m256 _mm256_nmsub_ps(__m256, __m256, __m256);
__m256d _mm256_nmsub_pd(__m256d, __m256d, __m256d);
__m256i _mm256_cmov_si256(__m256i, __m256i, __m256i);
__m256 _mm256_frcz_ps(__m256);
__m256d _mm256_frcz_pd(__m256d);
__m256 _mm256_permute2_ps(__m256, __m256, __m256i, int);
__m256d _mm256_permute2_pd(__m256d, __m256d, __m256i, int);

/* LWP intrinsics */
void __llwpcb(void *);
void *__slwpcb(void);
void __lwpval32(unsigned int, unsigned int, unsigned int);
unsigned char __lwpins32(unsigned int, unsigned int, unsigned int);

void __lwpval64(unsigned __int64, unsigned int, unsigned int);
unsigned char __lwpins64(unsigned __int64, unsigned int, unsigned int);
#line 277 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\ammintrin.h"

/*BMI intrinsics */
unsigned int _bextr_u32(unsigned int, unsigned int, unsigned int);
unsigned int _andn_u32(unsigned int, unsigned int);
unsigned int _tzcnt_u32(unsigned int);
unsigned int _lzcnt_u32(unsigned int);
unsigned int _blsr_u32(unsigned int);
unsigned int _blsmsk_u32(unsigned int);
unsigned int _blsi_u32(unsigned int);

unsigned __int64 _bextr_u64(unsigned __int64, unsigned int, unsigned int);
unsigned __int64 _andn_u64(unsigned __int64, unsigned __int64);
unsigned __int64 _tzcnt_u64(unsigned __int64);
unsigned __int64 _lzcnt_u64(unsigned __int64);
unsigned __int64 _blsr_u64(unsigned __int64);
unsigned __int64 _blsmsk_u64(unsigned __int64);
unsigned __int64 _blsi_u64(unsigned __int64);
#line 295 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\ammintrin.h"

/* TBM intrinsics */
unsigned int _bextri_u32(unsigned int, unsigned int);
unsigned int _blcfill_u32(unsigned int);
unsigned int _blsfill_u32(unsigned int);
unsigned int _blcs_u32(unsigned int);
unsigned int _tzmsk_u32(unsigned int);
unsigned int _blcic_u32(unsigned int);
unsigned int _blsic_u32(unsigned int);
unsigned int _t1mskc_u32(unsigned int);
unsigned int _blcmsk_u32(unsigned int);
unsigned int _blci_u32(unsigned int);

unsigned __int64 _bextri_u64(unsigned __int64, unsigned int);
unsigned __int64 _blcfill_u64(unsigned __int64);
unsigned __int64 _blsfill_u64(unsigned __int64);
unsigned __int64 _blcs_u64(unsigned __int64);
unsigned __int64 _tzmsk_u64(unsigned __int64);
unsigned __int64 _blcic_u64(unsigned __int64);
unsigned __int64 _blsic_u64(unsigned __int64);
unsigned __int64 _t1mskc_u64(unsigned __int64);
unsigned __int64 _blcmsk_u64(unsigned __int64);
unsigned __int64 _blci_u64(unsigned __int64);
#line 319 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\ammintrin.h"

void _mm_monitorx(void const *, unsigned int, unsigned int);
void _mm_mwaitx(unsigned int, unsigned int, unsigned int);

void _mm_clzero(void const *);


// Secure Nested Paging
typedef struct rmp_seg {
    unsigned __int64 rmp_gpa;
    __int8           rmp_entry;
    __int8           rmp_pageSize;
    __int8           rmp_pageMark;
    __int8           rmp_reserved;
    __int32          rmp_ASID;
} rmp_seg;

unsigned int __rmpupdate(unsigned __int64, rmp_seg *, int);
unsigned int __pvalidate(unsigned __int64, int, int, int *);
unsigned int __psmash(unsigned __int64);
unsigned int __rmpadjust(unsigned __int64, int, int);
#line 341 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\ammintrin.h"


//TLB extension
void __svm_invlpgb(void*, int);
void __svm_tlbsync(void);



}; /* End "C" */
#line 351 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\ammintrin.h"

#line 353 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\ammintrin.h"
#line 354 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\ammintrin.h"
#line 355 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\ammintrin.h"
#pragma external_header(pop)
#line 34 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"
    #line 35 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"

    

#line 39 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"

    


#line 44 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"

    


#line 49 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"
#line 50 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"


extern "C" {
#line 54 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"


void * _AddressOfReturnAddress(void);










int _cvt_dtoi_fast (double);
int _cvt_dtoi_sat (double);
int _cvt_dtoi_sent (double);
long long _cvt_dtoll_fast (double);
long long _cvt_dtoll_sat (double);
long long _cvt_dtoll_sent (double);
unsigned _cvt_dtoui_fast (double);
unsigned _cvt_dtoui_sat (double);
unsigned _cvt_dtoui_sent (double);
unsigned long long _cvt_dtoull_fast (double);
unsigned long long _cvt_dtoull_sat (double);
unsigned long long _cvt_dtoull_sent (double);
int _cvt_ftoi_fast (float);
int _cvt_ftoi_sat (float);
int _cvt_ftoi_sent (float);
long long _cvt_ftoll_fast (float);
long long _cvt_ftoll_sat (float);
long long _cvt_ftoll_sent (float);
unsigned _cvt_ftoui_fast (float);
unsigned _cvt_ftoui_sat (float);
unsigned _cvt_ftoui_sent (float);
unsigned long long _cvt_ftoull_fast (float);
unsigned long long _cvt_ftoull_sat (float);
unsigned long long _cvt_ftoull_sent (float);




long _interlockedadd(long volatile * _Addend, long _Value);

__int64 _interlockedadd64(__int64 volatile * _Addend, __int64 _Value);







short _InterlockedAnd16_np(short volatile * _Value, short _Mask);
__int64 _InterlockedAnd64_np(__int64 volatile * _Value, __int64 _Mask);
char _InterlockedAnd8_np(char volatile * _Value, char _Mask);
long _InterlockedAnd_np(long volatile * _Value, long _Mask);
unsigned char _InterlockedCompareExchange128_np(__int64 volatile * _Destination, __int64 _ExchangeHigh, __int64 _ExchangeLow, __int64 * _ComparandResult);
short _InterlockedCompareExchange16_np(short volatile * _Destination, short _Exchange, short _Comparand);
__int64 _InterlockedCompareExchange64_np(__int64 volatile * _Destination, __int64 _Exchange, __int64 _Comparand);
void * _InterlockedCompareExchangePointer(void * volatile * _Destination, void * _Exchange, void * _Comparand);


void * _InterlockedCompareExchangePointer_np(void * volatile * _Destination, void * _Exchange, void * _Comparand);

long _InterlockedCompareExchange_np(long volatile * _Destination, long _Exchange, long _Comparand);









void * _InterlockedExchangePointer(void * volatile * _Target, void * _Value);











short _InterlockedOr16_np(short volatile * _Value, short _Mask);
__int64 _InterlockedOr64_np(__int64 volatile * _Value, __int64 _Mask);
char _InterlockedOr8_np(char volatile * _Value, char _Mask);
long _InterlockedOr_np(long volatile * _Value, long _Mask);
short _InterlockedXor16_np(short volatile * _Value, short _Mask);
__int64 _InterlockedXor64_np(__int64 volatile * _Value, __int64 _Mask);
char _InterlockedXor8_np(char volatile * _Value, char _Mask);
long _InterlockedXor_np(long volatile * _Value, long _Mask);








void _ReadBarrier(void);






void * _ReturnAddress(void);

void _WriteBarrier(void);









void __addgsbyte(unsigned long, unsigned char);
void __addgsdword(unsigned long, unsigned long);
void __addgsqword(unsigned long, unsigned __int64);
void __addgsword(unsigned long, unsigned short);




void __clts(void);
void __code_seg(const char *);
void __cpuid(int[4], int);
void __cpuidex(int[4], int, int);
void __cdecl __debugbreak(void);

__int64 __emul(int, int);
unsigned __int64 __emulu(unsigned int, unsigned int);
__declspec(noreturn) void __fastfail(unsigned int);
void __faststorefence(void);
unsigned int __getcallerseflags(void);
void __halt(void);


unsigned char __inbyte(unsigned short);
void __inbytestring(unsigned short, unsigned char *, unsigned long);



void __incgsbyte(unsigned long);
void __incgsdword(unsigned long);
void __incgsqword(unsigned long);
void __incgsword(unsigned long);




unsigned long __indword(unsigned short);
void __indwordstring(unsigned short, unsigned long *, unsigned long);
void __int2c(void);
void __invlpg(void *);
unsigned short __inword(unsigned short);
void __inwordstring(unsigned short, unsigned short *, unsigned long);
void __lidt(void *);
unsigned __int64 __ll_lshift(unsigned __int64, int);
__int64 __ll_rshift(__int64, int);
void __movsb(unsigned char *, unsigned char const *, size_t);
void __movsd(unsigned long *, unsigned long const *, size_t);
void __movsq(unsigned long long *, unsigned long long const *, size_t);
void __movsw(unsigned short *, unsigned short const *, size_t);
__int64 __mulh(__int64, __int64);
void __nop(void);
void __nvreg_restore_fence(void);
void __nvreg_save_fence(void);
void __outbyte(unsigned short, unsigned char);
void __outbytestring(unsigned short, unsigned char *, unsigned long);
void __outdword(unsigned short, unsigned long);
void __outdwordstring(unsigned short, unsigned long *, unsigned long);
void __outword(unsigned short, unsigned short);
void __outwordstring(unsigned short, unsigned short *, unsigned long);




unsigned __int64 __rdtsc(void);
unsigned __int64 __rdtscp(unsigned int *);
unsigned __int64 __readcr0(void);

unsigned __int64 __readcr2(void);

unsigned __int64 __readcr3(void);

unsigned __int64 __readcr4(void);

unsigned __int64 __readcr8(void);

unsigned __int64 __readdr(unsigned int);

unsigned __int64 __readeflags(void);





unsigned char __readgsbyte(unsigned long);
unsigned long __readgsdword(unsigned long);
unsigned __int64 __readgsqword(unsigned long);
unsigned short __readgsword(unsigned long);
unsigned __int64 __readmsr(unsigned long);
unsigned __int64 __readpmc(unsigned long);




unsigned long __segmentlimit(unsigned long);

unsigned __int64 __shiftleft128(unsigned __int64 _LowPart, unsigned __int64 _HighPart, unsigned char _Shift);
void __sidt(void *);

void __stosb(unsigned char *, unsigned char, size_t);
void __stosd(unsigned long *, unsigned long, size_t);
void __stosq(unsigned __int64 *, unsigned __int64, size_t);
void __stosw(unsigned short *, unsigned short, size_t);
void __svm_clgi(void);
void __svm_invlpga(void *, int);
void __svm_skinit(int);
void __svm_stgi(void);
void __svm_vmload(size_t);
void __svm_vmrun(size_t);
void __svm_vmsave(size_t);





void __ud2(void);
unsigned __int64 __ull_rshift(unsigned __int64, int);
unsigned __int64 __umulh(unsigned __int64, unsigned __int64);
void __vmx_off(void);
unsigned char __vmx_on(unsigned __int64 *);
unsigned char __vmx_vmclear(unsigned __int64 *);
unsigned char __vmx_vmlaunch(void);
unsigned char __vmx_vmptrld(unsigned __int64 *);
void __vmx_vmptrst(unsigned __int64 *);
unsigned char __vmx_vmread(size_t, size_t *);
unsigned char __vmx_vmresume(void);
unsigned char __vmx_vmwrite(size_t, size_t);
void __wbinvd(void);


void __writecr0(unsigned __int64);

void __writecr2(unsigned __int64);

void __writecr3(unsigned __int64);

void __writecr4(unsigned __int64);

void __writecr8(unsigned __int64);

void __writedr(unsigned int, unsigned __int64);

void __writeeflags(unsigned __int64);





void __writegsbyte(unsigned long, unsigned char);
void __writegsdword(unsigned long, unsigned long);
void __writegsqword(unsigned long, unsigned __int64);
void __writegsword(unsigned long, unsigned short);
void __writemsr(unsigned long, unsigned __int64);




unsigned char _bittest64(__int64 const *, __int64);
unsigned char _bittestandcomplement(long *, long);
unsigned char _bittestandcomplement64(__int64 *, __int64);
unsigned char _bittestandreset(long *, long);
unsigned char _bittestandreset64(__int64 *, __int64);
unsigned char _bittestandset(long *, long);
unsigned char _bittestandset64(__int64 *, __int64);
[[nodiscard]]   unsigned __int64 __cdecl _byteswap_uint64(  unsigned __int64);
[[nodiscard]]   unsigned long __cdecl _byteswap_ulong(  unsigned long);
[[nodiscard]]   unsigned short __cdecl _byteswap_ushort(  unsigned short);
void __cdecl _disable(void);
void __cdecl _enable(void);
unsigned char _interlockedbittestandreset(long volatile *, long);
unsigned char _interlockedbittestandreset64(__int64 volatile *, __int64);






unsigned char _interlockedbittestandset64(__int64 volatile *, __int64);













[[nodiscard]]   unsigned long __cdecl _lrotl(  unsigned long,   int);
[[nodiscard]]   unsigned long __cdecl _lrotr(  unsigned long,   int);




























































void _m_prefetch(void *);
void _m_prefetchw(volatile const void *);



































__m128i _mm_abs_epi16(__m128i);
__m128i _mm_abs_epi32(__m128i);
__m128i _mm_abs_epi8(__m128i);



__m128i _mm_add_epi16(__m128i, __m128i);
__m128i _mm_add_epi32(__m128i, __m128i);
__m128i _mm_add_epi64(__m128i, __m128i);
__m128i _mm_add_epi8(__m128i, __m128i);
__m128d _mm_add_pd(__m128d, __m128d);
__m128 _mm_add_ps(__m128, __m128);
__m128d _mm_add_sd(__m128d, __m128d);

__m128 _mm_add_ss(__m128, __m128);
__m128i _mm_adds_epi16(__m128i, __m128i);
__m128i _mm_adds_epi8(__m128i, __m128i);
__m128i _mm_adds_epu16(__m128i, __m128i);
__m128i _mm_adds_epu8(__m128i, __m128i);
__m128d _mm_addsub_pd(__m128d, __m128d);
__m128 _mm_addsub_ps(__m128, __m128);
__m128i _mm_alignr_epi8(__m128i, __m128i, int);

__m128d _mm_and_pd(__m128d, __m128d);
__m128 _mm_and_ps(__m128, __m128);
__m128i _mm_and_si128(__m128i, __m128i);
__m128d _mm_andnot_pd(__m128d, __m128d);
__m128 _mm_andnot_ps(__m128, __m128);
__m128i _mm_andnot_si128(__m128i, __m128i);
__m128i _mm_avg_epu16(__m128i, __m128i);
__m128i _mm_avg_epu8(__m128i, __m128i);
__m128i _mm_blend_epi16(__m128i, __m128i, int);
__m128d _mm_blend_pd(__m128d, __m128d, int);
__m128 _mm_blend_ps(__m128, __m128, int);
__m128i _mm_blendv_epi8(__m128i, __m128i, __m128i);
__m128d _mm_blendv_pd(__m128d, __m128d, __m128d);
__m128 _mm_blendv_ps(__m128, __m128, __m128);
void _mm_clflush(void const *);
void _mm_clflushopt(void const *);
void _mm_clwb(void const *);
void _mm_clzero(void const *);
__m128i _mm_cmpeq_epi16(__m128i, __m128i);
__m128i _mm_cmpeq_epi32(__m128i, __m128i);
__m128i _mm_cmpeq_epi64(__m128i, __m128i);
__m128i _mm_cmpeq_epi8(__m128i, __m128i);
__m128d _mm_cmpeq_pd(__m128d, __m128d);
__m128 _mm_cmpeq_ps(__m128, __m128);
__m128d _mm_cmpeq_sd(__m128d, __m128d);
__m128 _mm_cmpeq_ss(__m128, __m128);
int _mm_cmpestra(__m128i, int, __m128i, int, int);
int _mm_cmpestrc(__m128i, int, __m128i, int, int);
int _mm_cmpestri(__m128i, int, __m128i, int, int);
__m128i _mm_cmpestrm(__m128i, int, __m128i, int, int);
int _mm_cmpestro(__m128i, int, __m128i, int, int);
int _mm_cmpestrs(__m128i, int, __m128i, int, int);
int _mm_cmpestrz(__m128i, int, __m128i, int, int);
__m128d _mm_cmpge_pd(__m128d, __m128d);
__m128 _mm_cmpge_ps(__m128, __m128);
__m128d _mm_cmpge_sd(__m128d, __m128d);
__m128 _mm_cmpge_ss(__m128, __m128);
__m128i _mm_cmpgt_epi16(__m128i, __m128i);
__m128i _mm_cmpgt_epi32(__m128i, __m128i);
__m128i _mm_cmpgt_epi64(__m128i, __m128i);
__m128i _mm_cmpgt_epi8(__m128i, __m128i);
__m128d _mm_cmpgt_pd(__m128d, __m128d);
__m128 _mm_cmpgt_ps(__m128, __m128);
__m128d _mm_cmpgt_sd(__m128d, __m128d);
__m128 _mm_cmpgt_ss(__m128, __m128);
int _mm_cmpistra(__m128i, __m128i, int);
int _mm_cmpistrc(__m128i, __m128i, int);
int _mm_cmpistri(__m128i, __m128i, int);
__m128i _mm_cmpistrm(__m128i, __m128i, int);
int _mm_cmpistro(__m128i, __m128i, int);
int _mm_cmpistrs(__m128i, __m128i, int);
int _mm_cmpistrz(__m128i, __m128i, int);
__m128d _mm_cmple_pd(__m128d, __m128d);
__m128 _mm_cmple_ps(__m128, __m128);
__m128d _mm_cmple_sd(__m128d, __m128d);
__m128 _mm_cmple_ss(__m128, __m128);
__m128i _mm_cmplt_epi16(__m128i, __m128i);
__m128i _mm_cmplt_epi32(__m128i, __m128i);
__m128i _mm_cmplt_epi8(__m128i, __m128i);
__m128d _mm_cmplt_pd(__m128d, __m128d);
__m128 _mm_cmplt_ps(__m128, __m128);
__m128d _mm_cmplt_sd(__m128d, __m128d);
__m128 _mm_cmplt_ss(__m128, __m128);
__m128d _mm_cmpneq_pd(__m128d, __m128d);
__m128 _mm_cmpneq_ps(__m128, __m128);
__m128d _mm_cmpneq_sd(__m128d, __m128d);
__m128 _mm_cmpneq_ss(__m128, __m128);
__m128d _mm_cmpnge_pd(__m128d, __m128d);
__m128 _mm_cmpnge_ps(__m128, __m128);
__m128d _mm_cmpnge_sd(__m128d, __m128d);
__m128 _mm_cmpnge_ss(__m128, __m128);
__m128d _mm_cmpngt_pd(__m128d, __m128d);
__m128 _mm_cmpngt_ps(__m128, __m128);
__m128d _mm_cmpngt_sd(__m128d, __m128d);
__m128 _mm_cmpngt_ss(__m128, __m128);
__m128d _mm_cmpnle_pd(__m128d, __m128d);
__m128 _mm_cmpnle_ps(__m128, __m128);
__m128d _mm_cmpnle_sd(__m128d, __m128d);
__m128 _mm_cmpnle_ss(__m128, __m128);
__m128d _mm_cmpnlt_pd(__m128d, __m128d);
__m128 _mm_cmpnlt_ps(__m128, __m128);
__m128d _mm_cmpnlt_sd(__m128d, __m128d);
__m128 _mm_cmpnlt_ss(__m128, __m128);
__m128d _mm_cmpord_pd(__m128d, __m128d);
__m128 _mm_cmpord_ps(__m128, __m128);
__m128d _mm_cmpord_sd(__m128d, __m128d);
__m128 _mm_cmpord_ss(__m128, __m128);
__m128d _mm_cmpunord_pd(__m128d, __m128d);
__m128 _mm_cmpunord_ps(__m128, __m128);
__m128d _mm_cmpunord_sd(__m128d, __m128d);
__m128 _mm_cmpunord_ss(__m128, __m128);
int _mm_comieq_sd(__m128d, __m128d);
int _mm_comieq_ss(__m128, __m128);
int _mm_comige_sd(__m128d, __m128d);
int _mm_comige_ss(__m128, __m128);
int _mm_comigt_sd(__m128d, __m128d);
int _mm_comigt_ss(__m128, __m128);
int _mm_comile_sd(__m128d, __m128d);
int _mm_comile_ss(__m128, __m128);
int _mm_comilt_sd(__m128d, __m128d);
int _mm_comilt_ss(__m128, __m128);
int _mm_comineq_sd(__m128d, __m128d);
int _mm_comineq_ss(__m128, __m128);
unsigned int _mm_crc32_u16(unsigned int, unsigned short);
unsigned int _mm_crc32_u32(unsigned int, unsigned int);
unsigned __int64 _mm_crc32_u64(unsigned __int64, unsigned __int64);
unsigned int _mm_crc32_u8(unsigned int, unsigned char);


__m128 _mm_cvt_si2ss(__m128, int);
int _mm_cvt_ss2si(__m128);
__m128i _mm_cvtepi16_epi32(__m128i);
__m128i _mm_cvtepi16_epi64(__m128i);
__m128i _mm_cvtepi32_epi64(__m128i);
__m128d _mm_cvtepi32_pd(__m128i);
__m128 _mm_cvtepi32_ps(__m128i);
__m128i _mm_cvtepi8_epi16(__m128i);
__m128i _mm_cvtepi8_epi32(__m128i);
__m128i _mm_cvtepi8_epi64(__m128i);
__m128i _mm_cvtepu16_epi32(__m128i);
__m128i _mm_cvtepu16_epi64(__m128i);
__m128i _mm_cvtepu32_epi64(__m128i);
__m128i _mm_cvtepu8_epi16(__m128i);
__m128i _mm_cvtepu8_epi32(__m128i);
__m128i _mm_cvtepu8_epi64(__m128i);
__m128i _mm_cvtpd_epi32(__m128d);

__m128 _mm_cvtpd_ps(__m128d);

__m128i _mm_cvtps_epi32(__m128);
__m128d _mm_cvtps_pd(__m128);
int _mm_cvtsd_si32(__m128d);
__int64 _mm_cvtsd_si64(__m128d);
__int64 _mm_cvtsd_si64x(__m128d);
__m128 _mm_cvtsd_ss(__m128, __m128d);
int _mm_cvtsi128_si32(__m128i);
__int64 _mm_cvtsi128_si64(__m128i);
__int64 _mm_cvtsi128_si64x(__m128i);
__m128d _mm_cvtsi32_sd(__m128d, int);
__m128i _mm_cvtsi32_si128(int);
__m128d _mm_cvtsi64_sd(__m128d, __int64);
__m128i _mm_cvtsi64_si128(__int64);
__m128 _mm_cvtsi64_ss(__m128, __int64);
__m128d _mm_cvtsi64x_sd(__m128d, __int64);
__m128i _mm_cvtsi64x_si128(__int64);
__m128 _mm_cvtsi64x_ss(__m128, __int64);
__m128d _mm_cvtss_sd(__m128d, __m128);
__int64 _mm_cvtss_si64(__m128);
__int64 _mm_cvtss_si64x(__m128);

int _mm_cvtt_ss2si(__m128);
__m128i _mm_cvttpd_epi32(__m128d);

__m128i _mm_cvttps_epi32(__m128);
int _mm_cvttsd_si32(__m128d);
__int64 _mm_cvttsd_si64(__m128d);
__int64 _mm_cvttsd_si64x(__m128d);
__int64 _mm_cvttss_si64(__m128);
__int64 _mm_cvttss_si64x(__m128);
__m128d _mm_div_pd(__m128d, __m128d);
__m128 _mm_div_ps(__m128, __m128);
__m128d _mm_div_sd(__m128d, __m128d);
__m128 _mm_div_ss(__m128, __m128);
__m128d _mm_dp_pd(__m128d, __m128d, int);
__m128 _mm_dp_ps(__m128, __m128, int);
int _mm_extract_epi16(__m128i, int);
int _mm_extract_epi32(__m128i, int);
__int64 _mm_extract_epi64(__m128i, int);
int _mm_extract_epi8(__m128i, int);
int _mm_extract_ps(__m128, int);
__m128i _mm_extract_si64(__m128i, __m128i);
__m128i _mm_extracti_si64(__m128i, int, int);
unsigned int _mm_getcsr(void);
__m128i _mm_hadd_epi16(__m128i, __m128i);
__m128i _mm_hadd_epi32(__m128i, __m128i);
__m128d _mm_hadd_pd(__m128d, __m128d);


__m128 _mm_hadd_ps(__m128, __m128);
__m128i _mm_hadds_epi16(__m128i, __m128i);

__m128i _mm_hsub_epi16(__m128i, __m128i);
__m128i _mm_hsub_epi32(__m128i, __m128i);
__m128d _mm_hsub_pd(__m128d, __m128d);


__m128 _mm_hsub_ps(__m128, __m128);
__m128i _mm_hsubs_epi16(__m128i, __m128i);

__m128i _mm_insert_epi16(__m128i, int, int);
__m128i _mm_insert_epi32(__m128i, int, int);
__m128i _mm_insert_epi64(__m128i, __int64, int);
__m128i _mm_insert_epi8(__m128i, int, int);
__m128 _mm_insert_ps(__m128, __m128, int);
__m128i _mm_insert_si64(__m128i, __m128i);
__m128i _mm_inserti_si64(__m128i, __m128i, int, int);
__m128i _mm_lddqu_si128(__m128i const *);
void _mm_lfence(void);
__m128d _mm_load1_pd(double const *);
__m128d _mm_load_pd(double const *);
__m128 _mm_load_ps(float const *);
__m128 _mm_load_ps1(float const *);
__m128d _mm_load_sd(double const *);
__m128i _mm_load_si128(__m128i const *);
__m128 _mm_load_ss(float const *);
__m128d _mm_loaddup_pd(double const *);
__m128d _mm_loadh_pd(__m128d, double const *);
__m128 _mm_loadh_pi(__m128, __m64 const *);
__m128i _mm_loadl_epi64(__m128i const *);
__m128d _mm_loadl_pd(__m128d, double const *);
__m128 _mm_loadl_pi(__m128, __m64 const *);
__m128d _mm_loadr_pd(double const *);
__m128 _mm_loadr_ps(float const *);
__m128d _mm_loadu_pd(double const *);
__m128 _mm_loadu_ps(float const *);
__m128i _mm_loadu_si128(__m128i const *);
__m128i _mm_madd_epi16(__m128i, __m128i);
__m128i _mm_maddubs_epi16(__m128i, __m128i);

void _mm_maskmoveu_si128(__m128i, __m128i, char *);
__m128i _mm_max_epi16(__m128i, __m128i);
__m128i _mm_max_epi32(__m128i, __m128i);
__m128i _mm_max_epi8(__m128i, __m128i);
__m128i _mm_max_epu16(__m128i, __m128i);
__m128i _mm_max_epu32(__m128i, __m128i);
__m128i _mm_max_epu8(__m128i, __m128i);
__m128d _mm_max_pd(__m128d, __m128d);
__m128 _mm_max_ps(__m128, __m128);
__m128d _mm_max_sd(__m128d, __m128d);
__m128 _mm_max_ss(__m128, __m128);
void _mm_mfence(void);
__m128i _mm_min_epi16(__m128i, __m128i);
__m128i _mm_min_epi32(__m128i, __m128i);
__m128i _mm_min_epi8(__m128i, __m128i);
__m128i _mm_min_epu16(__m128i, __m128i);
__m128i _mm_min_epu32(__m128i, __m128i);
__m128i _mm_min_epu8(__m128i, __m128i);
__m128d _mm_min_pd(__m128d, __m128d);
__m128 _mm_min_ps(__m128, __m128);
__m128d _mm_min_sd(__m128d, __m128d);
__m128 _mm_min_ss(__m128, __m128);
__m128i _mm_minpos_epu16(__m128i);
void _mm_monitor(void const *, unsigned int, unsigned int);
__m128i _mm_move_epi64(__m128i);
__m128d _mm_move_sd(__m128d, __m128d);
__m128 _mm_move_ss(__m128, __m128);
__m128d _mm_movedup_pd(__m128d);
__m128 _mm_movehdup_ps(__m128);
__m128 _mm_movehl_ps(__m128, __m128);
__m128 _mm_moveldup_ps(__m128);
__m128 _mm_movelh_ps(__m128, __m128);
int _mm_movemask_epi8(__m128i);
int _mm_movemask_pd(__m128d);
int _mm_movemask_ps(__m128);


__m128i _mm_mpsadbw_epu8(__m128i, __m128i, int);
__m128i _mm_mul_epi32(__m128i, __m128i);
__m128i _mm_mul_epu32(__m128i, __m128i);
__m128d _mm_mul_pd(__m128d, __m128d);
__m128 _mm_mul_ps(__m128, __m128);
__m128d _mm_mul_sd(__m128d, __m128d);
__m128 _mm_mul_ss(__m128, __m128);

__m128i _mm_mulhi_epi16(__m128i, __m128i);
__m128i _mm_mulhi_epu16(__m128i, __m128i);
__m128i _mm_mulhrs_epi16(__m128i, __m128i);

__m128i _mm_mullo_epi16(__m128i, __m128i);
__m128i _mm_mullo_epi32(__m128i, __m128i);
void _mm_mwait(unsigned int, unsigned int);
__m128d _mm_or_pd(__m128d, __m128d);
__m128 _mm_or_ps(__m128, __m128);
__m128i _mm_or_si128(__m128i, __m128i);
__m128i _mm_packs_epi16(__m128i, __m128i);
__m128i _mm_packs_epi32(__m128i, __m128i);
__m128i _mm_packus_epi16(__m128i, __m128i);
__m128i _mm_packus_epi32(__m128i, __m128i);
int _mm_popcnt_u32(unsigned int);
__int64 _mm_popcnt_u64(unsigned __int64);
void _mm_prefetch(char const *, int);
__m128 _mm_rcp_ps(__m128);
__m128 _mm_rcp_ss(__m128);
__m128d _mm_round_pd(__m128d, int);
__m128 _mm_round_ps(__m128, int);
__m128d _mm_round_sd(__m128d, __m128d, int);
__m128 _mm_round_ss(__m128, __m128, int);
__m128 _mm_rsqrt_ps(__m128);
__m128 _mm_rsqrt_ss(__m128);
__m128i _mm_sad_epu8(__m128i, __m128i);
__m128i _mm_set1_epi16(short);
__m128i _mm_set1_epi32(int);

__m128i _mm_set1_epi64x(__int64);
__m128i _mm_set1_epi8(char);
__m128d _mm_set1_pd(double);



__m128i _mm_set_epi16(short, short, short, short, short, short, short, short);
__m128i _mm_set_epi32(int, int, int, int);

__m128i _mm_set_epi64x(__int64, __int64);
__m128i _mm_set_epi8(char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char);
__m128d _mm_set_pd(double, double);



__m128 _mm_set_ps(float, float, float, float);
__m128 _mm_set_ps1(float);
__m128d _mm_set_sd(double);
__m128 _mm_set_ss(float);
void _mm_setcsr(unsigned int);
__m128i _mm_setl_epi64(__m128i);
__m128i _mm_setr_epi16(short, short, short, short, short, short, short, short);
__m128i _mm_setr_epi32(int, int, int, int);

__m128i _mm_setr_epi64x(__int64, __int64);
__m128i _mm_setr_epi8(char, char, char, char, char, char, char, char, char, char, char, char, char, char, char, char);
__m128d _mm_setr_pd(double, double);



__m128 _mm_setr_ps(float, float, float, float);
__m128d _mm_setzero_pd(void);
__m128 _mm_setzero_ps(void);
__m128i _mm_setzero_si128(void);

void _mm_sfence(void);
__m128i _mm_shuffle_epi32(__m128i, int);
__m128i _mm_shuffle_epi8(__m128i, __m128i);
__m128d _mm_shuffle_pd(__m128d, __m128d, int);

__m128 _mm_shuffle_ps(__m128, __m128, unsigned int);
__m128i _mm_shufflehi_epi16(__m128i, int);
__m128i _mm_shufflelo_epi16(__m128i, int);
__m128i _mm_sign_epi16(__m128i, __m128i);
__m128i _mm_sign_epi32(__m128i, __m128i);
__m128i _mm_sign_epi8(__m128i, __m128i);



__m128i _mm_sll_epi16(__m128i, __m128i);
__m128i _mm_sll_epi32(__m128i, __m128i);
__m128i _mm_sll_epi64(__m128i, __m128i);
__m128i _mm_slli_epi16(__m128i, int);
__m128i _mm_slli_epi32(__m128i, int);
__m128i _mm_slli_epi64(__m128i, int);
__m128i _mm_slli_si128(__m128i, int);
__m128d _mm_sqrt_pd(__m128d);
__m128 _mm_sqrt_ps(__m128);
__m128d _mm_sqrt_sd(__m128d, __m128d);
__m128 _mm_sqrt_ss(__m128);
__m128i _mm_sra_epi16(__m128i, __m128i);
__m128i _mm_sra_epi32(__m128i, __m128i);
__m128i _mm_srai_epi16(__m128i, int);
__m128i _mm_srai_epi32(__m128i, int);
__m128i _mm_srl_epi16(__m128i, __m128i);
__m128i _mm_srl_epi32(__m128i, __m128i);
__m128i _mm_srl_epi64(__m128i, __m128i);
__m128i _mm_srli_epi16(__m128i, int);
__m128i _mm_srli_epi32(__m128i, int);
__m128i _mm_srli_epi64(__m128i, int);
__m128i _mm_srli_si128(__m128i, int);
void _mm_store1_pd(double *, __m128d);
void _mm_store_pd(double *, __m128d);
void _mm_store_ps(float *, __m128);
void _mm_store_ps1(float *, __m128);
void _mm_store_sd(double *, __m128d);
void _mm_store_si128(__m128i *, __m128i);
void _mm_store_ss(float *, __m128);
void _mm_storeh_pd(double *, __m128d);
void _mm_storeh_pi(__m64 *, __m128);
void _mm_storel_epi64(__m128i *, __m128i);
void _mm_storel_pd(double *, __m128d);
void _mm_storel_pi(__m64 *, __m128);
void _mm_storer_pd(double *, __m128d);
void _mm_storer_ps(float *, __m128);
void _mm_storeu_pd(double *, __m128d);
void _mm_storeu_ps(float *, __m128);
void _mm_storeu_si128(__m128i *, __m128i);


#line 869 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"
__m128i _mm_stream_load_si128(const __m128i *);
#line 871 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"
void _mm_stream_pd(double *, __m128d);

void _mm_stream_ps(float *, __m128);
void _mm_stream_sd(double *, __m128d);
void _mm_stream_si128(__m128i *, __m128i);
void _mm_stream_si32(int *, int);
void _mm_stream_si64x(__int64 *, __int64);
void _mm_stream_ss(float *, __m128);
__m128i _mm_sub_epi16(__m128i, __m128i);
__m128i _mm_sub_epi32(__m128i, __m128i);
__m128i _mm_sub_epi64(__m128i, __m128i);
__m128i _mm_sub_epi8(__m128i, __m128i);
__m128d _mm_sub_pd(__m128d, __m128d);
__m128 _mm_sub_ps(__m128, __m128);
__m128d _mm_sub_sd(__m128d, __m128d);

__m128 _mm_sub_ss(__m128, __m128);
__m128i _mm_subs_epi16(__m128i, __m128i);
__m128i _mm_subs_epi8(__m128i, __m128i);
__m128i _mm_subs_epu16(__m128i, __m128i);
__m128i _mm_subs_epu8(__m128i, __m128i);
int _mm_testc_si128(__m128i, __m128i);
int _mm_testnzc_si128(__m128i, __m128i);
int _mm_testz_si128(__m128i, __m128i);
int _mm_ucomieq_sd(__m128d, __m128d);
int _mm_ucomieq_ss(__m128, __m128);
int _mm_ucomige_sd(__m128d, __m128d);
int _mm_ucomige_ss(__m128, __m128);
int _mm_ucomigt_sd(__m128d, __m128d);
int _mm_ucomigt_ss(__m128, __m128);
int _mm_ucomile_sd(__m128d, __m128d);
int _mm_ucomile_ss(__m128, __m128);
int _mm_ucomilt_sd(__m128d, __m128d);
int _mm_ucomilt_ss(__m128, __m128);
int _mm_ucomineq_sd(__m128d, __m128d);
int _mm_ucomineq_ss(__m128, __m128);
__m128i _mm_unpackhi_epi16(__m128i, __m128i);
__m128i _mm_unpackhi_epi32(__m128i, __m128i);
__m128i _mm_unpackhi_epi64(__m128i, __m128i);
__m128i _mm_unpackhi_epi8(__m128i, __m128i);
__m128d _mm_unpackhi_pd(__m128d, __m128d);
__m128 _mm_unpackhi_ps(__m128, __m128);
__m128i _mm_unpacklo_epi16(__m128i, __m128i);
__m128i _mm_unpacklo_epi32(__m128i, __m128i);
__m128i _mm_unpacklo_epi64(__m128i, __m128i);
__m128i _mm_unpacklo_epi8(__m128i, __m128i);
__m128d _mm_unpacklo_pd(__m128d, __m128d);
__m128 _mm_unpacklo_ps(__m128, __m128);
__m128d _mm_xor_pd(__m128d, __m128d);
__m128 _mm_xor_ps(__m128, __m128);
__m128i _mm_xor_si128(__m128i, __m128i);
__int64 _mul128(__int64 _Multiplier, __int64 _Multiplicand, __int64 * _HighProduct);
unsigned int __cdecl _rotl(  unsigned int _Value,   int _Shift);
unsigned short __cdecl _rotl16(unsigned short _Value, unsigned char _Shift);
unsigned __int64 __cdecl _rotl64(  unsigned __int64 _Value,   int _Shift);
unsigned char __cdecl _rotl8(unsigned char _Value, unsigned char _Shift);
unsigned int __cdecl _rotr(  unsigned int _Value,   int _Shift);
unsigned short __cdecl _rotr16(unsigned short _Value, unsigned char _Shift);
unsigned __int64 __cdecl _rotr64(  unsigned __int64 _Value,   int _Shift);
unsigned char __cdecl _rotr8(unsigned char _Value, unsigned char _Shift);
int __cdecl _setjmp(jmp_buf);
int __cdecl _setjmpex(jmp_buf);
void _rsm(void);
void _lgdt(void *);
void _sgdt(void *);
void _clac(void);
void _stac(void);
unsigned char __cdecl _addcarry_u8(unsigned char, unsigned char, unsigned char, unsigned char *);
unsigned char __cdecl _subborrow_u8(unsigned char, unsigned char, unsigned char, unsigned char *);
unsigned char __cdecl _addcarry_u16(unsigned char, unsigned short, unsigned short, unsigned short *);
unsigned char __cdecl _subborrow_u16(unsigned char, unsigned short, unsigned short, unsigned short *);
unsigned char __cdecl _addcarry_u32(unsigned char, unsigned int, unsigned int, unsigned int *);
unsigned char __cdecl _subborrow_u32(unsigned char, unsigned int, unsigned int, unsigned int *);
unsigned char __cdecl _addcarry_u64(unsigned char, unsigned __int64, unsigned __int64, unsigned __int64 *);
unsigned char __cdecl _subborrow_u64(unsigned char, unsigned __int64, unsigned __int64, unsigned __int64 *);
void _mm_monitorx(void const *, unsigned int, unsigned int);
void _mm_mwaitx(unsigned int, unsigned int, unsigned int);
unsigned int __rmpupdate(unsigned __int64, rmp_seg *, int);
unsigned int __psmash(unsigned __int64);
unsigned int __rmpadjust(unsigned __int64, int, int);
unsigned int __pvalidate(unsigned __int64, int, int, int*);
void __svm_invlpgb(void*, int);
void __svm_tlbsync(void);



    





#line 964 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"

#line 966 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"


}
#line 970 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"
#pragma warning(pop) 
#line 972 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\intrin.h"
#pragma external_header(pop)
#line 112 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
#pragma warning(pop)
#line 114 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"



#line 118 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"























#line 142 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
#line 143 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"


#pragma external_header(push)
#line 1 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\assert.h"
//
// assert.h
//
//      Copyright (c) Microsoft Corporation. All rights reserved.
//
// Defines the assert macro and related functionality.
//


#line 11 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\assert.h"



#pragma warning(push)
#pragma warning(disable: 4324  4514 4574 4710 4793 4820 4995 4996 28719 28726 28727 )


__pragma(pack(push, 8)) extern "C" {











    __declspec(dllimport) void __cdecl _wassert(
          wchar_t const* _Message,
          wchar_t const* _File,
            unsigned       _Line
        );

    




#line 42 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\\assert.h"



} __pragma(pack(pop))

#pragma warning(pop) 
#pragma external_header(pop)
#line 146 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

#pragma warning(push)
#pragma warning(disable : 4005 4668)
// C4005/4668: Old header issue
#pragma external_header(push)
#line 1 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\stdint.h"
//
// stdint.h
//
//      Copyright (c) Microsoft Corporation. All rights reserved.
//
// The C Standard Library <stdint.h> header.
//
#pragma once






#pragma warning(push)
#pragma warning(disable:   4514 4820 )

typedef signed char        int8_t;
typedef short              int16_t;
typedef int                int32_t;
typedef long long          int64_t;
typedef unsigned char      uint8_t;
typedef unsigned short     uint16_t;
typedef unsigned int       uint32_t;
typedef unsigned long long uint64_t;

typedef signed char        int_least8_t;
typedef short              int_least16_t;
typedef int                int_least32_t;
typedef long long          int_least64_t;
typedef unsigned char      uint_least8_t;
typedef unsigned short     uint_least16_t;
typedef unsigned int       uint_least32_t;
typedef unsigned long long uint_least64_t;

typedef signed char        int_fast8_t;
typedef int                int_fast16_t;
typedef int                int_fast32_t;
typedef long long          int_fast64_t;
typedef unsigned char      uint_fast8_t;
typedef unsigned int       uint_fast16_t;
typedef unsigned int       uint_fast32_t;
typedef unsigned long long uint_fast64_t;

typedef long long          intmax_t;
typedef unsigned long long uintmax_t;

// These macros must exactly match those in the Windows SDK's intsafe.h.








































    
    
    




#line 97 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\stdint.h"









    // SIZE_MAX definition must match exactly with limits.h for modules support.
    
        
    

#line 112 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\stdint.h"
#line 113 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\stdint.h"























#pragma warning(pop) 

#line 139 "C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.31.31103\\include\\stdint.h"
#pragma external_header(pop)
#line 151 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
#pragma warning(pop)

/****************************************************************************
 *
 * Conditional intrinsics
 *
 ****************************************************************************/






#line 165 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"


#line 168 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"



#line 172 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

#line 174 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

#line 176 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"











#line 188 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

namespace DirectX
{

/****************************************************************************
 *
 * Constant definitions
 *
 ****************************************************************************/























#line 221 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

constexpr float XM_PI        = 3.141592654f;
constexpr float XM_2PI       = 6.283185307f;
constexpr float XM_1DIVPI    = 0.318309886f;
constexpr float XM_1DIV2PI   = 0.159154943f;
constexpr float XM_PIDIV2    = 1.570796327f;
constexpr float XM_PIDIV4    = 0.785398163f;

constexpr uint32_t XM_SELECT_0   = 0x00000000;
constexpr uint32_t XM_SELECT_1   = 0xFFFFFFFF;

constexpr uint32_t XM_PERMUTE_0X = 0;
constexpr uint32_t XM_PERMUTE_0Y = 1;
constexpr uint32_t XM_PERMUTE_0Z = 2;
constexpr uint32_t XM_PERMUTE_0W = 3;
constexpr uint32_t XM_PERMUTE_1X = 4;
constexpr uint32_t XM_PERMUTE_1Y = 5;
constexpr uint32_t XM_PERMUTE_1Z = 6;
constexpr uint32_t XM_PERMUTE_1W = 7;

constexpr uint32_t XM_SWIZZLE_X  = 0;
constexpr uint32_t XM_SWIZZLE_Y  = 1;
constexpr uint32_t XM_SWIZZLE_Z  = 2;
constexpr uint32_t XM_SWIZZLE_W  = 3;

constexpr uint32_t XM_CRMASK_CR6         = 0x000000F0;
constexpr uint32_t XM_CRMASK_CR6TRUE     = 0x00000080;
constexpr uint32_t XM_CRMASK_CR6FALSE    = 0x00000020;
constexpr uint32_t XM_CRMASK_CR6BOUNDS   = XM_CRMASK_CR6FALSE;

constexpr size_t XM_CACHE_LINE_SIZE = 64;


/****************************************************************************
 *
 * Macros
 *
 ****************************************************************************/









#line 269 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

// Unit conversion

inline constexpr float XMConvertToRadians(float fDegrees) { return fDegrees * (XM_PI / 180.0f); }
inline constexpr float XMConvertToDegrees(float fRadians) { return fRadians * (180.0f / XM_PI); }

// Condition register evaluation proceeding a recording (R) comparison

inline bool XMComparisonAllTrue(uint32_t CR) { return (((CR) & XM_CRMASK_CR6TRUE) == XM_CRMASK_CR6TRUE); }
inline bool XMComparisonAnyTrue(uint32_t CR) { return (((CR) & XM_CRMASK_CR6FALSE) != XM_CRMASK_CR6FALSE); }
inline bool XMComparisonAllFalse(uint32_t CR) { return (((CR) & XM_CRMASK_CR6FALSE) == XM_CRMASK_CR6FALSE); }
inline bool XMComparisonAnyFalse(uint32_t CR) { return (((CR) & XM_CRMASK_CR6TRUE) != XM_CRMASK_CR6TRUE); }
inline bool XMComparisonMixed(uint32_t CR) { return (((CR) & XM_CRMASK_CR6) == 0); }
inline bool XMComparisonAllInBounds(uint32_t CR) { return (((CR) & XM_CRMASK_CR6BOUNDS) == XM_CRMASK_CR6BOUNDS); }
inline bool XMComparisonAnyOutOfBounds(uint32_t CR) { return (((CR) & XM_CRMASK_CR6BOUNDS) != XM_CRMASK_CR6BOUNDS); }


/****************************************************************************
 *
 * Data types
 *
 ****************************************************************************/

#pragma warning(push)
#pragma warning(disable:4068 4201 4365 4324 4820)
// C4068: ignore unknown pragmas
// C4201: nonstandard extension used : nameless struct/union
// C4365: Off by default noise
// C4324/4820: padding warnings






//------------------------------------------------------------------------------









#line 315 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

//------------------------------------------------------------------------------
// Vector intrinsic: Four 32 bit floating point components aligned on a 16 byte
// boundary and mapped to hardware vector registers

typedef __m128 XMVECTOR;




#line 326 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

// Fix-up for (1st-3rd) XMVECTOR parameters that are pass-in-register for x86, ARM, ARM64, and vector call; by reference otherwise

typedef const XMVECTOR FXMVECTOR;


#line 333 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

// Fix-up for (4th) XMVECTOR parameter to pass in-register for ARM, ARM64, and x64 vector call; by reference otherwise

typedef const XMVECTOR GXMVECTOR;


#line 340 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

// Fix-up for (5th & 6th) XMVECTOR parameter to pass in-register for ARM64 and vector call; by reference otherwise

typedef const XMVECTOR HXMVECTOR;


#line 347 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

// Fix-up for (7th+) XMVECTOR parameters to pass by reference
typedef const XMVECTOR& CXMVECTOR;

//------------------------------------------------------------------------------
// Conversion types for constants
__declspec(align(16)) struct XMVECTORF32
{
    union
    {
        float f[4];
        XMVECTOR v;
    };

    inline operator XMVECTOR() const { return v; }
    inline operator const float*() const { return f; }

    inline operator __m128i() const { return _mm_castps_si128(v); }
    inline operator __m128d() const { return _mm_castps_pd(v); }
#line 367 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
};

__declspec(align(16)) struct XMVECTORI32
{
    union
    {
        int32_t i[4];
        XMVECTOR v;
    };

    inline operator XMVECTOR() const { return v; }

    inline operator __m128i() const { return _mm_castps_si128(v); }
    inline operator __m128d() const { return _mm_castps_pd(v); }
#line 382 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
};

__declspec(align(16)) struct XMVECTORU8
{
    union
    {
        uint8_t u[16];
        XMVECTOR v;
    };

    inline operator XMVECTOR() const { return v; }

    inline operator __m128i() const { return _mm_castps_si128(v); }
    inline operator __m128d() const { return _mm_castps_pd(v); }
#line 397 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
};

__declspec(align(16)) struct XMVECTORU32
{
    union
    {
        uint32_t u[4];
        XMVECTOR v;
    };

    inline operator XMVECTOR() const { return v; }

    inline operator __m128i() const { return _mm_castps_si128(v); }
    inline operator __m128d() const { return _mm_castps_pd(v); }
#line 412 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
};

//------------------------------------------------------------------------------
// Vector operators


XMVECTOR    __vectorcall     operator+ (FXMVECTOR V);
XMVECTOR    __vectorcall     operator- (FXMVECTOR V);

XMVECTOR&   __vectorcall     operator+= (XMVECTOR& V1, FXMVECTOR V2);
XMVECTOR&   __vectorcall     operator-= (XMVECTOR& V1, FXMVECTOR V2);
XMVECTOR&   __vectorcall     operator*= (XMVECTOR& V1, FXMVECTOR V2);
XMVECTOR&   __vectorcall     operator/= (XMVECTOR& V1, FXMVECTOR V2);

XMVECTOR&   operator*= (XMVECTOR& V, float S);
XMVECTOR&   operator/= (XMVECTOR& V, float S);

XMVECTOR    __vectorcall     operator+ (FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     operator- (FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     operator* (FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     operator/ (FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     operator* (FXMVECTOR V, float S);
XMVECTOR    __vectorcall     operator* (float S, FXMVECTOR V);
XMVECTOR    __vectorcall     operator/ (FXMVECTOR V, float S);
#line 437 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

//------------------------------------------------------------------------------
// Matrix type: Sixteen 32 bit floating point components aligned on a
// 16 byte boundary and mapped to four hardware vector registers

struct XMMATRIX;

// Fix-up for (1st) XMMATRIX parameter to pass in-register for ARM64 and vector call; by reference otherwise

typedef const XMMATRIX FXMMATRIX;


#line 450 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

// Fix-up for (2nd+) XMMATRIX parameters to pass by reference
typedef const XMMATRIX& CXMMATRIX;




__declspec(align(16)) struct XMMATRIX
#line 459 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
{














    XMVECTOR r[4];
#line 476 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

    XMMATRIX() = default;

    XMMATRIX(const XMMATRIX&) = default;



#line 484 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
    XMMATRIX& operator=(const XMMATRIX&) = default;

    XMMATRIX(XMMATRIX&&) = default;
    XMMATRIX& operator=(XMMATRIX&&) = default;
#line 489 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

    constexpr XMMATRIX(FXMVECTOR R0, FXMVECTOR R1, FXMVECTOR R2, CXMVECTOR R3) : r{ R0,R1,R2,R3 } {}
    XMMATRIX(float m00, float m01, float m02, float m03,
             float m10, float m11, float m12, float m13,
             float m20, float m21, float m22, float m23,
             float m30, float m31, float m32, float m33);
    explicit XMMATRIX(  const float *pArray);






    XMMATRIX    operator+ () const { return *this; }
    XMMATRIX    operator- () const;

    XMMATRIX&   __vectorcall     operator+= (FXMMATRIX M);
    XMMATRIX&   __vectorcall     operator-= (FXMMATRIX M);
    XMMATRIX&   __vectorcall     operator*= (FXMMATRIX M);
    XMMATRIX&   operator*= (float S);
    XMMATRIX&   operator/= (float S);

    XMMATRIX    __vectorcall     operator+ (FXMMATRIX M) const;
    XMMATRIX    __vectorcall     operator- (FXMMATRIX M) const;
    XMMATRIX    __vectorcall     operator* (FXMMATRIX M) const;
    XMMATRIX    operator* (float S) const;
    XMMATRIX    operator/ (float S) const;

    friend XMMATRIX     __vectorcall     operator* (float S, FXMMATRIX M);
};

//------------------------------------------------------------------------------
// 2D Vector; 32 bit floating point components
struct XMFLOAT2
{
    float x;
    float y;

    XMFLOAT2() = default;

    XMFLOAT2(const XMFLOAT2&) = default;
    XMFLOAT2& operator=(const XMFLOAT2&) = default;

    XMFLOAT2(XMFLOAT2&&) = default;
    XMFLOAT2& operator=(XMFLOAT2&&) = default;

    constexpr XMFLOAT2(float _x, float _y) : x(_x), y(_y) {}
    explicit XMFLOAT2(  const float *pArray) : x(pArray[0]), y(pArray[1]) {}
};

// 2D Vector; 32 bit floating point components aligned on a 16 byte boundary
__declspec(align(16)) struct XMFLOAT2A : public XMFLOAT2
{
    XMFLOAT2A() = default;

    XMFLOAT2A(const XMFLOAT2A&) = default;
    XMFLOAT2A& operator=(const XMFLOAT2A&) = default;

    XMFLOAT2A(XMFLOAT2A&&) = default;
    XMFLOAT2A& operator=(XMFLOAT2A&&) = default;

    constexpr XMFLOAT2A(float _x, float _y) : XMFLOAT2(_x, _y) {}
    explicit XMFLOAT2A(  const float *pArray) : XMFLOAT2(pArray) {}
};

//------------------------------------------------------------------------------
// 2D Vector; 32 bit signed integer components
struct XMINT2
{
    int32_t x;
    int32_t y;

    XMINT2() = default;

    XMINT2(const XMINT2&) = default;
    XMINT2& operator=(const XMINT2&) = default;

    XMINT2(XMINT2&&) = default;
    XMINT2& operator=(XMINT2&&) = default;

    constexpr XMINT2(int32_t _x, int32_t _y) : x(_x), y(_y) {}
    explicit XMINT2(  const int32_t *pArray) : x(pArray[0]), y(pArray[1]) {}
};

// 2D Vector; 32 bit unsigned integer components
struct XMUINT2
{
    uint32_t x;
    uint32_t y;

    XMUINT2() = default;

    XMUINT2(const XMUINT2&) = default;
    XMUINT2& operator=(const XMUINT2&) = default;

    XMUINT2(XMUINT2&&) = default;
    XMUINT2& operator=(XMUINT2&&) = default;

    constexpr XMUINT2(uint32_t _x, uint32_t _y) : x(_x), y(_y) {}
    explicit XMUINT2(  const uint32_t *pArray) : x(pArray[0]), y(pArray[1]) {}
};

//------------------------------------------------------------------------------
// 3D Vector; 32 bit floating point components
struct XMFLOAT3
{
    float x;
    float y;
    float z;

    XMFLOAT3() = default;

    XMFLOAT3(const XMFLOAT3&) = default;
    XMFLOAT3& operator=(const XMFLOAT3&) = default;

    XMFLOAT3(XMFLOAT3&&) = default;
    XMFLOAT3& operator=(XMFLOAT3&&) = default;

    constexpr XMFLOAT3(float _x, float _y, float _z) : x(_x), y(_y), z(_z) {}
    explicit XMFLOAT3(  const float *pArray) : x(pArray[0]), y(pArray[1]), z(pArray[2]) {}
};

// 3D Vector; 32 bit floating point components aligned on a 16 byte boundary
__declspec(align(16)) struct XMFLOAT3A : public XMFLOAT3
{
    XMFLOAT3A() = default;

    XMFLOAT3A(const XMFLOAT3A&) = default;
    XMFLOAT3A& operator=(const XMFLOAT3A&) = default;

    XMFLOAT3A(XMFLOAT3A&&) = default;
    XMFLOAT3A& operator=(XMFLOAT3A&&) = default;

    constexpr XMFLOAT3A(float _x, float _y, float _z) : XMFLOAT3(_x, _y, _z) {}
    explicit XMFLOAT3A(  const float *pArray) : XMFLOAT3(pArray) {}
};

//------------------------------------------------------------------------------
// 3D Vector; 32 bit signed integer components
struct XMINT3
{
    int32_t x;
    int32_t y;
    int32_t z;

    XMINT3() = default;

    XMINT3(const XMINT3&) = default;
    XMINT3& operator=(const XMINT3&) = default;

    XMINT3(XMINT3&&) = default;
    XMINT3& operator=(XMINT3&&) = default;

    constexpr XMINT3(int32_t _x, int32_t _y, int32_t _z) : x(_x), y(_y), z(_z) {}
    explicit XMINT3(  const int32_t *pArray) : x(pArray[0]), y(pArray[1]), z(pArray[2]) {}
};

// 3D Vector; 32 bit unsigned integer components
struct XMUINT3
{
    uint32_t x;
    uint32_t y;
    uint32_t z;

    XMUINT3() = default;

    XMUINT3(const XMUINT3&) = default;
    XMUINT3& operator=(const XMUINT3&) = default;

    XMUINT3(XMUINT3&&) = default;
    XMUINT3& operator=(XMUINT3&&) = default;

    constexpr XMUINT3(uint32_t _x, uint32_t _y, uint32_t _z) : x(_x), y(_y), z(_z) {}
    explicit XMUINT3(  const uint32_t *pArray) : x(pArray[0]), y(pArray[1]), z(pArray[2]) {}
};

//------------------------------------------------------------------------------
// 4D Vector; 32 bit floating point components
struct XMFLOAT4
{
    float x;
    float y;
    float z;
    float w;

    XMFLOAT4() = default;

    XMFLOAT4(const XMFLOAT4&) = default;
    XMFLOAT4& operator=(const XMFLOAT4&) = default;

    XMFLOAT4(XMFLOAT4&&) = default;
    XMFLOAT4& operator=(XMFLOAT4&&) = default;

    constexpr XMFLOAT4(float _x, float _y, float _z, float _w) : x(_x), y(_y), z(_z), w(_w) {}
    explicit XMFLOAT4(  const float *pArray) : x(pArray[0]), y(pArray[1]), z(pArray[2]), w(pArray[3]) {}
};

// 4D Vector; 32 bit floating point components aligned on a 16 byte boundary
__declspec(align(16)) struct XMFLOAT4A : public XMFLOAT4
{
    XMFLOAT4A() = default;

    XMFLOAT4A(const XMFLOAT4A&) = default;
    XMFLOAT4A& operator=(const XMFLOAT4A&) = default;

    XMFLOAT4A(XMFLOAT4A&&) = default;
    XMFLOAT4A& operator=(XMFLOAT4A&&) = default;

    constexpr XMFLOAT4A(float _x, float _y, float _z, float _w) : XMFLOAT4(_x, _y, _z, _w) {}
    explicit XMFLOAT4A(  const float *pArray) : XMFLOAT4(pArray) {}
};

//------------------------------------------------------------------------------
// 4D Vector; 32 bit signed integer components
struct XMINT4
{
    int32_t x;
    int32_t y;
    int32_t z;
    int32_t w;

    XMINT4() = default;

    XMINT4(const XMINT4&) = default;
    XMINT4& operator=(const XMINT4&) = default;

    XMINT4(XMINT4&&) = default;
    XMINT4& operator=(XMINT4&&) = default;

    constexpr XMINT4(int32_t _x, int32_t _y, int32_t _z, int32_t _w) : x(_x), y(_y), z(_z), w(_w) {}
    explicit XMINT4(  const int32_t *pArray) : x(pArray[0]), y(pArray[1]), z(pArray[2]), w(pArray[3]) {}
};

// 4D Vector; 32 bit unsigned integer components
struct XMUINT4
{
    uint32_t x;
    uint32_t y;
    uint32_t z;
    uint32_t w;

    XMUINT4() = default;

    XMUINT4(const XMUINT4&) = default;
    XMUINT4& operator=(const XMUINT4&) = default;

    XMUINT4(XMUINT4&&) = default;
    XMUINT4& operator=(XMUINT4&&) = default;

    constexpr XMUINT4(uint32_t _x, uint32_t _y, uint32_t _z, uint32_t _w) : x(_x), y(_y), z(_z), w(_w) {}
    explicit XMUINT4(  const uint32_t *pArray) : x(pArray[0]), y(pArray[1]), z(pArray[2]), w(pArray[3]) {}
};

//------------------------------------------------------------------------------
// 3x3 Matrix: 32 bit floating point components
struct XMFLOAT3X3
{
    union
    {
        struct
        {
            float _11, _12, _13;
            float _21, _22, _23;
            float _31, _32, _33;
        };
        float m[3][3];
    };

    XMFLOAT3X3() = default;

    XMFLOAT3X3(const XMFLOAT3X3&) = default;
    XMFLOAT3X3& operator=(const XMFLOAT3X3&) = default;

    XMFLOAT3X3(XMFLOAT3X3&&) = default;
    XMFLOAT3X3& operator=(XMFLOAT3X3&&) = default;

    constexpr XMFLOAT3X3(float m00, float m01, float m02,
                            float m10, float m11, float m12,
                            float m20, float m21, float m22)
        : _11(m00), _12(m01), _13(m02),
          _21(m10), _22(m11), _23(m12),
          _31(m20), _32(m21), _33(m22) {}
    explicit XMFLOAT3X3(  const float *pArray);

    float       operator() (size_t Row, size_t Column) const { return m[Row][Column]; }
    float&      operator() (size_t Row, size_t Column) { return m[Row][Column]; }
};

//------------------------------------------------------------------------------
// 4x3 Row-major Matrix: 32 bit floating point components
struct XMFLOAT4X3
{
    union
    {
        struct
        {
            float _11, _12, _13;
            float _21, _22, _23;
            float _31, _32, _33;
            float _41, _42, _43;
        };
        float m[4][3];
        float f[12];
    };

    XMFLOAT4X3() = default;

    XMFLOAT4X3(const XMFLOAT4X3&) = default;
    XMFLOAT4X3& operator=(const XMFLOAT4X3&) = default;

    XMFLOAT4X3(XMFLOAT4X3&&) = default;
    XMFLOAT4X3& operator=(XMFLOAT4X3&&) = default;

    constexpr XMFLOAT4X3(float m00, float m01, float m02,
                            float m10, float m11, float m12,
                            float m20, float m21, float m22,
                            float m30, float m31, float m32)
        : _11(m00), _12(m01), _13(m02),
          _21(m10), _22(m11), _23(m12),
          _31(m20), _32(m21), _33(m22),
          _41(m30), _42(m31), _43(m32) {}
    explicit XMFLOAT4X3(  const float *pArray);

    float       operator() (size_t Row, size_t Column) const { return m[Row][Column]; }
    float&      operator() (size_t Row, size_t Column) { return m[Row][Column]; }
};

// 4x3 Row-major Matrix: 32 bit floating point components aligned on a 16 byte boundary
__declspec(align(16)) struct XMFLOAT4X3A : public XMFLOAT4X3
{
    XMFLOAT4X3A() = default;

    XMFLOAT4X3A(const XMFLOAT4X3A&) = default;
    XMFLOAT4X3A& operator=(const XMFLOAT4X3A&) = default;

    XMFLOAT4X3A(XMFLOAT4X3A&&) = default;
    XMFLOAT4X3A& operator=(XMFLOAT4X3A&&) = default;

    constexpr XMFLOAT4X3A(float m00, float m01, float m02,
                            float m10, float m11, float m12,
                            float m20, float m21, float m22,
                            float m30, float m31, float m32) :
        XMFLOAT4X3(m00,m01,m02,m10,m11,m12,m20,m21,m22,m30,m31,m32) {}
    explicit XMFLOAT4X3A(  const float *pArray) : XMFLOAT4X3(pArray) {}
};

//------------------------------------------------------------------------------
// 3x4 Column-major Matrix: 32 bit floating point components
struct XMFLOAT3X4
{
    union
    {
        struct
        {
            float _11, _12, _13, _14;
            float _21, _22, _23, _24;
            float _31, _32, _33, _34;
        };
        float m[3][4];
        float f[12];
    };

    XMFLOAT3X4() = default;

    XMFLOAT3X4(const XMFLOAT3X4&) = default;
    XMFLOAT3X4& operator=(const XMFLOAT3X4&) = default;

    XMFLOAT3X4(XMFLOAT3X4&&) = default;
    XMFLOAT3X4& operator=(XMFLOAT3X4&&) = default;

    constexpr XMFLOAT3X4(float m00, float m01, float m02, float m03,
                            float m10, float m11, float m12, float m13,
                            float m20, float m21, float m22, float m23)
        : _11(m00), _12(m01), _13(m02), _14(m03),
          _21(m10), _22(m11), _23(m12), _24(m13),
          _31(m20), _32(m21), _33(m22), _34(m23) {}
    explicit XMFLOAT3X4(  const float *pArray);

    float       operator() (size_t Row, size_t Column) const { return m[Row][Column]; }
    float&      operator() (size_t Row, size_t Column) { return m[Row][Column]; }
};

// 3x4 Column-major Matrix: 32 bit floating point components aligned on a 16 byte boundary
__declspec(align(16)) struct XMFLOAT3X4A : public XMFLOAT3X4
{
    XMFLOAT3X4A() = default;

    XMFLOAT3X4A(const XMFLOAT3X4A&) = default;
    XMFLOAT3X4A& operator=(const XMFLOAT3X4A&) = default;

    XMFLOAT3X4A(XMFLOAT3X4A&&) = default;
    XMFLOAT3X4A& operator=(XMFLOAT3X4A&&) = default;

    constexpr XMFLOAT3X4A(float m00, float m01, float m02, float m03,
                             float m10, float m11, float m12, float m13,
                             float m20, float m21, float m22, float m23) :
        XMFLOAT3X4(m00, m01, m02, m03, m10, m11, m12, m13, m20, m21, m22, m23) {}
    explicit XMFLOAT3X4A(  const float *pArray) : XMFLOAT3X4(pArray) {}
};

//------------------------------------------------------------------------------
// 4x4 Matrix: 32 bit floating point components
struct XMFLOAT4X4
{
    union
    {
        struct
        {
            float _11, _12, _13, _14;
            float _21, _22, _23, _24;
            float _31, _32, _33, _34;
            float _41, _42, _43, _44;
        };
        float m[4][4];
    };

    XMFLOAT4X4() = default;

    XMFLOAT4X4(const XMFLOAT4X4&) = default;
    XMFLOAT4X4& operator=(const XMFLOAT4X4&) = default;

    XMFLOAT4X4(XMFLOAT4X4&&) = default;
    XMFLOAT4X4& operator=(XMFLOAT4X4&&) = default;

    constexpr XMFLOAT4X4(float m00, float m01, float m02, float m03,
                            float m10, float m11, float m12, float m13,
                            float m20, float m21, float m22, float m23,
                            float m30, float m31, float m32, float m33)
        : _11(m00), _12(m01), _13(m02), _14(m03),
          _21(m10), _22(m11), _23(m12), _24(m13),
          _31(m20), _32(m21), _33(m22), _34(m23),
          _41(m30), _42(m31), _43(m32), _44(m33) {}
    explicit XMFLOAT4X4(  const float *pArray);

    float       operator() (size_t Row, size_t Column) const { return m[Row][Column]; }
    float&      operator() (size_t Row, size_t Column) { return m[Row][Column]; }
};

// 4x4 Matrix: 32 bit floating point components aligned on a 16 byte boundary
__declspec(align(16)) struct XMFLOAT4X4A : public XMFLOAT4X4
{
    XMFLOAT4X4A() = default;

    XMFLOAT4X4A(const XMFLOAT4X4A&) = default;
    XMFLOAT4X4A& operator=(const XMFLOAT4X4A&) = default;

    XMFLOAT4X4A(XMFLOAT4X4A&&) = default;
    XMFLOAT4X4A& operator=(XMFLOAT4X4A&&) = default;

    constexpr XMFLOAT4X4A(float m00, float m01, float m02, float m03,
                             float m10, float m11, float m12, float m13,
                             float m20, float m21, float m22, float m23,
                             float m30, float m31, float m32, float m33)
        : XMFLOAT4X4(m00,m01,m02,m03,m10,m11,m12,m13,m20,m21,m22,m23,m30,m31,m32,m33) {}
    explicit XMFLOAT4X4A(  const float *pArray) : XMFLOAT4X4(pArray) {}
};

////////////////////////////////////////////////////////////////////////////////





#pragma warning(pop)

/****************************************************************************
 *
 * Data conversion operations
 *
 ****************************************************************************/

XMVECTOR    __vectorcall     XMConvertVectorIntToFloat(FXMVECTOR VInt, uint32_t DivExponent);
XMVECTOR    __vectorcall     XMConvertVectorFloatToInt(FXMVECTOR VFloat, uint32_t MulExponent);
XMVECTOR    __vectorcall     XMConvertVectorUIntToFloat(FXMVECTOR VUInt, uint32_t DivExponent);
XMVECTOR    __vectorcall     XMConvertVectorFloatToUInt(FXMVECTOR VFloat, uint32_t MulExponent);





#line 970 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

XMVECTOR    __vectorcall     XMVectorSetBinaryConstant(uint32_t C0, uint32_t C1, uint32_t C2, uint32_t C3);
XMVECTOR    __vectorcall     XMVectorSplatConstant(int32_t IntConstant, uint32_t DivExponent);
XMVECTOR    __vectorcall     XMVectorSplatConstantInt(int32_t IntConstant);

/****************************************************************************
 *
 * Load operations
 *
 ****************************************************************************/

XMVECTOR    __vectorcall     XMLoadInt(  const uint32_t* pSource);
XMVECTOR    __vectorcall     XMLoadFloat(  const float* pSource);

XMVECTOR    __vectorcall     XMLoadInt2(  const uint32_t* pSource);
XMVECTOR    __vectorcall     XMLoadInt2A(  const uint32_t* PSource);
XMVECTOR    __vectorcall     XMLoadFloat2(  const XMFLOAT2* pSource);
XMVECTOR    __vectorcall     XMLoadFloat2A(  const XMFLOAT2A* pSource);
XMVECTOR    __vectorcall     XMLoadSInt2(  const XMINT2* pSource);
XMVECTOR    __vectorcall     XMLoadUInt2(  const XMUINT2* pSource);

XMVECTOR    __vectorcall     XMLoadInt3(  const uint32_t* pSource);
XMVECTOR    __vectorcall     XMLoadInt3A(  const uint32_t* pSource);
XMVECTOR    __vectorcall     XMLoadFloat3(  const XMFLOAT3* pSource);
XMVECTOR    __vectorcall     XMLoadFloat3A(  const XMFLOAT3A* pSource);
XMVECTOR    __vectorcall     XMLoadSInt3(  const XMINT3* pSource);
XMVECTOR    __vectorcall     XMLoadUInt3(  const XMUINT3* pSource);

XMVECTOR    __vectorcall     XMLoadInt4(  const uint32_t* pSource);
XMVECTOR    __vectorcall     XMLoadInt4A(  const uint32_t* pSource);
XMVECTOR    __vectorcall     XMLoadFloat4(  const XMFLOAT4* pSource);
XMVECTOR    __vectorcall     XMLoadFloat4A(  const XMFLOAT4A* pSource);
XMVECTOR    __vectorcall     XMLoadSInt4(  const XMINT4* pSource);
XMVECTOR    __vectorcall     XMLoadUInt4(  const XMUINT4* pSource);

XMMATRIX    __vectorcall     XMLoadFloat3x3(  const XMFLOAT3X3* pSource);
XMMATRIX    __vectorcall     XMLoadFloat4x3(  const XMFLOAT4X3* pSource);
XMMATRIX    __vectorcall     XMLoadFloat4x3A(  const XMFLOAT4X3A* pSource);
XMMATRIX    __vectorcall     XMLoadFloat3x4(  const XMFLOAT3X4* pSource);
XMMATRIX    __vectorcall     XMLoadFloat3x4A(  const XMFLOAT3X4A* pSource);
XMMATRIX    __vectorcall     XMLoadFloat4x4(  const XMFLOAT4X4* pSource);
XMMATRIX    __vectorcall     XMLoadFloat4x4A(  const XMFLOAT4X4A* pSource);

/****************************************************************************
 *
 * Store operations
 *
 ****************************************************************************/

void        __vectorcall     XMStoreInt(  uint32_t* pDestination,   FXMVECTOR V);
void        __vectorcall     XMStoreFloat(  float* pDestination,   FXMVECTOR V);

void        __vectorcall     XMStoreInt2(  uint32_t* pDestination,   FXMVECTOR V);
void        __vectorcall     XMStoreInt2A(  uint32_t* pDestination,   FXMVECTOR V);
void        __vectorcall     XMStoreFloat2(  XMFLOAT2* pDestination,   FXMVECTOR V);
void        __vectorcall     XMStoreFloat2A(  XMFLOAT2A* pDestination,   FXMVECTOR V);
void        __vectorcall     XMStoreSInt2(  XMINT2* pDestination,   FXMVECTOR V);
void        __vectorcall     XMStoreUInt2(  XMUINT2* pDestination,   FXMVECTOR V);

void        __vectorcall     XMStoreInt3(  uint32_t* pDestination,   FXMVECTOR V);
void        __vectorcall     XMStoreInt3A(  uint32_t* pDestination,   FXMVECTOR V);
void        __vectorcall     XMStoreFloat3(  XMFLOAT3* pDestination,   FXMVECTOR V);
void        __vectorcall     XMStoreFloat3A(  XMFLOAT3A* pDestination,   FXMVECTOR V);
void        __vectorcall     XMStoreSInt3(  XMINT3* pDestination,   FXMVECTOR V);
void        __vectorcall     XMStoreUInt3(  XMUINT3* pDestination,   FXMVECTOR V);

void        __vectorcall     XMStoreInt4(  uint32_t* pDestination,   FXMVECTOR V);
void        __vectorcall     XMStoreInt4A(  uint32_t* pDestination,   FXMVECTOR V);
void        __vectorcall     XMStoreFloat4(  XMFLOAT4* pDestination,   FXMVECTOR V);
void        __vectorcall     XMStoreFloat4A(  XMFLOAT4A* pDestination,   FXMVECTOR V);
void        __vectorcall     XMStoreSInt4(  XMINT4* pDestination,   FXMVECTOR V);
void        __vectorcall     XMStoreUInt4(  XMUINT4* pDestination,   FXMVECTOR V);

void        __vectorcall     XMStoreFloat3x3(  XMFLOAT3X3* pDestination,   FXMMATRIX M);
void        __vectorcall     XMStoreFloat4x3(  XMFLOAT4X3* pDestination,   FXMMATRIX M);
void        __vectorcall     XMStoreFloat4x3A(  XMFLOAT4X3A* pDestination,   FXMMATRIX M);
void        __vectorcall     XMStoreFloat3x4(  XMFLOAT3X4* pDestination,   FXMMATRIX M);
void        __vectorcall     XMStoreFloat3x4A(  XMFLOAT3X4A* pDestination,   FXMMATRIX M);
void        __vectorcall     XMStoreFloat4x4(  XMFLOAT4X4* pDestination,   FXMMATRIX M);
void        __vectorcall     XMStoreFloat4x4A(  XMFLOAT4X4A* pDestination,   FXMMATRIX M);

/****************************************************************************
 *
 * General vector operations
 *
 ****************************************************************************/

XMVECTOR    __vectorcall     XMVectorZero();
XMVECTOR    __vectorcall     XMVectorSet(float x, float y, float z, float w);
XMVECTOR    __vectorcall     XMVectorSetInt(uint32_t x, uint32_t y, uint32_t z, uint32_t w);
XMVECTOR    __vectorcall     XMVectorReplicate(float Value);
XMVECTOR    __vectorcall     XMVectorReplicatePtr(  const float *pValue);
XMVECTOR    __vectorcall     XMVectorReplicateInt(uint32_t Value);
XMVECTOR    __vectorcall     XMVectorReplicateIntPtr(  const uint32_t *pValue);
XMVECTOR    __vectorcall     XMVectorTrueInt();
XMVECTOR    __vectorcall     XMVectorFalseInt();
XMVECTOR    __vectorcall     XMVectorSplatX(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorSplatY(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorSplatZ(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorSplatW(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorSplatOne();
XMVECTOR    __vectorcall     XMVectorSplatInfinity();
XMVECTOR    __vectorcall     XMVectorSplatQNaN();
XMVECTOR    __vectorcall     XMVectorSplatEpsilon();
XMVECTOR    __vectorcall     XMVectorSplatSignMask();

float       __vectorcall     XMVectorGetByIndex(FXMVECTOR V, size_t i);
float       __vectorcall     XMVectorGetX(FXMVECTOR V);
float       __vectorcall     XMVectorGetY(FXMVECTOR V);
float       __vectorcall     XMVectorGetZ(FXMVECTOR V);
float       __vectorcall     XMVectorGetW(FXMVECTOR V);

void        __vectorcall     XMVectorGetByIndexPtr(  float *f,   FXMVECTOR V,   size_t i);
void        __vectorcall     XMVectorGetXPtr(  float *x,   FXMVECTOR V);
void        __vectorcall     XMVectorGetYPtr(  float *y,   FXMVECTOR V);
void        __vectorcall     XMVectorGetZPtr(  float *z,   FXMVECTOR V);
void        __vectorcall     XMVectorGetWPtr(  float *w,   FXMVECTOR V);

uint32_t    __vectorcall     XMVectorGetIntByIndex(FXMVECTOR V, size_t i);
uint32_t    __vectorcall     XMVectorGetIntX(FXMVECTOR V);
uint32_t    __vectorcall     XMVectorGetIntY(FXMVECTOR V);
uint32_t    __vectorcall     XMVectorGetIntZ(FXMVECTOR V);
uint32_t    __vectorcall     XMVectorGetIntW(FXMVECTOR V);

void        __vectorcall     XMVectorGetIntByIndexPtr(  uint32_t *x,   FXMVECTOR V,   size_t i);
void        __vectorcall     XMVectorGetIntXPtr(  uint32_t *x,   FXMVECTOR V);
void        __vectorcall     XMVectorGetIntYPtr(  uint32_t *y,   FXMVECTOR V);
void        __vectorcall     XMVectorGetIntZPtr(  uint32_t *z,   FXMVECTOR V);
void        __vectorcall     XMVectorGetIntWPtr(  uint32_t *w,   FXMVECTOR V);

XMVECTOR    __vectorcall     XMVectorSetByIndex(FXMVECTOR V,float f, size_t i);
XMVECTOR    __vectorcall     XMVectorSetX(FXMVECTOR V, float x);
XMVECTOR    __vectorcall     XMVectorSetY(FXMVECTOR V, float y);
XMVECTOR    __vectorcall     XMVectorSetZ(FXMVECTOR V, float z);
XMVECTOR    __vectorcall     XMVectorSetW(FXMVECTOR V, float w);

XMVECTOR    __vectorcall     XMVectorSetByIndexPtr(  FXMVECTOR V,   const float *f,   size_t i);
XMVECTOR    __vectorcall     XMVectorSetXPtr(  FXMVECTOR V,   const float *x);
XMVECTOR    __vectorcall     XMVectorSetYPtr(  FXMVECTOR V,   const float *y);
XMVECTOR    __vectorcall     XMVectorSetZPtr(  FXMVECTOR V,   const float *z);
XMVECTOR    __vectorcall     XMVectorSetWPtr(  FXMVECTOR V,   const float *w);

XMVECTOR    __vectorcall     XMVectorSetIntByIndex(FXMVECTOR V, uint32_t x, size_t i);
XMVECTOR    __vectorcall     XMVectorSetIntX(FXMVECTOR V, uint32_t x);
XMVECTOR    __vectorcall     XMVectorSetIntY(FXMVECTOR V, uint32_t y);
XMVECTOR    __vectorcall     XMVectorSetIntZ(FXMVECTOR V, uint32_t z);
XMVECTOR    __vectorcall     XMVectorSetIntW(FXMVECTOR V, uint32_t w);

XMVECTOR    __vectorcall     XMVectorSetIntByIndexPtr(  FXMVECTOR V,   const uint32_t *x,   size_t i);
XMVECTOR    __vectorcall     XMVectorSetIntXPtr(  FXMVECTOR V,   const uint32_t *x);
XMVECTOR    __vectorcall     XMVectorSetIntYPtr(  FXMVECTOR V,   const uint32_t *y);
XMVECTOR    __vectorcall     XMVectorSetIntZPtr(  FXMVECTOR V,   const uint32_t *z);
XMVECTOR    __vectorcall     XMVectorSetIntWPtr(  FXMVECTOR V,   const uint32_t *w);



#line 1127 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

XMVECTOR    __vectorcall     XMVectorSwizzle(FXMVECTOR V, uint32_t E0, uint32_t E1, uint32_t E2, uint32_t E3);
XMVECTOR    __vectorcall     XMVectorPermute(FXMVECTOR V1, FXMVECTOR V2, uint32_t PermuteX, uint32_t PermuteY, uint32_t PermuteZ, uint32_t PermuteW);
XMVECTOR    __vectorcall     XMVectorSelectControl(uint32_t VectorIndex0, uint32_t VectorIndex1, uint32_t VectorIndex2, uint32_t VectorIndex3);
XMVECTOR    __vectorcall     XMVectorSelect(FXMVECTOR V1, FXMVECTOR V2, FXMVECTOR Control);
XMVECTOR    __vectorcall     XMVectorMergeXY(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorMergeZW(FXMVECTOR V1, FXMVECTOR V2);






#line 1141 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

XMVECTOR    __vectorcall     XMVectorShiftLeft(FXMVECTOR V1, FXMVECTOR V2, uint32_t Elements);
XMVECTOR    __vectorcall     XMVectorRotateLeft(FXMVECTOR V, uint32_t Elements);
XMVECTOR    __vectorcall     XMVectorRotateRight(FXMVECTOR V, uint32_t Elements);
XMVECTOR    __vectorcall     XMVectorInsert(FXMVECTOR VD, FXMVECTOR VS, uint32_t VSLeftRotateElements,
                                           uint32_t Select0, uint32_t Select1, uint32_t Select2, uint32_t Select3);

XMVECTOR    __vectorcall     XMVectorEqual(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorEqualR(  uint32_t* pCR,   FXMVECTOR V1,   FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorEqualInt(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorEqualIntR(  uint32_t* pCR,   FXMVECTOR V,   FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorNearEqual(FXMVECTOR V1, FXMVECTOR V2, FXMVECTOR Epsilon);
XMVECTOR    __vectorcall     XMVectorNotEqual(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorNotEqualInt(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorGreater(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorGreaterR(  uint32_t* pCR,   FXMVECTOR V1,   FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorGreaterOrEqual(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorGreaterOrEqualR(  uint32_t* pCR,   FXMVECTOR V1,   FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorLess(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorLessOrEqual(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorInBounds(FXMVECTOR V, FXMVECTOR Bounds);
XMVECTOR    __vectorcall     XMVectorInBoundsR(  uint32_t* pCR,   FXMVECTOR V,   FXMVECTOR Bounds);

XMVECTOR    __vectorcall     XMVectorIsNaN(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorIsInfinite(FXMVECTOR V);

XMVECTOR    __vectorcall     XMVectorMin(FXMVECTOR V1,FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorMax(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorRound(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorTruncate(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorFloor(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorCeiling(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorClamp(FXMVECTOR V, FXMVECTOR Min, FXMVECTOR Max);
XMVECTOR    __vectorcall     XMVectorSaturate(FXMVECTOR V);

XMVECTOR    __vectorcall     XMVectorAndInt(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorAndCInt(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorOrInt(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorNorInt(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorXorInt(FXMVECTOR V1, FXMVECTOR V2);

XMVECTOR    __vectorcall     XMVectorNegate(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorAdd(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorSum(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorAddAngles(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorSubtract(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorSubtractAngles(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorMultiply(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorMultiplyAdd(FXMVECTOR V1, FXMVECTOR V2, FXMVECTOR V3);
XMVECTOR    __vectorcall     XMVectorDivide(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorNegativeMultiplySubtract(FXMVECTOR V1, FXMVECTOR V2, FXMVECTOR V3);
XMVECTOR    __vectorcall     XMVectorScale(FXMVECTOR V, float ScaleFactor);
XMVECTOR    __vectorcall     XMVectorReciprocalEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorReciprocal(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorSqrtEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorSqrt(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorReciprocalSqrtEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorReciprocalSqrt(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorExp2(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorExpE(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorExp(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorLog2(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorLogE(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorLog(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorPow(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorAbs(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorMod(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVectorModAngles(FXMVECTOR Angles);
XMVECTOR    __vectorcall     XMVectorSin(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorSinEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorCos(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorCosEst(FXMVECTOR V);
void        __vectorcall     XMVectorSinCos(  XMVECTOR* pSin,   XMVECTOR* pCos,   FXMVECTOR V);
void        __vectorcall     XMVectorSinCosEst(  XMVECTOR* pSin,   XMVECTOR* pCos,   FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorTan(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorTanEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorSinH(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorCosH(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorTanH(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorASin(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorASinEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorACos(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorACosEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorATan(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorATanEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVectorATan2(FXMVECTOR Y, FXMVECTOR X);
XMVECTOR    __vectorcall     XMVectorATan2Est(FXMVECTOR Y, FXMVECTOR X);
XMVECTOR    __vectorcall     XMVectorLerp(FXMVECTOR V0, FXMVECTOR V1, float t);
XMVECTOR    __vectorcall     XMVectorLerpV(FXMVECTOR V0, FXMVECTOR V1, FXMVECTOR T);
XMVECTOR    __vectorcall     XMVectorHermite(FXMVECTOR Position0, FXMVECTOR Tangent0, FXMVECTOR Position1, GXMVECTOR Tangent1, float t);
XMVECTOR    __vectorcall     XMVectorHermiteV(FXMVECTOR Position0, FXMVECTOR Tangent0, FXMVECTOR Position1, GXMVECTOR Tangent1, HXMVECTOR T);
XMVECTOR    __vectorcall     XMVectorCatmullRom(FXMVECTOR Position0, FXMVECTOR Position1, FXMVECTOR Position2, GXMVECTOR Position3, float t);
XMVECTOR    __vectorcall     XMVectorCatmullRomV(FXMVECTOR Position0, FXMVECTOR Position1, FXMVECTOR Position2, GXMVECTOR Position3, HXMVECTOR T);
XMVECTOR    __vectorcall     XMVectorBaryCentric(FXMVECTOR Position0, FXMVECTOR Position1, FXMVECTOR Position2, float f, float g);
XMVECTOR    __vectorcall     XMVectorBaryCentricV(FXMVECTOR Position0, FXMVECTOR Position1, FXMVECTOR Position2, GXMVECTOR F, HXMVECTOR G);

/****************************************************************************
 *
 * 2D vector operations
 *
 ****************************************************************************/

bool        __vectorcall     XMVector2Equal(FXMVECTOR V1, FXMVECTOR V2);
uint32_t    __vectorcall     XMVector2EqualR(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector2EqualInt(FXMVECTOR V1, FXMVECTOR V2);
uint32_t    __vectorcall     XMVector2EqualIntR(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector2NearEqual(FXMVECTOR V1, FXMVECTOR V2, FXMVECTOR Epsilon);
bool        __vectorcall     XMVector2NotEqual(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector2NotEqualInt(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector2Greater(FXMVECTOR V1, FXMVECTOR V2);
uint32_t    __vectorcall     XMVector2GreaterR(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector2GreaterOrEqual(FXMVECTOR V1, FXMVECTOR V2);
uint32_t    __vectorcall     XMVector2GreaterOrEqualR(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector2Less(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector2LessOrEqual(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector2InBounds(FXMVECTOR V, FXMVECTOR Bounds);

bool        __vectorcall     XMVector2IsNaN(FXMVECTOR V);
bool        __vectorcall     XMVector2IsInfinite(FXMVECTOR V);

XMVECTOR    __vectorcall     XMVector2Dot(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVector2Cross(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVector2LengthSq(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector2ReciprocalLengthEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector2ReciprocalLength(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector2LengthEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector2Length(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector2NormalizeEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector2Normalize(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector2ClampLength(FXMVECTOR V, float LengthMin, float LengthMax);
XMVECTOR    __vectorcall     XMVector2ClampLengthV(FXMVECTOR V, FXMVECTOR LengthMin, FXMVECTOR LengthMax);
XMVECTOR    __vectorcall     XMVector2Reflect(FXMVECTOR Incident, FXMVECTOR Normal);
XMVECTOR    __vectorcall     XMVector2Refract(FXMVECTOR Incident, FXMVECTOR Normal, float RefractionIndex);
XMVECTOR    __vectorcall     XMVector2RefractV(FXMVECTOR Incident, FXMVECTOR Normal, FXMVECTOR RefractionIndex);
XMVECTOR    __vectorcall     XMVector2Orthogonal(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector2AngleBetweenNormalsEst(FXMVECTOR N1, FXMVECTOR N2);
XMVECTOR    __vectorcall     XMVector2AngleBetweenNormals(FXMVECTOR N1, FXMVECTOR N2);
XMVECTOR    __vectorcall     XMVector2AngleBetweenVectors(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVector2LinePointDistance(FXMVECTOR LinePoint1, FXMVECTOR LinePoint2, FXMVECTOR Point);
XMVECTOR    __vectorcall     XMVector2IntersectLine(FXMVECTOR Line1Point1, FXMVECTOR Line1Point2, FXMVECTOR Line2Point1, GXMVECTOR Line2Point2);
XMVECTOR    __vectorcall     XMVector2Transform(FXMVECTOR V, FXMMATRIX M);
XMFLOAT4*   __vectorcall     XMVector2TransformStream(  XMFLOAT4* pOutputStream,
                                                      size_t OutputStride,
                                                      const XMFLOAT2* pInputStream,
                                                      size_t InputStride,   size_t VectorCount,   FXMMATRIX M);
XMVECTOR    __vectorcall     XMVector2TransformCoord(FXMVECTOR V, FXMMATRIX M);
XMFLOAT2*   __vectorcall     XMVector2TransformCoordStream(  XMFLOAT2* pOutputStream,
                                                            size_t OutputStride,
                                                            const XMFLOAT2* pInputStream,
                                                            size_t InputStride,   size_t VectorCount,   FXMMATRIX M);
XMVECTOR    __vectorcall     XMVector2TransformNormal(FXMVECTOR V, FXMMATRIX M);
XMFLOAT2*   __vectorcall     XMVector2TransformNormalStream(  XMFLOAT2* pOutputStream,
                                                             size_t OutputStride,
                                                             const XMFLOAT2* pInputStream,
                                                             size_t InputStride,   size_t VectorCount,   FXMMATRIX M);

/****************************************************************************
 *
 * 3D vector operations
 *
 ****************************************************************************/

bool        __vectorcall     XMVector3Equal(FXMVECTOR V1, FXMVECTOR V2);
uint32_t    __vectorcall     XMVector3EqualR(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector3EqualInt(FXMVECTOR V1, FXMVECTOR V2);
uint32_t    __vectorcall     XMVector3EqualIntR(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector3NearEqual(FXMVECTOR V1, FXMVECTOR V2, FXMVECTOR Epsilon);
bool        __vectorcall     XMVector3NotEqual(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector3NotEqualInt(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector3Greater(FXMVECTOR V1, FXMVECTOR V2);
uint32_t    __vectorcall     XMVector3GreaterR(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector3GreaterOrEqual(FXMVECTOR V1, FXMVECTOR V2);
uint32_t    __vectorcall     XMVector3GreaterOrEqualR(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector3Less(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector3LessOrEqual(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector3InBounds(FXMVECTOR V, FXMVECTOR Bounds);

bool        __vectorcall     XMVector3IsNaN(FXMVECTOR V);
bool        __vectorcall     XMVector3IsInfinite(FXMVECTOR V);

XMVECTOR    __vectorcall     XMVector3Dot(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVector3Cross(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVector3LengthSq(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector3ReciprocalLengthEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector3ReciprocalLength(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector3LengthEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector3Length(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector3NormalizeEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector3Normalize(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector3ClampLength(FXMVECTOR V, float LengthMin, float LengthMax);
XMVECTOR    __vectorcall     XMVector3ClampLengthV(FXMVECTOR V, FXMVECTOR LengthMin, FXMVECTOR LengthMax);
XMVECTOR    __vectorcall     XMVector3Reflect(FXMVECTOR Incident, FXMVECTOR Normal);
XMVECTOR    __vectorcall     XMVector3Refract(FXMVECTOR Incident, FXMVECTOR Normal, float RefractionIndex);
XMVECTOR    __vectorcall     XMVector3RefractV(FXMVECTOR Incident, FXMVECTOR Normal, FXMVECTOR RefractionIndex);
XMVECTOR    __vectorcall     XMVector3Orthogonal(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector3AngleBetweenNormalsEst(FXMVECTOR N1, FXMVECTOR N2);
XMVECTOR    __vectorcall     XMVector3AngleBetweenNormals(FXMVECTOR N1, FXMVECTOR N2);
XMVECTOR    __vectorcall     XMVector3AngleBetweenVectors(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVector3LinePointDistance(FXMVECTOR LinePoint1, FXMVECTOR LinePoint2, FXMVECTOR Point);
void        __vectorcall     XMVector3ComponentsFromNormal(  XMVECTOR* pParallel,   XMVECTOR* pPerpendicular,   FXMVECTOR V,   FXMVECTOR Normal);
XMVECTOR    __vectorcall     XMVector3Rotate(FXMVECTOR V, FXMVECTOR RotationQuaternion);
XMVECTOR    __vectorcall     XMVector3InverseRotate(FXMVECTOR V, FXMVECTOR RotationQuaternion);
XMVECTOR    __vectorcall     XMVector3Transform(FXMVECTOR V, FXMMATRIX M);
XMFLOAT4*   __vectorcall     XMVector3TransformStream(  XMFLOAT4* pOutputStream,
                                                       size_t OutputStride,
                                                       const XMFLOAT3* pInputStream,
                                                       size_t InputStride,   size_t VectorCount,   FXMMATRIX M);
XMVECTOR    __vectorcall     XMVector3TransformCoord(FXMVECTOR V, FXMMATRIX M);
XMFLOAT3*   __vectorcall     XMVector3TransformCoordStream(  XMFLOAT3* pOutputStream,
                                                            size_t OutputStride,
                                                            const XMFLOAT3* pInputStream,
                                                            size_t InputStride,   size_t VectorCount,   FXMMATRIX M);
XMVECTOR    __vectorcall     XMVector3TransformNormal(FXMVECTOR V, FXMMATRIX M);
XMFLOAT3*   __vectorcall     XMVector3TransformNormalStream(  XMFLOAT3* pOutputStream,
                                                             size_t OutputStride,
                                                             const XMFLOAT3* pInputStream,
                                                             size_t InputStride,   size_t VectorCount,   FXMMATRIX M);
XMVECTOR    __vectorcall     XMVector3Project(FXMVECTOR V, float ViewportX, float ViewportY, float ViewportWidth, float ViewportHeight, float ViewportMinZ, float ViewportMaxZ,
                                             FXMMATRIX Projection, CXMMATRIX View, CXMMATRIX World);
XMFLOAT3*   __vectorcall     XMVector3ProjectStream(  XMFLOAT3* pOutputStream,
                                                     size_t OutputStride,
                                                     const XMFLOAT3* pInputStream,
                                                     size_t InputStride,   size_t VectorCount,
                                                     float ViewportX,   float ViewportY,   float ViewportWidth,   float ViewportHeight,   float ViewportMinZ,   float ViewportMaxZ,
                                                     FXMMATRIX Projection,   CXMMATRIX View,   CXMMATRIX World);
XMVECTOR    __vectorcall     XMVector3Unproject(FXMVECTOR V, float ViewportX, float ViewportY, float ViewportWidth, float ViewportHeight, float ViewportMinZ, float ViewportMaxZ,
                                               FXMMATRIX Projection, CXMMATRIX View, CXMMATRIX World);
XMFLOAT3*   __vectorcall     XMVector3UnprojectStream(  XMFLOAT3* pOutputStream,
                                                       size_t OutputStride,
                                                       const XMFLOAT3* pInputStream,
                                                       size_t InputStride,   size_t VectorCount,
                                                       float ViewportX,   float ViewportY,   float ViewportWidth,   float ViewportHeight,   float ViewportMinZ,   float ViewportMaxZ,
                                                       FXMMATRIX Projection,   CXMMATRIX View,   CXMMATRIX World);

/****************************************************************************
 *
 * 4D vector operations
 *
 ****************************************************************************/

bool        __vectorcall     XMVector4Equal(FXMVECTOR V1, FXMVECTOR V2);
uint32_t    __vectorcall     XMVector4EqualR(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector4EqualInt(FXMVECTOR V1, FXMVECTOR V2);
uint32_t    __vectorcall     XMVector4EqualIntR(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector4NearEqual(FXMVECTOR V1, FXMVECTOR V2, FXMVECTOR Epsilon);
bool        __vectorcall     XMVector4NotEqual(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector4NotEqualInt(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector4Greater(FXMVECTOR V1, FXMVECTOR V2);
uint32_t    __vectorcall     XMVector4GreaterR(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector4GreaterOrEqual(FXMVECTOR V1, FXMVECTOR V2);
uint32_t    __vectorcall     XMVector4GreaterOrEqualR(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector4Less(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector4LessOrEqual(FXMVECTOR V1, FXMVECTOR V2);
bool        __vectorcall     XMVector4InBounds(FXMVECTOR V, FXMVECTOR Bounds);

bool        __vectorcall     XMVector4IsNaN(FXMVECTOR V);
bool        __vectorcall     XMVector4IsInfinite(FXMVECTOR V);

XMVECTOR    __vectorcall     XMVector4Dot(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVector4Cross(FXMVECTOR V1, FXMVECTOR V2, FXMVECTOR V3);
XMVECTOR    __vectorcall     XMVector4LengthSq(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector4ReciprocalLengthEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector4ReciprocalLength(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector4LengthEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector4Length(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector4NormalizeEst(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector4Normalize(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector4ClampLength(FXMVECTOR V, float LengthMin, float LengthMax);
XMVECTOR    __vectorcall     XMVector4ClampLengthV(FXMVECTOR V, FXMVECTOR LengthMin, FXMVECTOR LengthMax);
XMVECTOR    __vectorcall     XMVector4Reflect(FXMVECTOR Incident, FXMVECTOR Normal);
XMVECTOR    __vectorcall     XMVector4Refract(FXMVECTOR Incident, FXMVECTOR Normal, float RefractionIndex);
XMVECTOR    __vectorcall     XMVector4RefractV(FXMVECTOR Incident, FXMVECTOR Normal, FXMVECTOR RefractionIndex);
XMVECTOR    __vectorcall     XMVector4Orthogonal(FXMVECTOR V);
XMVECTOR    __vectorcall     XMVector4AngleBetweenNormalsEst(FXMVECTOR N1, FXMVECTOR N2);
XMVECTOR    __vectorcall     XMVector4AngleBetweenNormals(FXMVECTOR N1, FXMVECTOR N2);
XMVECTOR    __vectorcall     XMVector4AngleBetweenVectors(FXMVECTOR V1, FXMVECTOR V2);
XMVECTOR    __vectorcall     XMVector4Transform(FXMVECTOR V, FXMMATRIX M);
XMFLOAT4*   __vectorcall     XMVector4TransformStream(  XMFLOAT4* pOutputStream,
                                                       size_t OutputStride,
                                                       const XMFLOAT4* pInputStream,
                                                       size_t InputStride,   size_t VectorCount,   FXMMATRIX M);

/****************************************************************************
 *
 * Matrix operations
 *
 ****************************************************************************/

bool        __vectorcall     XMMatrixIsNaN(FXMMATRIX M);
bool        __vectorcall     XMMatrixIsInfinite(FXMMATRIX M);
bool        __vectorcall     XMMatrixIsIdentity(FXMMATRIX M);

XMMATRIX    __vectorcall     XMMatrixMultiply(FXMMATRIX M1, CXMMATRIX M2);
XMMATRIX    __vectorcall     XMMatrixMultiplyTranspose(FXMMATRIX M1, CXMMATRIX M2);
XMMATRIX    __vectorcall     XMMatrixTranspose(FXMMATRIX M);
XMMATRIX    __vectorcall     XMMatrixInverse(  XMVECTOR* pDeterminant,   FXMMATRIX M);
XMVECTOR    __vectorcall     XMMatrixDeterminant(FXMMATRIX M);
 
bool        __vectorcall     XMMatrixDecompose(  XMVECTOR *outScale,   XMVECTOR *outRotQuat,   XMVECTOR *outTrans,   FXMMATRIX M);

XMMATRIX    __vectorcall     XMMatrixIdentity();
XMMATRIX    __vectorcall     XMMatrixSet(float m00, float m01, float m02, float m03,
                                        float m10, float m11, float m12, float m13,
                                        float m20, float m21, float m22, float m23,
                                        float m30, float m31, float m32, float m33);
XMMATRIX    __vectorcall     XMMatrixTranslation(float OffsetX, float OffsetY, float OffsetZ);
XMMATRIX    __vectorcall     XMMatrixTranslationFromVector(FXMVECTOR Offset);
XMMATRIX    __vectorcall     XMMatrixScaling(float ScaleX, float ScaleY, float ScaleZ);
XMMATRIX    __vectorcall     XMMatrixScalingFromVector(FXMVECTOR Scale);
XMMATRIX    __vectorcall     XMMatrixRotationX(float Angle);
XMMATRIX    __vectorcall     XMMatrixRotationY(float Angle);
XMMATRIX    __vectorcall     XMMatrixRotationZ(float Angle);
XMMATRIX    __vectorcall     XMMatrixRotationRollPitchYaw(float Pitch, float Yaw, float Roll);
XMMATRIX    __vectorcall     XMMatrixRotationRollPitchYawFromVector(FXMVECTOR Angles);
XMMATRIX    __vectorcall     XMMatrixRotationNormal(FXMVECTOR NormalAxis, float Angle);
XMMATRIX    __vectorcall     XMMatrixRotationAxis(FXMVECTOR Axis, float Angle);
XMMATRIX    __vectorcall     XMMatrixRotationQuaternion(FXMVECTOR Quaternion);
XMMATRIX    __vectorcall     XMMatrixTransformation2D(FXMVECTOR ScalingOrigin, float ScalingOrientation, FXMVECTOR Scaling,
                                                     FXMVECTOR RotationOrigin, float Rotation, GXMVECTOR Translation);
XMMATRIX    __vectorcall     XMMatrixTransformation(FXMVECTOR ScalingOrigin, FXMVECTOR ScalingOrientationQuaternion, FXMVECTOR Scaling,
                                                   GXMVECTOR RotationOrigin, HXMVECTOR RotationQuaternion, HXMVECTOR Translation);
XMMATRIX    __vectorcall     XMMatrixAffineTransformation2D(FXMVECTOR Scaling, FXMVECTOR RotationOrigin, float Rotation, FXMVECTOR Translation);
XMMATRIX    __vectorcall     XMMatrixAffineTransformation(FXMVECTOR Scaling, FXMVECTOR RotationOrigin, FXMVECTOR RotationQuaternion, GXMVECTOR Translation);
XMMATRIX    __vectorcall     XMMatrixReflect(FXMVECTOR ReflectionPlane);
XMMATRIX    __vectorcall     XMMatrixShadow(FXMVECTOR ShadowPlane, FXMVECTOR LightPosition);

XMMATRIX    __vectorcall     XMMatrixLookAtLH(FXMVECTOR EyePosition, FXMVECTOR FocusPosition, FXMVECTOR UpDirection);
XMMATRIX    __vectorcall     XMMatrixLookAtRH(FXMVECTOR EyePosition, FXMVECTOR FocusPosition, FXMVECTOR UpDirection);
XMMATRIX    __vectorcall     XMMatrixLookToLH(FXMVECTOR EyePosition, FXMVECTOR EyeDirection, FXMVECTOR UpDirection);
XMMATRIX    __vectorcall     XMMatrixLookToRH(FXMVECTOR EyePosition, FXMVECTOR EyeDirection, FXMVECTOR UpDirection);
XMMATRIX    __vectorcall     XMMatrixPerspectiveLH(float ViewWidth, float ViewHeight, float NearZ, float FarZ);
XMMATRIX    __vectorcall     XMMatrixPerspectiveRH(float ViewWidth, float ViewHeight, float NearZ, float FarZ);
XMMATRIX    __vectorcall     XMMatrixPerspectiveFovLH(float FovAngleY, float AspectRatio, float NearZ, float FarZ);
XMMATRIX    __vectorcall     XMMatrixPerspectiveFovRH(float FovAngleY, float AspectRatio, float NearZ, float FarZ);
XMMATRIX    __vectorcall     XMMatrixPerspectiveOffCenterLH(float ViewLeft, float ViewRight, float ViewBottom, float ViewTop, float NearZ, float FarZ);
XMMATRIX    __vectorcall     XMMatrixPerspectiveOffCenterRH(float ViewLeft, float ViewRight, float ViewBottom, float ViewTop, float NearZ, float FarZ);
XMMATRIX    __vectorcall     XMMatrixOrthographicLH(float ViewWidth, float ViewHeight, float NearZ, float FarZ);
XMMATRIX    __vectorcall     XMMatrixOrthographicRH(float ViewWidth, float ViewHeight, float NearZ, float FarZ);
XMMATRIX    __vectorcall     XMMatrixOrthographicOffCenterLH(float ViewLeft, float ViewRight, float ViewBottom, float ViewTop, float NearZ, float FarZ);
XMMATRIX    __vectorcall     XMMatrixOrthographicOffCenterRH(float ViewLeft, float ViewRight, float ViewBottom, float ViewTop, float NearZ, float FarZ);


/****************************************************************************
 *
 * Quaternion operations
 *
 ****************************************************************************/

bool        __vectorcall     XMQuaternionEqual(FXMVECTOR Q1, FXMVECTOR Q2);
bool        __vectorcall     XMQuaternionNotEqual(FXMVECTOR Q1, FXMVECTOR Q2);

bool        __vectorcall     XMQuaternionIsNaN(FXMVECTOR Q);
bool        __vectorcall     XMQuaternionIsInfinite(FXMVECTOR Q);
bool        __vectorcall     XMQuaternionIsIdentity(FXMVECTOR Q);

XMVECTOR    __vectorcall     XMQuaternionDot(FXMVECTOR Q1, FXMVECTOR Q2);
XMVECTOR    __vectorcall     XMQuaternionMultiply(FXMVECTOR Q1, FXMVECTOR Q2);
XMVECTOR    __vectorcall     XMQuaternionLengthSq(FXMVECTOR Q);
XMVECTOR    __vectorcall     XMQuaternionReciprocalLength(FXMVECTOR Q);
XMVECTOR    __vectorcall     XMQuaternionLength(FXMVECTOR Q);
XMVECTOR    __vectorcall     XMQuaternionNormalizeEst(FXMVECTOR Q);
XMVECTOR    __vectorcall     XMQuaternionNormalize(FXMVECTOR Q);
XMVECTOR    __vectorcall     XMQuaternionConjugate(FXMVECTOR Q);
XMVECTOR    __vectorcall     XMQuaternionInverse(FXMVECTOR Q);
XMVECTOR    __vectorcall     XMQuaternionLn(FXMVECTOR Q);
XMVECTOR    __vectorcall     XMQuaternionExp(FXMVECTOR Q);
XMVECTOR    __vectorcall     XMQuaternionSlerp(FXMVECTOR Q0, FXMVECTOR Q1, float t);
XMVECTOR    __vectorcall     XMQuaternionSlerpV(FXMVECTOR Q0, FXMVECTOR Q1, FXMVECTOR T);
XMVECTOR    __vectorcall     XMQuaternionSquad(FXMVECTOR Q0, FXMVECTOR Q1, FXMVECTOR Q2, GXMVECTOR Q3, float t);
XMVECTOR    __vectorcall     XMQuaternionSquadV(FXMVECTOR Q0, FXMVECTOR Q1, FXMVECTOR Q2, GXMVECTOR Q3, HXMVECTOR T);
void        __vectorcall     XMQuaternionSquadSetup(  XMVECTOR* pA,   XMVECTOR* pB,   XMVECTOR* pC,   FXMVECTOR Q0,   FXMVECTOR Q1,   FXMVECTOR Q2,   GXMVECTOR Q3);
XMVECTOR    __vectorcall     XMQuaternionBaryCentric(FXMVECTOR Q0, FXMVECTOR Q1, FXMVECTOR Q2, float f, float g);
XMVECTOR    __vectorcall     XMQuaternionBaryCentricV(FXMVECTOR Q0, FXMVECTOR Q1, FXMVECTOR Q2, GXMVECTOR F, HXMVECTOR G);

XMVECTOR    __vectorcall     XMQuaternionIdentity();
XMVECTOR    __vectorcall     XMQuaternionRotationRollPitchYaw(float Pitch, float Yaw, float Roll);
XMVECTOR    __vectorcall     XMQuaternionRotationRollPitchYawFromVector(FXMVECTOR Angles);
XMVECTOR    __vectorcall     XMQuaternionRotationNormal(FXMVECTOR NormalAxis, float Angle);
XMVECTOR    __vectorcall     XMQuaternionRotationAxis(FXMVECTOR Axis, float Angle);
XMVECTOR    __vectorcall     XMQuaternionRotationMatrix(FXMMATRIX M);

void        __vectorcall     XMQuaternionToAxisAngle(  XMVECTOR* pAxis,   float* pAngle,   FXMVECTOR Q);

/****************************************************************************
 *
 * Plane operations
 *
 ****************************************************************************/

bool        __vectorcall     XMPlaneEqual(FXMVECTOR P1, FXMVECTOR P2);
bool        __vectorcall     XMPlaneNearEqual(FXMVECTOR P1, FXMVECTOR P2, FXMVECTOR Epsilon);
bool        __vectorcall     XMPlaneNotEqual(FXMVECTOR P1, FXMVECTOR P2);

bool        __vectorcall     XMPlaneIsNaN(FXMVECTOR P);
bool        __vectorcall     XMPlaneIsInfinite(FXMVECTOR P);

XMVECTOR    __vectorcall     XMPlaneDot(FXMVECTOR P, FXMVECTOR V);
XMVECTOR    __vectorcall     XMPlaneDotCoord(FXMVECTOR P, FXMVECTOR V);
XMVECTOR    __vectorcall     XMPlaneDotNormal(FXMVECTOR P, FXMVECTOR V);
XMVECTOR    __vectorcall     XMPlaneNormalizeEst(FXMVECTOR P);
XMVECTOR    __vectorcall     XMPlaneNormalize(FXMVECTOR P);
XMVECTOR    __vectorcall     XMPlaneIntersectLine(FXMVECTOR P, FXMVECTOR LinePoint1, FXMVECTOR LinePoint2);
void        __vectorcall     XMPlaneIntersectPlane(  XMVECTOR* pLinePoint1,   XMVECTOR* pLinePoint2,   FXMVECTOR P1,   FXMVECTOR P2);
XMVECTOR    __vectorcall     XMPlaneTransform(FXMVECTOR P, FXMMATRIX M);
XMFLOAT4*   __vectorcall     XMPlaneTransformStream(  XMFLOAT4* pOutputStream,
                                                     size_t OutputStride,
                                                     const XMFLOAT4* pInputStream,
                                                     size_t InputStride,   size_t PlaneCount,   FXMMATRIX M);

XMVECTOR    __vectorcall     XMPlaneFromPointNormal(FXMVECTOR Point, FXMVECTOR Normal);
XMVECTOR    __vectorcall     XMPlaneFromPoints(FXMVECTOR Point1, FXMVECTOR Point2, FXMVECTOR Point3);

/****************************************************************************
 *
 * Color operations
 *
 ****************************************************************************/

bool        __vectorcall     XMColorEqual(FXMVECTOR C1, FXMVECTOR C2);
bool        __vectorcall     XMColorNotEqual(FXMVECTOR C1, FXMVECTOR C2);
bool        __vectorcall     XMColorGreater(FXMVECTOR C1, FXMVECTOR C2);
bool        __vectorcall     XMColorGreaterOrEqual(FXMVECTOR C1, FXMVECTOR C2);
bool        __vectorcall     XMColorLess(FXMVECTOR C1, FXMVECTOR C2);
bool        __vectorcall     XMColorLessOrEqual(FXMVECTOR C1, FXMVECTOR C2);

bool        __vectorcall     XMColorIsNaN(FXMVECTOR C);
bool        __vectorcall     XMColorIsInfinite(FXMVECTOR C);

XMVECTOR    __vectorcall     XMColorNegative(FXMVECTOR C);
XMVECTOR    __vectorcall     XMColorModulate(FXMVECTOR C1, FXMVECTOR C2);
XMVECTOR    __vectorcall     XMColorAdjustSaturation(FXMVECTOR C, float Saturation);
XMVECTOR    __vectorcall     XMColorAdjustContrast(FXMVECTOR C, float Contrast);

XMVECTOR    __vectorcall     XMColorRGBToHSL( FXMVECTOR rgb );
XMVECTOR    __vectorcall     XMColorHSLToRGB( FXMVECTOR hsl );

XMVECTOR    __vectorcall     XMColorRGBToHSV( FXMVECTOR rgb );
XMVECTOR    __vectorcall     XMColorHSVToRGB( FXMVECTOR hsv );

XMVECTOR    __vectorcall     XMColorRGBToYUV( FXMVECTOR rgb );
XMVECTOR    __vectorcall     XMColorYUVToRGB( FXMVECTOR yuv );

XMVECTOR    __vectorcall     XMColorRGBToYUV_HD( FXMVECTOR rgb );
XMVECTOR    __vectorcall     XMColorYUVToRGB_HD( FXMVECTOR yuv );

XMVECTOR    __vectorcall     XMColorRGBToXYZ( FXMVECTOR rgb );
XMVECTOR    __vectorcall     XMColorXYZToRGB( FXMVECTOR xyz );

XMVECTOR    __vectorcall     XMColorXYZToSRGB( FXMVECTOR xyz );
XMVECTOR    __vectorcall     XMColorSRGBToXYZ( FXMVECTOR srgb );

XMVECTOR    __vectorcall     XMColorRGBToSRGB( FXMVECTOR rgb );
XMVECTOR    __vectorcall     XMColorSRGBToRGB( FXMVECTOR srgb );


/****************************************************************************
 *
 * Miscellaneous operations
 *
 ****************************************************************************/

bool            XMVerifyCPUSupport();

XMVECTOR    __vectorcall     XMFresnelTerm(FXMVECTOR CosIncidentAngle, FXMVECTOR RefractionIndex);

bool            XMScalarNearEqual(float S1, float S2, float Epsilon);
float           XMScalarModAngle(float Value);

float           XMScalarSin(float Value);
float           XMScalarSinEst(float Value);

float           XMScalarCos(float Value);
float           XMScalarCosEst(float Value);

void            XMScalarSinCos(  float* pSin,   float* pCos, float Value);
void            XMScalarSinCosEst(  float* pSin,   float* pCos, float Value);

float           XMScalarASin(float Value);
float           XMScalarASinEst(float Value);

float           XMScalarACos(float Value);
float           XMScalarACosEst(float Value);

/****************************************************************************
 *
 * Templates
 *
 ****************************************************************************/




#line 1634 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

template<class T> inline T XMMin(T a, T b) { return (a < b) ? a : b; }
template<class T> inline T XMMax(T a, T b) { return (a > b) ? a : b; }

//------------------------------------------------------------------------------



// PermuteHelper internal template (SSE only)
namespace Internal
{
    // Slow path fallback for permutes that do not map to a single SSE shuffle opcode.
    template<uint32_t Shuffle, bool WhichX, bool WhichY, bool WhichZ, bool WhichW> struct PermuteHelper
    {
        static XMVECTOR     __vectorcall     Permute(FXMVECTOR v1, FXMVECTOR v2)
        {
            static const XMVECTORU32 selectMask =
            { { {
                    WhichX ? 0xFFFFFFFF : 0,
                    WhichY ? 0xFFFFFFFF : 0,
                    WhichZ ? 0xFFFFFFFF : 0,
                    WhichW ? 0xFFFFFFFF : 0,
            } } };

            XMVECTOR shuffled1 = _mm_shuffle_ps( v1, v1, Shuffle );
            XMVECTOR shuffled2 = _mm_shuffle_ps( v2, v2, Shuffle );

            XMVECTOR masked1 = _mm_andnot_ps(selectMask, shuffled1);
            XMVECTOR masked2 = _mm_and_ps(selectMask, shuffled2);

            return _mm_or_ps(masked1, masked2);
        }
    };

    // Fast path for permutes that only read from the first vector.
    template<uint32_t Shuffle> struct PermuteHelper<Shuffle, false, false, false, false>
    {
        static XMVECTOR     __vectorcall     Permute(FXMVECTOR v1, FXMVECTOR) { return _mm_shuffle_ps( v1, v1, Shuffle ); }
    };

    // Fast path for permutes that only read from the second vector.
    template<uint32_t Shuffle> struct PermuteHelper<Shuffle, true, true, true, true>
    {
        static XMVECTOR     __vectorcall     Permute(FXMVECTOR, FXMVECTOR v2){ return _mm_shuffle_ps( v2, v2, Shuffle ); }
    };

    // Fast path for permutes that read XY from the first vector, ZW from the second.
    template<uint32_t Shuffle> struct PermuteHelper<Shuffle, false, false, true, true>
    {
        static XMVECTOR     __vectorcall     Permute(FXMVECTOR v1, FXMVECTOR v2) { return _mm_shuffle_ps(v1, v2, Shuffle); }
    };

    // Fast path for permutes that read XY from the second vector, ZW from the first.
    template<uint32_t Shuffle> struct PermuteHelper<Shuffle, true, true, false, false>
    {
        static XMVECTOR     __vectorcall     Permute(FXMVECTOR v1, FXMVECTOR v2) { return _mm_shuffle_ps(v2, v1, Shuffle); }
    };
}

#line 1694 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

// General permute template
template<uint32_t PermuteX, uint32_t PermuteY, uint32_t PermuteZ, uint32_t PermuteW>
    inline XMVECTOR     __vectorcall     XMVectorPermute(FXMVECTOR V1, FXMVECTOR V2)
{
    static_assert(PermuteX <= 7, "PermuteX template parameter out of range");
    static_assert(PermuteY <= 7, "PermuteY template parameter out of range");
    static_assert(PermuteZ <= 7, "PermuteZ template parameter out of range");
    static_assert(PermuteW <= 7, "PermuteW template parameter out of range");


    const uint32_t Shuffle = (((PermuteW & 3) << 6) | ((PermuteZ & 3) << 4) | ((PermuteY & 3) << 2) | ((PermuteX & 3)));

    const bool WhichX = PermuteX > 3;
    const bool WhichY = PermuteY > 3;
    const bool WhichZ = PermuteZ > 3;
    const bool WhichW = PermuteW > 3;

    return Internal::PermuteHelper<Shuffle, WhichX, WhichY, WhichZ, WhichW>::Permute(V1, V2);




#line 1718 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
}

// Special-case permute templates
template<> inline XMVECTOR      __vectorcall     XMVectorPermute<0,1,2,3>(FXMVECTOR V1, FXMVECTOR) { return V1; }
template<> inline XMVECTOR      __vectorcall     XMVectorPermute<4,5,6,7>(FXMVECTOR, FXMVECTOR V2) { return V2; }


template<> inline XMVECTOR      __vectorcall     XMVectorPermute<0,1,4,5>(FXMVECTOR V1, FXMVECTOR V2) { return _mm_movelh_ps(V1,V2); }
template<> inline XMVECTOR      __vectorcall     XMVectorPermute<6,7,2,3>(FXMVECTOR V1, FXMVECTOR V2) { return _mm_movehl_ps(V1,V2); }
template<> inline XMVECTOR      __vectorcall     XMVectorPermute<0,4,1,5>(FXMVECTOR V1, FXMVECTOR V2) { return _mm_unpacklo_ps(V1,V2); }
template<> inline XMVECTOR      __vectorcall     XMVectorPermute<2,6,3,7>(FXMVECTOR V1, FXMVECTOR V2) { return _mm_unpackhi_ps(V1,V2); }
template<> inline XMVECTOR      __vectorcall     XMVectorPermute<2,3,6,7>(FXMVECTOR V1, FXMVECTOR V2) { return _mm_castpd_ps(_mm_unpackhi_pd(_mm_castps_pd(V1), _mm_castps_pd(V2))); }
#line 1731 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
















#line 1748 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"







































#line 1788 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

//------------------------------------------------------------------------------

// General swizzle template
template<uint32_t SwizzleX, uint32_t SwizzleY, uint32_t SwizzleZ, uint32_t SwizzleW>
    inline XMVECTOR     __vectorcall     XMVectorSwizzle(FXMVECTOR V)
{
    static_assert(SwizzleX <= 3, "SwizzleX template parameter out of range");
    static_assert(SwizzleY <= 3, "SwizzleY template parameter out of range");
    static_assert(SwizzleZ <= 3, "SwizzleZ template parameter out of range");
    static_assert(SwizzleW <= 3, "SwizzleW template parameter out of range");


    return _mm_shuffle_ps( V, V, (((SwizzleW) << 6) | ((SwizzleZ) << 4) | ((SwizzleY) << 2) | ((SwizzleX))) );




#line 1807 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
}

// Specialized swizzles
template<> inline XMVECTOR      __vectorcall     XMVectorSwizzle<0,1,2,3>(FXMVECTOR V) { return V; }


template<> inline XMVECTOR      __vectorcall     XMVectorSwizzle<0,1,0,1>(FXMVECTOR V) { return _mm_movelh_ps(V,V); }
template<> inline XMVECTOR      __vectorcall     XMVectorSwizzle<2,3,2,3>(FXMVECTOR V) { return _mm_movehl_ps(V,V); }
template<> inline XMVECTOR      __vectorcall     XMVectorSwizzle<0,0,1,1>(FXMVECTOR V) { return _mm_unpacklo_ps(V,V); }
template<> inline XMVECTOR      __vectorcall     XMVectorSwizzle<2,2,3,3>(FXMVECTOR V) { return _mm_unpackhi_ps(V,V); }
#line 1818 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"




#line 1823 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"



#line 1827 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"


































#line 1862 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

//------------------------------------------------------------------------------

template<uint32_t Elements>
    inline XMVECTOR     __vectorcall     XMVectorShiftLeft(FXMVECTOR V1, FXMVECTOR V2)
{
    static_assert( Elements < 4, "Elements template parameter out of range" );
    return XMVectorPermute<Elements, (Elements + 1), (Elements + 2), (Elements + 3)>(V1, V2);
}

template<uint32_t Elements>
    inline XMVECTOR     __vectorcall     XMVectorRotateLeft(FXMVECTOR V)
{
    static_assert( Elements < 4, "Elements template parameter out of range" );
    return XMVectorSwizzle<Elements & 3, (Elements + 1) & 3, (Elements + 2) & 3, (Elements + 3) & 3>(V);
}

template<uint32_t Elements>
    inline XMVECTOR     __vectorcall     XMVectorRotateRight(FXMVECTOR V)
{
    static_assert( Elements < 4, "Elements template parameter out of range" );
    return XMVectorSwizzle<(4 - Elements) & 3, (5 - Elements) & 3, (6 - Elements) & 3, (7 - Elements) & 3>(V);
}

template<uint32_t VSLeftRotateElements, uint32_t Select0, uint32_t Select1, uint32_t Select2, uint32_t Select3>
    inline XMVECTOR     __vectorcall     XMVectorInsert(FXMVECTOR VD, FXMVECTOR VS)
{
    XMVECTOR Control = XMVectorSelectControl(Select0&1, Select1&1, Select2&1, Select3&1);
    return XMVectorSelect( VD, XMVectorRotateLeft<VSLeftRotateElements>(VS), Control );
}

/****************************************************************************
 *
 * Globals
 *
 ****************************************************************************/

// The purpose of the following global constants is to prevent redundant
// reloading of the constants when they are referenced by more than one
// separate inline math routine called within the same function.  Declaring
// a constant locally within a routine is sufficient to prevent redundant
// reloads of that constant when that single routine is called multiple
// times in a function, but if the constant is used (and declared) in a
// separate math routine it would be reloaded.



#line 1910 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"

extern const __declspec(selectany) XMVECTORF32 g_XMSinCoefficients0      = { { { -0.16666667f, +0.0083333310f, -0.00019840874f, +2.7525562e-06f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMSinCoefficients1      = { { { -2.3889859e-08f, -0.16665852f /*Est1*/, +0.0083139502f /*Est2*/, -0.00018524670f /*Est3*/ } } };
extern const __declspec(selectany) XMVECTORF32 g_XMCosCoefficients0      = { { { -0.5f, +0.041666638f, -0.0013888378f, +2.4760495e-05f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMCosCoefficients1      = { { { -2.6051615e-07f, -0.49992746f /*Est1*/, +0.041493919f /*Est2*/, -0.0012712436f /*Est3*/ } } };
extern const __declspec(selectany) XMVECTORF32 g_XMTanCoefficients0      = { { { 1.0f, 0.333333333f, 0.133333333f, 5.396825397e-2f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMTanCoefficients1      = { { { 2.186948854e-2f, 8.863235530e-3f, 3.592128167e-3f, 1.455834485e-3f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMTanCoefficients2      = { { { 5.900274264e-4f, 2.391290764e-4f, 9.691537707e-5f, 3.927832950e-5f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMArcCoefficients0      = { { { +1.5707963050f, -0.2145988016f, +0.0889789874f, -0.0501743046f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMArcCoefficients1      = { { { +0.0308918810f, -0.0170881256f, +0.0066700901f, -0.0012624911f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMATanCoefficients0     = { { { -0.3333314528f, +0.1999355085f, -0.1420889944f, +0.1065626393f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMATanCoefficients1     = { { { -0.0752896400f, +0.0429096138f, -0.0161657367f, +0.0028662257f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMATanEstCoefficients0  = { { { +0.999866f, +0.999866f, +0.999866f, +0.999866f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMATanEstCoefficients1  = { { { -0.3302995f, +0.180141f, -0.085133f, +0.0208351f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMTanEstCoefficients    = { { { 2.484f, -1.954923183e-1f, 2.467401101f, XM_1DIVPI } } };
extern const __declspec(selectany) XMVECTORF32 g_XMArcEstCoefficients    = { { { +1.5707288f, -0.2121144f, +0.0742610f, -0.0187293f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMPiConstants0          = { { { XM_PI, XM_2PI, XM_1DIVPI, XM_1DIV2PI } } };
extern const __declspec(selectany) XMVECTORF32 g_XMIdentityR0            = { { { 1.0f, 0.0f, 0.0f, 0.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMIdentityR1            = { { { 0.0f, 1.0f, 0.0f, 0.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMIdentityR2            = { { { 0.0f, 0.0f, 1.0f, 0.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMIdentityR3            = { { { 0.0f, 0.0f, 0.0f, 1.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNegIdentityR0         = { { { -1.0f, 0.0f, 0.0f, 0.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNegIdentityR1         = { { { 0.0f, -1.0f, 0.0f, 0.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNegIdentityR2         = { { { 0.0f, 0.0f, -1.0f, 0.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNegIdentityR3         = { { { 0.0f, 0.0f, 0.0f, -1.0f } } };
extern const __declspec(selectany) XMVECTORU32 g_XMNegativeZero          = { { { 0x80000000, 0x80000000, 0x80000000, 0x80000000 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMNegate3               = { { { 0x80000000, 0x80000000, 0x80000000, 0x00000000 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMMaskXY                = { { { 0xFFFFFFFF, 0xFFFFFFFF, 0x00000000, 0x00000000 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMMask3                 = { { { 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0x00000000 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMMaskX                 = { { { 0xFFFFFFFF, 0x00000000, 0x00000000, 0x00000000 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMMaskY                 = { { { 0x00000000, 0xFFFFFFFF, 0x00000000, 0x00000000 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMMaskZ                 = { { { 0x00000000, 0x00000000, 0xFFFFFFFF, 0x00000000 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMMaskW                 = { { { 0x00000000, 0x00000000, 0x00000000, 0xFFFFFFFF } } };
extern const __declspec(selectany) XMVECTORF32 g_XMOne                   = { { { 1.0f, 1.0f, 1.0f, 1.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMOne3                  = { { { 1.0f, 1.0f, 1.0f, 0.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMZero                  = { { { 0.0f, 0.0f, 0.0f, 0.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMTwo                   = { { { 2.f, 2.f, 2.f, 2.f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMFour                  = { { { 4.f, 4.f, 4.f, 4.f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMSix                   = { { { 6.f, 6.f, 6.f, 6.f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNegativeOne           = { { { -1.0f, -1.0f, -1.0f, -1.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMOneHalf               = { { { 0.5f, 0.5f, 0.5f, 0.5f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNegativeOneHalf       = { { { -0.5f, -0.5f, -0.5f, -0.5f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNegativeTwoPi         = { { { -XM_2PI, -XM_2PI, -XM_2PI, -XM_2PI } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNegativePi            = { { { -XM_PI, -XM_PI, -XM_PI, -XM_PI } } };
extern const __declspec(selectany) XMVECTORF32 g_XMHalfPi                = { { { XM_PIDIV2, XM_PIDIV2, XM_PIDIV2, XM_PIDIV2 } } };
extern const __declspec(selectany) XMVECTORF32 g_XMPi                    = { { { XM_PI, XM_PI, XM_PI, XM_PI } } };
extern const __declspec(selectany) XMVECTORF32 g_XMReciprocalPi          = { { { XM_1DIVPI, XM_1DIVPI, XM_1DIVPI, XM_1DIVPI } } };
extern const __declspec(selectany) XMVECTORF32 g_XMTwoPi                 = { { { XM_2PI, XM_2PI, XM_2PI, XM_2PI } } };
extern const __declspec(selectany) XMVECTORF32 g_XMReciprocalTwoPi       = { { { XM_1DIV2PI, XM_1DIV2PI, XM_1DIV2PI, XM_1DIV2PI } } };
extern const __declspec(selectany) XMVECTORF32 g_XMEpsilon               = { { { 1.192092896e-7f, 1.192092896e-7f, 1.192092896e-7f, 1.192092896e-7f } } };
extern const __declspec(selectany) XMVECTORI32 g_XMInfinity              = { { { 0x7F800000, 0x7F800000, 0x7F800000, 0x7F800000 } } };
extern const __declspec(selectany) XMVECTORI32 g_XMQNaN                  = { { { 0x7FC00000, 0x7FC00000, 0x7FC00000, 0x7FC00000 } } };
extern const __declspec(selectany) XMVECTORI32 g_XMQNaNTest              = { { { 0x007FFFFF, 0x007FFFFF, 0x007FFFFF, 0x007FFFFF } } };
extern const __declspec(selectany) XMVECTORI32 g_XMAbsMask               = { { { 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF } } };
extern const __declspec(selectany) XMVECTORI32 g_XMFltMin                = { { { 0x00800000, 0x00800000, 0x00800000, 0x00800000 } } };
extern const __declspec(selectany) XMVECTORI32 g_XMFltMax                = { { { 0x7F7FFFFF, 0x7F7FFFFF, 0x7F7FFFFF, 0x7F7FFFFF } } };
extern const __declspec(selectany) XMVECTORU32 g_XMNegOneMask            = { { { 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF } } };
extern const __declspec(selectany) XMVECTORU32 g_XMMaskA8R8G8B8          = { { { 0x00FF0000, 0x0000FF00, 0x000000FF, 0xFF000000 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMFlipA8R8G8B8          = { { { 0x00000000, 0x00000000, 0x00000000, 0x80000000 } } };
extern const __declspec(selectany) XMVECTORF32 g_XMFixAA8R8G8B8          = { { { 0.0f, 0.0f, 0.0f, float(0x80000000U) } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNormalizeA8R8G8B8     = { { { 1.0f / (255.0f*float(0x10000)), 1.0f / (255.0f*float(0x100)), 1.0f / 255.0f, 1.0f / (255.0f*float(0x1000000)) } } };
extern const __declspec(selectany) XMVECTORU32 g_XMMaskA2B10G10R10       = { { { 0x000003FF, 0x000FFC00, 0x3FF00000, 0xC0000000 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMFlipA2B10G10R10       = { { { 0x00000200, 0x00080000, 0x20000000, 0x80000000 } } };
extern const __declspec(selectany) XMVECTORF32 g_XMFixAA2B10G10R10       = { { { -512.0f, -512.0f*float(0x400), -512.0f*float(0x100000), float(0x80000000U) } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNormalizeA2B10G10R10  = { { { 1.0f / 511.0f, 1.0f / (511.0f*float(0x400)), 1.0f / (511.0f*float(0x100000)), 1.0f / (3.0f*float(0x40000000)) } } };
extern const __declspec(selectany) XMVECTORU32 g_XMMaskX16Y16            = { { { 0x0000FFFF, 0xFFFF0000, 0x00000000, 0x00000000 } } };
extern const __declspec(selectany) XMVECTORI32 g_XMFlipX16Y16            = { { { 0x00008000, 0x00000000, 0x00000000, 0x00000000 } } };
extern const __declspec(selectany) XMVECTORF32 g_XMFixX16Y16             = { { { -32768.0f, 0.0f, 0.0f, 0.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNormalizeX16Y16       = { { { 1.0f / 32767.0f, 1.0f / (32767.0f*65536.0f), 0.0f, 0.0f } } };
extern const __declspec(selectany) XMVECTORU32 g_XMMaskX16Y16Z16W16      = { { { 0x0000FFFF, 0x0000FFFF, 0xFFFF0000, 0xFFFF0000 } } };
extern const __declspec(selectany) XMVECTORI32 g_XMFlipX16Y16Z16W16      = { { { 0x00008000, 0x00008000, 0x00000000, 0x00000000 } } };
extern const __declspec(selectany) XMVECTORF32 g_XMFixX16Y16Z16W16       = { { { -32768.0f, -32768.0f, 0.0f, 0.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNormalizeX16Y16Z16W16 = { { { 1.0f / 32767.0f, 1.0f / 32767.0f, 1.0f / (32767.0f*65536.0f), 1.0f / (32767.0f*65536.0f) } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNoFraction            = { { { 8388608.0f, 8388608.0f, 8388608.0f, 8388608.0f } } };
extern const __declspec(selectany) XMVECTORI32 g_XMMaskByte              = { { { 0x000000FF, 0x000000FF, 0x000000FF, 0x000000FF } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNegateX               = { { { -1.0f, 1.0f, 1.0f, 1.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNegateY               = { { { 1.0f, -1.0f, 1.0f, 1.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNegateZ               = { { { 1.0f, 1.0f, -1.0f, 1.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMNegateW               = { { { 1.0f, 1.0f, 1.0f, -1.0f } } };
extern const __declspec(selectany) XMVECTORU32 g_XMSelect0101            = { { { XM_SELECT_0, XM_SELECT_1, XM_SELECT_0, XM_SELECT_1 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMSelect1010            = { { { XM_SELECT_1, XM_SELECT_0, XM_SELECT_1, XM_SELECT_0 } } };
extern const __declspec(selectany) XMVECTORI32 g_XMOneHalfMinusEpsilon   = { { { 0x3EFFFFFD, 0x3EFFFFFD, 0x3EFFFFFD, 0x3EFFFFFD } } };
extern const __declspec(selectany) XMVECTORU32 g_XMSelect1000            = { { { XM_SELECT_1, XM_SELECT_0, XM_SELECT_0, XM_SELECT_0 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMSelect1100            = { { { XM_SELECT_1, XM_SELECT_1, XM_SELECT_0, XM_SELECT_0 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMSelect1110            = { { { XM_SELECT_1, XM_SELECT_1, XM_SELECT_1, XM_SELECT_0 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMSelect1011            = { { { XM_SELECT_1, XM_SELECT_0, XM_SELECT_1, XM_SELECT_1 } } };
extern const __declspec(selectany) XMVECTORF32 g_XMFixupY16              = { { { 1.0f, 1.0f / 65536.0f, 0.0f, 0.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMFixupY16W16           = { { { 1.0f, 1.0f, 1.0f / 65536.0f, 1.0f / 65536.0f } } };
extern const __declspec(selectany) XMVECTORU32 g_XMFlipY                 = { { { 0, 0x80000000, 0, 0 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMFlipZ                 = { { { 0, 0, 0x80000000, 0 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMFlipW                 = { { { 0, 0, 0, 0x80000000 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMFlipYZ                = { { { 0, 0x80000000, 0x80000000, 0 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMFlipZW                = { { { 0, 0, 0x80000000, 0x80000000 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMFlipYW                = { { { 0, 0x80000000, 0, 0x80000000 } } };
extern const __declspec(selectany) XMVECTORI32 g_XMMaskDec4              = { { { 0x3FF, 0x3FF << 10, 0x3FF << 20, static_cast<int>(0xC0000000) } } };
extern const __declspec(selectany) XMVECTORI32 g_XMXorDec4               = { { { 0x200, 0x200 << 10, 0x200 << 20, 0 } } };
extern const __declspec(selectany) XMVECTORF32 g_XMAddUDec4              = { { { 0, 0, 0, 32768.0f*65536.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMAddDec4               = { { { -512.0f, -512.0f*1024.0f, -512.0f*1024.0f*1024.0f, 0 } } };
extern const __declspec(selectany) XMVECTORF32 g_XMMulDec4               = { { { 1.0f, 1.0f / 1024.0f, 1.0f / (1024.0f*1024.0f), 1.0f / (1024.0f*1024.0f*1024.0f) } } };
extern const __declspec(selectany) XMVECTORU32 g_XMMaskByte4             = { { { 0xFF, 0xFF00, 0xFF0000, 0xFF000000 } } };
extern const __declspec(selectany) XMVECTORI32 g_XMXorByte4              = { { { 0x80, 0x8000, 0x800000, 0x00000000 } } };
extern const __declspec(selectany) XMVECTORF32 g_XMAddByte4              = { { { -128.0f, -128.0f*256.0f, -128.0f*65536.0f, 0 } } };
extern const __declspec(selectany) XMVECTORF32 g_XMFixUnsigned           = { { { 32768.0f*65536.0f, 32768.0f*65536.0f, 32768.0f*65536.0f, 32768.0f*65536.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMMaxInt                = { { { 65536.0f*32768.0f - 128.0f, 65536.0f*32768.0f - 128.0f, 65536.0f*32768.0f - 128.0f, 65536.0f*32768.0f - 128.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMMaxUInt               = { { { 65536.0f*65536.0f - 256.0f, 65536.0f*65536.0f - 256.0f, 65536.0f*65536.0f - 256.0f, 65536.0f*65536.0f - 256.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMUnsignedFix           = { { { 32768.0f*65536.0f, 32768.0f*65536.0f, 32768.0f*65536.0f, 32768.0f*65536.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMsrgbScale             = { { { 12.92f, 12.92f, 12.92f, 1.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMsrgbA                 = { { { 0.055f, 0.055f, 0.055f, 0.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMsrgbA1                = { { { 1.055f, 1.055f, 1.055f, 1.0f } } };
extern const __declspec(selectany) XMVECTORI32 g_XMExponentBias          = { { { 127, 127, 127, 127 } } };
extern const __declspec(selectany) XMVECTORI32 g_XMSubnormalExponent     = { { { -126, -126, -126, -126 } } };
extern const __declspec(selectany) XMVECTORI32 g_XMNumTrailing           = { { { 23, 23, 23, 23 } } };
extern const __declspec(selectany) XMVECTORI32 g_XMMinNormal             = { { { 0x00800000, 0x00800000, 0x00800000, 0x00800000 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMNegInfinity           = { { { 0xFF800000, 0xFF800000, 0xFF800000, 0xFF800000 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMNegQNaN               = { { { 0xFFC00000, 0xFFC00000, 0xFFC00000, 0xFFC00000 } } };
extern const __declspec(selectany) XMVECTORI32 g_XMBin128                = { { { 0x43000000, 0x43000000, 0x43000000, 0x43000000 } } };
extern const __declspec(selectany) XMVECTORU32 g_XMBinNeg150             = { { { 0xC3160000, 0xC3160000, 0xC3160000, 0xC3160000 } } };
extern const __declspec(selectany) XMVECTORI32 g_XM253                   = { { { 253, 253, 253, 253 } } };
extern const __declspec(selectany) XMVECTORF32 g_XMExpEst1               = { { { -6.93147182e-1f, -6.93147182e-1f, -6.93147182e-1f, -6.93147182e-1f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMExpEst2               = { { { +2.40226462e-1f, +2.40226462e-1f, +2.40226462e-1f, +2.40226462e-1f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMExpEst3               = { { { -5.55036440e-2f, -5.55036440e-2f, -5.55036440e-2f, -5.55036440e-2f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMExpEst4               = { { { +9.61597636e-3f, +9.61597636e-3f, +9.61597636e-3f, +9.61597636e-3f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMExpEst5               = { { { -1.32823968e-3f, -1.32823968e-3f, -1.32823968e-3f, -1.32823968e-3f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMExpEst6               = { { { +1.47491097e-4f, +1.47491097e-4f, +1.47491097e-4f, +1.47491097e-4f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMExpEst7               = { { { -1.08635004e-5f, -1.08635004e-5f, -1.08635004e-5f, -1.08635004e-5f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMLogEst0               = { { { +1.442693f, +1.442693f, +1.442693f, +1.442693f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMLogEst1               = { { { -0.721242f, -0.721242f, -0.721242f, -0.721242f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMLogEst2               = { { { +0.479384f, +0.479384f, +0.479384f, +0.479384f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMLogEst3               = { { { -0.350295f, -0.350295f, -0.350295f, -0.350295f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMLogEst4               = { { { +0.248590f, +0.248590f, +0.248590f, +0.248590f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMLogEst5               = { { { -0.145700f, -0.145700f, -0.145700f, -0.145700f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMLogEst6               = { { { +0.057148f, +0.057148f, +0.057148f, +0.057148f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMLogEst7               = { { { -0.010578f, -0.010578f, -0.010578f, -0.010578f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMLgE                   = { { { +1.442695f, +1.442695f, +1.442695f, +1.442695f } } };
extern const __declspec(selectany) XMVECTORF32 g_XMInvLgE                = { { { +6.93147182e-1f, +6.93147182e-1f, +6.93147182e-1f, +6.93147182e-1f } } };
extern const __declspec(selectany) XMVECTORF32 g_UByteMax                = { { { 255.0f, 255.0f, 255.0f, 255.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_ByteMin                 = { { { -127.0f, -127.0f, -127.0f, -127.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_ByteMax                 = { { { 127.0f, 127.0f, 127.0f, 127.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_ShortMin                = { { { -32767.0f, -32767.0f, -32767.0f, -32767.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_ShortMax                = { { { 32767.0f, 32767.0f, 32767.0f, 32767.0f } } };
extern const __declspec(selectany) XMVECTORF32 g_UShortMax               = { { { 65535.0f, 65535.0f, 65535.0f, 65535.0f } } };

/****************************************************************************
 *
 * Implementation
 *
 ****************************************************************************/

#pragma warning(push)
#pragma warning(disable:4068 4214 4204 4365 4616 4640 6001 6101)
// C4068/4616: ignore unknown pragmas
// C4214/4204: nonstandard extension used
// C4365/4640: Off by default noise
// C6001/6101: False positives







//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorSetBinaryConstant(uint32_t C0, uint32_t C1, uint32_t C2, uint32_t C3)
{







#line 2083 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"






#line 2090 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
    static const XMVECTORU32 g_vMask1 = { { { 1, 1, 1, 1 } } };
    // Move the parms to a vector
    __m128i vTemp = _mm_set_epi32(static_cast<int>(C3), static_cast<int>(C2), static_cast<int>(C1), static_cast<int>(C0));
    // Mask off the low bits
    vTemp = _mm_and_si128(vTemp,g_vMask1);
    // 0xFFFFFFFF on true bits
    vTemp = _mm_cmpeq_epi32(vTemp,g_vMask1);
    // 0xFFFFFFFF -> 1.0f, 0x00000000 -> 0.0f
    vTemp = _mm_and_si128(vTemp,g_XMOne);
    return _mm_castsi128_ps(vTemp);
#line 2101 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorSplatConstant(int32_t IntConstant, uint32_t DivExponent)
{
    (void)( (!!(IntConstant >= -16 && IntConstant <= 15)) || (_wassert(L"IntConstant >= -16 && IntConstant <= 15", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h", (unsigned)(2107)), 0) );
    (void)( (!!(DivExponent < 32)) || (_wassert(L"DivExponent < 32", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h", (unsigned)(2108)), 0) );







#line 2117 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"











#line 2129 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
    // Splat the int
    __m128i vScale = _mm_set1_epi32(IntConstant);
    // Convert to a float
    XMVECTOR vResult = _mm_cvtepi32_ps(vScale);
    // Convert DivExponent into 1.0f/(1<<DivExponent)
    uint32_t uScale = 0x3F800000U - (DivExponent << 23);
    // Splat the scalar value (It's really a float)
    vScale = _mm_set1_epi32(static_cast<int>(uScale));
    // Multiply by the reciprocal (Perform a right shift by DivExponent)
    vResult = _mm_mul_ps(vResult,_mm_castsi128_ps(vScale));
    return vResult;
#line 2141 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorSplatConstantInt(int32_t IntConstant)
{
    (void)( (!!(IntConstant >= -16 && IntConstant <= 15)) || (_wassert(L"IntConstant >= -16 && IntConstant <= 15", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h", (unsigned)(2147)), 0) );





#line 2154 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"


#line 2157 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
    __m128i V = _mm_set1_epi32( IntConstant );
    return _mm_castsi128_ps(V);
#line 2160 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
}

#pragma external_header(push)
#line 1 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
//-------------------------------------------------------------------------------------
// DirectXMathConvert.inl -- SIMD C++ Math library
//
// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
//
// http://go.microsoft.com/fwlink/?LinkID=615560
//-------------------------------------------------------------------------------------

#pragma once

/****************************************************************************
 *
 * Data conversion
 *
 ****************************************************************************/

//------------------------------------------------------------------------------

#pragma warning(push)
#pragma warning(disable:4701)
// C4701: false positives

inline XMVECTOR __vectorcall XMConvertVectorIntToFloat
(
    FXMVECTOR    VInt,
    uint32_t     DivExponent
)
{
    (void)( (!!(DivExponent<32)) || (_wassert(L"DivExponent<32", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(30)), 0) );









#line 41 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"



#line 45 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    // Convert to floats
    XMVECTOR vResult = _mm_cvtepi32_ps(_mm_castps_si128(VInt));
    // Convert DivExponent into 1.0f/(1<<DivExponent)
    uint32_t uScale = 0x3F800000U - (DivExponent << 23);
    // Splat the scalar value
    __m128i vScale = _mm_set1_epi32(static_cast<int>(uScale));
    vResult = _mm_mul_ps(vResult,_mm_castsi128_ps(vScale));
    return vResult;
#line 54 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMConvertVectorFloatToInt
(
    FXMVECTOR    VFloat,
    uint32_t     MulExponent
)
{
    (void)( (!!(MulExponent<32)) || (_wassert(L"MulExponent<32", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(64)), 0) );


















#line 84 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"










#line 95 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    XMVECTOR vResult = _mm_set_ps1(static_cast<float>(1U << MulExponent));
    vResult = _mm_mul_ps(vResult,VFloat);
    // In case of positive overflow, detect it
    XMVECTOR vOverflow = _mm_cmpgt_ps(vResult,g_XMMaxInt);
    // Float to int conversion
    __m128i vResulti = _mm_cvttps_epi32(vResult);
    // If there was positive overflow, set to 0x7FFFFFFF
    vResult = _mm_and_ps(vOverflow,g_XMAbsMask);
    vOverflow = _mm_andnot_ps(vOverflow,_mm_castsi128_ps(vResulti));
    vOverflow = _mm_or_ps(vOverflow,vResult);
    return vOverflow;
#line 107 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMConvertVectorUIntToFloat
(
    FXMVECTOR     VUInt,
    uint32_t      DivExponent
)
{
    (void)( (!!(DivExponent<32)) || (_wassert(L"DivExponent<32", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(117)), 0) );








#line 127 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"



#line 131 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    // For the values that are higher than 0x7FFFFFFF, a fixup is needed
    // Determine which ones need the fix.
    XMVECTOR vMask = _mm_and_ps(VUInt,g_XMNegativeZero);
    // Force all values positive
    XMVECTOR vResult = _mm_xor_ps(VUInt,vMask);
    // Convert to floats
    vResult = _mm_cvtepi32_ps(_mm_castps_si128(vResult));
    // Convert 0x80000000 -> 0xFFFFFFFF
    __m128i iMask = _mm_srai_epi32(_mm_castps_si128(vMask),31);
    // For only the ones that are too big, add the fixup
    vMask = _mm_and_ps(_mm_castsi128_ps(iMask),g_XMFixUnsigned);
    vResult = _mm_add_ps(vResult,vMask);
    // Convert DivExponent into 1.0f/(1<<DivExponent)
    uint32_t uScale = 0x3F800000U - (DivExponent << 23);
    // Splat
    iMask = _mm_set1_epi32(static_cast<int>(uScale));
    vResult = _mm_mul_ps(vResult,_mm_castsi128_ps(iMask));
    return vResult;
#line 150 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMConvertVectorFloatToUInt
(
    FXMVECTOR     VFloat,
    uint32_t      MulExponent
)
{
    (void)( (!!(MulExponent<32)) || (_wassert(L"MulExponent<32", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(160)), 0) );


















#line 180 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"









#line 190 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    XMVECTOR vResult = _mm_set_ps1(static_cast<float>(1U << MulExponent));
    vResult = _mm_mul_ps(vResult,VFloat);
    // Clamp to >=0
    vResult = _mm_max_ps(vResult,g_XMZero);
    // Any numbers that are too big, set to 0xFFFFFFFFU
    XMVECTOR vOverflow = _mm_cmpgt_ps(vResult,g_XMMaxUInt);
    XMVECTOR vValue = g_XMUnsignedFix;
    // Too large for a signed integer?
    XMVECTOR vMask = _mm_cmpge_ps(vResult,vValue);
    // Zero for number's lower than 0x80000000, 32768.0f*65536.0f otherwise
    vValue = _mm_and_ps(vValue,vMask);
    // Perform fixup only on numbers too large (Keeps low bit precision)
    vResult = _mm_sub_ps(vResult,vValue);
    __m128i vResulti = _mm_cvttps_epi32(vResult);
    // Convert from signed to unsigned pnly if greater than 0x80000000
    vMask = _mm_and_ps(vMask,g_XMNegativeZero);
    vResult = _mm_xor_ps(_mm_castsi128_ps(vResulti),vMask);
    // On those that are too large, set to 0xFFFFFFFF
    vResult = _mm_or_ps(vResult,vOverflow);
    return vResult;
#line 211 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

#pragma warning(pop)

/****************************************************************************
 *
 * Vector and matrix load operations
 *
 ****************************************************************************/

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadInt(const uint32_t* pSource)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(225)), 0) );







#line 234 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"


#line 237 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    return _mm_load_ss( reinterpret_cast<const float*>(pSource) );
#line 239 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadFloat(const float* pSource)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(245)), 0) );







#line 254 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"


#line 257 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    return _mm_load_ss( pSource );
#line 259 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadInt2
(
    const uint32_t* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(268)), 0) );







#line 277 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"



#line 281 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    __m128 x = _mm_load_ss( reinterpret_cast<const float*>(pSource) );
    __m128 y = _mm_load_ss( reinterpret_cast<const float*>(pSource+1) );
    return _mm_unpacklo_ps( x, y );
#line 285 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadInt2A
(
    const uint32_t* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(294)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(295)), 0) );







#line 304 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"



#line 308 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    __m128i V = _mm_loadl_epi64( reinterpret_cast<const __m128i*>(pSource) );
    return _mm_castsi128_ps(V);
#line 311 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadFloat2
(
    const XMFLOAT2* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(320)), 0) );







#line 329 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"



#line 333 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    __m128 x = _mm_load_ss( &pSource->x );
    __m128 y = _mm_load_ss( &pSource->y );
    return _mm_unpacklo_ps( x, y );
#line 337 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadFloat2A
(
    const XMFLOAT2A* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(346)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(347)), 0) );







#line 356 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"



#line 360 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    __m128i V = _mm_loadl_epi64( reinterpret_cast<const __m128i*>(pSource) );
    return _mm_castsi128_ps(V);
#line 363 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadSInt2
(
    const XMINT2* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(372)), 0) );







#line 381 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"




#line 386 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    __m128 x = _mm_load_ss( reinterpret_cast<const float*>(&pSource->x) );
    __m128 y = _mm_load_ss( reinterpret_cast<const float*>(&pSource->y) );
    __m128 V = _mm_unpacklo_ps( x, y );
    return _mm_cvtepi32_ps(_mm_castps_si128(V));
#line 391 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadUInt2
(
    const XMUINT2* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(400)), 0) );







#line 409 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"




#line 414 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    __m128 x = _mm_load_ss( reinterpret_cast<const float*>(&pSource->x) );
    __m128 y = _mm_load_ss( reinterpret_cast<const float*>(&pSource->y) );
    __m128 V = _mm_unpacklo_ps( x, y );
    // For the values that are higher than 0x7FFFFFFF, a fixup is needed
    // Determine which ones need the fix.
    XMVECTOR vMask = _mm_and_ps(V,g_XMNegativeZero);
    // Force all values positive
    XMVECTOR vResult = _mm_xor_ps(V,vMask);
    // Convert to floats
    vResult = _mm_cvtepi32_ps(_mm_castps_si128(vResult));
    // Convert 0x80000000 -> 0xFFFFFFFF
    __m128i iMask = _mm_srai_epi32(_mm_castps_si128(vMask),31);
    // For only the ones that are too big, add the fixup
    vMask = _mm_and_ps(_mm_castsi128_ps(iMask),g_XMFixUnsigned);
    vResult = _mm_add_ps(vResult,vMask);
    return vResult;
#line 431 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadInt3
(
    const uint32_t* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(440)), 0) );







#line 449 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"




#line 454 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    __m128 x = _mm_load_ss( reinterpret_cast<const float*>(pSource) );
    __m128 y = _mm_load_ss( reinterpret_cast<const float*>(pSource+1) );
    __m128 z = _mm_load_ss( reinterpret_cast<const float*>(pSource+2) );
    __m128 xy = _mm_unpacklo_ps( x, y );
    return _mm_movelh_ps( xy, z );
#line 460 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadInt3A
(
    const uint32_t* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(469)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(470)), 0) );







#line 479 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"



#line 483 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    // Reads an extra integer which is zero'd
    __m128i V = _mm_load_si128( reinterpret_cast<const __m128i*>(pSource) );
    V = _mm_and_si128( V, g_XMMask3 );
    return _mm_castsi128_ps(V);
#line 488 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadFloat3
(
    const XMFLOAT3* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(497)), 0) );







#line 506 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"




#line 511 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    __m128 x = _mm_load_ss( &pSource->x );
    __m128 y = _mm_load_ss( &pSource->y );
    __m128 z = _mm_load_ss( &pSource->z );
    __m128 xy = _mm_unpacklo_ps( x, y );
    return _mm_movelh_ps( xy, z );
#line 517 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadFloat3A
(
    const XMFLOAT3A* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(526)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(527)), 0) );







#line 536 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"



#line 540 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    // Reads an extra float which is zero'd
    __m128 V = _mm_load_ps( &pSource->x );
    return _mm_and_ps( V, g_XMMask3 );
#line 544 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadSInt3
(
    const XMINT3* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(553)), 0) );









#line 564 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"





#line 570 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    __m128 x = _mm_load_ss( reinterpret_cast<const float*>(&pSource->x) );
    __m128 y = _mm_load_ss( reinterpret_cast<const float*>(&pSource->y) );
    __m128 z = _mm_load_ss( reinterpret_cast<const float*>(&pSource->z) );
    __m128 xy = _mm_unpacklo_ps( x, y );
    __m128 V = _mm_movelh_ps( xy, z );
    return _mm_cvtepi32_ps(_mm_castps_si128(V));
#line 577 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadUInt3
(
    const XMUINT3* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(586)), 0) );







#line 595 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"





#line 601 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    __m128 x = _mm_load_ss( reinterpret_cast<const float*>(&pSource->x) );
    __m128 y = _mm_load_ss( reinterpret_cast<const float*>(&pSource->y) );
    __m128 z = _mm_load_ss( reinterpret_cast<const float*>(&pSource->z) );
    __m128 xy = _mm_unpacklo_ps( x, y );
    __m128 V = _mm_movelh_ps( xy, z );
    // For the values that are higher than 0x7FFFFFFF, a fixup is needed
    // Determine which ones need the fix.
    XMVECTOR vMask = _mm_and_ps(V,g_XMNegativeZero);
    // Force all values positive
    XMVECTOR vResult = _mm_xor_ps(V,vMask);
    // Convert to floats
    vResult = _mm_cvtepi32_ps(_mm_castps_si128(vResult));
    // Convert 0x80000000 -> 0xFFFFFFFF
    __m128i iMask = _mm_srai_epi32(_mm_castps_si128(vMask),31);
    // For only the ones that are too big, add the fixup
    vMask = _mm_and_ps(_mm_castsi128_ps(iMask),g_XMFixUnsigned);
    vResult = _mm_add_ps(vResult,vMask);
    return vResult;
#line 620 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadInt4
(
    const uint32_t* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(629)), 0) );








#line 639 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"

#line 641 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    __m128i V = _mm_loadu_si128( reinterpret_cast<const __m128i*>(pSource) );
    return _mm_castsi128_ps(V);
#line 644 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadInt4A
(
    const uint32_t* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(653)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(654)), 0) );







#line 663 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"

#line 665 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    __m128i V = _mm_load_si128( reinterpret_cast<const __m128i*>(pSource) );
    return _mm_castsi128_ps(V);
#line 668 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadFloat4
(
    const XMFLOAT4* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(677)), 0) );







#line 686 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"

#line 688 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    return _mm_loadu_ps( &pSource->x );
#line 690 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadFloat4A
(
    const XMFLOAT4A* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(699)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(700)), 0) );







#line 709 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"

#line 711 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    return _mm_load_ps( &pSource->x );
#line 713 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadSInt4
(
    const XMINT4* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(722)), 0) );









#line 733 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"


#line 736 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    __m128i V = _mm_loadu_si128( reinterpret_cast<const __m128i*>(pSource) );
    return _mm_cvtepi32_ps(V);
#line 739 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMLoadUInt4
(
    const XMUINT4* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(748)), 0) );







#line 757 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"


#line 760 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    __m128i V = _mm_loadu_si128( reinterpret_cast<const __m128i*>(pSource) );
    // For the values that are higher than 0x7FFFFFFF, a fixup is needed
    // Determine which ones need the fix.
    XMVECTOR vMask = _mm_and_ps(_mm_castsi128_ps(V),g_XMNegativeZero);
    // Force all values positive
    XMVECTOR vResult = _mm_xor_ps(_mm_castsi128_ps(V),vMask);
    // Convert to floats
    vResult = _mm_cvtepi32_ps(_mm_castps_si128(vResult));
    // Convert 0x80000000 -> 0xFFFFFFFF
    __m128i iMask = _mm_srai_epi32(_mm_castps_si128(vMask),31);
    // For only the ones that are too big, add the fixup
    vMask = _mm_and_ps(_mm_castsi128_ps(iMask),g_XMFixUnsigned);
    vResult = _mm_add_ps(vResult,vMask);
    return vResult;
#line 775 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMLoadFloat3x3
(
    const XMFLOAT3X3* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(784)), 0) );























#line 809 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"











#line 821 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    __m128 Z = _mm_setzero_ps();

    __m128 V1 = _mm_loadu_ps( &pSource->m[0][0] );
    __m128 V2 = _mm_loadu_ps( &pSource->m[1][1] );
    __m128 V3 = _mm_load_ss( &pSource->m[2][2] );

    __m128 T1 = _mm_unpackhi_ps( V1, Z );
    __m128 T2 = _mm_unpacklo_ps( V2, Z );
    __m128 T3 = _mm_shuffle_ps( V3, T2, (((0) << 6) | ((1) << 4) | ((0) << 2) | ((0))) );
    __m128 T4 = _mm_movehl_ps( T2, T3 );
    __m128 T5 = _mm_movehl_ps( Z, T1 );

    XMMATRIX M;
    M.r[0] = _mm_movelh_ps( V1, T1 );
    M.r[1] = _mm_add_ps( T4, T5 );
    M.r[2] = _mm_shuffle_ps( V2, V3, (((1) << 6) | ((0) << 4) | ((3) << 2) | ((2))) );
    M.r[3] = g_XMIdentityR3;
    return M;
#line 840 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMLoadFloat4x3
(
    const XMFLOAT4X3* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(849)), 0) );
























#line 875 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"














#line 890 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    // Use unaligned load instructions to
    // load the 12 floats
    // vTemp1 = x1,y1,z1,x2
    XMVECTOR vTemp1 = _mm_loadu_ps(&pSource->m[0][0]);
    // vTemp2 = y2,z2,x3,y3
    XMVECTOR vTemp2 = _mm_loadu_ps(&pSource->m[1][1]);
    // vTemp4 = z3,x4,y4,z4
    XMVECTOR vTemp4 = _mm_loadu_ps(&pSource->m[2][2]);
    // vTemp3 = x3,y3,z3,z3
    XMVECTOR vTemp3 = _mm_shuffle_ps(vTemp2,vTemp4,(((0) << 6) | ((0) << 4) | ((3) << 2) | ((2))));
    // vTemp2 = y2,z2,x2,x2
    vTemp2 = _mm_shuffle_ps(vTemp2,vTemp1,(((3) << 6) | ((3) << 4) | ((1) << 2) | ((0))));
    // vTemp2 = x2,y2,z2,z2
    vTemp2 = _mm_shuffle_ps( vTemp2, vTemp2, (((1) << 6) | ((1) << 4) | ((0) << 2) | ((2))) );
    // vTemp1 = x1,y1,z1,0
    vTemp1 = _mm_and_ps(vTemp1,g_XMMask3);
    // vTemp2 = x2,y2,z2,0
    vTemp2 = _mm_and_ps(vTemp2,g_XMMask3);
    // vTemp3 = x3,y3,z3,0
    vTemp3 = _mm_and_ps(vTemp3,g_XMMask3);
    // vTemp4i = x4,y4,z4,0
    __m128i vTemp4i = _mm_srli_si128(_mm_castps_si128(vTemp4),32/8);
    // vTemp4i = x4,y4,z4,1.0f
    vTemp4i = _mm_or_si128(vTemp4i,g_XMIdentityR3);
    XMMATRIX M(vTemp1,
            vTemp2,
            vTemp3,
            _mm_castsi128_ps(vTemp4i));
    return M;
#line 920 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMLoadFloat4x3A
(
    const XMFLOAT4X3A* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(929)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(930)), 0) );
























#line 956 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"














#line 971 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    // Use aligned load instructions to
    // load the 12 floats
    // vTemp1 = x1,y1,z1,x2
    XMVECTOR vTemp1 = _mm_load_ps(&pSource->m[0][0]);
    // vTemp2 = y2,z2,x3,y3
    XMVECTOR vTemp2 = _mm_load_ps(&pSource->m[1][1]);
    // vTemp4 = z3,x4,y4,z4
    XMVECTOR vTemp4 = _mm_load_ps(&pSource->m[2][2]);
    // vTemp3 = x3,y3,z3,z3
    XMVECTOR vTemp3 = _mm_shuffle_ps(vTemp2,vTemp4,(((0) << 6) | ((0) << 4) | ((3) << 2) | ((2))));
    // vTemp2 = y2,z2,x2,x2
    vTemp2 = _mm_shuffle_ps(vTemp2,vTemp1,(((3) << 6) | ((3) << 4) | ((1) << 2) | ((0))));
    // vTemp2 = x2,y2,z2,z2
    vTemp2 = _mm_shuffle_ps( vTemp2, vTemp2, (((1) << 6) | ((1) << 4) | ((0) << 2) | ((2))) );
    // vTemp1 = x1,y1,z1,0
    vTemp1 = _mm_and_ps(vTemp1,g_XMMask3);
    // vTemp2 = x2,y2,z2,0
    vTemp2 = _mm_and_ps(vTemp2,g_XMMask3);
    // vTemp3 = x3,y3,z3,0
    vTemp3 = _mm_and_ps(vTemp3,g_XMMask3);
    // vTemp4i = x4,y4,z4,0
    __m128i vTemp4i = _mm_srli_si128(_mm_castps_si128(vTemp4),32/8);
    // vTemp4i = x4,y4,z4,1.0f
    vTemp4i = _mm_or_si128(vTemp4i,g_XMIdentityR3);
    XMMATRIX M(vTemp1,
            vTemp2,
            vTemp3,
            _mm_castsi128_ps(vTemp4i));
    return M;
#line 1001 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMLoadFloat3x4
(
    const XMFLOAT3X4* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1010)), 0) );
























#line 1036 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"



















#line 1056 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    XMMATRIX M;
    M.r[0] = _mm_loadu_ps(&pSource->_11);
    M.r[1] = _mm_loadu_ps(&pSource->_21);
    M.r[2] = _mm_loadu_ps(&pSource->_31);
    M.r[3] = g_XMIdentityR3;

    // x.x,x.y,y.x,y.y
    XMVECTOR vTemp1 = _mm_shuffle_ps(M.r[0], M.r[1], (((1) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    // x.z,x.w,y.z,y.w
    XMVECTOR vTemp3 = _mm_shuffle_ps(M.r[0], M.r[1], (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));
    // z.x,z.y,w.x,w.y
    XMVECTOR vTemp2 = _mm_shuffle_ps(M.r[2], M.r[3], (((1) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    // z.z,z.w,w.z,w.w
    XMVECTOR vTemp4 = _mm_shuffle_ps(M.r[2], M.r[3], (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));
    XMMATRIX mResult;

    // x.x,y.x,z.x,w.x
    mResult.r[0] = _mm_shuffle_ps(vTemp1, vTemp2, (((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));
    // x.y,y.y,z.y,w.y
    mResult.r[1] = _mm_shuffle_ps(vTemp1, vTemp2, (((3) << 6) | ((1) << 4) | ((3) << 2) | ((1))));
    // x.z,y.z,z.z,w.z
    mResult.r[2] = _mm_shuffle_ps(vTemp3, vTemp4, (((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));
    // x.w,y.w,z.w,w.w
    mResult.r[3] = _mm_shuffle_ps(vTemp3, vTemp4, (((3) << 6) | ((1) << 4) | ((3) << 2) | ((1))));
    return mResult;
#line 1082 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMLoadFloat3x4A
(
    const XMFLOAT3X4A* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1091)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1092)), 0) );
























#line 1118 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"



















#line 1138 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    XMMATRIX M;
    M.r[0] = _mm_load_ps(&pSource->_11);
    M.r[1] = _mm_load_ps(&pSource->_21);
    M.r[2] = _mm_load_ps(&pSource->_31);
    M.r[3] = g_XMIdentityR3;

    // x.x,x.y,y.x,y.y
    XMVECTOR vTemp1 = _mm_shuffle_ps(M.r[0], M.r[1], (((1) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    // x.z,x.w,y.z,y.w
    XMVECTOR vTemp3 = _mm_shuffle_ps(M.r[0], M.r[1], (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));
    // z.x,z.y,w.x,w.y
    XMVECTOR vTemp2 = _mm_shuffle_ps(M.r[2], M.r[3], (((1) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    // z.z,z.w,w.z,w.w
    XMVECTOR vTemp4 = _mm_shuffle_ps(M.r[2], M.r[3], (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));
    XMMATRIX mResult;

    // x.x,y.x,z.x,w.x
    mResult.r[0] = _mm_shuffle_ps(vTemp1, vTemp2, (((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));
    // x.y,y.y,z.y,w.y
    mResult.r[1] = _mm_shuffle_ps(vTemp1, vTemp2, (((3) << 6) | ((1) << 4) | ((3) << 2) | ((1))));
    // x.z,y.z,z.z,w.z
    mResult.r[2] = _mm_shuffle_ps(vTemp3, vTemp4, (((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));
    // x.w,y.w,z.w,w.w
    mResult.r[3] = _mm_shuffle_ps(vTemp3, vTemp4, (((3) << 6) | ((1) << 4) | ((3) << 2) | ((1))));
    return mResult;
#line 1164 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMLoadFloat4x4
(
    const XMFLOAT4X4* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1173)), 0) );
























#line 1199 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"






#line 1206 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    XMMATRIX M;
    M.r[0] = _mm_loadu_ps( &pSource->_11 );
    M.r[1] = _mm_loadu_ps( &pSource->_21 );
    M.r[2] = _mm_loadu_ps( &pSource->_31 );
    M.r[3] = _mm_loadu_ps( &pSource->_41 );
    return M;
#line 1213 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMLoadFloat4x4A
(
    const XMFLOAT4X4A* pSource
)
{
    (void)( (!!(pSource)) || (_wassert(L"pSource", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1222)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pSource) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1223)), 0) );
























#line 1249 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"






#line 1256 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    XMMATRIX M;
    M.r[0] = _mm_load_ps( &pSource->_11 );
    M.r[1] = _mm_load_ps( &pSource->_21 );
    M.r[2] = _mm_load_ps( &pSource->_31 );
    M.r[3] = _mm_load_ps( &pSource->_41 );
    return M;
#line 1263 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

/****************************************************************************
 *
 * Vector and matrix store operations
 *
 ****************************************************************************/

inline void __vectorcall XMStoreInt
(
    uint32_t*    pDestination,
    FXMVECTOR V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1277)), 0) );


#line 1281 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"

#line 1283 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    _mm_store_ss( reinterpret_cast<float*>(pDestination), V );
#line 1285 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreFloat
(
    float*    pDestination,
    FXMVECTOR V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1295)), 0) );


#line 1299 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"

#line 1301 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    _mm_store_ss( pDestination, V );
#line 1303 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreInt2
(
    uint32_t*    pDestination,
    FXMVECTOR V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1313)), 0) );



#line 1318 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"


#line 1321 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    XMVECTOR T = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    _mm_store_ss( reinterpret_cast<float*>(&pDestination[0]), V );
    _mm_store_ss( reinterpret_cast<float*>(&pDestination[1]), T );
#line 1325 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreInt2A
(
    uint32_t*    pDestination,
    FXMVECTOR V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1335)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1336)), 0) );



#line 1341 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"


#line 1344 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    _mm_storel_epi64( reinterpret_cast<__m128i*>(pDestination), _mm_castps_si128(V) );
#line 1346 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreFloat2
(
    XMFLOAT2* pDestination,
    FXMVECTOR  V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1356)), 0) );



#line 1361 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"


#line 1364 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    XMVECTOR T = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    _mm_store_ss( &pDestination->x, V );
    _mm_store_ss( &pDestination->y, T );
#line 1368 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreFloat2A
(
    XMFLOAT2A*   pDestination,
    FXMVECTOR     V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1378)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1379)), 0) );



#line 1384 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"


#line 1387 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    _mm_storel_epi64( reinterpret_cast<__m128i*>(pDestination), _mm_castps_si128(V) );
#line 1389 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreSInt2
(
    XMINT2* pDestination,
    FXMVECTOR V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1399)), 0) );



#line 1404 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"



#line 1408 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    // In case of positive overflow, detect it
    XMVECTOR vOverflow = _mm_cmpgt_ps(V,g_XMMaxInt);
    // Float to int conversion
    __m128i vResulti = _mm_cvttps_epi32(V);
    // If there was positive overflow, set to 0x7FFFFFFF
    XMVECTOR vResult = _mm_and_ps(vOverflow,g_XMAbsMask);
    vOverflow = _mm_andnot_ps(vOverflow,_mm_castsi128_ps(vResulti));
    vOverflow = _mm_or_ps(vOverflow,vResult);
    // Write two ints
    XMVECTOR T = _mm_shuffle_ps( vOverflow, vOverflow, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    _mm_store_ss( reinterpret_cast<float*>(&pDestination->x), vOverflow );
    _mm_store_ss( reinterpret_cast<float*>(&pDestination->y), T );
#line 1421 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreUInt2
(
    XMUINT2* pDestination,
    FXMVECTOR V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1431)), 0) );



#line 1436 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"



#line 1440 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    // Clamp to >=0
    XMVECTOR vResult = _mm_max_ps(V,g_XMZero);
    // Any numbers that are too big, set to 0xFFFFFFFFU
    XMVECTOR vOverflow = _mm_cmpgt_ps(vResult,g_XMMaxUInt);
    XMVECTOR vValue = g_XMUnsignedFix;
    // Too large for a signed integer?
    XMVECTOR vMask = _mm_cmpge_ps(vResult,vValue);
    // Zero for number's lower than 0x80000000, 32768.0f*65536.0f otherwise
    vValue = _mm_and_ps(vValue,vMask);
    // Perform fixup only on numbers too large (Keeps low bit precision)
    vResult = _mm_sub_ps(vResult,vValue);
    __m128i vResulti = _mm_cvttps_epi32(vResult);
    // Convert from signed to unsigned pnly if greater than 0x80000000
    vMask = _mm_and_ps(vMask,g_XMNegativeZero);
    vResult = _mm_xor_ps(_mm_castsi128_ps(vResulti),vMask);
    // On those that are too large, set to 0xFFFFFFFF
    vResult = _mm_or_ps(vResult,vOverflow);
    // Write two uints
    XMVECTOR T = _mm_shuffle_ps( vResult, vResult, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    _mm_store_ss( reinterpret_cast<float*>(&pDestination->x), vResult );
    _mm_store_ss( reinterpret_cast<float*>(&pDestination->y), T );
#line 1462 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreInt3
(
    uint32_t*    pDestination,
    FXMVECTOR V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1472)), 0) );




#line 1478 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"



#line 1482 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    XMVECTOR T1 = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    XMVECTOR T2 = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    _mm_store_ss( reinterpret_cast<float*>(pDestination), V );
    _mm_store_ss( reinterpret_cast<float*>(&pDestination[1]), T1 );
    _mm_store_ss( reinterpret_cast<float*>(&pDestination[2]), T2 );
#line 1488 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreInt3A
(
    uint32_t*    pDestination,
    FXMVECTOR V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1498)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1499)), 0) );




#line 1505 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"



#line 1509 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    XMVECTOR T = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    _mm_storel_epi64( reinterpret_cast<__m128i*>(pDestination), _mm_castps_si128(V) );
    _mm_store_ss( reinterpret_cast<float*>(&pDestination[2]), T );
#line 1513 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreFloat3
(
    XMFLOAT3* pDestination,
    FXMVECTOR V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1523)), 0) );




#line 1529 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"



#line 1533 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    XMVECTOR T1 = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    XMVECTOR T2 = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    _mm_store_ss( &pDestination->x, V );
    _mm_store_ss( &pDestination->y, T1 );
    _mm_store_ss( &pDestination->z, T2 );
#line 1539 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreFloat3A
(
    XMFLOAT3A*   pDestination,
    FXMVECTOR     V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1549)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1550)), 0) );




#line 1556 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"



#line 1560 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    XMVECTOR T = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    _mm_storel_epi64( reinterpret_cast<__m128i*>(pDestination), _mm_castps_si128(V) );
    _mm_store_ss( &pDestination->z, T );
#line 1564 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreSInt3
(
    XMINT3* pDestination,
    FXMVECTOR V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1574)), 0) );




#line 1580 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"




#line 1585 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    // In case of positive overflow, detect it
    XMVECTOR vOverflow = _mm_cmpgt_ps(V,g_XMMaxInt);
    // Float to int conversion
    __m128i vResulti = _mm_cvttps_epi32(V);
    // If there was positive overflow, set to 0x7FFFFFFF
    XMVECTOR vResult = _mm_and_ps(vOverflow,g_XMAbsMask);
    vOverflow = _mm_andnot_ps(vOverflow,_mm_castsi128_ps(vResulti));
    vOverflow = _mm_or_ps(vOverflow,vResult);
    // Write 3 uints
    XMVECTOR T1 = _mm_shuffle_ps( vOverflow, vOverflow, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    XMVECTOR T2 = _mm_shuffle_ps( vOverflow, vOverflow, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    _mm_store_ss( reinterpret_cast<float*>(&pDestination->x), vOverflow );
    _mm_store_ss( reinterpret_cast<float*>(&pDestination->y), T1 );
    _mm_store_ss( reinterpret_cast<float*>(&pDestination->z), T2 );
#line 1600 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreUInt3
(
    XMUINT3* pDestination,
    FXMVECTOR V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1610)), 0) );




#line 1616 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"




#line 1621 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    // Clamp to >=0
    XMVECTOR vResult = _mm_max_ps(V,g_XMZero);
    // Any numbers that are too big, set to 0xFFFFFFFFU
    XMVECTOR vOverflow = _mm_cmpgt_ps(vResult,g_XMMaxUInt);
    XMVECTOR vValue = g_XMUnsignedFix;
    // Too large for a signed integer?
    XMVECTOR vMask = _mm_cmpge_ps(vResult,vValue);
    // Zero for number's lower than 0x80000000, 32768.0f*65536.0f otherwise
    vValue = _mm_and_ps(vValue,vMask);
    // Perform fixup only on numbers too large (Keeps low bit precision)
    vResult = _mm_sub_ps(vResult,vValue);
    __m128i vResulti = _mm_cvttps_epi32(vResult);
    // Convert from signed to unsigned pnly if greater than 0x80000000
    vMask = _mm_and_ps(vMask,g_XMNegativeZero);
    vResult = _mm_xor_ps(_mm_castsi128_ps(vResulti),vMask);
    // On those that are too large, set to 0xFFFFFFFF
    vResult = _mm_or_ps(vResult,vOverflow);
    // Write 3 uints
    XMVECTOR T1 = _mm_shuffle_ps( vResult, vResult, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    XMVECTOR T2 = _mm_shuffle_ps( vResult, vResult, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    _mm_store_ss( reinterpret_cast<float*>(&pDestination->x), vResult );
    _mm_store_ss( reinterpret_cast<float*>(&pDestination->y), T1 );
    _mm_store_ss( reinterpret_cast<float*>(&pDestination->z), T2 );
#line 1645 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreInt4
(
    uint32_t*    pDestination,
    FXMVECTOR V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1655)), 0) );





#line 1662 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"

#line 1664 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    _mm_storeu_si128( reinterpret_cast<__m128i*>(pDestination), _mm_castps_si128(V) );
#line 1666 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreInt4A
(
    uint32_t*    pDestination,
    FXMVECTOR V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1676)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1677)), 0) );





#line 1684 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"

#line 1686 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    _mm_store_si128( reinterpret_cast<__m128i*>(pDestination), _mm_castps_si128(V) );
#line 1688 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreFloat4
(
    XMFLOAT4* pDestination,
    FXMVECTOR  V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1698)), 0) );





#line 1705 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"

#line 1707 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    _mm_storeu_ps( &pDestination->x, V );
#line 1709 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreFloat4A
(
    XMFLOAT4A*   pDestination,
    FXMVECTOR     V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1719)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1720)), 0) );





#line 1727 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"

#line 1729 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    _mm_store_ps( &pDestination->x, V );
#line 1731 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreSInt4
(
    XMINT4* pDestination,
    FXMVECTOR V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1741)), 0) );





#line 1748 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"


#line 1751 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    // In case of positive overflow, detect it
    XMVECTOR vOverflow = _mm_cmpgt_ps(V,g_XMMaxInt);
    // Float to int conversion
    __m128i vResulti = _mm_cvttps_epi32(V);
    // If there was positive overflow, set to 0x7FFFFFFF
    XMVECTOR vResult = _mm_and_ps(vOverflow,g_XMAbsMask);
    vOverflow = _mm_andnot_ps(vOverflow,_mm_castsi128_ps(vResulti));
    vOverflow = _mm_or_ps(vOverflow,vResult);
    _mm_storeu_si128( reinterpret_cast<__m128i*>(pDestination), _mm_castps_si128(vOverflow) );
#line 1761 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreUInt4
(
    XMUINT4* pDestination,
    FXMVECTOR V
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1771)), 0) );





#line 1778 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"


#line 1781 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    // Clamp to >=0
    XMVECTOR vResult = _mm_max_ps(V,g_XMZero);
    // Any numbers that are too big, set to 0xFFFFFFFFU
    XMVECTOR vOverflow = _mm_cmpgt_ps(vResult,g_XMMaxUInt);
    XMVECTOR vValue = g_XMUnsignedFix;
    // Too large for a signed integer?
    XMVECTOR vMask = _mm_cmpge_ps(vResult,vValue);
    // Zero for number's lower than 0x80000000, 32768.0f*65536.0f otherwise
    vValue = _mm_and_ps(vValue,vMask);
    // Perform fixup only on numbers too large (Keeps low bit precision)
    vResult = _mm_sub_ps(vResult,vValue);
    __m128i vResulti = _mm_cvttps_epi32(vResult);
    // Convert from signed to unsigned pnly if greater than 0x80000000
    vMask = _mm_and_ps(vMask,g_XMNegativeZero);
    vResult = _mm_xor_ps(_mm_castsi128_ps(vResulti),vMask);
    // On those that are too large, set to 0xFFFFFFFF
    vResult = _mm_or_ps(vResult,vOverflow);
    _mm_storeu_si128( reinterpret_cast<__m128i*>(pDestination), _mm_castps_si128(vResult) );
#line 1800 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreFloat3x3
(
    XMFLOAT3X3* pDestination,
    FXMMATRIX   M
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1810)), 0) );














#line 1826 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"









#line 1836 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    XMVECTOR vTemp1 = M.r[0];
    XMVECTOR vTemp2 = M.r[1];
    XMVECTOR vTemp3 = M.r[2];
    XMVECTOR vWork = _mm_shuffle_ps(vTemp1,vTemp2,(((0) << 6) | ((0) << 4) | ((2) << 2) | ((2))));
    vTemp1 = _mm_shuffle_ps(vTemp1,vWork,(((2) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    _mm_storeu_ps(&pDestination->m[0][0],vTemp1);
    vTemp2 = _mm_shuffle_ps(vTemp2,vTemp3,(((1) << 6) | ((0) << 4) | ((2) << 2) | ((1))));
    _mm_storeu_ps(&pDestination->m[1][1],vTemp2);
    vTemp3 = _mm_shuffle_ps( vTemp3, vTemp3, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    _mm_store_ss(&pDestination->m[2][2],vTemp3);
#line 1847 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreFloat4x3
(
    XMFLOAT4X3* pDestination,
    FXMMATRIX M
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1857)), 0) );


















#line 1877 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"











#line 1889 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    XMVECTOR vTemp1 = M.r[0];
    XMVECTOR vTemp2 = M.r[1];
    XMVECTOR vTemp3 = M.r[2];
    XMVECTOR vTemp4 = M.r[3];
    XMVECTOR vTemp2x = _mm_shuffle_ps(vTemp2,vTemp3,(((1) << 6) | ((0) << 4) | ((2) << 2) | ((1))));
    vTemp2 = _mm_shuffle_ps(vTemp2,vTemp1,(((2) << 6) | ((2) << 4) | ((0) << 2) | ((0))));
    vTemp1 = _mm_shuffle_ps(vTemp1,vTemp2,(((0) << 6) | ((2) << 4) | ((1) << 2) | ((0))));
    vTemp3 = _mm_shuffle_ps(vTemp3,vTemp4,(((0) << 6) | ((0) << 4) | ((2) << 2) | ((2))));
    vTemp3 = _mm_shuffle_ps(vTemp3,vTemp4,(((2) << 6) | ((1) << 4) | ((2) << 2) | ((0))));
    _mm_storeu_ps(&pDestination->m[0][0],vTemp1);
    _mm_storeu_ps(&pDestination->m[1][1],vTemp2x);
    _mm_storeu_ps(&pDestination->m[2][2],vTemp3);
#line 1902 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreFloat4x3A
(
    XMFLOAT4X3A*    pDestination,
    FXMMATRIX       M
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1912)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1913)), 0) );


















#line 1933 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"











#line 1945 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    // x1,y1,z1,w1
    XMVECTOR vTemp1 = M.r[0];
    // x2,y2,z2,w2
    XMVECTOR vTemp2 = M.r[1];
    // x3,y3,z3,w3
    XMVECTOR vTemp3 = M.r[2];
    // x4,y4,z4,w4
    XMVECTOR vTemp4 = M.r[3];
    // z1,z1,x2,y2
    XMVECTOR vTemp = _mm_shuffle_ps(vTemp1,vTemp2,(((1) << 6) | ((0) << 4) | ((2) << 2) | ((2))));
    // y2,z2,x3,y3 (Final)
    vTemp2 = _mm_shuffle_ps(vTemp2,vTemp3,(((1) << 6) | ((0) << 4) | ((2) << 2) | ((1))));
    // x1,y1,z1,x2 (Final)
    vTemp1 = _mm_shuffle_ps(vTemp1,vTemp,(((2) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    // z3,z3,x4,x4
    vTemp3 = _mm_shuffle_ps(vTemp3,vTemp4,(((0) << 6) | ((0) << 4) | ((2) << 2) | ((2))));
    // z3,x4,y4,z4 (Final)
    vTemp3 = _mm_shuffle_ps(vTemp3,vTemp4,(((2) << 6) | ((1) << 4) | ((2) << 2) | ((0))));
    // Store in 3 operations
    _mm_store_ps(&pDestination->m[0][0],vTemp1);
    _mm_store_ps(&pDestination->m[1][1],vTemp2);
    _mm_store_ps(&pDestination->m[2][2],vTemp3);
#line 1968 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreFloat3x4
(
    XMFLOAT3X4* pDestination,
    FXMMATRIX M
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(1978)), 0) );

















#line 1997 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"









#line 2007 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    // x.x,x.y,y.x,y.y
    XMVECTOR vTemp1 = _mm_shuffle_ps(M.r[0], M.r[1], (((1) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    // x.z,x.w,y.z,y.w
    XMVECTOR vTemp3 = _mm_shuffle_ps(M.r[0], M.r[1], (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));
    // z.x,z.y,w.x,w.y
    XMVECTOR vTemp2 = _mm_shuffle_ps(M.r[2], M.r[3], (((1) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    // z.z,z.w,w.z,w.w
    XMVECTOR vTemp4 = _mm_shuffle_ps(M.r[2], M.r[3], (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));

    // x.x,y.x,z.x,w.x
    XMVECTOR r0 = _mm_shuffle_ps(vTemp1, vTemp2, (((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));
    // x.y,y.y,z.y,w.y
    XMVECTOR r1 = _mm_shuffle_ps(vTemp1, vTemp2, (((3) << 6) | ((1) << 4) | ((3) << 2) | ((1))));
    // x.z,y.z,z.z,w.z
    XMVECTOR r2 = _mm_shuffle_ps(vTemp3, vTemp4, (((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));

    _mm_storeu_ps(&pDestination->m[0][0], r0);
    _mm_storeu_ps(&pDestination->m[1][0], r1);
    _mm_storeu_ps(&pDestination->m[2][0], r2);
#line 2027 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreFloat3x4A
(
    XMFLOAT3X4A* pDestination,
    FXMMATRIX M
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(2037)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(2038)), 0) );

















#line 2057 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"









#line 2067 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    // x.x,x.y,y.x,y.y
    XMVECTOR vTemp1 = _mm_shuffle_ps(M.r[0], M.r[1], (((1) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    // x.z,x.w,y.z,y.w
    XMVECTOR vTemp3 = _mm_shuffle_ps(M.r[0], M.r[1], (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));
    // z.x,z.y,w.x,w.y
    XMVECTOR vTemp2 = _mm_shuffle_ps(M.r[2], M.r[3], (((1) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    // z.z,z.w,w.z,w.w
    XMVECTOR vTemp4 = _mm_shuffle_ps(M.r[2], M.r[3], (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));

    // x.x,y.x,z.x,w.x
    XMVECTOR r0 = _mm_shuffle_ps(vTemp1, vTemp2, (((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));
    // x.y,y.y,z.y,w.y
    XMVECTOR r1 = _mm_shuffle_ps(vTemp1, vTemp2, (((3) << 6) | ((1) << 4) | ((3) << 2) | ((1))));
    // x.z,y.z,z.z,w.z
    XMVECTOR r2 = _mm_shuffle_ps(vTemp3, vTemp4, (((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));

    _mm_store_ps(&pDestination->m[0][0], r0);
    _mm_store_ps(&pDestination->m[1][0], r1);
    _mm_store_ps(&pDestination->m[2][0], r2);
#line 2087 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreFloat4x4
(
    XMFLOAT4X4* pDestination,
    FXMMATRIX M
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(2097)), 0) );






















#line 2121 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"




#line 2126 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    _mm_storeu_ps( &pDestination->_11, M.r[0] );
    _mm_storeu_ps( &pDestination->_21, M.r[1] );
    _mm_storeu_ps( &pDestination->_31, M.r[2] );
    _mm_storeu_ps( &pDestination->_41, M.r[3] );
#line 2131 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

//------------------------------------------------------------------------------

inline void __vectorcall XMStoreFloat4x4A
(
    XMFLOAT4X4A*    pDestination,
    FXMMATRIX       M
)
{
    (void)( (!!(pDestination)) || (_wassert(L"pDestination", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(2141)), 0) );
    (void)( (!!((reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0)) || (_wassert(L"(reinterpret_cast<uintptr_t>(pDestination) & 0xF) == 0", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl", (unsigned)(2142)), 0) );






















#line 2166 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"




#line 2171 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
    _mm_store_ps( &pDestination->_11, M.r[0] );
    _mm_store_ps( &pDestination->_21, M.r[1] );
    _mm_store_ps( &pDestination->_31, M.r[2] );
    _mm_store_ps( &pDestination->_41, M.r[3] );
#line 2176 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathConvert.inl"
}

#pragma external_header(pop)
#line 2163 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
#pragma external_header(push)
#line 1 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
//-------------------------------------------------------------------------------------
// DirectXMathVector.inl -- SIMD C++ Math library
//
// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
//
// http://go.microsoft.com/fwlink/?LinkID=615560
//-------------------------------------------------------------------------------------

#pragma once




#line 16 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
















#line 33 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

/****************************************************************************
 *
 * General Vector
 *
 ****************************************************************************/

//------------------------------------------------------------------------------
// Assignment operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------
// Return a vector with all elements equaling zero
inline XMVECTOR __vectorcall XMVectorZero()
{



#line 52 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 54 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_setzero_ps();
#line 56 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Initialize a vector with four floating point values
inline XMVECTOR __vectorcall XMVectorSet
(
    float x,
    float y,
    float z,
    float w
)
{



#line 72 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"







#line 80 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_set_ps( w, z, y, x );
#line 82 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Initialize a vector with four integer values
inline XMVECTOR __vectorcall XMVectorSetInt
(
    uint32_t x,
    uint32_t y,
    uint32_t z,
    uint32_t w
)
{



#line 98 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 102 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i V = _mm_set_epi32(static_cast<int>(w), static_cast<int>(z), static_cast<int>(y), static_cast<int>(x));
    return _mm_castsi128_ps(V);
#line 105 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Initialize a vector with a replicated floating point value
inline XMVECTOR __vectorcall XMVectorReplicate
(
    float Value
)
{







#line 122 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 124 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_set_ps1( Value );
#line 126 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Initialize a vector with a replicated floating point value passed by pointer

inline XMVECTOR __vectorcall XMVectorReplicatePtr
(
    const float *pValue
)
{








#line 145 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 147 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 149 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_load_ps1( pValue );
#line 151 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Initialize a vector with a replicated integer value
inline XMVECTOR __vectorcall XMVectorReplicateInt
(
    uint32_t Value
)
{







#line 168 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 170 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i vTemp = _mm_set1_epi32(static_cast<int>(Value));
    return _mm_castsi128_ps(vTemp);
#line 173 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Initialize a vector with a replicated integer value passed by pointer

inline XMVECTOR __vectorcall XMVectorReplicateIntPtr
(
    const uint32_t *pValue
)
{








#line 192 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 194 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_load_ps1(reinterpret_cast<const float *>(pValue));
#line 196 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Initialize a vector with all bits set (true mask)
inline XMVECTOR __vectorcall XMVectorTrueInt()
{



#line 206 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 208 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i V = _mm_set1_epi32(-1);
    return _mm_castsi128_ps(V);
#line 211 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Initialize a vector with all bits clear (false mask)
inline XMVECTOR __vectorcall XMVectorFalseInt()
{



#line 221 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 223 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_setzero_ps();
#line 225 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Replicate the x component of the vector
inline XMVECTOR __vectorcall XMVectorSplatX
(
    FXMVECTOR V
)
{







#line 242 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 244 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 246 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
#line 248 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Replicate the y component of the vector
inline XMVECTOR __vectorcall XMVectorSplatY
(
    FXMVECTOR V
)
{







#line 265 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 267 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
#line 269 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Replicate the z component of the vector
inline XMVECTOR __vectorcall XMVectorSplatZ
(
    FXMVECTOR V
)
{







#line 286 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 288 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
#line 290 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Replicate the w component of the vector
inline XMVECTOR __vectorcall XMVectorSplatW
(
    FXMVECTOR V
)
{







#line 307 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 309 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
#line 311 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Return a vector of 1.0f,1.0f,1.0f,1.0f
inline XMVECTOR __vectorcall XMVectorSplatOne()
{







#line 325 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 327 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return g_XMOne;
#line 329 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Return a vector of INF,INF,INF,INF
inline XMVECTOR __vectorcall XMVectorSplatInfinity()
{







#line 343 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 345 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return g_XMInfinity;
#line 347 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Return a vector of Q_NAN,Q_NAN,Q_NAN,Q_NAN
inline XMVECTOR __vectorcall XMVectorSplatQNaN()
{







#line 361 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 363 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return g_XMQNaN;
#line 365 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Return a vector of 1.192092896e-7f,1.192092896e-7f,1.192092896e-7f,1.192092896e-7f
inline XMVECTOR __vectorcall XMVectorSplatEpsilon()
{







#line 379 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 381 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return g_XMEpsilon;
#line 383 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Return a vector of -0.0f (0x80000000),-0.0f,-0.0f,-0.0f
inline XMVECTOR __vectorcall XMVectorSplatSignMask()
{







#line 397 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 399 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i V = _mm_set1_epi32(static_cast<int>(0x80000000));
    return _mm_castsi128_ps(V);
#line 402 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Return a floating point value via an index. This is not a recommended
// function to use due to performance loss.
inline float __vectorcall XMVectorGetByIndex(FXMVECTOR V, size_t i)
{
    (void)( (!!(i < 4)) || (_wassert(L"i < 4", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(409)), 0) );
    ;


#line 414 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTORF32 U;
    U.v = V;
    return U.f[i];
#line 418 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Return the X component in an FPU register.
inline float __vectorcall XMVectorGetX(FXMVECTOR V)
{


#line 427 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 429 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_cvtss_f32(V);
#line 431 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Return the Y component in an FPU register.
inline float __vectorcall XMVectorGetY(FXMVECTOR V)
{


#line 439 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 441 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    return _mm_cvtss_f32(vTemp);
#line 444 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Return the Z component in an FPU register.
inline float __vectorcall XMVectorGetZ(FXMVECTOR V)
{


#line 452 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 454 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    return _mm_cvtss_f32(vTemp);
#line 457 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Return the W component in an FPU register.
inline float __vectorcall XMVectorGetW(FXMVECTOR V)
{


#line 465 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 467 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    return _mm_cvtss_f32(vTemp);
#line 470 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

// Store a component indexed by i into a 32 bit float location in memory.

inline void __vectorcall XMVectorGetByIndexPtr(float *f, FXMVECTOR V, size_t i)
{
    (void)( (!!(f != nullptr)) || (_wassert(L"f != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(478)), 0) );
    (void)( (!!(i < 4)) || (_wassert(L"i < 4", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(479)), 0) );
    ;


#line 484 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTORF32 U;
    U.v = V;
    *f = U.f[i];
#line 488 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

// Store the X component into a 32 bit float location in memory.

inline void __vectorcall XMVectorGetXPtr(float *x, FXMVECTOR V)
{
    (void)( (!!(x != nullptr)) || (_wassert(L"x != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(496)), 0) );


#line 500 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 502 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    _mm_store_ss(x,V);
#line 504 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Store the Y component into a 32 bit float location in memory.

inline void __vectorcall XMVectorGetYPtr(float *y, FXMVECTOR V)
{
    (void)( (!!(y != nullptr)) || (_wassert(L"y != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(510)), 0) );


#line 514 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 516 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 518 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    _mm_store_ss(y,vResult);
#line 521 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Store the Z component into a 32 bit float location in memory.

inline void __vectorcall XMVectorGetZPtr(float *z, FXMVECTOR V)
{
    (void)( (!!(z != nullptr)) || (_wassert(L"z != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(527)), 0) );


#line 531 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 533 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 535 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    _mm_store_ss(z,vResult);
#line 538 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Store the W component into a 32 bit float location in memory.

inline void __vectorcall XMVectorGetWPtr(float *w, FXMVECTOR V)
{
    (void)( (!!(w != nullptr)) || (_wassert(L"w != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(544)), 0) );


#line 548 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 550 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 552 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    _mm_store_ss(w,vResult);
#line 555 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

// Return an integer value via an index. This is not a recommended
// function to use due to performance loss.
inline uint32_t __vectorcall XMVectorGetIntByIndex(FXMVECTOR V, size_t i)
{
    (void)( (!!(i < 4)) || (_wassert(L"i < 4", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(563)), 0) );
    ;


#line 568 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTORU32 U;
    U.v = V;
    return U.u[i];
#line 572 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

// Return the X component in an integer register.
inline uint32_t __vectorcall XMVectorGetIntX(FXMVECTOR V)
{


#line 582 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 584 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return static_cast<uint32_t>(_mm_cvtsi128_si32(_mm_castps_si128(V)));
#line 586 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Return the Y component in an integer register.
inline uint32_t __vectorcall XMVectorGetIntY(FXMVECTOR V)
{


#line 594 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 596 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 599 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i vResulti = _mm_shuffle_epi32(_mm_castps_si128(V),(((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))));
    return static_cast<uint32_t>(_mm_cvtsi128_si32(vResulti));
#line 602 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Return the Z component in an integer register.
inline uint32_t __vectorcall XMVectorGetIntZ(FXMVECTOR V)
{


#line 610 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 612 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 615 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i vResulti = _mm_shuffle_epi32(_mm_castps_si128(V),(((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))));
    return static_cast<uint32_t>(_mm_cvtsi128_si32(vResulti));
#line 618 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Return the W component in an integer register.
inline uint32_t __vectorcall XMVectorGetIntW(FXMVECTOR V)
{


#line 626 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 628 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 631 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i vResulti = _mm_shuffle_epi32(_mm_castps_si128(V),(((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))));
    return static_cast<uint32_t>(_mm_cvtsi128_si32(vResulti));
#line 634 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

// Store a component indexed by i into a 32 bit integer location in memory.

inline void __vectorcall XMVectorGetIntByIndexPtr(uint32_t *x, FXMVECTOR V, size_t i)
{
    (void)( (!!(x != nullptr)) || (_wassert(L"x != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(642)), 0) );
    (void)( (!!(i < 4)) || (_wassert(L"i < 4", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(643)), 0) );
    ;


#line 648 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTORU32 U;
    U.v = V;
    *x = U.u[i];
#line 652 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

// Store the X component into a 32 bit integer location in memory.

inline void __vectorcall XMVectorGetIntXPtr(uint32_t *x, FXMVECTOR V)
{
    (void)( (!!(x != nullptr)) || (_wassert(L"x != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(660)), 0) );


#line 664 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 666 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    _mm_store_ss(reinterpret_cast<float *>(x),V);
#line 668 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Store the Y component into a 32 bit integer location in memory.

inline void __vectorcall XMVectorGetIntYPtr(uint32_t *y, FXMVECTOR V)
{
    (void)( (!!(y != nullptr)) || (_wassert(L"y != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(674)), 0) );


#line 678 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 680 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 683 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    _mm_store_ss(reinterpret_cast<float *>(y),vResult);
#line 686 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Store the Z component into a 32 bit integer locaCantion in memory.

inline void __vectorcall XMVectorGetIntZPtr(uint32_t *z, FXMVECTOR V)
{
    (void)( (!!(z != nullptr)) || (_wassert(L"z != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(692)), 0) );


#line 696 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 698 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 701 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    _mm_store_ss(reinterpret_cast<float *>(z),vResult);
#line 704 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Store the W component into a 32 bit integer location in memory.

inline void __vectorcall XMVectorGetIntWPtr(uint32_t *w, FXMVECTOR V)
{
    (void)( (!!(w != nullptr)) || (_wassert(L"w != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(710)), 0) );


#line 714 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 716 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 719 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    _mm_store_ss(reinterpret_cast<float *>(w),vResult);
#line 722 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

// Set a single indexed floating point component
inline XMVECTOR __vectorcall XMVectorSetByIndex(FXMVECTOR V, float f, size_t i)
{
    (void)( (!!(i < 4)) || (_wassert(L"i < 4", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(729)), 0) );
    ;
    XMVECTORF32 U;
    U.v = V;
    U.f[i] = f;
    return U.v;
}

//------------------------------------------------------------------------------

// Sets the X component of a vector to a passed floating point value
inline XMVECTOR __vectorcall XMVectorSetX(FXMVECTOR V, float x)
{








#line 751 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 753 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_set_ss(x);
    vResult = _mm_move_ss(V,vResult);
    return vResult;
#line 757 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Sets the Y component of a vector to a passed floating point value
inline XMVECTOR __vectorcall XMVectorSetY(FXMVECTOR V, float y)
{








#line 771 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 773 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 777 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Swap y and x
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((3) << 6) | ((2) << 4) | ((0) << 2) | ((1))) );
    // Convert input to vector
    XMVECTOR vTemp = _mm_set_ss(y);
    // Replace the x component
    vResult = _mm_move_ss(vResult,vTemp);
    // Swap y and x again
    vResult = _mm_shuffle_ps( vResult, vResult, (((3) << 6) | ((2) << 4) | ((0) << 2) | ((1))) );
    return vResult;
#line 787 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}
// Sets the Z component of a vector to a passed floating point value
inline XMVECTOR __vectorcall XMVectorSetZ(FXMVECTOR V, float z)
{








#line 800 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 802 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 806 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Swap z and x
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((3) << 6) | ((0) << 4) | ((1) << 2) | ((2))) );
    // Convert input to vector
    XMVECTOR vTemp = _mm_set_ss(z);
    // Replace the x component
    vResult = _mm_move_ss(vResult,vTemp);
    // Swap z and x again
    vResult = _mm_shuffle_ps( vResult, vResult, (((3) << 6) | ((0) << 4) | ((1) << 2) | ((2))) );
    return vResult;
#line 816 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Sets the W component of a vector to a passed floating point value
inline XMVECTOR __vectorcall XMVectorSetW(FXMVECTOR V, float w)
{








#line 830 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 832 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 836 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Swap w and x
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((0) << 6) | ((2) << 4) | ((1) << 2) | ((3))) );
    // Convert input to vector
    XMVECTOR vTemp = _mm_set_ss(w);
    // Replace the x component
    vResult = _mm_move_ss(vResult,vTemp);
    // Swap w and x again
    vResult = _mm_shuffle_ps( vResult, vResult, (((0) << 6) | ((2) << 4) | ((1) << 2) | ((3))) );
    return vResult;
#line 846 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

// Sets a component of a vector to a floating point value passed by pointer

inline XMVECTOR __vectorcall XMVectorSetByIndexPtr(FXMVECTOR V, const float *f, size_t i)
{
    (void)( (!!(f != nullptr)) || (_wassert(L"f != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(854)), 0) );
    (void)( (!!(i < 4)) || (_wassert(L"i < 4", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(855)), 0) );
    ;
    XMVECTORF32 U;
    U.v = V;
    U.f[i] = *f;
    return U.v;
}

//------------------------------------------------------------------------------

// Sets the X component of a vector to a floating point value passed by pointer

inline XMVECTOR __vectorcall XMVectorSetXPtr(FXMVECTOR V, const float *x)
{
    (void)( (!!(x != nullptr)) || (_wassert(L"x != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(869)), 0) );








#line 879 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 881 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_load_ss(x);
    vResult = _mm_move_ss(V,vResult);
    return vResult;
#line 885 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Sets the Y component of a vector to a floating point value passed by pointer

inline XMVECTOR __vectorcall XMVectorSetYPtr(FXMVECTOR V, const float *y)
{
    (void)( (!!(y != nullptr)) || (_wassert(L"y != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(891)), 0) );








#line 901 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 903 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Swap y and x
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((3) << 6) | ((2) << 4) | ((0) << 2) | ((1))) );
    // Convert input to vector
    XMVECTOR vTemp = _mm_load_ss(y);
    // Replace the x component
    vResult = _mm_move_ss(vResult,vTemp);
    // Swap y and x again
    vResult = _mm_shuffle_ps( vResult, vResult, (((3) << 6) | ((2) << 4) | ((0) << 2) | ((1))) );
    return vResult;
#line 913 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Sets the Z component of a vector to a floating point value passed by pointer

inline XMVECTOR __vectorcall XMVectorSetZPtr(FXMVECTOR V, const float *z)
{
    (void)( (!!(z != nullptr)) || (_wassert(L"z != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(919)), 0) );








#line 929 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 931 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Swap z and x
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((3) << 6) | ((0) << 4) | ((1) << 2) | ((2))) );
    // Convert input to vector
    XMVECTOR vTemp = _mm_load_ss(z);
    // Replace the x component
    vResult = _mm_move_ss(vResult,vTemp);
    // Swap z and x again
    vResult = _mm_shuffle_ps( vResult, vResult, (((3) << 6) | ((0) << 4) | ((1) << 2) | ((2))) );
    return vResult;
#line 941 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Sets the W component of a vector to a floating point value passed by pointer

inline XMVECTOR __vectorcall XMVectorSetWPtr(FXMVECTOR V, const float *w)
{
    (void)( (!!(w != nullptr)) || (_wassert(L"w != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(947)), 0) );








#line 957 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 959 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Swap w and x
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((0) << 6) | ((2) << 4) | ((1) << 2) | ((3))) );
    // Convert input to vector
    XMVECTOR vTemp = _mm_load_ss(w);
    // Replace the x component
    vResult = _mm_move_ss(vResult,vTemp);
    // Swap w and x again
    vResult = _mm_shuffle_ps( vResult, vResult, (((0) << 6) | ((2) << 4) | ((1) << 2) | ((3))) );
    return vResult;
#line 969 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

// Sets a component of a vector to an integer passed by value
inline XMVECTOR __vectorcall XMVectorSetIntByIndex(FXMVECTOR V, uint32_t x, size_t i)
{
    (void)( (!!(i < 4)) || (_wassert(L"i < 4", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(976)), 0) );
    ;
    XMVECTORU32 tmp;
    tmp.v = V;
    tmp.u[i] = x;
    return tmp;
}

//------------------------------------------------------------------------------

// Sets the X component of a vector to an integer passed by value
inline XMVECTOR __vectorcall XMVectorSetIntX(FXMVECTOR V, uint32_t x)
{








#line 998 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1000 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i vTemp = _mm_cvtsi32_si128(static_cast<int>(x));
    XMVECTOR vResult = _mm_move_ss(V,_mm_castsi128_ps(vTemp));
    return vResult;
#line 1004 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Sets the Y component of a vector to an integer passed by value
inline XMVECTOR __vectorcall XMVectorSetIntY(FXMVECTOR V, uint32_t y)
{








#line 1018 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1020 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 1024 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Swap y and x
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((3) << 6) | ((2) << 4) | ((0) << 2) | ((1))) );
    // Convert input to vector
    __m128i vTemp = _mm_cvtsi32_si128(static_cast<int>(y));
    // Replace the x component
    vResult = _mm_move_ss(vResult,_mm_castsi128_ps(vTemp));
    // Swap y and x again
    vResult = _mm_shuffle_ps( vResult, vResult, (((3) << 6) | ((2) << 4) | ((0) << 2) | ((1))) );
    return vResult;
#line 1034 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Sets the Z component of a vector to an integer passed by value
inline XMVECTOR __vectorcall XMVectorSetIntZ(FXMVECTOR V, uint32_t z)
{








#line 1048 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1050 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 1054 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Swap z and x
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((3) << 6) | ((0) << 4) | ((1) << 2) | ((2))) );
    // Convert input to vector
    __m128i vTemp = _mm_cvtsi32_si128(static_cast<int>(z));
    // Replace the x component
    vResult = _mm_move_ss(vResult,_mm_castsi128_ps(vTemp));
    // Swap z and x again
    vResult = _mm_shuffle_ps( vResult, vResult, (((3) << 6) | ((0) << 4) | ((1) << 2) | ((2))) );
    return vResult;
#line 1064 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Sets the W component of a vector to an integer passed by value
inline XMVECTOR __vectorcall XMVectorSetIntW(FXMVECTOR V, uint32_t w)
{








#line 1078 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1080 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 1084 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Swap w and x
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((0) << 6) | ((2) << 4) | ((1) << 2) | ((3))) );
    // Convert input to vector
    __m128i vTemp = _mm_cvtsi32_si128(static_cast<int>(w));
    // Replace the x component
    vResult = _mm_move_ss(vResult,_mm_castsi128_ps(vTemp));
    // Swap w and x again
    vResult = _mm_shuffle_ps( vResult, vResult, (((0) << 6) | ((2) << 4) | ((1) << 2) | ((3))) );
    return vResult;
#line 1094 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

// Sets a component of a vector to an integer value passed by pointer

inline XMVECTOR __vectorcall XMVectorSetIntByIndexPtr(FXMVECTOR V, const uint32_t *x, size_t i)
{
    (void)( (!!(x != nullptr)) || (_wassert(L"x != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(1102)), 0) );
    (void)( (!!(i < 4)) || (_wassert(L"i < 4", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(1103)), 0) );
    ;
    XMVECTORU32 tmp;
    tmp.v = V;
    tmp.u[i] = *x;
    return tmp;
}

//------------------------------------------------------------------------------

// Sets the X component of a vector to an integer value passed by pointer

inline XMVECTOR __vectorcall XMVectorSetIntXPtr(FXMVECTOR V, const uint32_t *x)
{
    (void)( (!!(x != nullptr)) || (_wassert(L"x != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(1117)), 0) );








#line 1127 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1129 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_load_ss(reinterpret_cast<const float *>(x));
    XMVECTOR vResult = _mm_move_ss(V,vTemp);
    return vResult;
#line 1133 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Sets the Y component of a vector to an integer value passed by pointer

inline XMVECTOR __vectorcall XMVectorSetIntYPtr(FXMVECTOR V, const uint32_t *y)
{
    (void)( (!!(y != nullptr)) || (_wassert(L"y != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(1139)), 0) );








#line 1149 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1151 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Swap y and x
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((3) << 6) | ((2) << 4) | ((0) << 2) | ((1))) );
    // Convert input to vector
    XMVECTOR vTemp = _mm_load_ss(reinterpret_cast<const float *>(y));
    // Replace the x component
    vResult = _mm_move_ss(vResult,vTemp);
    // Swap y and x again
    vResult = _mm_shuffle_ps( vResult, vResult, (((3) << 6) | ((2) << 4) | ((0) << 2) | ((1))) );
    return vResult;
#line 1161 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Sets the Z component of a vector to an integer value passed by pointer

inline XMVECTOR __vectorcall XMVectorSetIntZPtr(FXMVECTOR V, const uint32_t *z)
{
    (void)( (!!(z != nullptr)) || (_wassert(L"z != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(1167)), 0) );








#line 1177 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1179 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Swap z and x
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((3) << 6) | ((0) << 4) | ((1) << 2) | ((2))) );
    // Convert input to vector
    XMVECTOR vTemp = _mm_load_ss(reinterpret_cast<const float *>(z));
    // Replace the x component
    vResult = _mm_move_ss(vResult,vTemp);
    // Swap z and x again
    vResult = _mm_shuffle_ps( vResult, vResult, (((3) << 6) | ((0) << 4) | ((1) << 2) | ((2))) );
    return vResult;
#line 1189 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

// Sets the W component of a vector to an integer value passed by pointer

inline XMVECTOR __vectorcall XMVectorSetIntWPtr(FXMVECTOR V, const uint32_t *w)
{
    (void)( (!!(w != nullptr)) || (_wassert(L"w != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(1195)), 0) );








#line 1205 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1207 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Swap w and x
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((0) << 6) | ((2) << 4) | ((1) << 2) | ((3))) );
    // Convert input to vector
    XMVECTOR vTemp = _mm_load_ss(reinterpret_cast<const float *>(w));
    // Replace the x component
    vResult = _mm_move_ss(vResult,vTemp);
    // Swap w and x again
    vResult = _mm_shuffle_ps( vResult, vResult, (((0) << 6) | ((2) << 4) | ((1) << 2) | ((3))) );
    return vResult;
#line 1217 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorSwizzle
(
    FXMVECTOR V,
    uint32_t E0,
    uint32_t E1,
    uint32_t E2,
    uint32_t E3
)
{
    (void)( (!!((E0 < 4) && (E1 < 4) && (E2 < 4) && (E3 < 4))) || (_wassert(L"(E0 < 4) && (E1 < 4) && (E2 < 4) && (E3 < 4)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(1230)), 0) );
    ;










#line 1243 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



















#line 1263 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 1267 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    auto aPtr = reinterpret_cast<const uint32_t*>(&V);

    XMVECTOR Result;
    auto pWork = reinterpret_cast<uint32_t*>(&Result);

    pWork[0] = aPtr[E0];
    pWork[1] = aPtr[E1];
    pWork[2] = aPtr[E2];
    pWork[3] = aPtr[E3];

    return Result;
#line 1279 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
inline XMVECTOR __vectorcall XMVectorPermute
(
    FXMVECTOR V1,
    FXMVECTOR V2,
    uint32_t PermuteX,
    uint32_t PermuteY,
    uint32_t PermuteZ,
    uint32_t PermuteW
)
{
    (void)( (!!(PermuteX <= 7 && PermuteY <= 7 && PermuteZ <= 7 && PermuteW <= 7)) || (_wassert(L"PermuteX <= 7 && PermuteY <= 7 && PermuteZ <= 7 && PermuteW <= 7", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(1292)), 0) );
    ;



























#line 1322 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"















#line 1338 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

    const uint32_t *aPtr[2];
    aPtr[0] = reinterpret_cast<const uint32_t*>(&V1);
    aPtr[1] = reinterpret_cast<const uint32_t*>(&V2);

    XMVECTOR Result;
    auto pWork = reinterpret_cast<uint32_t*>(&Result);

    const uint32_t i0 = PermuteX & 3;
    const uint32_t vi0 = PermuteX >> 2;
    pWork[0] = aPtr[vi0][i0];

    const uint32_t i1 = PermuteY & 3;
    const uint32_t vi1 = PermuteY >> 2;
    pWork[1] = aPtr[vi1][i1];

    const uint32_t i2 = PermuteZ & 3;
    const uint32_t vi2 = PermuteZ >> 2;
    pWork[2] = aPtr[vi2][i2];

    const uint32_t i3 = PermuteW & 3;
    const uint32_t vi3 = PermuteW >> 2;
    pWork[3] = aPtr[vi3][i3];

    return Result;
#line 1364 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Define a control vector to be used in XMVectorSelect
// operations.  The four integers specified in XMVectorSelectControl
// serve as indices to select between components in two vectors.
// The first index controls selection for the first component of
// the vectors involved in a select operation, the second index
// controls selection for the second component etc.  A value of
// zero for an index causes the corresponding component from the first
// vector to be selected whereas a one causes the component from the
// second vector to be selected instead.

inline XMVECTOR __vectorcall XMVectorSelectControl
(
    uint32_t VectorIndex0,
    uint32_t VectorIndex1,
    uint32_t VectorIndex2,
    uint32_t VectorIndex3
)
{

    // x=Index0,y=Index1,z=Index2,w=Index3
    __m128i vTemp = _mm_set_epi32(static_cast<int>(VectorIndex3), static_cast<int>(VectorIndex2), static_cast<int>(VectorIndex1), static_cast<int>(VectorIndex0));
    // Any non-zero entries become 0xFFFFFFFF else 0
    vTemp = _mm_cmpgt_epi32(vTemp,g_XMZero);
    return _mm_castsi128_ps(vTemp);






























#line 1422 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorSelect
(
    FXMVECTOR V1,
    FXMVECTOR V2,
    FXMVECTOR Control
)
{










#line 1444 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1446 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp1 = _mm_andnot_ps(Control,V1);
    XMVECTOR vTemp2 = _mm_and_ps(V2,Control);
    return _mm_or_ps(vTemp1,vTemp2);
#line 1450 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorMergeXY
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 1471 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1473 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_unpacklo_ps( V1, V2 );
#line 1475 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorMergeZW
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 1496 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1498 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_unpackhi_ps( V1, V2 );
#line 1500 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorShiftLeft(FXMVECTOR V1, FXMVECTOR V2, uint32_t Elements)
{
    (void)( (!!(Elements < 4)) || (_wassert(L"Elements < 4", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(1506)), 0) );
    ;
    return XMVectorPermute(V1, V2, Elements, ((Elements) + 1), ((Elements) + 2), ((Elements) + 3));
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorRotateLeft(FXMVECTOR V, uint32_t Elements)
{
    (void)( (!!(Elements < 4)) || (_wassert(L"Elements < 4", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(1515)), 0) );
    ;
    return XMVectorSwizzle( V, Elements & 3, (Elements + 1) & 3, (Elements + 2) & 3, (Elements + 3) & 3 );
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorRotateRight(FXMVECTOR V, uint32_t Elements)
{
    (void)( (!!(Elements < 4)) || (_wassert(L"Elements < 4", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(1524)), 0) );
    ;
    return XMVectorSwizzle( V, (4 - (Elements)) & 3, (5 - (Elements)) & 3, (6 - (Elements)) & 3, (7 - (Elements)) & 3 );
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorInsert(FXMVECTOR VD, FXMVECTOR VS, uint32_t VSLeftRotateElements,
                                  uint32_t Select0, uint32_t Select1, uint32_t Select2, uint32_t Select3)
{
    XMVECTOR Control = XMVectorSelectControl(Select0&1, Select1&1, Select2&1, Select3&1);
    return XMVectorSelect( VD, XMVectorRotateLeft(VS, VSLeftRotateElements), Control );
}

//------------------------------------------------------------------------------
// Comparison operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 1561 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1563 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_cmpeq_ps( V1, V2 );
#line 1565 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------


inline XMVECTOR __vectorcall XMVectorEqualR
(
    uint32_t*    pCR,
    FXMVECTOR V1,
    FXMVECTOR V2
)
{
    (void)( (!!(pCR != nullptr)) || (_wassert(L"pCR != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(1577)), 0) );





















#line 1600 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

















#line 1618 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpeq_ps(V1,V2);
    uint32_t CR = 0;
    int iTest = _mm_movemask_ps(vTemp);
    if (iTest==0xf)
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if (!iTest)
    {
        // All elements are not greater
        CR = XM_CRMASK_CR6FALSE;
    }
    *pCR = CR;
    return vTemp;
#line 1633 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Treat the components of the vectors as unsigned integers and
// compare individual bits between the two.  This is useful for
// comparing control vectors and result vectors returned from
// other comparison operations.

inline XMVECTOR __vectorcall XMVectorEqualInt
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 1658 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1660 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i V = _mm_cmpeq_epi32( _mm_castps_si128(V1),_mm_castps_si128(V2) );
    return _mm_castsi128_ps(V);
#line 1663 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------


inline XMVECTOR __vectorcall XMVectorEqualIntR
(
    uint32_t*    pCR,
    FXMVECTOR V1,
    FXMVECTOR V2
)
{
    (void)( (!!(pCR != nullptr)) || (_wassert(L"pCR != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(1675)), 0) );

















#line 1694 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

















#line 1712 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i V = _mm_cmpeq_epi32( _mm_castps_si128(V1),_mm_castps_si128(V2) );
    int iTemp = _mm_movemask_ps(_mm_castsi128_ps(V));
    uint32_t CR = 0;
    if (iTemp==0x0F)
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if (!iTemp)
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    *pCR = CR;
    return _mm_castsi128_ps(V);
#line 1726 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorNearEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2,
    FXMVECTOR Epsilon
)
{




















#line 1758 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 1761 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Get the difference
    XMVECTOR vDelta = _mm_sub_ps(V1,V2);
    // Get the absolute value of the difference
    XMVECTOR vTemp = _mm_setzero_ps();
    vTemp = _mm_sub_ps(vTemp,vDelta);
    vTemp = _mm_max_ps(vTemp,vDelta);
    vTemp = _mm_cmple_ps(vTemp,Epsilon);
    return vTemp;
#line 1770 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorNotEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 1791 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1793 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_cmpneq_ps( V1, V2 );
#line 1795 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorNotEqualInt
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 1816 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1818 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i V = _mm_cmpeq_epi32( _mm_castps_si128(V1),_mm_castps_si128(V2) );
    return _mm_xor_ps(_mm_castsi128_ps(V),g_XMNegOneMask);
#line 1821 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorGreater
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 1842 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1844 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_cmpgt_ps( V1, V2 );
#line 1846 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------


inline XMVECTOR __vectorcall XMVectorGreaterR
(
    uint32_t*    pCR,
    FXMVECTOR V1,
    FXMVECTOR V2
)
{
    (void)( (!!(pCR != nullptr)) || (_wassert(L"pCR != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(1858)), 0) );






















#line 1882 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

















#line 1900 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpgt_ps(V1,V2);
    uint32_t CR = 0;
    int iTest = _mm_movemask_ps(vTemp);
    if (iTest==0xf)
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if (!iTest)
    {
        // All elements are not greater
        CR = XM_CRMASK_CR6FALSE;
    }
    *pCR = CR;
    return vTemp;
#line 1915 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorGreaterOrEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 1936 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 1938 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_cmpge_ps( V1, V2 );
#line 1940 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------


inline XMVECTOR __vectorcall XMVectorGreaterOrEqualR
(
    uint32_t*    pCR,
    FXMVECTOR V1,
    FXMVECTOR V2
)
{
    (void)( (!!(pCR != nullptr)) || (_wassert(L"pCR != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(1952)), 0) );






















#line 1976 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

















#line 1994 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpge_ps(V1,V2);
    uint32_t CR = 0;
    int iTest = _mm_movemask_ps(vTemp);
    if (iTest==0xf)
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if (!iTest)
    {
        // All elements are not greater
        CR = XM_CRMASK_CR6FALSE;
    }
    *pCR = CR;
    return vTemp;
#line 2009 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorLess
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 2030 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 2032 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_cmplt_ps( V1, V2 );
#line 2034 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorLessOrEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 2055 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 2057 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_cmple_ps( V1, V2 );
#line 2059 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorInBounds
(
    FXMVECTOR V,
    FXMVECTOR Bounds
)
{










#line 2080 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"









#line 2090 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Test if less than or equal
    XMVECTOR vTemp1 = _mm_cmple_ps(V,Bounds);
    // Negate the bounds
    XMVECTOR vTemp2 = _mm_mul_ps(Bounds,g_XMNegativeOne);
    // Test if greater or equal (Reversed)
    vTemp2 = _mm_cmple_ps(vTemp2,V);
    // Blend answers
    vTemp1 = _mm_and_ps(vTemp1,vTemp2);
    return vTemp1;
#line 2100 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------


inline XMVECTOR __vectorcall XMVectorInBoundsR
(
    uint32_t*    pCR,
    FXMVECTOR V,
    FXMVECTOR Bounds
)
{
    (void)( (!!(pCR != nullptr)) || (_wassert(L"pCR != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(2112)), 0) );


















#line 2132 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



















#line 2152 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Test if less than or equal
    XMVECTOR vTemp1 = _mm_cmple_ps(V,Bounds);
    // Negate the bounds
    XMVECTOR vTemp2 = _mm_mul_ps(Bounds,g_XMNegativeOne);
    // Test if greater or equal (Reversed)
    vTemp2 = _mm_cmple_ps(vTemp2,V);
    // Blend answers
    vTemp1 = _mm_and_ps(vTemp1,vTemp2);

    uint32_t CR = 0;
    if (_mm_movemask_ps(vTemp1)==0xf) {
        // All elements are in bounds
        CR = XM_CRMASK_CR6BOUNDS;
    }
    *pCR = CR;
    return vTemp1;
#line 2169 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------


#pragma float_control(push)
#pragma float_control(precise, on)
#line 2177 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

inline XMVECTOR __vectorcall XMVectorIsNaN
(
    FXMVECTOR V
)
{










#line 2194 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 2199 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Test against itself. NaN is always not equal
    return _mm_cmpneq_ps(V,V);
#line 2202 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}


#pragma float_control(pop)
#line 2207 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorIsInfinite
(
    FXMVECTOR V
)
{










#line 2226 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






#line 2233 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Mask off the sign bit
    __m128 vTemp = _mm_and_ps(V,g_XMAbsMask);
    // Compare to infinity
    vTemp = _mm_cmpeq_ps(vTemp,g_XMInfinity);
    // If any are infinity, the signs are true.
    return vTemp;
#line 2240 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Rounding and clamping operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorMin
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 2265 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 2267 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_min_ps( V1, V2 );
#line 2269 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorMax
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 2290 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 2292 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_max_ps( V1, V2 );
#line 2294 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

namespace Internal
{
    // Round to nearest (even) a.k.a. banker's rounding
    inline float round_to_nearest( float x )
    {
        float i = floorf(x);
        x -= i;
        if(x < 0.5f)
            return i;
        if(x > 0.5f)
            return i + 1.f;

        float int_part;
        (void)modff( i / 2.f, &int_part );
        if ( (2.f*int_part) == i )
        {
            return i;
        }

        return i + 1.f;
    }
}


#pragma float_control(push)
#pragma float_control(precise, on)
#line 2325 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

inline XMVECTOR __vectorcall XMVectorRound
(
    FXMVECTOR V
)
{










#line 2342 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"












#line 2355 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 2357 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128 sign = _mm_and_ps( V, g_XMNegativeZero );
    __m128 sMagic = _mm_or_ps( g_XMNoFraction, sign );
    __m128 R1 = _mm_add_ps( V, sMagic );
    R1 = _mm_sub_ps( R1, sMagic );
    __m128 R2 = _mm_and_ps( V, g_XMAbsMask );
    __m128 mask = _mm_cmple_ps( R2, g_XMNoFraction );
    R2 = _mm_andnot_ps(mask,V);
    R1 = _mm_and_ps(R1,mask);
    XMVECTOR vResult = _mm_xor_ps(R1, R2);
    return vResult;
#line 2368 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}


#pragma float_control(pop)
#line 2373 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorTruncate
(
    FXMVECTOR V
)
{
























#line 2406 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"













#line 2420 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 2422 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // To handle NAN, INF and numbers greater than 8388608, use masking
    // Get the abs value
    __m128i vTest = _mm_and_si128(_mm_castps_si128(V),g_XMAbsMask);
    // Test for greater than 8388608 (All floats with NO fractionals, NAN and INF
    vTest = _mm_cmplt_epi32(vTest,g_XMNoFraction);
    // Convert to int and back to float for rounding with truncation
    __m128i vInt = _mm_cvttps_epi32(V);
    // Convert back to floats
    XMVECTOR vResult = _mm_cvtepi32_ps(vInt);
    // All numbers less than 8388608 will use the round to int
    vResult = _mm_and_ps(vResult,_mm_castsi128_ps(vTest));
    // All others, use the ORIGINAL value
    vTest = _mm_andnot_si128(vTest,_mm_castps_si128(V));
    vResult = _mm_or_ps(vResult,_mm_castsi128_ps(vTest));
    return vResult;
#line 2438 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorFloor
(
    FXMVECTOR V
)
{








#line 2456 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
















#line 2473 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 2475 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // To handle NAN, INF and numbers greater than 8388608, use masking
    __m128i vTest = _mm_and_si128(_mm_castps_si128(V),g_XMAbsMask);
    vTest = _mm_cmplt_epi32(vTest,g_XMNoFraction);
    // Truncate
    __m128i vInt = _mm_cvttps_epi32(V);
    XMVECTOR vResult = _mm_cvtepi32_ps(vInt);
    __m128 vLarger = _mm_cmpgt_ps( vResult, V );
    // 0 -> 0, 0xffffffff -> -1.0f
    vLarger = _mm_cvtepi32_ps( _mm_castps_si128( vLarger ) );
    vResult = _mm_add_ps( vResult, vLarger );
    // All numbers less than 8388608 will use the round to int
    vResult = _mm_and_ps(vResult,_mm_castsi128_ps(vTest));
    // All others, use the ORIGINAL value
    vTest = _mm_andnot_si128(vTest,_mm_castps_si128(V));
    vResult = _mm_or_ps(vResult,_mm_castsi128_ps(vTest));
    return vResult;
#line 2492 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorCeiling
(
    FXMVECTOR V
)
{








#line 2510 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
















#line 2527 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 2529 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // To handle NAN, INF and numbers greater than 8388608, use masking
    __m128i vTest = _mm_and_si128(_mm_castps_si128(V),g_XMAbsMask);
    vTest = _mm_cmplt_epi32(vTest,g_XMNoFraction);
    // Truncate
    __m128i vInt = _mm_cvttps_epi32(V);
    XMVECTOR vResult = _mm_cvtepi32_ps(vInt);
    __m128 vSmaller = _mm_cmplt_ps( vResult, V );
    // 0 -> 0, 0xffffffff -> -1.0f
    vSmaller = _mm_cvtepi32_ps( _mm_castps_si128( vSmaller ) );
    vResult = _mm_sub_ps( vResult, vSmaller );
    // All numbers less than 8388608 will use the round to int
    vResult = _mm_and_ps(vResult,_mm_castsi128_ps(vTest));
    // All others, use the ORIGINAL value
    vTest = _mm_andnot_si128(vTest,_mm_castps_si128(V));
    vResult = _mm_or_ps(vResult,_mm_castsi128_ps(vTest));
    return vResult;
#line 2546 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorClamp
(
    FXMVECTOR V,
    FXMVECTOR Min,
    FXMVECTOR Max
)
{
    (void)( (!!(XMVector4LessOrEqual(Min, Max))) || (_wassert(L"XMVector4LessOrEqual(Min, Max)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(2557)), 0) );








#line 2567 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 2572 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult;
    vResult = _mm_max_ps(Min, V);
    vResult = _mm_min_ps(Max, vResult);
    return vResult;
#line 2577 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorSaturate
(
    FXMVECTOR V
)
{






#line 2593 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 2598 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Set <0 to 0
    XMVECTOR vResult = _mm_max_ps(V,g_XMZero);
    // Set>1 to 1
    return _mm_min_ps(vResult,g_XMOne);
#line 2603 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Bitwise logical operations
//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorAndInt
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 2626 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 2628 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_and_ps(V1,V2);
#line 2630 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorAndCInt
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 2651 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 2653 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i V = _mm_andnot_si128( _mm_castps_si128(V2), _mm_castps_si128(V1) );
    return _mm_castsi128_ps(V);
#line 2656 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorOrInt
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 2677 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 2679 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i V = _mm_or_si128( _mm_castps_si128(V1), _mm_castps_si128(V2) );
    return _mm_castsi128_ps(V);
#line 2682 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorNorInt
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 2703 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 2706 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i Result;
    Result = _mm_or_si128( _mm_castps_si128(V1), _mm_castps_si128(V2) );
    Result = _mm_andnot_si128( Result,g_XMNegOneMask);
    return _mm_castsi128_ps(Result);
#line 2711 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorXorInt
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 2732 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 2734 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i V = _mm_xor_si128( _mm_castps_si128(V1), _mm_castps_si128(V2) );
    return _mm_castsi128_ps(V);
#line 2737 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Computation operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorNegate
(
    FXMVECTOR V
)
{










#line 2761 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 2763 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR Z;

    Z = _mm_setzero_ps();

    return _mm_sub_ps( Z, V );
#line 2769 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorAdd
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 2790 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 2792 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_add_ps( V1, V2 );
#line 2794 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorSum
(
    FXMVECTOR V
)
{









#line 2813 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"










#line 2824 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 2827 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_shuffle_ps( V, V, (((2) << 6) | ((3) << 4) | ((0) << 2) | ((1))) );
    XMVECTOR vTemp2 = _mm_add_ps(V, vTemp);
    vTemp = _mm_shuffle_ps( vTemp2, vTemp2, (((1) << 6) | ((0) << 4) | ((3) << 2) | ((2))) );
    return _mm_add_ps(vTemp, vTemp2);
#line 2832 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorAddAngles
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{




















#line 2863 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"













#line 2877 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Adjust the angles
    XMVECTOR vResult = _mm_add_ps(V1,V2);
    // Less than Pi?
    XMVECTOR vOffset = _mm_cmplt_ps(vResult,g_XMNegativePi);
    vOffset = _mm_and_ps(vOffset,g_XMTwoPi);
    // Add 2Pi to all entries less than -Pi
    vResult = _mm_add_ps(vResult,vOffset);
    // Greater than or equal to Pi?
    vOffset = _mm_cmpge_ps(vResult,g_XMPi);
    vOffset = _mm_and_ps(vOffset,g_XMTwoPi);
    // Sub 2Pi to all entries greater than Pi
    vResult = _mm_sub_ps(vResult,vOffset);
    return vResult;
#line 2891 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorSubtract
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 2912 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 2914 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_sub_ps( V1, V2 );
#line 2916 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorSubtractAngles
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{




















#line 2947 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"













#line 2961 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Adjust the angles
    XMVECTOR vResult = _mm_sub_ps(V1,V2);
    // Less than Pi?
    XMVECTOR vOffset = _mm_cmplt_ps(vResult,g_XMNegativePi);
    vOffset = _mm_and_ps(vOffset,g_XMTwoPi);
    // Add 2Pi to all entries less than -Pi
    vResult = _mm_add_ps(vResult,vOffset);
    // Greater than or equal to Pi?
    vOffset = _mm_cmpge_ps(vResult,g_XMPi);
    vOffset = _mm_and_ps(vOffset,g_XMTwoPi);
    // Sub 2Pi to all entries greater than Pi
    vResult = _mm_sub_ps(vResult,vOffset);
    return vResult;
#line 2975 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorMultiply
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{








#line 2994 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 2996 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_mul_ps( V1, V2 );
#line 2998 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorMultiplyAdd
(
    FXMVECTOR V1,
    FXMVECTOR V2,
    FXMVECTOR V3
)
{








#line 3018 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"





#line 3024 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 3026 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_mul_ps( V1, V2 );
    return _mm_add_ps(vResult, V3 );
#line 3029 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorDivide
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{








#line 3048 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"











#line 3060 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_div_ps( V1, V2 );
#line 3062 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorNegativeMultiplySubtract
(
    FXMVECTOR V1,
    FXMVECTOR V2,
    FXMVECTOR V3
)
{








#line 3082 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"





#line 3088 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 3090 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR R = _mm_mul_ps( V1, V2 );
    return _mm_sub_ps( V3, R );
#line 3093 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorScale
(
    FXMVECTOR V,
    float    ScaleFactor
)
{








#line 3112 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 3114 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
   XMVECTOR vResult = _mm_set_ps1(ScaleFactor);
   return _mm_mul_ps(vResult,V);
#line 3117 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorReciprocalEst
(
    FXMVECTOR V
)
{








#line 3135 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 3137 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_rcp_ps(V);
#line 3139 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorReciprocal
(
    FXMVECTOR V
)
{








#line 3157 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"











#line 3169 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_div_ps(g_XMOne,V);
#line 3171 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Return an estimated square root
inline XMVECTOR __vectorcall XMVectorSqrtEst
(
    FXMVECTOR V
)
{








#line 3189 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"











#line 3201 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_sqrt_ps(V);
#line 3203 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorSqrt
(
    FXMVECTOR V
)
{








#line 3221 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

















#line 3239 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_sqrt_ps(V);
#line 3241 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorReciprocalSqrtEst
(
    FXMVECTOR V
)
{








#line 3259 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 3261 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    return _mm_rsqrt_ps(V);
#line 3263 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorReciprocalSqrt
(
    FXMVECTOR V
)
{








#line 3281 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"











#line 3293 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_sqrt_ps(V);
    vResult = _mm_div_ps(g_XMOne,vResult);
    return vResult;
#line 3297 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorExp2
(
    FXMVECTOR V
)
{










#line 3317 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"





















































#line 3371 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i itrunc = _mm_cvttps_epi32(V);
    __m128 ftrunc = _mm_cvtepi32_ps(itrunc);
    __m128 y = _mm_sub_ps(V, ftrunc);
    __m128 poly = _mm_mul_ps(g_XMExpEst7, y);
    poly = _mm_add_ps(g_XMExpEst6, poly);
    poly = _mm_mul_ps(poly, y);
    poly = _mm_add_ps(g_XMExpEst5, poly);
    poly = _mm_mul_ps(poly, y);
    poly = _mm_add_ps(g_XMExpEst4, poly);
    poly = _mm_mul_ps(poly, y);
    poly = _mm_add_ps(g_XMExpEst3, poly);
    poly = _mm_mul_ps(poly, y);
    poly = _mm_add_ps(g_XMExpEst2, poly);
    poly = _mm_mul_ps(poly, y);
    poly = _mm_add_ps(g_XMExpEst1, poly);
    poly = _mm_mul_ps(poly, y);
    poly = _mm_add_ps(g_XMOne, poly);

    __m128i biased = _mm_add_epi32(itrunc, g_XMExponentBias);
    biased = _mm_slli_epi32(biased, 23);
    __m128 result0 = _mm_div_ps(_mm_castsi128_ps(biased), poly);

    biased = _mm_add_epi32(itrunc, g_XM253);
    biased = _mm_slli_epi32(biased, 23);
    __m128 result1 = _mm_div_ps(_mm_castsi128_ps(biased), poly);
    result1 = _mm_mul_ps(g_XMMinNormal.v, result1);

    // Use selection to handle the cases
    //  if (V is NaN) -> QNaN;
    //  else if (V sign bit set)
    //      if (V > -150)
    //         if (V.exponent < -126) -> result1
    //         else -> result0
    //      else -> +0
    //  else
    //      if (V < 128) -> result0
    //      else -> +inf

    __m128i comp = _mm_cmplt_epi32( _mm_castps_si128(V), g_XMBin128);
    __m128i select0 = _mm_and_si128(comp, _mm_castps_si128(result0));
    __m128i select1 = _mm_andnot_si128(comp, g_XMInfinity);
    __m128i result2 = _mm_or_si128(select0, select1);

    comp = _mm_cmplt_epi32(itrunc, g_XMSubnormalExponent);
    select1 = _mm_and_si128(comp, _mm_castps_si128(result1));
    select0 = _mm_andnot_si128(comp, _mm_castps_si128(result0));
    __m128i result3 = _mm_or_si128(select0, select1);

    comp = _mm_cmplt_epi32(_mm_castps_si128(V), g_XMBinNeg150);
    select0 = _mm_and_si128(comp, result3);
    select1 = _mm_andnot_si128(comp, g_XMZero);
    __m128i result4 = _mm_or_si128(select0, select1);

    __m128i sign = _mm_and_si128(_mm_castps_si128(V), g_XMNegativeZero);
    comp = _mm_cmpeq_epi32(sign, g_XMNegativeZero);
    select0 = _mm_and_si128(comp, result4);
    select1 = _mm_andnot_si128(comp, result2);
    __m128i result5 = _mm_or_si128(select0, select1);

    __m128i t0 = _mm_and_si128(_mm_castps_si128(V), g_XMQNaNTest);
    __m128i t1 = _mm_and_si128(_mm_castps_si128(V), g_XMInfinity);
    t0 = _mm_cmpeq_epi32(t0, g_XMZero);
    t1 = _mm_cmpeq_epi32(t1, g_XMInfinity);
    __m128i isNaN = _mm_andnot_si128(t0, t1);

    select0 = _mm_and_si128(isNaN, g_XMQNaN);
    select1 = _mm_andnot_si128(isNaN, result5);
    __m128i vResult = _mm_or_si128(select0, select1);

    return _mm_castsi128_ps(vResult);
#line 3442 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorExpE
(
    FXMVECTOR V
)
{










#line 3462 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

























































#line 3520 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // expE(V) = exp2(vin*log2(e))
    __m128 Ve = _mm_mul_ps(g_XMLgE, V);

    __m128i itrunc = _mm_cvttps_epi32(Ve);
    __m128 ftrunc = _mm_cvtepi32_ps(itrunc);
    __m128 y = _mm_sub_ps(Ve, ftrunc);
    __m128 poly = _mm_mul_ps(g_XMExpEst7, y);
    poly = _mm_add_ps(g_XMExpEst6, poly);
    poly = _mm_mul_ps(poly, y);
    poly = _mm_add_ps(g_XMExpEst5, poly);
    poly = _mm_mul_ps(poly, y);
    poly = _mm_add_ps(g_XMExpEst4, poly);
    poly = _mm_mul_ps(poly, y);
    poly = _mm_add_ps(g_XMExpEst3, poly);
    poly = _mm_mul_ps(poly, y);
    poly = _mm_add_ps(g_XMExpEst2, poly);
    poly = _mm_mul_ps(poly, y);
    poly = _mm_add_ps(g_XMExpEst1, poly);
    poly = _mm_mul_ps(poly, y);
    poly = _mm_add_ps(g_XMOne, poly);

    __m128i biased = _mm_add_epi32(itrunc, g_XMExponentBias);
    biased = _mm_slli_epi32(biased, 23);
    __m128 result0 = _mm_div_ps(_mm_castsi128_ps(biased), poly);

    biased = _mm_add_epi32(itrunc, g_XM253);
    biased = _mm_slli_epi32(biased, 23);
    __m128 result1 = _mm_div_ps(_mm_castsi128_ps(biased), poly);
    result1 = _mm_mul_ps(g_XMMinNormal.v, result1);

    // Use selection to handle the cases
    //  if (V is NaN) -> QNaN;
    //  else if (V sign bit set)
    //      if (V > -150)
    //         if (V.exponent < -126) -> result1
    //         else -> result0
    //      else -> +0
    //  else
    //      if (V < 128) -> result0
    //      else -> +inf

    __m128i comp = _mm_cmplt_epi32( _mm_castps_si128(Ve), g_XMBin128);
    __m128i select0 = _mm_and_si128(comp, _mm_castps_si128(result0));
    __m128i select1 = _mm_andnot_si128(comp, g_XMInfinity);
    __m128i result2 = _mm_or_si128(select0, select1);

    comp = _mm_cmplt_epi32(itrunc, g_XMSubnormalExponent);
    select1 = _mm_and_si128(comp, _mm_castps_si128(result1));
    select0 = _mm_andnot_si128(comp, _mm_castps_si128(result0));
    __m128i result3 = _mm_or_si128(select0, select1);

    comp = _mm_cmplt_epi32(_mm_castps_si128(Ve), g_XMBinNeg150);
    select0 = _mm_and_si128(comp, result3);
    select1 = _mm_andnot_si128(comp, g_XMZero);
    __m128i result4 = _mm_or_si128(select0, select1);

    __m128i sign = _mm_and_si128(_mm_castps_si128(Ve), g_XMNegativeZero);
    comp = _mm_cmpeq_epi32(sign, g_XMNegativeZero);
    select0 = _mm_and_si128(comp, result4);
    select1 = _mm_andnot_si128(comp, result2);
    __m128i result5 = _mm_or_si128(select0, select1);

    __m128i t0 = _mm_and_si128(_mm_castps_si128(Ve), g_XMQNaNTest);
    __m128i t1 = _mm_and_si128(_mm_castps_si128(Ve), g_XMInfinity);
    t0 = _mm_cmpeq_epi32(t0, g_XMZero);
    t1 = _mm_cmpeq_epi32(t1, g_XMInfinity);
    __m128i isNaN = _mm_andnot_si128(t0, t1);

    select0 = _mm_and_si128(isNaN, g_XMQNaN);
    select1 = _mm_andnot_si128(isNaN, result5);
    __m128i vResult = _mm_or_si128(select0, select1);

    return _mm_castsi128_ps(vResult);
#line 3594 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorExp
(
    FXMVECTOR V
)
{
    return XMVectorExp2(V);
}

//------------------------------------------------------------------------------



namespace Internal
{
    inline __m128i multi_sll_epi32(__m128i value, __m128i count)
    {
        __m128i v = _mm_shuffle_epi32(value, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))));
        __m128i c = _mm_shuffle_epi32(count, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))));
        c = _mm_and_si128(c, g_XMMaskX);
        __m128i r0 = _mm_sll_epi32(v, c);

        v = _mm_shuffle_epi32(value, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))));
        c = _mm_shuffle_epi32(count, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))));
        c = _mm_and_si128(c, g_XMMaskX);
        __m128i r1 = _mm_sll_epi32(v, c);

        v = _mm_shuffle_epi32(value, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))));
        c = _mm_shuffle_epi32(count, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))));
        c = _mm_and_si128(c, g_XMMaskX);
        __m128i r2 = _mm_sll_epi32(v, c);

        v = _mm_shuffle_epi32(value, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))));
        c = _mm_shuffle_epi32(count, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))));
        c = _mm_and_si128(c, g_XMMaskX);
        __m128i r3 = _mm_sll_epi32(v, c);

        // (r0,r0,r1,r1)
        __m128 r01 = _mm_shuffle_ps(_mm_castsi128_ps(r0), _mm_castsi128_ps(r1), (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))));
        // (r2,r2,r3,r3)
        __m128 r23 = _mm_shuffle_ps(_mm_castsi128_ps(r2), _mm_castsi128_ps(r3), (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))));
        // (r0,r1,r2,r3)
        __m128 result = _mm_shuffle_ps(r01, r23, (((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));
        return _mm_castps_si128(result);
    }

    inline __m128i multi_srl_epi32(__m128i value, __m128i count)
    {
        __m128i v = _mm_shuffle_epi32(value, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))));
        __m128i c = _mm_shuffle_epi32(count, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))));
        c = _mm_and_si128(c, g_XMMaskX);
        __m128i r0 = _mm_srl_epi32(v, c);

        v = _mm_shuffle_epi32(value, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))));
        c = _mm_shuffle_epi32(count, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))));
        c = _mm_and_si128(c, g_XMMaskX);
        __m128i r1 = _mm_srl_epi32(v, c);

        v = _mm_shuffle_epi32(value, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))));
        c = _mm_shuffle_epi32(count, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))));
        c = _mm_and_si128(c, g_XMMaskX);
        __m128i r2 = _mm_srl_epi32(v, c);

        v = _mm_shuffle_epi32(value, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))));
        c = _mm_shuffle_epi32(count, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))));
        c = _mm_and_si128(c, g_XMMaskX);
        __m128i r3 = _mm_srl_epi32(v, c);

        // (r0,r0,r1,r1)
        __m128 r01 = _mm_shuffle_ps(_mm_castsi128_ps(r0), _mm_castsi128_ps(r1), (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))));
        // (r2,r2,r3,r3)
        __m128 r23 = _mm_shuffle_ps(_mm_castsi128_ps(r2), _mm_castsi128_ps(r3), (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))));
        // (r0,r1,r2,r3)
        __m128 result = _mm_shuffle_ps(r01, r23, (((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));
        return _mm_castps_si128(result);
    }

    inline __m128i GetLeadingBit(const __m128i value)
    {
        static const XMVECTORI32 g_XM0000FFFF = { { { 0x0000FFFF, 0x0000FFFF, 0x0000FFFF, 0x0000FFFF } } };
        static const XMVECTORI32 g_XM000000FF = { { { 0x000000FF, 0x000000FF, 0x000000FF, 0x000000FF } } };
        static const XMVECTORI32 g_XM0000000F = { { { 0x0000000F, 0x0000000F, 0x0000000F, 0x0000000F } } };
        static const XMVECTORI32 g_XM00000003 = { { { 0x00000003, 0x00000003, 0x00000003, 0x00000003 } } };

        __m128i v = value, r, c, b, s;

        c = _mm_cmpgt_epi32(v, g_XM0000FFFF);   // c = (v > 0xFFFF)
        b = _mm_srli_epi32(c, 31);              // b = (c ? 1 : 0)
        r = _mm_slli_epi32(b, 4);               // r = (b << 4)
        v = multi_srl_epi32(v, r);              // v = (v >> r)

        c = _mm_cmpgt_epi32(v, g_XM000000FF);   // c = (v > 0xFF)
        b = _mm_srli_epi32(c, 31);              // b = (c ? 1 : 0)
        s = _mm_slli_epi32(b, 3);               // s = (b << 3)
        v = multi_srl_epi32(v, s);              // v = (v >> s)
        r = _mm_or_si128(r, s);                 // r = (r | s)

        c = _mm_cmpgt_epi32(v, g_XM0000000F);   // c = (v > 0xF)
        b = _mm_srli_epi32(c, 31);              // b = (c ? 1 : 0)
        s = _mm_slli_epi32(b, 2);               // s = (b << 2)
        v = multi_srl_epi32(v, s);              // v = (v >> s)
        r = _mm_or_si128(r, s);                 // r = (r | s)

        c = _mm_cmpgt_epi32(v, g_XM00000003);   // c = (v > 0x3)
        b = _mm_srli_epi32(c, 31);              // b = (c ? 1 : 0)
        s = _mm_slli_epi32(b, 1);               // s = (b << 1)
        v = multi_srl_epi32(v, s);              // v = (v >> s)
        r = _mm_or_si128(r, s);                 // r = (r | s)

        s = _mm_srli_epi32(v, 1);
        r = _mm_or_si128(r, s);
        return r;
    }
} // namespace Internal

#line 3713 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
















































#line 3762 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorLog2
(
    FXMVECTOR V
)
{












#line 3783 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




























































#line 3844 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i rawBiased = _mm_and_si128(_mm_castps_si128(V), g_XMInfinity);
    __m128i trailing = _mm_and_si128(_mm_castps_si128(V), g_XMQNaNTest);
    __m128i isExponentZero = _mm_cmpeq_epi32(g_XMZero, rawBiased);

    // Compute exponent and significand for normals.
    __m128i biased = _mm_srli_epi32(rawBiased, 23);
    __m128i exponentNor = _mm_sub_epi32(biased, g_XMExponentBias);
    __m128i trailingNor = trailing;

    // Compute exponent and significand for subnormals.
    __m128i leading = Internal::GetLeadingBit(trailing);
    __m128i shift = _mm_sub_epi32(g_XMNumTrailing, leading);
    __m128i exponentSub = _mm_sub_epi32(g_XMSubnormalExponent, shift);
    __m128i trailingSub = Internal::multi_sll_epi32(trailing, shift);
    trailingSub = _mm_and_si128(trailingSub, g_XMQNaNTest);

    __m128i select0 = _mm_and_si128(isExponentZero, exponentSub);
    __m128i select1 = _mm_andnot_si128(isExponentZero, exponentNor);
    __m128i e = _mm_or_si128(select0, select1);

    select0 = _mm_and_si128(isExponentZero, trailingSub);
    select1 = _mm_andnot_si128(isExponentZero, trailingNor);
    __m128i t = _mm_or_si128(select0, select1);

    // Compute the approximation.
    __m128i tmp = _mm_or_si128(g_XMOne, t);
    __m128 y = _mm_sub_ps(_mm_castsi128_ps(tmp), g_XMOne);

    __m128 log2 = _mm_mul_ps(g_XMLogEst7, y);
    log2 = _mm_add_ps(g_XMLogEst6, log2);
    log2 = _mm_mul_ps(log2, y);
    log2 = _mm_add_ps(g_XMLogEst5, log2);
    log2 = _mm_mul_ps(log2, y);
    log2 = _mm_add_ps(g_XMLogEst4, log2);
    log2 = _mm_mul_ps(log2, y);
    log2 = _mm_add_ps(g_XMLogEst3, log2);
    log2 = _mm_mul_ps(log2, y);
    log2 = _mm_add_ps(g_XMLogEst2, log2);
    log2 = _mm_mul_ps(log2, y);
    log2 = _mm_add_ps(g_XMLogEst1, log2);
    log2 = _mm_mul_ps(log2, y);
    log2 = _mm_add_ps(g_XMLogEst0, log2);
    log2 = _mm_mul_ps(log2, y);
    log2 = _mm_add_ps(log2, _mm_cvtepi32_ps(e));

    //  if (x is NaN) -> QNaN
    //  else if (V is positive)
    //      if (V is infinite) -> +inf
    //      else -> log2(V)
    //  else
    //      if (V is zero) -> -inf
    //      else -> -QNaN

    __m128i isInfinite = _mm_and_si128(_mm_castps_si128(V), g_XMAbsMask);
    isInfinite = _mm_cmpeq_epi32(isInfinite, g_XMInfinity);

    __m128i isGreaterZero = _mm_cmpgt_epi32(_mm_castps_si128(V), g_XMZero);
    __m128i isNotFinite = _mm_cmpgt_epi32(_mm_castps_si128(V), g_XMInfinity);
    __m128i isPositive = _mm_andnot_si128(isNotFinite, isGreaterZero);

    __m128i isZero = _mm_and_si128(_mm_castps_si128(V), g_XMAbsMask);
    isZero = _mm_cmpeq_epi32(isZero, g_XMZero);

    __m128i t0 = _mm_and_si128(_mm_castps_si128(V), g_XMQNaNTest);
    __m128i t1 = _mm_and_si128(_mm_castps_si128(V), g_XMInfinity);
    t0 = _mm_cmpeq_epi32(t0, g_XMZero);
    t1 = _mm_cmpeq_epi32(t1, g_XMInfinity);
    __m128i isNaN = _mm_andnot_si128(t0, t1);

    select0 = _mm_and_si128(isInfinite, g_XMInfinity);
    select1 = _mm_andnot_si128(isInfinite, _mm_castps_si128(log2));
    __m128i result = _mm_or_si128(select0, select1);

    select0 = _mm_and_si128(isZero, g_XMNegInfinity);
    select1 = _mm_andnot_si128(isZero, g_XMNegQNaN);
    tmp = _mm_or_si128(select0, select1);

    select0 = _mm_and_si128(isPositive, result);
    select1 = _mm_andnot_si128(isPositive, tmp);
    result = _mm_or_si128(select0, select1);

    select0 = _mm_and_si128(isNaN, g_XMQNaN);
    select1 = _mm_andnot_si128(isNaN, result);
    result = _mm_or_si128(select0, select1);

    return _mm_castsi128_ps(result);
#line 3931 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorLogE
(
    FXMVECTOR V
)
{










#line 3951 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






























































#line 4014 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i rawBiased = _mm_and_si128(_mm_castps_si128(V), g_XMInfinity);
    __m128i trailing = _mm_and_si128(_mm_castps_si128(V), g_XMQNaNTest);
    __m128i isExponentZero = _mm_cmpeq_epi32(g_XMZero, rawBiased);

    // Compute exponent and significand for normals.
    __m128i biased = _mm_srli_epi32(rawBiased, 23);
    __m128i exponentNor = _mm_sub_epi32(biased, g_XMExponentBias);
    __m128i trailingNor = trailing;

    // Compute exponent and significand for subnormals.
    __m128i leading = Internal::GetLeadingBit(trailing);
    __m128i shift = _mm_sub_epi32(g_XMNumTrailing, leading);
    __m128i exponentSub = _mm_sub_epi32(g_XMSubnormalExponent, shift);
    __m128i trailingSub = Internal::multi_sll_epi32(trailing, shift);
    trailingSub = _mm_and_si128(trailingSub, g_XMQNaNTest);

    __m128i select0 = _mm_and_si128(isExponentZero, exponentSub);
    __m128i select1 = _mm_andnot_si128(isExponentZero, exponentNor);
    __m128i e = _mm_or_si128(select0, select1);

    select0 = _mm_and_si128(isExponentZero, trailingSub);
    select1 = _mm_andnot_si128(isExponentZero, trailingNor);
    __m128i t = _mm_or_si128(select0, select1);

    // Compute the approximation.
    __m128i tmp = _mm_or_si128(g_XMOne, t);
    __m128 y = _mm_sub_ps(_mm_castsi128_ps(tmp), g_XMOne);

    __m128 log2 = _mm_mul_ps(g_XMLogEst7, y);
    log2 = _mm_add_ps(g_XMLogEst6, log2);
    log2 = _mm_mul_ps(log2, y);
    log2 = _mm_add_ps(g_XMLogEst5, log2);
    log2 = _mm_mul_ps(log2, y);
    log2 = _mm_add_ps(g_XMLogEst4, log2);
    log2 = _mm_mul_ps(log2, y);
    log2 = _mm_add_ps(g_XMLogEst3, log2);
    log2 = _mm_mul_ps(log2, y);
    log2 = _mm_add_ps(g_XMLogEst2, log2);
    log2 = _mm_mul_ps(log2, y);
    log2 = _mm_add_ps(g_XMLogEst1, log2);
    log2 = _mm_mul_ps(log2, y);
    log2 = _mm_add_ps(g_XMLogEst0, log2);
    log2 = _mm_mul_ps(log2, y);
    log2 = _mm_add_ps(log2, _mm_cvtepi32_ps(e));

    log2 = _mm_mul_ps(g_XMInvLgE, log2);

    //  if (x is NaN) -> QNaN
    //  else if (V is positive)
    //      if (V is infinite) -> +inf
    //      else -> log2(V)
    //  else
    //      if (V is zero) -> -inf
    //      else -> -QNaN

    __m128i isInfinite = _mm_and_si128(_mm_castps_si128(V), g_XMAbsMask);
    isInfinite = _mm_cmpeq_epi32(isInfinite, g_XMInfinity);

    __m128i isGreaterZero = _mm_cmpgt_epi32(_mm_castps_si128(V), g_XMZero);
    __m128i isNotFinite = _mm_cmpgt_epi32(_mm_castps_si128(V), g_XMInfinity);
    __m128i isPositive = _mm_andnot_si128(isNotFinite, isGreaterZero);

    __m128i isZero = _mm_and_si128(_mm_castps_si128(V), g_XMAbsMask);
    isZero = _mm_cmpeq_epi32(isZero, g_XMZero);

    __m128i t0 = _mm_and_si128(_mm_castps_si128(V), g_XMQNaNTest);
    __m128i t1 = _mm_and_si128(_mm_castps_si128(V), g_XMInfinity);
    t0 = _mm_cmpeq_epi32(t0, g_XMZero);
    t1 = _mm_cmpeq_epi32(t1, g_XMInfinity);
    __m128i isNaN = _mm_andnot_si128(t0, t1);

    select0 = _mm_and_si128(isInfinite, g_XMInfinity);
    select1 = _mm_andnot_si128(isInfinite, _mm_castps_si128(log2));
    __m128i result = _mm_or_si128(select0, select1);

    select0 = _mm_and_si128(isZero, g_XMNegInfinity);
    select1 = _mm_andnot_si128(isZero, g_XMNegQNaN);
    tmp = _mm_or_si128(select0, select1);

    select0 = _mm_and_si128(isPositive, result);
    select1 = _mm_andnot_si128(isPositive, tmp);
    result = _mm_or_si128(select0, select1);

    select0 = _mm_and_si128(isNaN, g_XMQNaN);
    select1 = _mm_andnot_si128(isNaN, result);
    result = _mm_or_si128(select0, select1);

    return _mm_castsi128_ps(result);
#line 4103 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorLog
(
    FXMVECTOR V
)
{
    return XMVectorLog2(V);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorPow
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{










#line 4134 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"







#line 4142 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __declspec(align(16)) float a[4];
    __declspec(align(16)) float b[4];
    _mm_store_ps( a, V1 );
    _mm_store_ps( b, V2 );
    XMVECTOR vResult = _mm_setr_ps(
        powf(a[0],b[0]),
        powf(a[1],b[1]),
        powf(a[2],b[2]),
        powf(a[3],b[3]));
    return vResult;
#line 4153 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorAbs
(
    FXMVECTOR V
)
{








#line 4171 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 4173 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_setzero_ps();
    vResult = _mm_sub_ps(vResult,V);
    vResult = _mm_max_ps(vResult,V);
    return vResult;
#line 4178 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorMod
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{
    // V1 % V2 = V1 - V2 * truncate(V1 / V2)








#line 4198 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 4202 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_div_ps(V1, V2);
    vResult = XMVectorTruncate(vResult);
    vResult = _mm_mul_ps(vResult,V2);
    vResult = _mm_sub_ps(V1,vResult);
    return vResult;
#line 4208 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorModAngles
(
    FXMVECTOR Angles
)
{











#line 4229 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"





#line 4235 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Modulo the range of the given angles such that -XM_PI <= Angles < XM_PI
    XMVECTOR vResult = _mm_mul_ps(Angles,g_XMReciprocalTwoPi);
    // Use the inline function due to complexity for rounding
    vResult = XMVectorRound(vResult);
    vResult = _mm_mul_ps(vResult,g_XMTwoPi);
    vResult = _mm_sub_ps(Angles,vResult);
    return vResult;
#line 4243 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorSin
(
    FXMVECTOR V
)
{
    // 11-degree minimax approximation









#line 4263 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"































#line 4295 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Force the value within the bounds of pi
    XMVECTOR x = XMVectorModAngles(V);

    // Map in [-pi/2,pi/2] with sin(y) = sin(x).
    __m128 sign = _mm_and_ps(x, g_XMNegativeZero);
    __m128 c = _mm_or_ps(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
    __m128 absx = _mm_andnot_ps(sign, x);  // |x|
    __m128 rflx = _mm_sub_ps(c, x);
    __m128 comp = _mm_cmple_ps(absx, g_XMHalfPi);
    __m128 select0 = _mm_and_ps(comp, x);
    __m128 select1 = _mm_andnot_ps(comp, rflx);
    x = _mm_or_ps(select0, select1);

    __m128 x2 = _mm_mul_ps(x, x);

    // Compute polynomial approximation
    const XMVECTOR SC1 = g_XMSinCoefficients1;
    XMVECTOR vConstants = _mm_shuffle_ps( SC1, SC1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    __m128 Result = _mm_mul_ps(vConstants, x2);

    const XMVECTOR SC0 = g_XMSinCoefficients0;
    vConstants = _mm_shuffle_ps( SC0, SC0, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( SC0, SC0, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( SC0, SC0, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( SC0, SC0, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);
    Result = _mm_add_ps(Result, g_XMOne);
    Result = _mm_mul_ps(Result, x);
    return Result;
#line 4335 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorCos
(
    FXMVECTOR V
)
{
    // 10-degree minimax approximation









#line 4355 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
































#line 4388 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Map V to x in [-pi,pi].
    XMVECTOR x = XMVectorModAngles(V);

    // Map in [-pi/2,pi/2] with cos(y) = sign*cos(x).
    XMVECTOR sign = _mm_and_ps(x, g_XMNegativeZero);
    __m128 c = _mm_or_ps(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
    __m128 absx = _mm_andnot_ps(sign, x);  // |x|
    __m128 rflx = _mm_sub_ps(c, x);
    __m128 comp = _mm_cmple_ps(absx, g_XMHalfPi);
    __m128 select0 = _mm_and_ps(comp, x);
    __m128 select1 = _mm_andnot_ps(comp, rflx);
    x = _mm_or_ps(select0, select1);
    select0 = _mm_and_ps(comp, g_XMOne);
    select1 = _mm_andnot_ps(comp, g_XMNegativeOne);
    sign = _mm_or_ps(select0, select1);

    __m128 x2 = _mm_mul_ps(x, x);

    // Compute polynomial approximation
    const XMVECTOR CC1 = g_XMCosCoefficients1;
    XMVECTOR vConstants = _mm_shuffle_ps( CC1, CC1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    __m128 Result = _mm_mul_ps(vConstants, x2);

    const XMVECTOR CC0 = g_XMCosCoefficients0;
    vConstants = _mm_shuffle_ps( CC0, CC0, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( CC0, CC0, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( CC0, CC0, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( CC0, CC0, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);
    Result = _mm_add_ps(Result, g_XMOne);
    Result = _mm_mul_ps(Result, sign);
    return Result;
#line 4431 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------


inline void __vectorcall XMVectorSinCos
(
    XMVECTOR* pSin,
    XMVECTOR* pCos,
    FXMVECTOR V
)
{
    (void)( (!!(pSin != nullptr)) || (_wassert(L"pSin != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(4443)), 0) );
    (void)( (!!(pCos != nullptr)) || (_wassert(L"pCos != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(4444)), 0) );

    // 11/10-degree minimax approximation


















#line 4466 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

















































#line 4516 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Force the value within the bounds of pi
    XMVECTOR x = XMVectorModAngles(V);

    // Map in [-pi/2,pi/2] with sin(y) = sin(x), cos(y) = sign*cos(x).
    XMVECTOR sign = _mm_and_ps(x, g_XMNegativeZero);
    __m128 c = _mm_or_ps(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
    __m128 absx = _mm_andnot_ps(sign, x);  // |x|
    __m128 rflx = _mm_sub_ps(c, x);
    __m128 comp = _mm_cmple_ps(absx, g_XMHalfPi);
    __m128 select0 = _mm_and_ps(comp, x);
    __m128 select1 = _mm_andnot_ps(comp, rflx);
    x = _mm_or_ps(select0, select1);
    select0 = _mm_and_ps(comp, g_XMOne);
    select1 = _mm_andnot_ps(comp, g_XMNegativeOne);
    sign = _mm_or_ps(select0, select1);

    __m128 x2 = _mm_mul_ps(x, x);

    // Compute polynomial approximation of sine
    const XMVECTOR SC1 = g_XMSinCoefficients1;
    XMVECTOR vConstants = _mm_shuffle_ps( SC1, SC1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    __m128 Result = _mm_mul_ps(vConstants, x2);

    const XMVECTOR SC0 = g_XMSinCoefficients0;
    vConstants = _mm_shuffle_ps( SC0, SC0, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( SC0, SC0, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( SC0, SC0, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( SC0, SC0, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);
    Result = _mm_add_ps(Result, g_XMOne);
    Result = _mm_mul_ps(Result, x);
    *pSin = Result;

    // Compute polynomial approximation of cosine
    const XMVECTOR CC1 = g_XMCosCoefficients1;
    vConstants = _mm_shuffle_ps( CC1, CC1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    Result = _mm_mul_ps(vConstants, x2);

    const XMVECTOR CC0 = g_XMCosCoefficients0;
    vConstants = _mm_shuffle_ps( CC0, CC0, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( CC0, CC0, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( CC0, CC0, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( CC0, CC0, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);
    Result = _mm_add_ps(Result, g_XMOne);
    Result = _mm_mul_ps(Result, sign);
    *pCos = Result;
#line 4584 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorTan
(
    FXMVECTOR V
)
{
    // Cody and Waite algorithm to compute tangent.









#line 4604 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

    static const XMVECTORF32 TanCoefficients0 = { { { 1.0f, -4.667168334e-1f, 2.566383229e-2f, -3.118153191e-4f } } };
    static const XMVECTORF32 TanCoefficients1 = { { { 4.981943399e-7f, -1.333835001e-1f, 3.424887824e-3f, -1.786170734e-5f } } };
    static const XMVECTORF32 TanConstants     = { { { 1.570796371f, 6.077100628e-11f, 0.000244140625f, 0.63661977228f /*2 / Pi*/ } } };
    static const XMVECTORU32 Mask             = { { { 0x1, 0x1, 0x1, 0x1 } } };

    XMVECTOR TwoDivPi = XMVectorSplatW(TanConstants.v);

    XMVECTOR Zero = XMVectorZero();

    XMVECTOR C0 = XMVectorSplatX(TanConstants.v);
    XMVECTOR C1 = XMVectorSplatY(TanConstants.v);
    XMVECTOR Epsilon = XMVectorSplatZ(TanConstants.v);

    XMVECTOR VA = XMVectorMultiply(V, TwoDivPi);

    VA = XMVectorRound(VA);

    XMVECTOR VC = XMVectorNegativeMultiplySubtract(VA, C0, V);

    XMVECTOR VB = XMVectorAbs(VA);

    VC = XMVectorNegativeMultiplySubtract(VA, C1, VC);



#line 4631 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    reinterpret_cast<__m128i *>(&VB)[0] = _mm_cvttps_epi32(VB);





#line 4638 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

    XMVECTOR VC2 = XMVectorMultiply(VC, VC);

    XMVECTOR T7 = XMVectorSplatW(TanCoefficients1.v);
    XMVECTOR T6 = XMVectorSplatZ(TanCoefficients1.v);
    XMVECTOR T4 = XMVectorSplatX(TanCoefficients1.v);
    XMVECTOR T3 = XMVectorSplatW(TanCoefficients0.v);
    XMVECTOR T5 = XMVectorSplatY(TanCoefficients1.v);
    XMVECTOR T2 = XMVectorSplatZ(TanCoefficients0.v);
    XMVECTOR T1 = XMVectorSplatY(TanCoefficients0.v);
    XMVECTOR T0 = XMVectorSplatX(TanCoefficients0.v);

    XMVECTOR VBIsEven = XMVectorAndInt(VB, Mask.v);
    VBIsEven = XMVectorEqualInt(VBIsEven, Zero);

    XMVECTOR N = XMVectorMultiplyAdd(VC2, T7, T6);
    XMVECTOR D = XMVectorMultiplyAdd(VC2, T4, T3);
    N = XMVectorMultiplyAdd(VC2, N, T5);
    D = XMVectorMultiplyAdd(VC2, D, T2);
    N = XMVectorMultiply(VC2, N);
    D = XMVectorMultiplyAdd(VC2, D, T1);
    N = XMVectorMultiplyAdd(VC, N, VC);
    XMVECTOR VCNearZero = XMVectorInBounds(VC, Epsilon);
    D = XMVectorMultiplyAdd(VC2, D, T0);

    N = XMVectorSelect(N, VC, VCNearZero);
    D = XMVectorSelect(D, g_XMOne.v, VCNearZero);

    XMVECTOR R0 = XMVectorNegate(N);
    XMVECTOR R1 = XMVectorDivide(N,D);
    R0 = XMVectorDivide(D,R0);

    XMVECTOR VIsZero = XMVectorEqual(V, Zero);

    XMVECTOR Result = XMVectorSelect(R0, R1, VBIsEven);

    Result = XMVectorSelect(Result, Zero, VIsZero);

    return Result;

#line 4679 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorSinH
(
    FXMVECTOR V
)
{








#line 4697 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"








#line 4706 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    static const XMVECTORF32 Scale = { { { 1.442695040888963f, 1.442695040888963f, 1.442695040888963f, 1.442695040888963f } } }; // 1.0f / ln(2.0f)

    XMVECTOR V1 = _mm_mul_ps(V, Scale);
    V1 = _mm_add_ps(V1,g_XMNegativeOne);
    XMVECTOR V2 = _mm_mul_ps(V, Scale);
    V2 = _mm_sub_ps(g_XMNegativeOne,V2);
    XMVECTOR E1 = XMVectorExp(V1);
    XMVECTOR E2 = XMVectorExp(V2);

    return _mm_sub_ps(E1, E2);
#line 4717 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorCosH
(
    FXMVECTOR V
)
{








#line 4735 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"







#line 4743 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    static const XMVECTORF32 Scale = { { { 1.442695040888963f, 1.442695040888963f, 1.442695040888963f, 1.442695040888963f } } }; // 1.0f / ln(2.0f)

    XMVECTOR V1 = _mm_mul_ps(V,Scale.v);
    V1 = _mm_add_ps(V1,g_XMNegativeOne.v);
    XMVECTOR V2 = _mm_mul_ps(V, Scale.v);
    V2 = _mm_sub_ps(g_XMNegativeOne.v,V2);
    XMVECTOR E1 = XMVectorExp(V1);
    XMVECTOR E2 = XMVectorExp(V2);
    return _mm_add_ps(E1, E2);
#line 4753 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorTanH
(
    FXMVECTOR V
)
{








#line 4771 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"







#line 4779 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    static const XMVECTORF32 Scale = { { { 2.8853900817779268f, 2.8853900817779268f, 2.8853900817779268f, 2.8853900817779268f } } }; // 2.0f / ln(2.0f)

    XMVECTOR E = _mm_mul_ps(V, Scale.v);
    E = XMVectorExp(E);
    E = _mm_mul_ps(E,g_XMOneHalf.v);
    E = _mm_add_ps(E,g_XMOneHalf.v);
    E = _mm_div_ps(g_XMOne.v,E);
    return _mm_sub_ps(g_XMOne.v,E);
#line 4788 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorASin
(
    FXMVECTOR V
)
{
    // 7-degree minimax approximation









#line 4808 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"





































#line 4846 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128 nonnegative = _mm_cmpge_ps(V, g_XMZero);
    __m128 mvalue = _mm_sub_ps(g_XMZero, V);
    __m128 x = _mm_max_ps(V, mvalue);  // |V|

    // Compute (1-|V|), clamp to zero to avoid sqrt of negative number.
    __m128 oneMValue = _mm_sub_ps(g_XMOne, x);
    __m128 clampOneMValue = _mm_max_ps(g_XMZero, oneMValue);
    __m128 root = _mm_sqrt_ps(clampOneMValue);  // sqrt(1-|V|)

    // Compute polynomial approximation
    const XMVECTOR AC1 = g_XMArcCoefficients1;
    XMVECTOR vConstants = _mm_shuffle_ps( AC1, AC1, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    __m128 t0 = _mm_mul_ps(vConstants, x);

    vConstants = _mm_shuffle_ps( AC1, AC1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, x);

    vConstants = _mm_shuffle_ps( AC1, AC1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, x);

    vConstants = _mm_shuffle_ps( AC1, AC1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, x);

    const XMVECTOR AC0 = g_XMArcCoefficients0;
    vConstants = _mm_shuffle_ps( AC0, AC0, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, x);

    vConstants = _mm_shuffle_ps( AC0, AC0, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, x);

    vConstants = _mm_shuffle_ps( AC0, AC0, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, x);

    vConstants = _mm_shuffle_ps( AC0, AC0, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, root);

    __m128 t1 = _mm_sub_ps(g_XMPi, t0);
    t0 = _mm_and_ps(nonnegative, t0);
    t1 = _mm_andnot_ps(nonnegative, t1);
    t0 = _mm_or_ps(t0, t1);
    t0 = _mm_sub_ps(g_XMHalfPi, t0);
    return t0;
#line 4896 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorACos
(
    FXMVECTOR V
)
{
    // 7-degree minimax approximation









#line 4916 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




































#line 4953 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128 nonnegative = _mm_cmpge_ps(V, g_XMZero);
    __m128 mvalue = _mm_sub_ps(g_XMZero, V);
    __m128 x = _mm_max_ps(V, mvalue);  // |V|

    // Compute (1-|V|), clamp to zero to avoid sqrt of negative number.
    __m128 oneMValue = _mm_sub_ps(g_XMOne, x);
    __m128 clampOneMValue = _mm_max_ps(g_XMZero, oneMValue);
    __m128 root = _mm_sqrt_ps(clampOneMValue);  // sqrt(1-|V|)

    // Compute polynomial approximation
    const XMVECTOR AC1 = g_XMArcCoefficients1;
    XMVECTOR vConstants = _mm_shuffle_ps( AC1, AC1, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    __m128 t0 = _mm_mul_ps(vConstants, x);

    vConstants = _mm_shuffle_ps( AC1, AC1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, x);

    vConstants = _mm_shuffle_ps( AC1, AC1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, x);

    vConstants = _mm_shuffle_ps( AC1, AC1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, x);

    const XMVECTOR AC0 = g_XMArcCoefficients0;
    vConstants = _mm_shuffle_ps( AC0, AC0, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, x);

    vConstants = _mm_shuffle_ps( AC0, AC0, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, x);

    vConstants = _mm_shuffle_ps( AC0, AC0, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, x);

    vConstants = _mm_shuffle_ps( AC0, AC0, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, root);

    __m128 t1 = _mm_sub_ps(g_XMPi, t0);
    t0 = _mm_and_ps(nonnegative, t0);
    t1 = _mm_andnot_ps(nonnegative, t1);
    t0 = _mm_or_ps(t0, t1);
    return t0;
#line 5002 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorATan
(
    FXMVECTOR V
)
{
    // 17-degree minimax approximation









#line 5022 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"











































#line 5066 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128 absV = XMVectorAbs(V);
    __m128 invV = _mm_div_ps(g_XMOne, V);
    __m128 comp = _mm_cmpgt_ps(V, g_XMOne);
    __m128 select0 = _mm_and_ps(comp, g_XMOne);
    __m128 select1 = _mm_andnot_ps(comp, g_XMNegativeOne);
    __m128 sign = _mm_or_ps(select0, select1);
    comp = _mm_cmple_ps(absV, g_XMOne);
    select0 = _mm_and_ps(comp, g_XMZero);
    select1 = _mm_andnot_ps(comp, sign);
    sign = _mm_or_ps(select0, select1);
    select0 = _mm_and_ps(comp, V);
    select1 = _mm_andnot_ps(comp, invV);
    __m128 x = _mm_or_ps(select0, select1);

    __m128 x2 = _mm_mul_ps(x, x);

    // Compute polynomial approximation
    const XMVECTOR TC1 = g_XMATanCoefficients1;
    XMVECTOR vConstants = _mm_shuffle_ps( TC1, TC1, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    __m128 Result = _mm_mul_ps(vConstants, x2);

    vConstants = _mm_shuffle_ps( TC1, TC1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( TC1, TC1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( TC1, TC1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    const XMVECTOR TC0 = g_XMATanCoefficients0;
    vConstants = _mm_shuffle_ps( TC0, TC0, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( TC0, TC0, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( TC0, TC0, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( TC0, TC0, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);
    Result = _mm_add_ps(Result, g_XMOne);
    Result = _mm_mul_ps(Result, x);
    __m128 result1 = _mm_mul_ps(sign, g_XMHalfPi);
    result1 = _mm_sub_ps(result1, Result);

    comp = _mm_cmpeq_ps(sign, g_XMZero);
    select0 = _mm_and_ps(comp, Result);
    select1 = _mm_andnot_ps(comp, result1);
    Result = _mm_or_ps(select0, select1);
    return Result;
#line 5126 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorATan2
(
    FXMVECTOR Y,
    FXMVECTOR X
)
{








#line 5145 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

    // Return the inverse tangent of Y / X in the range of -Pi to Pi with the following exceptions:

    //     Y == 0 and X is Negative         -> Pi with the sign of Y
    //     y == 0 and x is positive         -> 0 with the sign of y
    //     Y != 0 and X == 0                -> Pi / 2 with the sign of Y
    //     Y != 0 and X is Negative         -> atan(y/x) + (PI with the sign of Y)
    //     X == -Infinity and Finite Y      -> Pi with the sign of Y
    //     X == +Infinity and Finite Y      -> 0 with the sign of Y
    //     Y == Infinity and X is Finite    -> Pi / 2 with the sign of Y
    //     Y == Infinity and X == -Infinity -> 3Pi / 4 with the sign of Y
    //     Y == Infinity and X == +Infinity -> Pi / 4 with the sign of Y

    static const XMVECTORF32 ATan2Constants = { { { XM_PI, XM_PIDIV2, XM_PIDIV4, XM_PI * 3.0f / 4.0f } } };

    XMVECTOR Zero = XMVectorZero();
    XMVECTOR ATanResultValid = XMVectorTrueInt();

    XMVECTOR Pi = XMVectorSplatX(ATan2Constants);
    XMVECTOR PiOverTwo = XMVectorSplatY(ATan2Constants);
    XMVECTOR PiOverFour = XMVectorSplatZ(ATan2Constants);
    XMVECTOR ThreePiOverFour = XMVectorSplatW(ATan2Constants);

    XMVECTOR YEqualsZero = XMVectorEqual(Y, Zero);
    XMVECTOR XEqualsZero = XMVectorEqual(X, Zero);
    XMVECTOR XIsPositive = XMVectorAndInt(X, g_XMNegativeZero.v);
    XIsPositive = XMVectorEqualInt(XIsPositive, Zero);
    XMVECTOR YEqualsInfinity = XMVectorIsInfinite(Y);
    XMVECTOR XEqualsInfinity = XMVectorIsInfinite(X);

    XMVECTOR YSign = XMVectorAndInt(Y, g_XMNegativeZero.v);
    Pi = XMVectorOrInt(Pi, YSign);
    PiOverTwo = XMVectorOrInt(PiOverTwo, YSign);
    PiOverFour = XMVectorOrInt(PiOverFour, YSign);
    ThreePiOverFour = XMVectorOrInt(ThreePiOverFour, YSign);

    XMVECTOR R1 = XMVectorSelect(Pi, YSign, XIsPositive);
    XMVECTOR R2 = XMVectorSelect(ATanResultValid, PiOverTwo, XEqualsZero);
    XMVECTOR R3 = XMVectorSelect(R2, R1, YEqualsZero);
    XMVECTOR R4 = XMVectorSelect(ThreePiOverFour, PiOverFour, XIsPositive);
    XMVECTOR R5 = XMVectorSelect(PiOverTwo, R4, XEqualsInfinity);
    XMVECTOR Result = XMVectorSelect(R3, R5, YEqualsInfinity);
    ATanResultValid = XMVectorEqualInt(Result, ATanResultValid);

    XMVECTOR V = XMVectorDivide(Y, X);

    XMVECTOR R0 = XMVectorATan(V);

    R1 = XMVectorSelect( Pi, g_XMNegativeZero, XIsPositive );
    R2 = XMVectorAdd(R0, R1);

    return XMVectorSelect(Result, R2, ATanResultValid);

#line 5199 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorSinEst
(
    FXMVECTOR V
)
{
    // 7-degree minimax approximation









#line 5219 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
























#line 5244 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Force the value within the bounds of pi
    XMVECTOR x = XMVectorModAngles(V);

    // Map in [-pi/2,pi/2] with sin(y) = sin(x).
    __m128 sign = _mm_and_ps(x, g_XMNegativeZero);
    __m128 c = _mm_or_ps(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
    __m128 absx = _mm_andnot_ps(sign, x);  // |x|
    __m128 rflx = _mm_sub_ps(c, x);
    __m128 comp = _mm_cmple_ps(absx, g_XMHalfPi);
    __m128 select0 = _mm_and_ps(comp, x);
    __m128 select1 = _mm_andnot_ps(comp, rflx);
    x = _mm_or_ps(select0, select1);

    __m128 x2 = _mm_mul_ps(x, x);

    // Compute polynomial approximation
    const XMVECTOR SEC = g_XMSinCoefficients1;
    XMVECTOR vConstants = _mm_shuffle_ps( SEC, SEC, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    __m128 Result = _mm_mul_ps(vConstants, x2);

    vConstants = _mm_shuffle_ps( SEC, SEC, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( SEC, SEC, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    Result = _mm_add_ps(Result, g_XMOne);
    Result = _mm_mul_ps(Result, x);
    return Result;
#line 5276 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorCosEst
(
    FXMVECTOR V
)
{
    // 6-degree minimax approximation









#line 5296 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

























#line 5322 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Map V to x in [-pi,pi].
    XMVECTOR x = XMVectorModAngles(V);

    // Map in [-pi/2,pi/2] with cos(y) = sign*cos(x).
    XMVECTOR sign = _mm_and_ps(x, g_XMNegativeZero);
    __m128 c = _mm_or_ps(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
    __m128 absx = _mm_andnot_ps(sign, x);  // |x|
    __m128 rflx = _mm_sub_ps(c, x);
    __m128 comp = _mm_cmple_ps(absx, g_XMHalfPi);
    __m128 select0 = _mm_and_ps(comp, x);
    __m128 select1 = _mm_andnot_ps(comp, rflx);
    x = _mm_or_ps(select0, select1);
    select0 = _mm_and_ps(comp, g_XMOne);
    select1 = _mm_andnot_ps(comp, g_XMNegativeOne);
    sign = _mm_or_ps(select0, select1);

    __m128 x2 = _mm_mul_ps(x, x);

    // Compute polynomial approximation
    const XMVECTOR CEC = g_XMCosCoefficients1;
    XMVECTOR vConstants = _mm_shuffle_ps( CEC, CEC, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    __m128 Result = _mm_mul_ps(vConstants, x2);

    vConstants = _mm_shuffle_ps( CEC, CEC, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( CEC, CEC, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    Result = _mm_add_ps(Result, g_XMOne);
    Result = _mm_mul_ps(Result, sign);
    return Result;
#line 5357 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------


inline void __vectorcall XMVectorSinCosEst
(
    XMVECTOR* pSin,
    XMVECTOR* pCos,
    FXMVECTOR  V
)
{
    (void)( (!!(pSin != nullptr)) || (_wassert(L"pSin != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(5369)), 0) );
    (void)( (!!(pCos != nullptr)) || (_wassert(L"pCos != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(5370)), 0) );

    // 7/6-degree minimax approximation


















#line 5392 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



































#line 5428 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Force the value within the bounds of pi
    XMVECTOR x = XMVectorModAngles(V);

    // Map in [-pi/2,pi/2] with sin(y) = sin(x), cos(y) = sign*cos(x).
    XMVECTOR sign = _mm_and_ps(x, g_XMNegativeZero);
    __m128 c = _mm_or_ps(g_XMPi, sign);  // pi when x >= 0, -pi when x < 0
    __m128 absx = _mm_andnot_ps(sign, x);  // |x|
    __m128 rflx = _mm_sub_ps(c, x);
    __m128 comp = _mm_cmple_ps(absx, g_XMHalfPi);
    __m128 select0 = _mm_and_ps(comp, x);
    __m128 select1 = _mm_andnot_ps(comp, rflx);
    x = _mm_or_ps(select0, select1);
    select0 = _mm_and_ps(comp, g_XMOne);
    select1 = _mm_andnot_ps(comp, g_XMNegativeOne);
    sign = _mm_or_ps(select0, select1);

    __m128 x2 = _mm_mul_ps(x, x);

    // Compute polynomial approximation for sine
    const XMVECTOR SEC = g_XMSinCoefficients1;
    XMVECTOR vConstants = _mm_shuffle_ps( SEC, SEC, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    __m128 Result = _mm_mul_ps(vConstants, x2);

    vConstants = _mm_shuffle_ps( SEC, SEC, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( SEC, SEC, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    Result = _mm_add_ps(Result, g_XMOne);
    Result = _mm_mul_ps(Result, x);
    *pSin = Result;

    // Compute polynomial approximation for cosine
    const XMVECTOR CEC = g_XMCosCoefficients1;
    vConstants = _mm_shuffle_ps( CEC, CEC, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    Result = _mm_mul_ps(vConstants, x2);

    vConstants = _mm_shuffle_ps( CEC, CEC, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( CEC, CEC, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    Result = _mm_add_ps(Result, g_XMOne);
    Result = _mm_mul_ps(Result, sign);
    *pCos = Result;
#line 5480 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorTanEst
(
    FXMVECTOR V
)
{








#line 5498 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

    XMVECTOR OneOverPi = XMVectorSplatW(g_XMTanEstCoefficients.v);

    XMVECTOR V1 = XMVectorMultiply(V, OneOverPi);
    V1 = XMVectorRound(V1);

    V1 = XMVectorNegativeMultiplySubtract(g_XMPi.v, V1, V);

    XMVECTOR T0 = XMVectorSplatX(g_XMTanEstCoefficients.v);
    XMVECTOR T1 = XMVectorSplatY(g_XMTanEstCoefficients.v);
    XMVECTOR T2 = XMVectorSplatZ(g_XMTanEstCoefficients.v);

    XMVECTOR V2T2 = XMVectorNegativeMultiplySubtract(V1, V1, T2);
    XMVECTOR V2 = XMVectorMultiply(V1, V1);
    XMVECTOR V1T0 = XMVectorMultiply(V1, T0);
    XMVECTOR V1T1 = XMVectorMultiply(V1, T1);

    XMVECTOR D = XMVectorReciprocalEst(V2T2);
    XMVECTOR N = XMVectorMultiplyAdd(V2, V1T1, V1T0);

    return XMVectorMultiply(N, D);

#line 5521 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorASinEst
(
    FXMVECTOR V
)
{
    // 3-degree minimax approximation








#line 5540 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
























#line 5565 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128 nonnegative = _mm_cmpge_ps(V, g_XMZero);
    __m128 mvalue = _mm_sub_ps(g_XMZero, V);
    __m128 x = _mm_max_ps(V, mvalue);  // |V|

    // Compute (1-|V|), clamp to zero to avoid sqrt of negative number.
    __m128 oneMValue = _mm_sub_ps(g_XMOne, x);
    __m128 clampOneMValue = _mm_max_ps(g_XMZero, oneMValue);
    __m128 root = _mm_sqrt_ps(clampOneMValue);  // sqrt(1-|V|)

    // Compute polynomial approximation
    const XMVECTOR AEC = g_XMArcEstCoefficients;
    XMVECTOR vConstants = _mm_shuffle_ps( AEC, AEC, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    __m128 t0 = _mm_mul_ps(vConstants, x);

    vConstants = _mm_shuffle_ps( AEC, AEC, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, x);

    vConstants = _mm_shuffle_ps( AEC, AEC, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, x);

    vConstants = _mm_shuffle_ps( AEC, AEC, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, root);

    __m128 t1 = _mm_sub_ps(g_XMPi, t0);
    t0 = _mm_and_ps(nonnegative, t0);
    t1 = _mm_andnot_ps(nonnegative, t1);
    t0 = _mm_or_ps(t0, t1);
    t0 = _mm_sub_ps(g_XMHalfPi, t0);
    return t0;
#line 5598 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorACosEst
(
    FXMVECTOR V
)
{
    // 3-degree minimax approximation









#line 5618 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"























#line 5642 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128 nonnegative = _mm_cmpge_ps(V, g_XMZero);
    __m128 mvalue = _mm_sub_ps(g_XMZero, V);
    __m128 x = _mm_max_ps(V, mvalue);  // |V|

    // Compute (1-|V|), clamp to zero to avoid sqrt of negative number.
    __m128 oneMValue = _mm_sub_ps(g_XMOne, x);
    __m128 clampOneMValue = _mm_max_ps(g_XMZero, oneMValue);
    __m128 root = _mm_sqrt_ps(clampOneMValue);  // sqrt(1-|V|)

    // Compute polynomial approximation
    const XMVECTOR AEC = g_XMArcEstCoefficients;
    XMVECTOR vConstants = _mm_shuffle_ps( AEC, AEC, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    __m128 t0 = _mm_mul_ps(vConstants, x);

    vConstants = _mm_shuffle_ps( AEC, AEC, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, x);

    vConstants = _mm_shuffle_ps( AEC, AEC, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, x);

    vConstants = _mm_shuffle_ps( AEC, AEC, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    t0 = _mm_add_ps(t0, vConstants);
    t0 = _mm_mul_ps(t0, root);

    __m128 t1 = _mm_sub_ps(g_XMPi, t0);
    t0 = _mm_and_ps(nonnegative, t0);
    t1 = _mm_andnot_ps(nonnegative, t1);
    t0 = _mm_or_ps(t0, t1);
    return t0;
#line 5674 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorATanEst
(
    FXMVECTOR V
)
{
    // 9-degree minimax approximation









#line 5694 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"































#line 5726 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128 absV = XMVectorAbs(V);
    __m128 invV = _mm_div_ps(g_XMOne, V);
    __m128 comp = _mm_cmpgt_ps(V, g_XMOne);
    __m128 select0 = _mm_and_ps(comp, g_XMOne);
    __m128 select1 = _mm_andnot_ps(comp, g_XMNegativeOne);
    __m128 sign = _mm_or_ps(select0, select1);
    comp = _mm_cmple_ps(absV, g_XMOne);
    select0 = _mm_and_ps(comp, g_XMZero);
    select1 = _mm_andnot_ps(comp, sign);
    sign = _mm_or_ps(select0, select1);
    select0 = _mm_and_ps(comp, V);
    select1 = _mm_andnot_ps(comp, invV);
    __m128 x = _mm_or_ps(select0, select1);

    __m128 x2 = _mm_mul_ps(x, x);

    // Compute polynomial approximation
    const XMVECTOR AEC = g_XMATanEstCoefficients1;
    XMVECTOR vConstants = _mm_shuffle_ps( AEC, AEC, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    __m128 Result = _mm_mul_ps(vConstants, x2);

    vConstants = _mm_shuffle_ps( AEC, AEC, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( AEC, AEC, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    vConstants = _mm_shuffle_ps( AEC, AEC, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    Result = _mm_add_ps(Result, vConstants);
    Result = _mm_mul_ps(Result, x2);

    // ATanEstCoefficients0 is already splatted
    Result = _mm_add_ps(Result, g_XMATanEstCoefficients0);
    Result = _mm_mul_ps(Result, x);
    __m128 result1 = _mm_mul_ps(sign, g_XMHalfPi);
    result1 = _mm_sub_ps(result1, Result);

    comp = _mm_cmpeq_ps(sign, g_XMZero);
    select0 = _mm_and_ps(comp, Result);
    select1 = _mm_andnot_ps(comp, result1);
    Result = _mm_or_ps(select0, select1);
    return Result;
#line 5771 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorATan2Est
(
    FXMVECTOR Y,
    FXMVECTOR X
)
{








#line 5790 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

    static const XMVECTORF32 ATan2Constants = { { { XM_PI, XM_PIDIV2, XM_PIDIV4, 2.3561944905f /* Pi*3/4 */ } } };

    const XMVECTOR Zero = XMVectorZero();
    XMVECTOR ATanResultValid = XMVectorTrueInt();

    XMVECTOR Pi = XMVectorSplatX(ATan2Constants);
    XMVECTOR PiOverTwo = XMVectorSplatY(ATan2Constants);
    XMVECTOR PiOverFour = XMVectorSplatZ(ATan2Constants);
    XMVECTOR ThreePiOverFour = XMVectorSplatW(ATan2Constants);

    XMVECTOR YEqualsZero = XMVectorEqual(Y, Zero);
    XMVECTOR XEqualsZero = XMVectorEqual(X, Zero);
    XMVECTOR XIsPositive = XMVectorAndInt(X, g_XMNegativeZero.v);
    XIsPositive = XMVectorEqualInt(XIsPositive, Zero);
    XMVECTOR YEqualsInfinity = XMVectorIsInfinite(Y);
    XMVECTOR XEqualsInfinity = XMVectorIsInfinite(X);

    XMVECTOR YSign = XMVectorAndInt(Y, g_XMNegativeZero.v);
    Pi = XMVectorOrInt(Pi, YSign);
    PiOverTwo = XMVectorOrInt(PiOverTwo, YSign);
    PiOverFour = XMVectorOrInt(PiOverFour, YSign);
    ThreePiOverFour = XMVectorOrInt(ThreePiOverFour, YSign);

    XMVECTOR R1 = XMVectorSelect(Pi, YSign, XIsPositive);
    XMVECTOR R2 = XMVectorSelect(ATanResultValid, PiOverTwo, XEqualsZero);
    XMVECTOR R3 = XMVectorSelect(R2, R1, YEqualsZero);
    XMVECTOR R4 = XMVectorSelect(ThreePiOverFour, PiOverFour, XIsPositive);
    XMVECTOR R5 = XMVectorSelect(PiOverTwo, R4, XEqualsInfinity);
    XMVECTOR Result = XMVectorSelect(R3, R5, YEqualsInfinity);
    ATanResultValid = XMVectorEqualInt(Result, ATanResultValid);

    XMVECTOR Reciprocal = XMVectorReciprocalEst(X);
    XMVECTOR V = XMVectorMultiply(Y, Reciprocal);
    XMVECTOR R0 = XMVectorATanEst(V);

    R1 = XMVectorSelect( Pi, g_XMNegativeZero, XIsPositive );
    R2 = XMVectorAdd(R0, R1);

    Result = XMVectorSelect(Result, R2, ATanResultValid);

    return Result;

#line 5834 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorLerp
(
    FXMVECTOR V0,
    FXMVECTOR V1,
    float    t
)
{
    // V0 + t * (V1 - V0)







#line 5854 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 5857 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR L = _mm_sub_ps( V1, V0 );
    XMVECTOR S = _mm_set_ps1( t );
    XMVECTOR Result = _mm_mul_ps( L, S );
    return _mm_add_ps( Result, V0 );
#line 5862 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorLerpV
(
    FXMVECTOR V0,
    FXMVECTOR V1,
    FXMVECTOR T
)
{
    // V0 + T * (V1 - V0)






#line 5881 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 5884 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR Length = _mm_sub_ps( V1, V0 );
    XMVECTOR Result = _mm_mul_ps( Length, T );
    return _mm_add_ps( Result, V0 );
#line 5888 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorHermite
(
    FXMVECTOR Position0,
    FXMVECTOR Tangent0,
    FXMVECTOR Position1,
    GXMVECTOR Tangent1,
    float    t
)
{
    // Result = (2 * t^3 - 3 * t^2 + 1) * Position0 +
    //          (t^3 - 2 * t^2 + t) * Tangent0 +
    //          (-2 * t^3 + 3 * t^2) * Position1 +
    //          (t^3 - t^2) * Tangent1


















#line 5924 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"













#line 5938 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    float t2 = t * t;
    float t3 = t * t2;

    XMVECTOR P0 = _mm_set_ps1(2.0f * t3 - 3.0f * t2 + 1.0f);
    XMVECTOR T0 = _mm_set_ps1(t3 - 2.0f * t2 + t);
    XMVECTOR P1 = _mm_set_ps1(-2.0f * t3 + 3.0f * t2);
    XMVECTOR T1 = _mm_set_ps1(t3 - t2);

    XMVECTOR vResult = _mm_mul_ps(P0, Position0);
    XMVECTOR vTemp = _mm_mul_ps(T0, Tangent0);
    vResult = _mm_add_ps(vResult,vTemp);
    vTemp = _mm_mul_ps(P1, Position1);
    vResult = _mm_add_ps(vResult,vTemp);
    vTemp = _mm_mul_ps(T1, Tangent1);
    vResult = _mm_add_ps(vResult,vTemp);
    return vResult;
#line 5955 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorHermiteV
(
    FXMVECTOR Position0,
    FXMVECTOR Tangent0,
    FXMVECTOR Position1,
    GXMVECTOR Tangent1,
    HXMVECTOR T
)
{
    // Result = (2 * t^3 - 3 * t^2 + 1) * Position0 +
    //          (t^3 - 2 * t^2 + t) * Tangent0 +
    //          (-2 * t^3 + 3 * t^2) * Position1 +
    //          (t^3 - t^2) * Tangent1


















#line 5991 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

























#line 6017 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    static const XMVECTORF32 CatMulT2 = { { { -3.0f, -2.0f, 3.0f, -1.0f } } };
    static const XMVECTORF32 CatMulT3 = { { { 2.0f, 1.0f, -2.0f, 1.0f } } };

    XMVECTOR T2 = _mm_mul_ps(T,T);
    XMVECTOR T3 = _mm_mul_ps(T,T2);
    // Mul by the constants against t^2
    T2 = _mm_mul_ps(T2,CatMulT2);
    // Mul by the constants against t^3
    T3 = _mm_mul_ps(T3,CatMulT3);
    // T3 now has the pre-result.
    T3 = _mm_add_ps(T3,T2);
    // I need to add t.y only
    T2 = _mm_and_ps(T,g_XMMaskY);
    T3 = _mm_add_ps(T3,T2);
    // Add 1.0f to x
    T3 = _mm_add_ps(T3,g_XMIdentityR0);
    // Now, I have the constants created
    // Mul the x constant to Position0
    XMVECTOR vResult = _mm_shuffle_ps( T3, T3, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    vResult = _mm_mul_ps(vResult,Position0);
    // Mul the y constant to Tangent0
    T2 = _mm_shuffle_ps( T3, T3, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    T2 = _mm_mul_ps(T2,Tangent0);
    vResult = _mm_add_ps(vResult,T2);
    // Mul the z constant to Position1
    T2 = _mm_shuffle_ps( T3, T3, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    T2 = _mm_mul_ps(T2,Position1);
    vResult = _mm_add_ps(vResult,T2);
    // Mul the w constant to Tangent1
    T3 = _mm_shuffle_ps( T3, T3, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    T3 = _mm_mul_ps(T3,Tangent1);
    vResult = _mm_add_ps(vResult,T3);
    return vResult;
#line 6051 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorCatmullRom
(
    FXMVECTOR Position0,
    FXMVECTOR Position1,
    FXMVECTOR Position2,
    GXMVECTOR Position3,
    float    t
)
{
    // Result = ((-t^3 + 2 * t^2 - t) * Position0 +
    //           (3 * t^3 - 5 * t^2 + 2) * Position1 +
    //           (-3 * t^3 + 4 * t^2 + t) * Position2 +
    //           (t^3 - t^2) * Position3) * 0.5


















#line 6087 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"














#line 6102 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    float t2 = t * t;
    float t3 = t * t2;

    XMVECTOR P0 = _mm_set_ps1((-t3 + 2.0f * t2 - t) * 0.5f);
    XMVECTOR P1 = _mm_set_ps1((3.0f * t3 - 5.0f * t2 + 2.0f) * 0.5f);
    XMVECTOR P2 = _mm_set_ps1((-3.0f * t3 + 4.0f * t2 + t) * 0.5f);
    XMVECTOR P3 = _mm_set_ps1((t3 - t2) * 0.5f);

    P0 = _mm_mul_ps(P0, Position0);
    P1 = _mm_mul_ps(P1, Position1);
    P2 = _mm_mul_ps(P2, Position2);
    P3 = _mm_mul_ps(P3, Position3);
    P0 = _mm_add_ps(P0,P1);
    P2 = _mm_add_ps(P2,P3);
    P0 = _mm_add_ps(P0,P2);
    return P0;
#line 6119 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorCatmullRomV
(
    FXMVECTOR Position0,
    FXMVECTOR Position1,
    FXMVECTOR Position2,
    GXMVECTOR Position3,
    HXMVECTOR T
)
{



























#line 6160 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




























#line 6189 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    static const XMVECTORF32 Catmul2 = { { { 2.0f, 2.0f, 2.0f, 2.0f } } };
    static const XMVECTORF32 Catmul3 = { { { 3.0f, 3.0f, 3.0f, 3.0f } } };
    static const XMVECTORF32 Catmul4 = { { { 4.0f, 4.0f, 4.0f, 4.0f } } };
    static const XMVECTORF32 Catmul5 = { { { 5.0f, 5.0f, 5.0f, 5.0f } } };
    // Cache T^2 and T^3
    XMVECTOR T2 = _mm_mul_ps(T,T);
    XMVECTOR T3 = _mm_mul_ps(T,T2);
    // Perform the Position0 term
    XMVECTOR vResult = _mm_add_ps(T2,T2);
    vResult = _mm_sub_ps(vResult,T);
    vResult = _mm_sub_ps(vResult,T3);
    vResult = _mm_mul_ps(vResult,Position0);
    // Perform the Position1 term and add
    XMVECTOR vTemp = _mm_mul_ps(T3,Catmul3);
    XMVECTOR vTemp2 = _mm_mul_ps(T2,Catmul5);
    vTemp = _mm_sub_ps(vTemp,vTemp2);
    vTemp = _mm_add_ps(vTemp,Catmul2);
    vTemp = _mm_mul_ps(vTemp,Position1);
    vResult = _mm_add_ps(vResult,vTemp);
    // Perform the Position2 term and add
    vTemp = _mm_mul_ps(T2,Catmul4);
    vTemp2 = _mm_mul_ps(T3,Catmul3);
    vTemp = _mm_sub_ps(vTemp,vTemp2);
    vTemp = _mm_add_ps(vTemp,T);
    vTemp = _mm_mul_ps(vTemp,Position2);
    vResult = _mm_add_ps(vResult,vTemp);
    // Position3 is the last term
    T3 = _mm_sub_ps(T3,T2);
    T3 = _mm_mul_ps(T3,Position3);
    vResult = _mm_add_ps(vResult,T3);
    // Multiply by 0.5f and exit
    vResult = _mm_mul_ps(vResult,g_XMOneHalf);
    return vResult;
#line 6223 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorBaryCentric
(
    FXMVECTOR Position0,
    FXMVECTOR Position1,
    FXMVECTOR Position2,
    float    f,
    float    g
)
{
    // Result = Position0 + f * (Position1 - Position0) + g * (Position2 - Position0)














#line 6252 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 6257 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR R1 = _mm_sub_ps(Position1,Position0);
    XMVECTOR SF = _mm_set_ps1(f);
    XMVECTOR R2 = _mm_sub_ps(Position2,Position0);
    XMVECTOR SG = _mm_set_ps1(g);
    R1 = _mm_mul_ps(R1,SF);
    R2 = _mm_mul_ps(R2,SG);
    R1 = _mm_add_ps(R1,Position0);
    R1 = _mm_add_ps(R1,R2);
    return R1;
#line 6267 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVectorBaryCentricV
(
    FXMVECTOR Position0,
    FXMVECTOR Position1,
    FXMVECTOR Position2,
    GXMVECTOR F,
    HXMVECTOR G
)
{
    // Result = Position0 + f * (Position1 - Position0) + g * (Position2 - Position0)











#line 6293 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 6298 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR R1 = _mm_sub_ps(Position1,Position0);
    XMVECTOR R2 = _mm_sub_ps(Position2,Position0);
    R1 = _mm_mul_ps(R1,F);
    R2 = _mm_mul_ps(R2,G);
    R1 = _mm_add_ps(R1,Position0);
    R1 = _mm_add_ps(R1,R2);
    return R1;
#line 6306 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

/****************************************************************************
 *
 * 2D Vector
 *
 ****************************************************************************/

//------------------------------------------------------------------------------
// Comparison operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector2Equal
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 6329 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 6332 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpeq_ps(V1,V2);
// z and w are don't care
    return (((_mm_movemask_ps(vTemp)&3)==3) != 0);
#line 6336 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}


//------------------------------------------------------------------------------

inline uint32_t __vectorcall XMVector2EqualR
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{















#line 6363 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"












#line 6376 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpeq_ps(V1,V2);
// z and w are don't care
    int iTest = _mm_movemask_ps(vTemp)&3;
    uint32_t CR = 0;
    if (iTest==3)
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if (!iTest)
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
#line 6390 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector2EqualInt
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 6403 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 6406 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
    return (((_mm_movemask_ps(_mm_castsi128_ps(vTemp))&3)==3) != 0);
#line 6409 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline uint32_t __vectorcall XMVector2EqualIntR
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{















#line 6435 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"












#line 6448 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
    int iTest = _mm_movemask_ps(_mm_castsi128_ps(vTemp))&3;
    uint32_t CR = 0;
    if (iTest==3)
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if (!iTest)
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
#line 6461 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector2NearEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2,
    FXMVECTOR Epsilon
)
{





#line 6478 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 6483 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Get the difference
    XMVECTOR vDelta = _mm_sub_ps(V1,V2);
    // Get the absolute value of the difference
    XMVECTOR vTemp = _mm_setzero_ps();
    vTemp = _mm_sub_ps(vTemp,vDelta);
    vTemp = _mm_max_ps(vTemp,vDelta);
    vTemp = _mm_cmple_ps(vTemp,Epsilon);
    // z and w are don't care
    return (((_mm_movemask_ps(vTemp)&3)==0x3) != 0);
#line 6493 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector2NotEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 6506 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 6509 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpeq_ps(V1,V2);
// z and w are don't care
    return (((_mm_movemask_ps(vTemp)&3)!=3) != 0);
#line 6513 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector2NotEqualInt
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 6526 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 6529 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
    return (((_mm_movemask_ps(_mm_castsi128_ps(vTemp))&3)!=3) != 0);
#line 6532 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector2Greater
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 6545 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 6548 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpgt_ps(V1,V2);
// z and w are don't care
    return (((_mm_movemask_ps(vTemp)&3)==3) != 0);
#line 6552 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline uint32_t __vectorcall XMVector2GreaterR
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{















#line 6578 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"












#line 6591 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpgt_ps(V1,V2);
    int iTest = _mm_movemask_ps(vTemp)&3;
    uint32_t CR = 0;
    if (iTest==3)
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if (!iTest)
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
#line 6604 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector2GreaterOrEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 6617 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 6620 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpge_ps(V1,V2);
    return (((_mm_movemask_ps(vTemp)&3)==3) != 0);
#line 6623 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline uint32_t __vectorcall XMVector2GreaterOrEqualR
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{















#line 6649 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"












#line 6662 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpge_ps(V1,V2);
    int iTest = _mm_movemask_ps(vTemp)&3;
    uint32_t CR = 0;
    if (iTest == 3)
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if (!iTest)
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
#line 6675 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector2Less
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 6688 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 6691 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmplt_ps(V1,V2);
    return (((_mm_movemask_ps(vTemp)&3)==3) != 0);
#line 6694 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector2LessOrEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 6707 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 6710 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmple_ps(V1,V2);
    return (((_mm_movemask_ps(vTemp)&3)==3) != 0);
#line 6713 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector2InBounds
(
    FXMVECTOR V,
    FXMVECTOR Bounds
)
{



#line 6727 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"












#line 6740 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Test if less than or equal
    XMVECTOR vTemp1 = _mm_cmple_ps(V,Bounds);
    // Negate the bounds
    XMVECTOR vTemp2 = _mm_mul_ps(Bounds,g_XMNegativeOne);
    // Test if greater or equal (Reversed)
    vTemp2 = _mm_cmple_ps(vTemp2,V);
    // Blend answers
    vTemp1 = _mm_and_ps(vTemp1,vTemp2);
    // x and y in bounds? (z and w are don't care)
    return (((_mm_movemask_ps(vTemp1)&0x3)==0x3) != 0);
#line 6751 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------


#pragma float_control(push)
#pragma float_control(precise, on)
#line 6759 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

inline bool __vectorcall XMVector2IsNaN
(
    FXMVECTOR V
)
{



#line 6769 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"





#line 6775 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Test against itself. NaN is always not equal
    XMVECTOR vTempNan = _mm_cmpneq_ps(V,V);
    // If x or y are NaN, the mask is non-zero
    return ((_mm_movemask_ps(vTempNan)&3) != 0);
#line 6780 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}


#pragma float_control(pop)
#line 6785 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector2IsInfinite
(
    FXMVECTOR V
)
{




#line 6798 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






#line 6805 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Mask off the sign bit
    __m128 vTemp = _mm_and_ps(V,g_XMAbsMask);
    // Compare to infinity
    vTemp = _mm_cmpeq_ps(vTemp,g_XMInfinity);
    // If x or z are infinity, the signs are true.
    return ((_mm_movemask_ps(vTemp)&3) != 0);
#line 6812 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Computation operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2Dot
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{









#line 6836 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 6841 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 6843 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 6848 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x and y
    XMVECTOR vLengthSq = _mm_mul_ps(V1,V2);
    // vTemp has y splatted
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    // x+y
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    return vLengthSq;
#line 6857 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2Cross
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{
    // [ V1.x*V2.y - V1.y*V2.x, V1.x*V2.y - V1.y*V2.x ]









#line 6878 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






#line 6885 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Swap x and y
    XMVECTOR vResult = _mm_shuffle_ps( V2, V2, (((0) << 6) | ((1) << 4) | ((0) << 2) | ((1))) );
    // Perform the muls
    vResult = _mm_mul_ps(vResult,V1);
    // Splat y
    XMVECTOR vTemp = _mm_shuffle_ps( vResult, vResult, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    // Sub the values
    vResult = _mm_sub_ss(vResult,vTemp);
    // Splat the cross product
    vResult = _mm_shuffle_ps( vResult, vResult, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    return vResult;
#line 6897 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2LengthSq
(
    FXMVECTOR V
)
{
    return XMVector2Dot(V, V);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2ReciprocalLengthEst
(
    FXMVECTOR V
)
{







#line 6924 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"







#line 6932 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 6935 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"





#line 6941 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x and y
    XMVECTOR vLengthSq = _mm_mul_ps(V,V);
    // vTemp has y splatted
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    // x+y
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    vLengthSq = _mm_rsqrt_ss(vLengthSq);
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    return vLengthSq;
#line 6951 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2ReciprocalLength
(
    FXMVECTOR V
)
{







#line 6968 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"













#line 6982 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 6986 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






#line 6993 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x and y
    XMVECTOR vLengthSq = _mm_mul_ps(V,V);
    // vTemp has y splatted
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    // x+y
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    vLengthSq = _mm_sqrt_ss(vLengthSq);
    vLengthSq = _mm_div_ss(g_XMOne,vLengthSq);
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    return vLengthSq;
#line 7004 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2LengthEst
(
    FXMVECTOR V
)
{







#line 7021 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"











#line 7033 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 7036 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"





#line 7042 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x and y
    XMVECTOR vLengthSq = _mm_mul_ps(V,V);
    // vTemp has y splatted
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    // x+y
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    vLengthSq = _mm_sqrt_ss(vLengthSq);
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    return vLengthSq;
#line 7052 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2Length
(
    FXMVECTOR V
)
{







#line 7069 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

















#line 7087 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 7090 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"





#line 7096 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x and y
    XMVECTOR vLengthSq = _mm_mul_ps(V,V);
    // vTemp has y splatted
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    // x+y
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    vLengthSq = _mm_sqrt_ps(vLengthSq);
    return vLengthSq;
#line 7106 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// XMVector2NormalizeEst uses a reciprocal estimate and
// returns QNaN on zero and infinite vectors.

inline XMVECTOR __vectorcall XMVector2NormalizeEst
(
    FXMVECTOR V
)
{







#line 7125 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"









#line 7135 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 7139 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






#line 7146 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x and y
    XMVECTOR vLengthSq = _mm_mul_ps(V,V);
    // vTemp has y splatted
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    // x+y
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    vLengthSq = _mm_rsqrt_ss(vLengthSq);
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    vLengthSq = _mm_mul_ps(vLengthSq,V);
    return vLengthSq;
#line 7157 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2Normalize
(
    FXMVECTOR V
)
{
















#line 7183 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



















#line 7203 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



















#line 7223 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






















#line 7246 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x and y only
    XMVECTOR vLengthSq = _mm_mul_ps(V,V);
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    // Prepare for the division
    XMVECTOR vResult = _mm_sqrt_ps(vLengthSq);
    // Create zero with a single instruction
    XMVECTOR vZeroMask = _mm_setzero_ps();
    // Test for a divide by zero (Must be FP to detect -0.0)
    vZeroMask = _mm_cmpneq_ps(vZeroMask,vResult);
    // Failsafe on zero (Or epsilon) length planes
    // If the length is infinity, set the elements to zero
    vLengthSq = _mm_cmpneq_ps(vLengthSq,g_XMInfinity);
    // Reciprocal mul to perform the normalization
    vResult = _mm_div_ps(V,vResult);
    // Any that are infinity, set to zero
    vResult = _mm_and_ps(vResult,vZeroMask);
    // Select qnan or result based on infinite length
    XMVECTOR vTemp1 = _mm_andnot_ps(vLengthSq,g_XMQNaN);
    XMVECTOR vTemp2 = _mm_and_ps(vResult,vLengthSq);
    vResult = _mm_or_ps(vTemp1,vTemp2);
    return vResult;
#line 7270 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2ClampLength
(
    FXMVECTOR V,
    float    LengthMin,
    float    LengthMax
)
{
    XMVECTOR ClampMax = XMVectorReplicate(LengthMax);
    XMVECTOR ClampMin = XMVectorReplicate(LengthMin);
    return XMVector2ClampLengthV(V, ClampMin, ClampMax);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2ClampLengthV
(
    FXMVECTOR V,
    FXMVECTOR LengthMin,
    FXMVECTOR LengthMax
)
{
    (void)( (!!((XMVectorGetY(LengthMin) == XMVectorGetX(LengthMin)))) || (_wassert(L"(XMVectorGetY(LengthMin) == XMVectorGetX(LengthMin))", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(7295)), 0) );
    (void)( (!!((XMVectorGetY(LengthMax) == XMVectorGetX(LengthMax)))) || (_wassert(L"(XMVectorGetY(LengthMax) == XMVectorGetX(LengthMax))", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(7296)), 0) );
    (void)( (!!(XMVector2GreaterOrEqual(LengthMin, g_XMZero))) || (_wassert(L"XMVector2GreaterOrEqual(LengthMin, g_XMZero)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(7297)), 0) );
    (void)( (!!(XMVector2GreaterOrEqual(LengthMax, g_XMZero))) || (_wassert(L"XMVector2GreaterOrEqual(LengthMax, g_XMZero)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(7298)), 0) );
    (void)( (!!(XMVector2GreaterOrEqual(LengthMax, LengthMin))) || (_wassert(L"XMVector2GreaterOrEqual(LengthMax, LengthMin)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(7299)), 0) );

    XMVECTOR LengthSq = XMVector2LengthSq(V);

    const XMVECTOR Zero = XMVectorZero();

    XMVECTOR RcpLength = XMVectorReciprocalSqrt(LengthSq);

    XMVECTOR InfiniteLength = XMVectorEqualInt(LengthSq, g_XMInfinity.v);
    XMVECTOR ZeroLength = XMVectorEqual(LengthSq, Zero);

    XMVECTOR Length = XMVectorMultiply(LengthSq, RcpLength);

    XMVECTOR Normal = XMVectorMultiply(V, RcpLength);

    XMVECTOR Select = XMVectorEqualInt(InfiniteLength, ZeroLength);
    Length = XMVectorSelect(LengthSq, Length, Select);
    Normal = XMVectorSelect(LengthSq, Normal, Select);

    XMVECTOR ControlMax = XMVectorGreater(Length, LengthMax);
    XMVECTOR ControlMin = XMVectorLess(Length, LengthMin);

    XMVECTOR ClampLength = XMVectorSelect(Length, LengthMax, ControlMax);
    ClampLength = XMVectorSelect(ClampLength, LengthMin, ControlMin);

    XMVECTOR Result = XMVectorMultiply(Normal, ClampLength);

    // Preserve the original vector (with no precision loss) if the length falls within the given range
    XMVECTOR Control = XMVectorEqualInt(ControlMax, ControlMin);
    Result = XMVectorSelect(Result, V, Control);

    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2Reflect
(
    FXMVECTOR Incident,
    FXMVECTOR Normal
)
{
    // Result = Incident - (2 * dot(Incident, Normal)) * Normal

    XMVECTOR Result;
    Result = XMVector2Dot(Incident, Normal);
    Result = XMVectorAdd(Result, Result);
    Result = XMVectorNegativeMultiplySubtract(Result, Normal, Incident);
    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2Refract
(
    FXMVECTOR Incident,
    FXMVECTOR Normal,
    float    RefractionIndex
)
{
    XMVECTOR Index = XMVectorReplicate(RefractionIndex);
    return XMVector2RefractV(Incident, Normal, Index);
}

//------------------------------------------------------------------------------

// Return the refraction of a 2D vector
inline XMVECTOR __vectorcall XMVector2RefractV
(
    FXMVECTOR Incident,
    FXMVECTOR Normal,
    FXMVECTOR RefractionIndex
)
{
    // Result = RefractionIndex * Incident - Normal * (RefractionIndex * dot(Incident, Normal) +
    // sqrt(1 - RefractionIndex * RefractionIndex * (1 - dot(Incident, Normal) * dot(Incident, Normal))))


























#line 7402 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




























#line 7431 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Result = RefractionIndex * Incident - Normal * (RefractionIndex * dot(Incident, Normal) +
    // sqrt(1 - RefractionIndex * RefractionIndex * (1 - dot(Incident, Normal) * dot(Incident, Normal))))
    // Get the 2D Dot product of Incident-Normal
    XMVECTOR IDotN = XMVector2Dot(Incident, Normal);
    // vTemp = 1.0f - RefractionIndex * RefractionIndex * (1.0f - IDotN * IDotN)
    XMVECTOR vTemp = _mm_mul_ps(IDotN,IDotN);
    vTemp = _mm_sub_ps(g_XMOne,vTemp);
    vTemp = _mm_mul_ps(vTemp,RefractionIndex);
    vTemp = _mm_mul_ps(vTemp,RefractionIndex);
    vTemp = _mm_sub_ps(g_XMOne,vTemp);
    // If any terms are <=0, sqrt() will fail, punt to zero
    XMVECTOR vMask = _mm_cmpgt_ps(vTemp,g_XMZero);
    // R = RefractionIndex * IDotN + sqrt(R)
    vTemp = _mm_sqrt_ps(vTemp);
    XMVECTOR vResult = _mm_mul_ps(RefractionIndex,IDotN);
    vTemp = _mm_add_ps(vTemp,vResult);
    // Result = RefractionIndex * Incident - Normal * R
    vResult = _mm_mul_ps(RefractionIndex,Incident);
    vTemp = _mm_mul_ps(vTemp,Normal);
    vResult = _mm_sub_ps(vResult,vTemp);
    vResult = _mm_and_ps(vResult,vMask);
    return vResult;
#line 7454 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2Orthogonal
(
    FXMVECTOR V
)
{










#line 7474 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






#line 7481 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((3) << 6) | ((2) << 4) | ((0) << 2) | ((1))) );
    vResult = _mm_mul_ps(vResult,g_XMNegateX);
    return vResult;
#line 7485 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2AngleBetweenNormalsEst
(
    FXMVECTOR N1,
    FXMVECTOR N2
)
{
    XMVECTOR Result = XMVector2Dot(N1, N2);
    Result = XMVectorClamp(Result, g_XMNegativeOne.v, g_XMOne.v);
    Result = XMVectorACosEst(Result);
    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2AngleBetweenNormals
(
    FXMVECTOR N1,
    FXMVECTOR N2
)
{
    XMVECTOR Result = XMVector2Dot(N1, N2);
    Result = XMVectorClamp(Result, g_XMNegativeOne, g_XMOne);
    Result = XMVectorACos(Result);
    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2AngleBetweenVectors
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{
    XMVECTOR L1 = XMVector2ReciprocalLength(V1);
    XMVECTOR L2 = XMVector2ReciprocalLength(V2);

    XMVECTOR Dot = XMVector2Dot(V1, V2);

    L1 = XMVectorMultiply(L1, L2);

    XMVECTOR CosAngle = XMVectorMultiply(Dot, L1);
    CosAngle = XMVectorClamp(CosAngle, g_XMNegativeOne.v, g_XMOne.v);

    return XMVectorACos(CosAngle);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2LinePointDistance
(
    FXMVECTOR LinePoint1,
    FXMVECTOR LinePoint2,
    FXMVECTOR Point
)
{
    // Given a vector PointVector from LinePoint1 to Point and a vector
    // LineVector from LinePoint1 to LinePoint2, the scaled distance
    // PointProjectionScale from LinePoint1 to the perpendicular projection
    // of PointVector onto the line is defined as:
    //
    //     PointProjectionScale = dot(PointVector, LineVector) / LengthSq(LineVector)

    XMVECTOR PointVector = XMVectorSubtract(Point, LinePoint1);
    XMVECTOR LineVector = XMVectorSubtract(LinePoint2, LinePoint1);

    XMVECTOR LengthSq = XMVector2LengthSq(LineVector);

    XMVECTOR PointProjectionScale = XMVector2Dot(PointVector, LineVector);
    PointProjectionScale = XMVectorDivide(PointProjectionScale, LengthSq);

    XMVECTOR DistanceVector = XMVectorMultiply(LineVector, PointProjectionScale);
    DistanceVector = XMVectorSubtract(PointVector, DistanceVector);

    return XMVector2Length(DistanceVector);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2IntersectLine
(
    FXMVECTOR Line1Point1,
    FXMVECTOR Line1Point2,
    FXMVECTOR Line2Point1,
    GXMVECTOR Line2Point2
)
{


































#line 7611 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR V1 = _mm_sub_ps(Line1Point2, Line1Point1);
    XMVECTOR V2 = _mm_sub_ps(Line2Point2, Line2Point1);
    XMVECTOR V3 = _mm_sub_ps(Line1Point1, Line2Point1);
    // Generate the cross products
    XMVECTOR C1 = XMVector2Cross(V1, V2);
    XMVECTOR C2 = XMVector2Cross(V2, V3);
    // If C1 is not close to epsilon, use the calculated value
    XMVECTOR vResultMask = _mm_setzero_ps();
    vResultMask = _mm_sub_ps(vResultMask,C1);
    vResultMask = _mm_max_ps(vResultMask,C1);
    // 0xFFFFFFFF if the calculated value is to be used
    vResultMask = _mm_cmpgt_ps(vResultMask,g_XMEpsilon);
    // If C1 is close to epsilon, which fail type is it? INFINITY or NAN?
    XMVECTOR vFailMask = _mm_setzero_ps();
    vFailMask = _mm_sub_ps(vFailMask,C2);
    vFailMask = _mm_max_ps(vFailMask,C2);
    vFailMask = _mm_cmple_ps(vFailMask,g_XMEpsilon);
    XMVECTOR vFail = _mm_and_ps(vFailMask,g_XMInfinity);
    vFailMask = _mm_andnot_ps(vFailMask,g_XMQNaN);
    // vFail is NAN or INF
    vFail = _mm_or_ps(vFail,vFailMask);
    // Intersection point = Line1Point1 + V1 * (C2 / C1)
    XMVECTOR vResult = _mm_div_ps(C2,C1);
    vResult = _mm_mul_ps(vResult,V1);
    vResult = _mm_add_ps(vResult,Line1Point1);
    // Use result, or failure value
    vResult = _mm_and_ps(vResult,vResultMask);
    vResultMask = _mm_andnot_ps(vResultMask,vFail);
    vResult = _mm_or_ps(vResult,vResultMask);
    return vResult;
#line 7642 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2Transform
(
    FXMVECTOR V,
    FXMMATRIX M
)
{










#line 7663 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 7667 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    vResult = _mm_mul_ps(vResult,M.r[0]);
    XMVECTOR vTemp = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    vTemp = _mm_mul_ps(vTemp,M.r[1]);
    vResult = _mm_add_ps(vResult,vTemp);
    vResult = _mm_add_ps(vResult,M.r[3]);
    return vResult;
#line 7675 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------


inline XMFLOAT4* __vectorcall XMVector2TransformStream
(
    XMFLOAT4*       pOutputStream,
    size_t          OutputStride,
    const XMFLOAT2* pInputStream,
    size_t          InputStride,
    size_t          VectorCount,
    FXMMATRIX       M
)
{
    (void)( (!!(pOutputStream != nullptr)) || (_wassert(L"pOutputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(7690)), 0) );
    (void)( (!!(pInputStream != nullptr)) || (_wassert(L"pInputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(7691)), 0) );

    (void)( (!!(InputStride >= sizeof(XMFLOAT2))) || (_wassert(L"InputStride >= sizeof(XMFLOAT2)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(7693)), 0) );
    ;

    (void)( (!!(OutputStride >= sizeof(XMFLOAT4))) || (_wassert(L"OutputStride >= sizeof(XMFLOAT4)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(7696)), 0) );
    ;




































#line 7735 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"







































































#line 7807 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    auto pInputVector = reinterpret_cast<const uint8_t*>(pInputStream);
    auto pOutputVector = reinterpret_cast<uint8_t*>(pOutputStream);

    const XMVECTOR row0 = M.r[0];
    const XMVECTOR row1 = M.r[1];
    const XMVECTOR row3 = M.r[3];

    size_t i = 0;
    size_t two = VectorCount >> 1;
    if ( two > 0 )
    {
        if ( InputStride == sizeof(XMFLOAT2) )
        {
            if ( !(reinterpret_cast<uintptr_t>(pOutputStream) & 0xF) && !(OutputStride & 0xF) )
            {
                // Packed input, aligned output
                for (size_t j = 0; j < two; ++j)
                {
                    XMVECTOR V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                    pInputVector += sizeof(XMFLOAT2)*2;

                    XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    XMVECTOR vTemp = _mm_mul_ps( Y, row1 );
                    XMVECTOR vTemp2 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );

                    _mm_stream_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                    pOutputVector += OutputStride;

                    Y = _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                    X = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );

                    vTemp = _mm_mul_ps( Y, row1 );
                    vTemp2 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );

                    _mm_stream_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                    pOutputVector += OutputStride;

                    i += 2;
                }
            }
            else
            {
                // Packed input, unaligned output
                for (size_t j = 0; j < two; ++j)
                {
                    XMVECTOR V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                    pInputVector += sizeof(XMFLOAT2)*2;

                    XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    XMVECTOR vTemp = _mm_mul_ps( Y, row1 );
                    XMVECTOR vTemp2 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );

                    _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                    pOutputVector += OutputStride;

                    Y = _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                    X = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );

                    vTemp = _mm_mul_ps( Y, row1 );
                    vTemp2 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );

                    _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                    pOutputVector += OutputStride;

                    i += 2;
                }
            }
        }
    }

    if ( !(reinterpret_cast<uintptr_t>(pInputVector) & 0xF) && !(InputStride & 0xF) )
    {
        if ( !(reinterpret_cast<uintptr_t>(pOutputStream) & 0xF) && !(OutputStride & 0xF) )
        {
            // Aligned input, aligned output
            for (; i < VectorCount; i++)
            {
                XMVECTOR V = _mm_castsi128_ps( _mm_loadl_epi64( reinterpret_cast<const __m128i*>(pInputVector) ) );
                pInputVector += InputStride;

                XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                XMVECTOR vTemp = _mm_mul_ps( Y, row1 );
                XMVECTOR vTemp2 = _mm_mul_ps( X, row0 );
                vTemp = _mm_add_ps( vTemp, row3 );
                vTemp = _mm_add_ps( vTemp, vTemp2 );

                _mm_stream_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                pOutputVector += OutputStride;
            }
        }
        else
        {
            // Aligned input, unaligned output
            for (; i < VectorCount; i++)
            {
                XMVECTOR V = _mm_castsi128_ps( _mm_loadl_epi64( reinterpret_cast<const __m128i*>(pInputVector) ) );
                pInputVector += InputStride;

                XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                XMVECTOR vTemp = _mm_mul_ps( Y, row1 );
                XMVECTOR vTemp2 = _mm_mul_ps( X, row0 );
                vTemp = _mm_add_ps( vTemp, row3 );
                vTemp = _mm_add_ps( vTemp, vTemp2 );

                _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                pOutputVector += OutputStride;
            }
        }
    }
    else
    {
        // Unaligned input
        for (; i < VectorCount; i++)
        {
            __m128 x = _mm_load_ss( reinterpret_cast<const float*>(pInputVector) );
            __m128 y = _mm_load_ss( reinterpret_cast<const float*>(pInputVector+4) );
            pInputVector += InputStride;

            XMVECTOR Y = _mm_shuffle_ps( y, y, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
            XMVECTOR X = _mm_shuffle_ps( x, x, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

            XMVECTOR vTemp = _mm_mul_ps( Y, row1 );
            XMVECTOR vTemp2 = _mm_mul_ps( X, row0 );
            vTemp = _mm_add_ps( vTemp, row3 );
            vTemp = _mm_add_ps( vTemp, vTemp2 );

            _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
            pOutputVector += OutputStride;
        }
    }

    _mm_sfence();

    return pOutputStream;
#line 7958 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2TransformCoord
(
    FXMVECTOR V,
    FXMMATRIX M
)
{
    XMVECTOR Y = XMVectorSplatY(V);
    XMVECTOR X = XMVectorSplatX(V);

    XMVECTOR Result = XMVectorMultiplyAdd(Y, M.r[1], M.r[3]);
    Result = XMVectorMultiplyAdd(X, M.r[0], Result);

    XMVECTOR W = XMVectorSplatW(Result);
    return XMVectorDivide( Result, W );
}

//------------------------------------------------------------------------------


inline XMFLOAT2* __vectorcall XMVector2TransformCoordStream
(
    XMFLOAT2*       pOutputStream,
    size_t          OutputStride,
    const XMFLOAT2* pInputStream,
    size_t          InputStride,
    size_t          VectorCount,
    FXMMATRIX       M
)
{
    (void)( (!!(pOutputStream != nullptr)) || (_wassert(L"pOutputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(7991)), 0) );
    (void)( (!!(pInputStream != nullptr)) || (_wassert(L"pInputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(7992)), 0) );

    (void)( (!!(InputStride >= sizeof(XMFLOAT2))) || (_wassert(L"InputStride >= sizeof(XMFLOAT2)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(7994)), 0) );
    ;

    (void)( (!!(OutputStride >= sizeof(XMFLOAT2))) || (_wassert(L"OutputStride >= sizeof(XMFLOAT2)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(7997)), 0) );
    ;








































#line 8040 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
































































































#line 8137 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    auto pInputVector = reinterpret_cast<const uint8_t*>(pInputStream);
    auto pOutputVector = reinterpret_cast<uint8_t*>(pOutputStream);

    const XMVECTOR row0 = M.r[0];
    const XMVECTOR row1 = M.r[1];
    const XMVECTOR row3 = M.r[3];

    size_t i = 0;
    size_t two = VectorCount >> 1;
    if ( two > 0 )
    {
        if ( InputStride == sizeof(XMFLOAT2) )
        {
            if ( OutputStride == sizeof(XMFLOAT2) )
            {
                if ( !(reinterpret_cast<uintptr_t>(pOutputStream) & 0xF) )
                {
                    // Packed input, aligned & packed output
                    for (size_t j = 0; j < two; ++j)
                    {
                        XMVECTOR V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                        pInputVector += sizeof(XMFLOAT2)*2;

                        // Result 1
                        XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        XMVECTOR vTemp = _mm_mul_ps( Y, row1 );
                        XMVECTOR vTemp2 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, row3 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );

                        XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                        XMVECTOR V1 = _mm_div_ps( vTemp, W );

                        // Result 2
                        Y = _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        X = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );

                        vTemp = _mm_mul_ps( Y, row1 );
                        vTemp2 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, row3 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                        XMVECTOR V2 = _mm_div_ps( vTemp, W );

                        vTemp = _mm_movelh_ps( V1, V2 );

                        _mm_stream_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                        pOutputVector += sizeof(XMFLOAT2)*2;

                        i += 2;
                    }
                }
                else
                {
                    // Packed input, unaligned & packed output
                    for (size_t j = 0; j < two; ++j)
                    {
                        XMVECTOR V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                        pInputVector += sizeof(XMFLOAT2)*2;

                        // Result 1
                        XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        XMVECTOR vTemp = _mm_mul_ps( Y, row1 );
                        XMVECTOR vTemp2 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, row3 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );

                        XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                        XMVECTOR V1 = _mm_div_ps( vTemp, W );

                        // Result 2
                        Y = _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        X = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );

                        vTemp = _mm_mul_ps( Y, row1 );
                        vTemp2 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, row3 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                        XMVECTOR V2 = _mm_div_ps( vTemp, W );

                        vTemp = _mm_movelh_ps( V1, V2 );

                        _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                        pOutputVector += sizeof(XMFLOAT2)*2;

                        i += 2;
                    }
                }
            }
            else
            {
                // Packed input, unpacked output
                for (size_t j = 0; j < two; ++j)
                {
                    XMVECTOR V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                    pInputVector += sizeof(XMFLOAT2)*2;

                    // Result 1
                    XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    XMVECTOR vTemp = _mm_mul_ps( Y, row1 );
                    XMVECTOR vTemp2 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );

                    XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                    vTemp = _mm_div_ps( vTemp, W );
                    vTemp2 = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );

                    _mm_store_ss( reinterpret_cast<float*>(pOutputVector), vTemp );
                    _mm_store_ss( reinterpret_cast<float*>(pOutputVector+4), vTemp2 );
                    pOutputVector += OutputStride;

                    // Result 2
                    Y = _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                    X = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );

                    vTemp = _mm_mul_ps( Y, row1 );
                    vTemp2 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );

                    W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                    vTemp = _mm_div_ps( vTemp, W );
                    vTemp2 = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );

                    _mm_store_ss( reinterpret_cast<float*>(pOutputVector), vTemp );
                    _mm_store_ss( reinterpret_cast<float*>(pOutputVector+4), vTemp2 );
                    pOutputVector += OutputStride;

                    i += 2;
                }
            }
        }
    }

    if ( !(reinterpret_cast<uintptr_t>(pInputVector) & 0xF) && !(InputStride & 0xF) )
    {
        // Aligned input
        for (; i < VectorCount; i++)
        {
            XMVECTOR V = _mm_castsi128_ps( _mm_loadl_epi64( reinterpret_cast<const __m128i*>(pInputVector) ) );
            pInputVector += InputStride;

            XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
            XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

            XMVECTOR vTemp = _mm_mul_ps( Y, row1 );
            XMVECTOR vTemp2 = _mm_mul_ps( X, row0 );
            vTemp = _mm_add_ps( vTemp, row3 );
            vTemp = _mm_add_ps( vTemp, vTemp2 );

            XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

            vTemp = _mm_div_ps( vTemp, W );
            vTemp2 = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );

            _mm_store_ss( reinterpret_cast<float*>(pOutputVector), vTemp );
            _mm_store_ss( reinterpret_cast<float*>(pOutputVector+4), vTemp2 );
            pOutputVector += OutputStride;
        }
    }
    else
    {
        // Unaligned input
        for (; i < VectorCount; i++)
        {
            __m128 x = _mm_load_ss( reinterpret_cast<const float*>(pInputVector) );
            __m128 y = _mm_load_ss( reinterpret_cast<const float*>(pInputVector+4) );
            pInputVector += InputStride;

            XMVECTOR Y = _mm_shuffle_ps( y, y, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
            XMVECTOR X = _mm_shuffle_ps( x, x, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

            XMVECTOR vTemp = _mm_mul_ps( Y, row1 );
            XMVECTOR vTemp2 = _mm_mul_ps( X, row0 );
            vTemp = _mm_add_ps( vTemp, row3 );
            vTemp = _mm_add_ps( vTemp, vTemp2 );

            XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

            vTemp = _mm_div_ps( vTemp, W );
            vTemp2 = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );

            _mm_store_ss( reinterpret_cast<float*>(pOutputVector), vTemp );
            _mm_store_ss( reinterpret_cast<float*>(pOutputVector+4), vTemp2 );
            pOutputVector += OutputStride;
        }
    }

    _mm_sfence();

    return pOutputStream;
#line 8345 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector2TransformNormal
(
    FXMVECTOR V,
    FXMMATRIX M
)
{










#line 8366 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 8370 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    vResult = _mm_mul_ps(vResult,M.r[0]);
    XMVECTOR vTemp = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    vTemp = _mm_mul_ps(vTemp,M.r[1]);
    vResult = _mm_add_ps(vResult,vTemp);
    return vResult;
#line 8377 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------


inline XMFLOAT2* __vectorcall XMVector2TransformNormalStream
(
    XMFLOAT2*       pOutputStream,
    size_t          OutputStride,
    const XMFLOAT2* pInputStream,
    size_t          InputStride,
    size_t          VectorCount,
    FXMMATRIX       M
)
{
    (void)( (!!(pOutputStream != nullptr)) || (_wassert(L"pOutputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(8392)), 0) );
    (void)( (!!(pInputStream != nullptr)) || (_wassert(L"pInputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(8393)), 0) );

    (void)( (!!(InputStride >= sizeof(XMFLOAT2))) || (_wassert(L"InputStride >= sizeof(XMFLOAT2)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(8395)), 0) );
    ;

    (void)( (!!(OutputStride >= sizeof(XMFLOAT2))) || (_wassert(L"OutputStride >= sizeof(XMFLOAT2)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(8398)), 0) );
    ;



































#line 8436 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
























































#line 8493 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    auto pInputVector = reinterpret_cast<const uint8_t*>(pInputStream);
    auto pOutputVector = reinterpret_cast<uint8_t*>(pOutputStream);

    const XMVECTOR row0 = M.r[0];
    const XMVECTOR row1 = M.r[1];

    size_t i = 0;
    size_t two = VectorCount >> 1;
    if ( two > 0 )
    {
        if ( InputStride == sizeof(XMFLOAT2) )
        {
            if ( OutputStride == sizeof(XMFLOAT2) )
            {
                if ( !(reinterpret_cast<uintptr_t>(pOutputStream) & 0xF) )
                {
                    // Packed input, aligned & packed output
                    for (size_t j = 0; j < two; ++j)
                    {
                        XMVECTOR V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                        pInputVector += sizeof(XMFLOAT2)*2;

                        // Result 1
                        XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        XMVECTOR vTemp = _mm_mul_ps( Y, row1 );
                        XMVECTOR vTemp2 = _mm_mul_ps( X, row0 );
                        XMVECTOR V1 = _mm_add_ps( vTemp, vTemp2 );

                        // Result 2
                        Y = _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        X = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );

                        vTemp = _mm_mul_ps( Y, row1 );
                        vTemp2 = _mm_mul_ps( X, row0 );
                        XMVECTOR V2 = _mm_add_ps( vTemp, vTemp2 );

                        vTemp = _mm_movelh_ps( V1, V2 );

                        _mm_stream_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                        pOutputVector += sizeof(XMFLOAT2)*2;

                        i += 2;
                    }
                }
                else
                {
                    // Packed input, unaligned & packed output
                    for (size_t j = 0; j < two; ++j)
                    {
                        XMVECTOR V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                        pInputVector += sizeof(XMFLOAT2)*2;

                        // Result 1
                        XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        XMVECTOR vTemp = _mm_mul_ps( Y, row1 );
                        XMVECTOR vTemp2 = _mm_mul_ps( X, row0 );
                        XMVECTOR V1 = _mm_add_ps( vTemp, vTemp2 );

                        // Result 2
                        Y = _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        X = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );

                        vTemp = _mm_mul_ps( Y, row1 );
                        vTemp2 = _mm_mul_ps( X, row0 );
                        XMVECTOR V2 = _mm_add_ps( vTemp, vTemp2 );

                        vTemp = _mm_movelh_ps( V1, V2 );

                        _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                        pOutputVector += sizeof(XMFLOAT2)*2;

                        i += 2;
                    }
                }
            }
            else
            {
                // Packed input, unpacked output
                for (size_t j = 0; j < two; ++j)
                {
                    XMVECTOR V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                    pInputVector += sizeof(XMFLOAT2)*2;

                    // Result 1
                    XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    XMVECTOR vTemp = _mm_mul_ps( Y, row1 );
                    XMVECTOR vTemp2 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp2 = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );

                    _mm_store_ss( reinterpret_cast<float*>(pOutputVector), vTemp );
                    _mm_store_ss( reinterpret_cast<float*>(pOutputVector+4), vTemp2 );
                    pOutputVector += OutputStride;

                    // Result 2
                    Y = _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                    X = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );

                    vTemp = _mm_mul_ps( Y, row1 );
                    vTemp2 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp2 = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );

                    _mm_store_ss( reinterpret_cast<float*>(pOutputVector), vTemp );
                    _mm_store_ss( reinterpret_cast<float*>(pOutputVector+4), vTemp2 );
                    pOutputVector += OutputStride;

                    i += 2;
                }
            }
        }
    }

    if ( !(reinterpret_cast<uintptr_t>(pInputVector) & 0xF) && !(InputStride & 0xF) )
    {
        // Aligned input
        for (; i < VectorCount; i++)
        {
            XMVECTOR V = _mm_castsi128_ps( _mm_loadl_epi64( reinterpret_cast<const __m128i*>(pInputVector) ) );
            pInputVector += InputStride;

            XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
            XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

            XMVECTOR vTemp = _mm_mul_ps( Y, row1 );
            XMVECTOR vTemp2 = _mm_mul_ps( X, row0 );
            vTemp = _mm_add_ps( vTemp, vTemp2 );
            vTemp2 = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );

            _mm_store_ss( reinterpret_cast<float*>(pOutputVector), vTemp );
            _mm_store_ss( reinterpret_cast<float*>(pOutputVector+4), vTemp2 );
            pOutputVector += OutputStride;
        }
    }
    else
    {
        // Unaligned input
        for (; i < VectorCount; i++)
        {
            __m128 x = _mm_load_ss( reinterpret_cast<const float*>(pInputVector) );
            __m128 y = _mm_load_ss( reinterpret_cast<const float*>(pInputVector+4) );
            pInputVector += InputStride;

            XMVECTOR Y = _mm_shuffle_ps( y, y, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
            XMVECTOR X = _mm_shuffle_ps( x, x, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

            XMVECTOR vTemp = _mm_mul_ps( Y, row1 );
            XMVECTOR vTemp2 = _mm_mul_ps( X, row0 );
            vTemp = _mm_add_ps( vTemp, vTemp2 );
            vTemp2 = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );

            _mm_store_ss( reinterpret_cast<float*>(pOutputVector), vTemp );
            _mm_store_ss( reinterpret_cast<float*>(pOutputVector+4), vTemp2 );
            pOutputVector += OutputStride;
        }
    }

    _mm_sfence();

    return pOutputStream;
#line 8660 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

/****************************************************************************
 *
 * 3D Vector
 *
 ****************************************************************************/

//------------------------------------------------------------------------------
// Comparison operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector3Equal
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 8683 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 8688 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpeq_ps(V1,V2);
    return (((_mm_movemask_ps(vTemp)&7)==7) != 0);
#line 8691 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline uint32_t __vectorcall XMVector3EqualR
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{















#line 8717 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"















#line 8733 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpeq_ps(V1,V2);
    int iTest = _mm_movemask_ps(vTemp)&7;
    uint32_t CR = 0;
    if (iTest==7)
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if (!iTest)
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
#line 8746 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector3EqualInt
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 8759 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 8764 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
    return (((_mm_movemask_ps(_mm_castsi128_ps(vTemp))&7)==7) != 0);
#line 8767 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline uint32_t __vectorcall XMVector3EqualIntR
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{















#line 8793 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"















#line 8809 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
    int iTemp = _mm_movemask_ps(_mm_castsi128_ps(vTemp))&7;
    uint32_t CR = 0;
    if (iTemp==7)
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if (!iTemp)
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
#line 8822 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector3NearEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2,
    FXMVECTOR Epsilon
)
{









#line 8843 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"





#line 8849 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Get the difference
    XMVECTOR vDelta = _mm_sub_ps(V1,V2);
    // Get the absolute value of the difference
    XMVECTOR vTemp = _mm_setzero_ps();
    vTemp = _mm_sub_ps(vTemp,vDelta);
    vTemp = _mm_max_ps(vTemp,vDelta);
    vTemp = _mm_cmple_ps(vTemp,Epsilon);
    // w is don't care
    return (((_mm_movemask_ps(vTemp)&7)==0x7) != 0);
#line 8859 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector3NotEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 8872 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 8877 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpeq_ps(V1,V2);
    return (((_mm_movemask_ps(vTemp)&7)!=7) != 0);
#line 8880 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector3NotEqualInt
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 8893 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 8898 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
    return (((_mm_movemask_ps(_mm_castsi128_ps(vTemp))&7)!=7) != 0);
#line 8901 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector3Greater
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 8914 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 8919 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpgt_ps(V1,V2);
    return (((_mm_movemask_ps(vTemp)&7)==7) != 0);
#line 8922 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline uint32_t __vectorcall XMVector3GreaterR
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{
















#line 8949 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"















#line 8965 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpgt_ps(V1,V2);
    uint32_t CR = 0;
    int iTest = _mm_movemask_ps(vTemp)&7;
    if (iTest==7)
    {
        CR =  XM_CRMASK_CR6TRUE;
    }
    else if (!iTest)
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
#line 8978 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector3GreaterOrEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 8991 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 8996 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpge_ps(V1,V2);
    return (((_mm_movemask_ps(vTemp)&7)==7) != 0);
#line 8999 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline uint32_t __vectorcall XMVector3GreaterOrEqualR
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{

















#line 9027 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"















#line 9043 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpge_ps(V1,V2);
    uint32_t CR = 0;
    int iTest = _mm_movemask_ps(vTemp)&7;
    if (iTest==7)
    {
        CR =  XM_CRMASK_CR6TRUE;
    }
    else if (!iTest)
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
#line 9056 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector3Less
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 9069 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 9074 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmplt_ps(V1,V2);
    return (((_mm_movemask_ps(vTemp)&7)==7) != 0);
#line 9077 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector3LessOrEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 9090 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 9095 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmple_ps(V1,V2);
    return (((_mm_movemask_ps(vTemp)&7)==7) != 0);
#line 9098 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector3InBounds
(
    FXMVECTOR V,
    FXMVECTOR Bounds
)
{




#line 9113 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"












#line 9126 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Test if less than or equal
    XMVECTOR vTemp1 = _mm_cmple_ps(V,Bounds);
    // Negate the bounds
    XMVECTOR vTemp2 = _mm_mul_ps(Bounds,g_XMNegativeOne);
    // Test if greater or equal (Reversed)
    vTemp2 = _mm_cmple_ps(vTemp2,V);
    // Blend answers
    vTemp1 = _mm_and_ps(vTemp1,vTemp2);
    // x,y and z in bounds? (w is don't care)
    return (((_mm_movemask_ps(vTemp1)&0x7)==0x7) != 0);


#line 9139 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------


#pragma float_control(push)
#pragma float_control(precise, on)
#line 9147 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

inline bool __vectorcall XMVector3IsNaN
(
    FXMVECTOR V
)
{






#line 9160 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






#line 9167 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Test against itself. NaN is always not equal
    XMVECTOR vTempNan = _mm_cmpneq_ps(V,V);
    // If x or y or z are NaN, the mask is non-zero
    return ((_mm_movemask_ps(vTempNan)&7) != 0);
#line 9172 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}


#pragma float_control(pop)
#line 9177 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector3IsInfinite
(
    FXMVECTOR V
)
{




#line 9190 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"








#line 9199 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Mask off the sign bit
    __m128 vTemp = _mm_and_ps(V,g_XMAbsMask);
    // Compare to infinity
    vTemp = _mm_cmpeq_ps(vTemp,g_XMInfinity);
    // If x,y or z are infinity, the signs are true.
    return ((_mm_movemask_ps(vTemp)&7) != 0);
#line 9206 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Computation operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3Dot
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{








#line 9229 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"







#line 9237 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 9239 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 9244 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product
    XMVECTOR vDot = _mm_mul_ps(V1,V2);
    // x=Dot.vector4_f32[1], y=Dot.vector4_f32[2]
    XMVECTOR vTemp = _mm_shuffle_ps( vDot, vDot, (((2) << 6) | ((1) << 4) | ((2) << 2) | ((1))) );
    // Result.vector4_f32[0] = x+y
    vDot = _mm_add_ss(vDot,vTemp);
    // x=Dot.vector4_f32[2]
    vTemp = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    // Result.vector4_f32[0] = (x+y)+z
    vDot = _mm_add_ss(vDot,vTemp);
    // Splat x
    return _mm_shuffle_ps( vDot, vDot, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
#line 9257 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3Cross
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{
    // [ V1.y*V2.z - V1.z*V2.y, V1.z*V2.x - V1.x*V2.z, V1.x*V2.y - V1.y*V2.x ]









#line 9278 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"













#line 9292 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // y1,z1,x1,w1
    XMVECTOR vTemp1 = _mm_shuffle_ps( V1, V1, (((3) << 6) | ((0) << 4) | ((2) << 2) | ((1))) );
    // z2,x2,y2,w2
    XMVECTOR vTemp2 = _mm_shuffle_ps( V2, V2, (((3) << 6) | ((1) << 4) | ((0) << 2) | ((2))) );
    // Perform the left operation
    XMVECTOR vResult = _mm_mul_ps(vTemp1,vTemp2);
    // z1,x1,y1,w1
    vTemp1 = _mm_shuffle_ps( vTemp1, vTemp1, (((3) << 6) | ((0) << 4) | ((2) << 2) | ((1))) );
    // y2,z2,x2,w2
    vTemp2 = _mm_shuffle_ps( vTemp2, vTemp2, (((3) << 6) | ((1) << 4) | ((0) << 2) | ((2))) );
    // Perform the right operation
    vTemp1 = _mm_mul_ps(vTemp1,vTemp2);
    // Subract the right from left, and return answer
    vResult = _mm_sub_ps(vResult,vTemp1);
    // Set w to zero
    return _mm_and_ps(vResult,g_XMMask3);
#line 9309 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3LengthSq
(
    FXMVECTOR V
)
{
    return XMVector3Dot(V, V);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3ReciprocalLengthEst
(
    FXMVECTOR V
)
{









#line 9338 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"










#line 9349 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 9352 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






#line 9359 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x,y and z
    XMVECTOR vLengthSq = _mm_mul_ps(V,V);
    // vTemp has z and y
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((1) << 6) | ((2) << 4) | ((1) << 2) | ((2))) );
    // x+z, y
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    // y,y,y,y
    vTemp = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    // x+z+y,??,??,??
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    // Splat the length squared
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    // Get the reciprocal
    vLengthSq = _mm_rsqrt_ps(vLengthSq);
    return vLengthSq;
#line 9375 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3ReciprocalLength
(
    FXMVECTOR V
)
{









#line 9394 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
















#line 9411 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 9415 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"







#line 9423 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
     // Perform the dot product
    XMVECTOR vDot = _mm_mul_ps(V,V);
    // x=Dot.y, y=Dot.z
    XMVECTOR vTemp = _mm_shuffle_ps( vDot, vDot, (((2) << 6) | ((1) << 4) | ((2) << 2) | ((1))) );
    // Result.x = x+y
    vDot = _mm_add_ss(vDot,vTemp);
    // x=Dot.z
    vTemp = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    // Result.x = (x+y)+z
    vDot = _mm_add_ss(vDot,vTemp);
    // Splat x
    vDot = _mm_shuffle_ps( vDot, vDot, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    // Get the reciprocal
    vDot = _mm_sqrt_ps(vDot);
    // Get the reciprocal
    vDot = _mm_div_ps(g_XMOne,vDot);
    return vDot;
#line 9441 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3LengthEst
(
    FXMVECTOR V
)
{









#line 9460 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"














#line 9475 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 9478 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






#line 9485 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x,y and z
    XMVECTOR vLengthSq = _mm_mul_ps(V,V);
    // vTemp has z and y
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((1) << 6) | ((2) << 4) | ((1) << 2) | ((2))) );
    // x+z, y
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    // y,y,y,y
    vTemp = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    // x+z+y,??,??,??
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    // Splat the length squared
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    // Get the length
    vLengthSq = _mm_sqrt_ps(vLengthSq);
    return vLengthSq;
#line 9501 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3Length
(
    FXMVECTOR V
)
{









#line 9520 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




















#line 9541 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 9544 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






#line 9551 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x,y and z
    XMVECTOR vLengthSq = _mm_mul_ps(V,V);
    // vTemp has z and y
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((1) << 6) | ((2) << 4) | ((1) << 2) | ((2))) );
    // x+z, y
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    // y,y,y,y
    vTemp = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    // x+z+y,??,??,??
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    // Splat the length squared
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    // Get the length
    vLengthSq = _mm_sqrt_ps(vLengthSq);
    return vLengthSq;
#line 9567 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// XMVector3NormalizeEst uses a reciprocal estimate and
// returns QNaN on zero and infinite vectors.

inline XMVECTOR __vectorcall XMVector3NormalizeEst
(
    FXMVECTOR V
)
{







#line 9586 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"











#line 9598 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 9602 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"







#line 9610 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
     // Perform the dot product
    XMVECTOR vDot = _mm_mul_ps(V,V);
    // x=Dot.y, y=Dot.z
    XMVECTOR vTemp = _mm_shuffle_ps( vDot, vDot, (((2) << 6) | ((1) << 4) | ((2) << 2) | ((1))) );
    // Result.x = x+y
    vDot = _mm_add_ss(vDot,vTemp);
    // x=Dot.z
    vTemp = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    // Result.x = (x+y)+z
    vDot = _mm_add_ss(vDot,vTemp);
    // Splat x
    vDot = _mm_shuffle_ps( vDot, vDot, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    // Get the reciprocal
    vDot = _mm_rsqrt_ps(vDot);
    // Perform the normalization
    vDot = _mm_mul_ps(vDot,V);
    return vDot;
#line 9628 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3Normalize
(
    FXMVECTOR V
)
{


















#line 9656 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"





















#line 9678 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



















#line 9698 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"























#line 9722 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x,y and z only
    XMVECTOR vLengthSq = _mm_mul_ps(V,V);
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((2) << 6) | ((1) << 4) | ((2) << 2) | ((1))) );
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    vTemp = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    // Prepare for the division
    XMVECTOR vResult = _mm_sqrt_ps(vLengthSq);
    // Create zero with a single instruction
    XMVECTOR vZeroMask = _mm_setzero_ps();
    // Test for a divide by zero (Must be FP to detect -0.0)
    vZeroMask = _mm_cmpneq_ps(vZeroMask,vResult);
    // Failsafe on zero (Or epsilon) length planes
    // If the length is infinity, set the elements to zero
    vLengthSq = _mm_cmpneq_ps(vLengthSq,g_XMInfinity);
    // Divide to perform the normalization
    vResult = _mm_div_ps(V,vResult);
    // Any that are infinity, set to zero
    vResult = _mm_and_ps(vResult,vZeroMask);
    // Select qnan or result based on infinite length
    XMVECTOR vTemp1 = _mm_andnot_ps(vLengthSq,g_XMQNaN);
    XMVECTOR vTemp2 = _mm_and_ps(vResult,vLengthSq);
    vResult = _mm_or_ps(vTemp1,vTemp2);
    return vResult;
#line 9748 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3ClampLength
(
    FXMVECTOR V,
    float    LengthMin,
    float    LengthMax
)
{
    XMVECTOR ClampMax = XMVectorReplicate(LengthMax);
    XMVECTOR ClampMin = XMVectorReplicate(LengthMin);

    return XMVector3ClampLengthV(V, ClampMin, ClampMax);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3ClampLengthV
(
    FXMVECTOR V,
    FXMVECTOR LengthMin,
    FXMVECTOR LengthMax
)
{
    (void)( (!!((XMVectorGetY(LengthMin) == XMVectorGetX(LengthMin)) && (XMVectorGetZ(LengthMin) == XMVectorGetX(LengthMin)))) || (_wassert(L"(XMVectorGetY(LengthMin) == XMVectorGetX(LengthMin)) && (XMVectorGetZ(LengthMin) == XMVectorGetX(LengthMin))", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(9774)), 0) );
    (void)( (!!((XMVectorGetY(LengthMax) == XMVectorGetX(LengthMax)) && (XMVectorGetZ(LengthMax) == XMVectorGetX(LengthMax)))) || (_wassert(L"(XMVectorGetY(LengthMax) == XMVectorGetX(LengthMax)) && (XMVectorGetZ(LengthMax) == XMVectorGetX(LengthMax))", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(9775)), 0) );
    (void)( (!!(XMVector3GreaterOrEqual(LengthMin, XMVectorZero()))) || (_wassert(L"XMVector3GreaterOrEqual(LengthMin, XMVectorZero())", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(9776)), 0) );
    (void)( (!!(XMVector3GreaterOrEqual(LengthMax, XMVectorZero()))) || (_wassert(L"XMVector3GreaterOrEqual(LengthMax, XMVectorZero())", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(9777)), 0) );
    (void)( (!!(XMVector3GreaterOrEqual(LengthMax, LengthMin))) || (_wassert(L"XMVector3GreaterOrEqual(LengthMax, LengthMin)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(9778)), 0) );

    XMVECTOR LengthSq = XMVector3LengthSq(V);

    const XMVECTOR Zero = XMVectorZero();

    XMVECTOR RcpLength = XMVectorReciprocalSqrt(LengthSq);

    XMVECTOR InfiniteLength = XMVectorEqualInt(LengthSq, g_XMInfinity.v);
    XMVECTOR ZeroLength = XMVectorEqual(LengthSq, Zero);

    XMVECTOR Normal = XMVectorMultiply(V, RcpLength);

    XMVECTOR Length = XMVectorMultiply(LengthSq, RcpLength);

    XMVECTOR Select = XMVectorEqualInt(InfiniteLength, ZeroLength);
    Length = XMVectorSelect(LengthSq, Length, Select);
    Normal = XMVectorSelect(LengthSq, Normal, Select);

    XMVECTOR ControlMax = XMVectorGreater(Length, LengthMax);
    XMVECTOR ControlMin = XMVectorLess(Length, LengthMin);

    XMVECTOR ClampLength = XMVectorSelect(Length, LengthMax, ControlMax);
    ClampLength = XMVectorSelect(ClampLength, LengthMin, ControlMin);

    XMVECTOR Result = XMVectorMultiply(Normal, ClampLength);

    // Preserve the original vector (with no precision loss) if the length falls within the given range
    XMVECTOR Control = XMVectorEqualInt(ControlMax, ControlMin);
    Result = XMVectorSelect(Result, V, Control);

    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3Reflect
(
    FXMVECTOR Incident,
    FXMVECTOR Normal
)
{
    // Result = Incident - (2 * dot(Incident, Normal)) * Normal

    XMVECTOR Result = XMVector3Dot(Incident, Normal);
    Result = XMVectorAdd(Result, Result);
    Result = XMVectorNegativeMultiplySubtract(Result, Normal, Incident);

    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3Refract
(
    FXMVECTOR Incident,
    FXMVECTOR Normal,
    float    RefractionIndex
)
{
    XMVECTOR Index = XMVectorReplicate(RefractionIndex);
    return XMVector3RefractV(Incident, Normal, Index);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3RefractV
(
    FXMVECTOR Incident,
    FXMVECTOR Normal,
    FXMVECTOR RefractionIndex
)
{
    // Result = RefractionIndex * Incident - Normal * (RefractionIndex * dot(Incident, Normal) +
    // sqrt(1 - RefractionIndex * RefractionIndex * (1 - dot(Incident, Normal) * dot(Incident, Normal))))






























#line 9884 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

































#line 9918 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Result = RefractionIndex * Incident - Normal * (RefractionIndex * dot(Incident, Normal) +
    // sqrt(1 - RefractionIndex * RefractionIndex * (1 - dot(Incident, Normal) * dot(Incident, Normal))))
    XMVECTOR IDotN = XMVector3Dot(Incident, Normal);
    // R = 1.0f - RefractionIndex * RefractionIndex * (1.0f - IDotN * IDotN)
    XMVECTOR R = _mm_mul_ps(IDotN, IDotN);
    R = _mm_sub_ps(g_XMOne,R);
    R = _mm_mul_ps(R, RefractionIndex);
    R = _mm_mul_ps(R, RefractionIndex);
    R = _mm_sub_ps(g_XMOne,R);

    XMVECTOR vResult = _mm_cmple_ps(R,g_XMZero);
    if (_mm_movemask_ps(vResult)==0x0f)
    {
        // Total internal reflection
        vResult = g_XMZero;
    }
    else
    {
        // R = RefractionIndex * IDotN + sqrt(R)
        R = _mm_sqrt_ps(R);
        vResult = _mm_mul_ps(RefractionIndex,IDotN);
        R = _mm_add_ps(R,vResult);
        // Result = RefractionIndex * Incident - Normal * R
        vResult = _mm_mul_ps(RefractionIndex, Incident);
        R = _mm_mul_ps(R,Normal);
        vResult = _mm_sub_ps(vResult,R);
    }
    return vResult;
#line 9947 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3Orthogonal
(
    FXMVECTOR V
)
{
    XMVECTOR Zero = XMVectorZero();
    XMVECTOR Z = XMVectorSplatZ(V);
    XMVECTOR YZYY = XMVectorSwizzle<XM_SWIZZLE_Y, XM_SWIZZLE_Z, XM_SWIZZLE_Y, XM_SWIZZLE_Y>(V);

    XMVECTOR NegativeV = XMVectorSubtract(Zero, V);

    XMVECTOR ZIsNegative = XMVectorLess(Z, Zero);
    XMVECTOR YZYYIsNegative = XMVectorLess(YZYY, Zero);

    XMVECTOR S = XMVectorAdd(YZYY, Z);
    XMVECTOR D = XMVectorSubtract(YZYY, Z);

    XMVECTOR Select = XMVectorEqualInt(ZIsNegative, YZYYIsNegative);

    XMVECTOR R0 = XMVectorPermute<XM_PERMUTE_1X, XM_PERMUTE_0X, XM_PERMUTE_0X, XM_PERMUTE_0X>(NegativeV, S);
    XMVECTOR R1 = XMVectorPermute<XM_PERMUTE_1X, XM_PERMUTE_0X, XM_PERMUTE_0X, XM_PERMUTE_0X>(V, D);

    return XMVectorSelect(R1, R0, Select);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3AngleBetweenNormalsEst
(
    FXMVECTOR N1,
    FXMVECTOR N2
)
{
    XMVECTOR Result = XMVector3Dot(N1, N2);
    Result = XMVectorClamp(Result, g_XMNegativeOne.v, g_XMOne.v);
    Result = XMVectorACosEst(Result);
    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3AngleBetweenNormals
(
    FXMVECTOR N1,
    FXMVECTOR N2
)
{
    XMVECTOR Result = XMVector3Dot(N1, N2);
    Result = XMVectorClamp(Result, g_XMNegativeOne.v, g_XMOne.v);
    Result = XMVectorACos(Result);
    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3AngleBetweenVectors
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{
    XMVECTOR L1 = XMVector3ReciprocalLength(V1);
    XMVECTOR L2 = XMVector3ReciprocalLength(V2);

    XMVECTOR Dot = XMVector3Dot(V1, V2);

    L1 = XMVectorMultiply(L1, L2);

    XMVECTOR CosAngle = XMVectorMultiply(Dot, L1);
    CosAngle = XMVectorClamp(CosAngle, g_XMNegativeOne.v, g_XMOne.v);

    return XMVectorACos(CosAngle);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3LinePointDistance
(
    FXMVECTOR LinePoint1,
    FXMVECTOR LinePoint2,
    FXMVECTOR Point
)
{
    // Given a vector PointVector from LinePoint1 to Point and a vector
    // LineVector from LinePoint1 to LinePoint2, the scaled distance
    // PointProjectionScale from LinePoint1 to the perpendicular projection
    // of PointVector onto the line is defined as:
    //
    //     PointProjectionScale = dot(PointVector, LineVector) / LengthSq(LineVector)

    XMVECTOR PointVector = XMVectorSubtract(Point, LinePoint1);
    XMVECTOR LineVector = XMVectorSubtract(LinePoint2, LinePoint1);

    XMVECTOR LengthSq = XMVector3LengthSq(LineVector);

    XMVECTOR PointProjectionScale = XMVector3Dot(PointVector, LineVector);
    PointProjectionScale = XMVectorDivide(PointProjectionScale, LengthSq);

    XMVECTOR DistanceVector = XMVectorMultiply(LineVector, PointProjectionScale);
    DistanceVector = XMVectorSubtract(PointVector, DistanceVector);

    return XMVector3Length(DistanceVector);
}

//------------------------------------------------------------------------------


inline void __vectorcall XMVector3ComponentsFromNormal
(
    XMVECTOR* pParallel,
    XMVECTOR* pPerpendicular,
    FXMVECTOR  V,
    FXMVECTOR  Normal
)
{
    (void)( (!!(pParallel != nullptr)) || (_wassert(L"pParallel != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(10066)), 0) );
    (void)( (!!(pPerpendicular != nullptr)) || (_wassert(L"pPerpendicular != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(10067)), 0) );

    XMVECTOR Scale = XMVector3Dot(V, Normal);

    XMVECTOR Parallel = XMVectorMultiply(Normal, Scale);

    *pParallel = Parallel;
    *pPerpendicular = XMVectorSubtract(V, Parallel);
}

//------------------------------------------------------------------------------
// Transform a vector using a rotation expressed as a unit quaternion

inline XMVECTOR __vectorcall XMVector3Rotate
(
    FXMVECTOR V,
    FXMVECTOR RotationQuaternion
)
{
    XMVECTOR A = XMVectorSelect(g_XMSelect1110.v, V, g_XMSelect1110.v);
    XMVECTOR Q = XMQuaternionConjugate(RotationQuaternion);
    XMVECTOR Result = XMQuaternionMultiply(Q, A);
    return XMQuaternionMultiply(Result, RotationQuaternion);
}

//------------------------------------------------------------------------------
// Transform a vector using the inverse of a rotation expressed as a unit quaternion

inline XMVECTOR __vectorcall XMVector3InverseRotate
(
    FXMVECTOR V,
    FXMVECTOR RotationQuaternion
)
{
    XMVECTOR A = XMVectorSelect(g_XMSelect1110.v, V, g_XMSelect1110.v);
    XMVECTOR Result = XMQuaternionMultiply(RotationQuaternion, A);
    XMVECTOR Q = XMQuaternionConjugate(RotationQuaternion);
    return XMQuaternionMultiply(Result, Q);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3Transform
(
    FXMVECTOR V,
    FXMMATRIX M
)
{












#line 10128 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 10133 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    vResult = _mm_mul_ps(vResult,M.r[0]);
    XMVECTOR vTemp = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    vTemp = _mm_mul_ps(vTemp,M.r[1]);
    vResult = _mm_add_ps(vResult,vTemp);
    vTemp = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    vTemp = _mm_mul_ps(vTemp,M.r[2]);
    vResult = _mm_add_ps(vResult,vTemp);
    vResult = _mm_add_ps(vResult,M.r[3]);
    return vResult;
#line 10144 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------







inline XMFLOAT4* __vectorcall XMVector3TransformStream
(
    XMFLOAT4*       pOutputStream,
    size_t          OutputStride,
    const XMFLOAT3* pInputStream,
    size_t          InputStride,
    size_t          VectorCount,
    FXMMATRIX       M
)
{
    (void)( (!!(pOutputStream != nullptr)) || (_wassert(L"pOutputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(10164)), 0) );
    (void)( (!!(pInputStream != nullptr)) || (_wassert(L"pInputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(10165)), 0) );

    (void)( (!!(InputStride >= sizeof(XMFLOAT3))) || (_wassert(L"InputStride >= sizeof(XMFLOAT3)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(10167)), 0) );
    ;

    (void)( (!!(OutputStride >= sizeof(XMFLOAT4))) || (_wassert(L"OutputStride >= sizeof(XMFLOAT4)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(10170)), 0) );
    ;






























#line 10203 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"























































































#line 10291 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    auto pInputVector = reinterpret_cast<const uint8_t*>(pInputStream);
    auto pOutputVector = reinterpret_cast<uint8_t*>(pOutputStream);

    const XMVECTOR row0 = M.r[0];
    const XMVECTOR row1 = M.r[1];
    const XMVECTOR row2 = M.r[2];
    const XMVECTOR row3 = M.r[3];

    size_t i = 0;
    size_t four = VectorCount >> 2;
    if ( four > 0 )
    {
        if (InputStride == sizeof(XMFLOAT3))
        {
            if ( !(reinterpret_cast<uintptr_t>(pOutputStream) & 0xF) && !(OutputStride & 0xF) )
            {
                // Packed input, aligned output
                for (size_t j = 0; j < four; ++j)
                {
                    __m128 V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                    __m128 L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                    __m128 L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                    pInputVector += sizeof(XMFLOAT3)*4;

                    // Unpack the 4 vectors (.w components are junk)
                    XMVECTOR V3 = _mm_shuffle_ps(L2,L3,(((0) << 6) | ((0) << 4) | ((3) << 2) | ((2)))); XMVECTOR V2 = _mm_shuffle_ps(L2,V1,(((3) << 6) | ((3) << 4) | ((1) << 2) | ((0)))); V2 = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((0) << 2) | ((2))) ); XMVECTOR V4 = _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );

                    // Result 1
                    XMVECTOR Z = _mm_shuffle_ps( V1, V1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    XMVECTOR Y = _mm_shuffle_ps( V1, V1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    XMVECTOR X = _mm_shuffle_ps( V1, V1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    XMVECTOR vTemp = _mm_mul_ps( Z, row2 );
                    XMVECTOR vTemp2 = _mm_mul_ps( Y, row1 );
                    XMVECTOR vTemp3 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );
                    _mm_stream_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                    pOutputVector += OutputStride;

                    // Result 2
                    Z = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V2, V2, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, row2 );
                    vTemp2 = _mm_mul_ps( Y, row1 );
                    vTemp3 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );
                    _mm_stream_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                    pOutputVector += OutputStride;

                    // Result 3
                    Z = _mm_shuffle_ps( V3, V3, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V3, V3, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V3, V3, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, row2 );
                    vTemp2 = _mm_mul_ps( Y, row1 );
                    vTemp3 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );
                    _mm_stream_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                    pOutputVector += OutputStride;

                    // Result 4
                    Z = _mm_shuffle_ps( V4, V4, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V4, V4, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V4, V4, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, row2 );
                    vTemp2 = _mm_mul_ps( Y, row1 );
                    vTemp3 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );
                    _mm_stream_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                    pOutputVector += OutputStride;

                    i += 4;
                }
            }
            else
            {
                // Packed input, unaligned output
                for (size_t j = 0; j < four; ++j)
                {
                    __m128 V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                    __m128 L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                    __m128 L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                    pInputVector += sizeof(XMFLOAT3)*4;

                    // Unpack the 4 vectors (.w components are junk)
                    XMVECTOR V3 = _mm_shuffle_ps(L2,L3,(((0) << 6) | ((0) << 4) | ((3) << 2) | ((2)))); XMVECTOR V2 = _mm_shuffle_ps(L2,V1,(((3) << 6) | ((3) << 4) | ((1) << 2) | ((0)))); V2 = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((0) << 2) | ((2))) ); XMVECTOR V4 = _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );

                    // Result 1
                    XMVECTOR Z = _mm_shuffle_ps( V1, V1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    XMVECTOR Y = _mm_shuffle_ps( V1, V1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    XMVECTOR X = _mm_shuffle_ps( V1, V1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    XMVECTOR vTemp = _mm_mul_ps( Z, row2 );
                    XMVECTOR vTemp2 = _mm_mul_ps( Y, row1 );
                    XMVECTOR vTemp3 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );
                    _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                    pOutputVector += OutputStride;

                    // Result 2
                    Z = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V2, V2, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, row2 );
                    vTemp2 = _mm_mul_ps( Y, row1 );
                    vTemp3 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );
                    _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                    pOutputVector += OutputStride;

                    // Result 3
                    Z = _mm_shuffle_ps( V3, V3, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V3, V3, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V3, V3, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, row2 );
                    vTemp2 = _mm_mul_ps( Y, row1 );
                    vTemp3 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );
                    _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                    pOutputVector += OutputStride;

                    // Result 4
                    Z = _mm_shuffle_ps( V4, V4, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V4, V4, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V4, V4, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, row2 );
                    vTemp2 = _mm_mul_ps( Y, row1 );
                    vTemp3 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );
                    _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
                    pOutputVector += OutputStride;

                    i += 4;
                }
            }
        }
    }

    if ( !(reinterpret_cast<uintptr_t>(pOutputStream) & 0xF) && !(OutputStride & 0xF) )
    {
        // Aligned output
        for (; i < VectorCount; ++i)
        {
            XMVECTOR V = XMLoadFloat3(reinterpret_cast<const XMFLOAT3*>(pInputVector));
            pInputVector += InputStride;

            XMVECTOR Z = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
            XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
            XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

            XMVECTOR vTemp = _mm_mul_ps( Z, row2 );
            XMVECTOR vTemp2 = _mm_mul_ps( Y, row1 );
            XMVECTOR vTemp3 = _mm_mul_ps( X, row0 );
            vTemp = _mm_add_ps( vTemp, row3 );
            vTemp = _mm_add_ps( vTemp, vTemp2 );
            vTemp = _mm_add_ps( vTemp, vTemp3 );

            _mm_stream_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
            pOutputVector += OutputStride;
        }
    }
    else
    {
        // Unaligned output
        for (; i < VectorCount; ++i)
        {
            XMVECTOR V = XMLoadFloat3(reinterpret_cast<const XMFLOAT3*>(pInputVector));
            pInputVector += InputStride;

            XMVECTOR Z = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
            XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
            XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

            XMVECTOR vTemp = _mm_mul_ps( Z, row2 );
            XMVECTOR vTemp2 = _mm_mul_ps( Y, row1 );
            XMVECTOR vTemp3 = _mm_mul_ps( X, row0 );
            vTemp = _mm_add_ps( vTemp, row3 );
            vTemp = _mm_add_ps( vTemp, vTemp2 );
            vTemp = _mm_add_ps( vTemp, vTemp3 );

            _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTemp );
            pOutputVector += OutputStride;
        }
    }

    _mm_sfence();

    return pOutputStream;
#line 10503 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}





//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3TransformCoord
(
    FXMVECTOR V,
    FXMMATRIX M
)
{
    XMVECTOR Z = XMVectorSplatZ(V);
    XMVECTOR Y = XMVectorSplatY(V);
    XMVECTOR X = XMVectorSplatX(V);

    XMVECTOR Result = XMVectorMultiplyAdd(Z, M.r[2], M.r[3]);
    Result = XMVectorMultiplyAdd(Y, M.r[1], Result);
    Result = XMVectorMultiplyAdd(X, M.r[0], Result);

    XMVECTOR W = XMVectorSplatW(Result);
    return XMVectorDivide( Result, W );
}

//------------------------------------------------------------------------------







inline XMFLOAT3* __vectorcall XMVector3TransformCoordStream
(
    XMFLOAT3*       pOutputStream,
    size_t          OutputStride,
    const XMFLOAT3* pInputStream,
    size_t          InputStride,
    size_t          VectorCount,
    FXMMATRIX       M
)
{
    (void)( (!!(pOutputStream != nullptr)) || (_wassert(L"pOutputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(10547)), 0) );
    (void)( (!!(pInputStream != nullptr)) || (_wassert(L"pInputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(10548)), 0) );

    (void)( (!!(InputStride >= sizeof(XMFLOAT3))) || (_wassert(L"InputStride >= sizeof(XMFLOAT3)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(10550)), 0) );
    ;

    (void)( (!!(OutputStride >= sizeof(XMFLOAT3))) || (_wassert(L"OutputStride >= sizeof(XMFLOAT3)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(10553)), 0) );
    ;


































#line 10590 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




















































































































#line 10707 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    auto pInputVector = reinterpret_cast<const uint8_t*>(pInputStream);
    auto pOutputVector = reinterpret_cast<uint8_t*>(pOutputStream);

    const XMVECTOR row0 = M.r[0];
    const XMVECTOR row1 = M.r[1];
    const XMVECTOR row2 = M.r[2];
    const XMVECTOR row3 = M.r[3];

    size_t i = 0;
    size_t four = VectorCount >> 2;
    if ( four > 0 )
    {
        if (InputStride == sizeof(XMFLOAT3))
        {
            if (OutputStride == sizeof(XMFLOAT3))
            {
                if ( !(reinterpret_cast<uintptr_t>(pOutputStream) & 0xF) )
                {
                    // Packed input, aligned & packed output
                    for (size_t j = 0; j < four; ++j)
                    {
                        __m128 V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                        __m128 L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                        __m128 L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                        pInputVector += sizeof(XMFLOAT3)*4;

                        // Unpack the 4 vectors (.w components are junk)
                        XMVECTOR V3 = _mm_shuffle_ps(L2,L3,(((0) << 6) | ((0) << 4) | ((3) << 2) | ((2)))); XMVECTOR V2 = _mm_shuffle_ps(L2,V1,(((3) << 6) | ((3) << 4) | ((1) << 2) | ((0)))); V2 = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((0) << 2) | ((2))) ); XMVECTOR V4 = _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );

                        // Result 1
                        XMVECTOR Z = _mm_shuffle_ps( V1, V1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        XMVECTOR Y = _mm_shuffle_ps( V1, V1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        XMVECTOR X = _mm_shuffle_ps( V1, V1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        XMVECTOR vTemp = _mm_mul_ps( Z, row2 );
                        XMVECTOR vTemp2 = _mm_mul_ps( Y, row1 );
                        XMVECTOR vTemp3 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, row3 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                        V1 = _mm_div_ps( vTemp, W );

                        // Result 2
                        Z = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V2, V2, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, row2 );
                        vTemp2 = _mm_mul_ps( Y, row1 );
                        vTemp3 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, row3 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                        V2 = _mm_div_ps( vTemp, W );

                        // Result 3
                        Z = _mm_shuffle_ps( V3, V3, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V3, V3, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V3, V3, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, row2 );
                        vTemp2 = _mm_mul_ps( Y, row1 );
                        vTemp3 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, row3 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                        V3 = _mm_div_ps( vTemp, W );

                        // Result 4
                        Z = _mm_shuffle_ps( V4, V4, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V4, V4, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V4, V4, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, row2 );
                        vTemp2 = _mm_mul_ps( Y, row1 );
                        vTemp3 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, row3 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                        V4 = _mm_div_ps( vTemp, W );

                        // Pack and store the vectors
                        vTemp = _mm_shuffle_ps(V2,V3,(((1) << 6) | ((0) << 4) | ((2) << 2) | ((1)))); V2 = _mm_shuffle_ps(V2,V1,(((2) << 6) | ((2) << 4) | ((0) << 2) | ((0)))); V1 = _mm_shuffle_ps(V1,V2,(((0) << 6) | ((2) << 4) | ((1) << 2) | ((0)))); V3 = _mm_shuffle_ps(V3,V4,(((0) << 6) | ((0) << 4) | ((2) << 2) | ((2)))); V3 = _mm_shuffle_ps(V3,V4,(((2) << 6) | ((1) << 4) | ((2) << 2) | ((0))));
                        _mm_stream_ps( reinterpret_cast<float*>(pOutputVector), V1 );
                        _mm_stream_ps( reinterpret_cast<float*>(pOutputVector+16), vTemp );
                        _mm_stream_ps( reinterpret_cast<float*>(pOutputVector+32), V3 );
                        pOutputVector += sizeof(XMFLOAT3)*4;
                        i += 4;
                    }
                }
                else
                {
                    // Packed input, unaligned & packed output
                    for (size_t j = 0; j < four; ++j)
                    {
                        __m128 V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                        __m128 L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                        __m128 L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                        pInputVector += sizeof(XMFLOAT3)*4;

                        // Unpack the 4 vectors (.w components are junk)
                        XMVECTOR V3 = _mm_shuffle_ps(L2,L3,(((0) << 6) | ((0) << 4) | ((3) << 2) | ((2)))); XMVECTOR V2 = _mm_shuffle_ps(L2,V1,(((3) << 6) | ((3) << 4) | ((1) << 2) | ((0)))); V2 = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((0) << 2) | ((2))) ); XMVECTOR V4 = _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );

                        // Result 1
                        XMVECTOR Z = _mm_shuffle_ps( V1, V1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        XMVECTOR Y = _mm_shuffle_ps( V1, V1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        XMVECTOR X = _mm_shuffle_ps( V1, V1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        XMVECTOR vTemp = _mm_mul_ps( Z, row2 );
                        XMVECTOR vTemp2 = _mm_mul_ps( Y, row1 );
                        XMVECTOR vTemp3 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, row3 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                        V1 = _mm_div_ps( vTemp, W );

                        // Result 2
                        Z = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V2, V2, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, row2 );
                        vTemp2 = _mm_mul_ps( Y, row1 );
                        vTemp3 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, row3 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                        V2 = _mm_div_ps( vTemp, W );

                        // Result 3
                        Z = _mm_shuffle_ps( V3, V3, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V3, V3, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V3, V3, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, row2 );
                        vTemp2 = _mm_mul_ps( Y, row1 );
                        vTemp3 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, row3 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                        V3 = _mm_div_ps( vTemp, W );

                        // Result 4
                        Z = _mm_shuffle_ps( V4, V4, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V4, V4, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V4, V4, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, row2 );
                        vTemp2 = _mm_mul_ps( Y, row1 );
                        vTemp3 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, row3 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                        V4 = _mm_div_ps( vTemp, W );

                        // Pack and store the vectors
                        vTemp = _mm_shuffle_ps(V2,V3,(((1) << 6) | ((0) << 4) | ((2) << 2) | ((1)))); V2 = _mm_shuffle_ps(V2,V1,(((2) << 6) | ((2) << 4) | ((0) << 2) | ((0)))); V1 = _mm_shuffle_ps(V1,V2,(((0) << 6) | ((2) << 4) | ((1) << 2) | ((0)))); V3 = _mm_shuffle_ps(V3,V4,(((0) << 6) | ((0) << 4) | ((2) << 2) | ((2)))); V3 = _mm_shuffle_ps(V3,V4,(((2) << 6) | ((1) << 4) | ((2) << 2) | ((0))));
                        _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), V1 );
                        _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector+16), vTemp );
                        _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector+32), V3 );
                        pOutputVector += sizeof(XMFLOAT3)*4;
                        i += 4;
                    }
                }
            }
            else
            {
                // Packed input, unpacked output
                for (size_t j = 0; j < four; ++j)
                {
                    __m128 V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                    __m128 L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                    __m128 L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                    pInputVector += sizeof(XMFLOAT3)*4;

                    // Unpack the 4 vectors (.w components are junk)
                    XMVECTOR V3 = _mm_shuffle_ps(L2,L3,(((0) << 6) | ((0) << 4) | ((3) << 2) | ((2)))); XMVECTOR V2 = _mm_shuffle_ps(L2,V1,(((3) << 6) | ((3) << 4) | ((1) << 2) | ((0)))); V2 = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((0) << 2) | ((2))) ); XMVECTOR V4 = _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );

                    // Result 1
                    XMVECTOR Z = _mm_shuffle_ps( V1, V1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    XMVECTOR Y = _mm_shuffle_ps( V1, V1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    XMVECTOR X = _mm_shuffle_ps( V1, V1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    XMVECTOR vTemp = _mm_mul_ps( Z, row2 );
                    XMVECTOR vTemp2 = _mm_mul_ps( Y, row1 );
                    XMVECTOR vTemp3 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );

                    XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                    vTemp = _mm_div_ps( vTemp, W );
                    XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                    pOutputVector += OutputStride;

                    // Result 2
                    Z = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V2, V2, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, row2 );
                    vTemp2 = _mm_mul_ps( Y, row1 );
                    vTemp3 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );

                    W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                    vTemp = _mm_div_ps( vTemp, W );
                    XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                    pOutputVector += OutputStride;

                    // Result 3
                    Z = _mm_shuffle_ps( V3, V3, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V3, V3, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V3, V3, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, row2 );
                    vTemp2 = _mm_mul_ps( Y, row1 );
                    vTemp3 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );

                    W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                    vTemp = _mm_div_ps( vTemp, W );
                    XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                    pOutputVector += OutputStride;

                    // Result 4
                    Z = _mm_shuffle_ps( V4, V4, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V4, V4, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V4, V4, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, row2 );
                    vTemp2 = _mm_mul_ps( Y, row1 );
                    vTemp3 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, row3 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );

                    W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                    vTemp = _mm_div_ps( vTemp, W );
                    XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                    pOutputVector += OutputStride;

                    i += 4;
                }
            }
        }
    }

    for (; i < VectorCount; i++)
    {
        XMVECTOR V = XMLoadFloat3(reinterpret_cast<const XMFLOAT3*>(pInputVector));
        pInputVector += InputStride;

        XMVECTOR Z = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
        XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
        XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

        XMVECTOR vTemp = _mm_mul_ps( Z, row2 );
        XMVECTOR vTemp2 = _mm_mul_ps( Y, row1 );
        XMVECTOR vTemp3 = _mm_mul_ps( X, row0 );
        vTemp = _mm_add_ps( vTemp, row3 );
        vTemp = _mm_add_ps( vTemp, vTemp2 );
        vTemp = _mm_add_ps( vTemp, vTemp3 );

        XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

        vTemp = _mm_div_ps( vTemp, W );

        XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
        pOutputVector += OutputStride;
    }

    _mm_sfence();

    return pOutputStream;
#line 11015 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}





//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3TransformNormal
(
    FXMVECTOR V,
    FXMMATRIX M
)
{












#line 11042 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 11047 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    vResult = _mm_mul_ps(vResult,M.r[0]);
    XMVECTOR vTemp = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    vTemp = _mm_mul_ps(vTemp,M.r[1]);
    vResult = _mm_add_ps(vResult,vTemp);
    vTemp = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    vTemp = _mm_mul_ps(vTemp,M.r[2]);
    vResult = _mm_add_ps(vResult,vTemp);
    return vResult;
#line 11057 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------







inline XMFLOAT3* __vectorcall XMVector3TransformNormalStream
(
    XMFLOAT3*       pOutputStream,
    size_t          OutputStride,
    const XMFLOAT3* pInputStream,
    size_t          InputStride,
    size_t          VectorCount,
    FXMMATRIX       M
)
{
    (void)( (!!(pOutputStream != nullptr)) || (_wassert(L"pOutputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(11077)), 0) );
    (void)( (!!(pInputStream != nullptr)) || (_wassert(L"pInputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(11078)), 0) );

    (void)( (!!(InputStride >= sizeof(XMFLOAT3))) || (_wassert(L"InputStride >= sizeof(XMFLOAT3)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(11080)), 0) );
    ;

    (void)( (!!(OutputStride >= sizeof(XMFLOAT3))) || (_wassert(L"OutputStride >= sizeof(XMFLOAT3)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(11083)), 0) );
    ;





























#line 11115 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

















































































#line 11197 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    auto pInputVector = reinterpret_cast<const uint8_t*>(pInputStream);
    auto pOutputVector = reinterpret_cast<uint8_t*>(pOutputStream);

    const XMVECTOR row0 = M.r[0];
    const XMVECTOR row1 = M.r[1];
    const XMVECTOR row2 = M.r[2];

    size_t i = 0;
    size_t four = VectorCount >> 2;
    if ( four > 0 )
    {
        if (InputStride == sizeof(XMFLOAT3))
        {
            if (OutputStride == sizeof(XMFLOAT3))
            {
                if ( !(reinterpret_cast<uintptr_t>(pOutputStream) & 0xF) )
                {
                    // Packed input, aligned & packed output
                    for (size_t j = 0; j < four; ++j)
                    {
                        __m128 V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                        __m128 L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                        __m128 L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                        pInputVector += sizeof(XMFLOAT3)*4;

                        // Unpack the 4 vectors (.w components are junk)
                        XMVECTOR V3 = _mm_shuffle_ps(L2,L3,(((0) << 6) | ((0) << 4) | ((3) << 2) | ((2)))); XMVECTOR V2 = _mm_shuffle_ps(L2,V1,(((3) << 6) | ((3) << 4) | ((1) << 2) | ((0)))); V2 = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((0) << 2) | ((2))) ); XMVECTOR V4 = _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );

                        // Result 1
                        XMVECTOR Z = _mm_shuffle_ps( V1, V1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        XMVECTOR Y = _mm_shuffle_ps( V1, V1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        XMVECTOR X = _mm_shuffle_ps( V1, V1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        XMVECTOR vTemp = _mm_mul_ps( Z, row2 );
                        XMVECTOR vTemp2 = _mm_mul_ps( Y, row1 );
                        XMVECTOR vTemp3 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        V1 = _mm_add_ps( vTemp, vTemp3 );

                        // Result 2
                        Z = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V2, V2, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, row2 );
                        vTemp2 = _mm_mul_ps( Y, row1 );
                        vTemp3 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        V2 = _mm_add_ps( vTemp, vTemp3 );

                        // Result 3
                        Z = _mm_shuffle_ps( V3, V3, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V3, V3, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V3, V3, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, row2 );
                        vTemp2 = _mm_mul_ps( Y, row1 );
                        vTemp3 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        V3 = _mm_add_ps( vTemp, vTemp3 );

                        // Result 4
                        Z = _mm_shuffle_ps( V4, V4, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V4, V4, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V4, V4, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, row2 );
                        vTemp2 = _mm_mul_ps( Y, row1 );
                        vTemp3 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        V4 = _mm_add_ps( vTemp, vTemp3 );

                        // Pack and store the vectors
                        vTemp = _mm_shuffle_ps(V2,V3,(((1) << 6) | ((0) << 4) | ((2) << 2) | ((1)))); V2 = _mm_shuffle_ps(V2,V1,(((2) << 6) | ((2) << 4) | ((0) << 2) | ((0)))); V1 = _mm_shuffle_ps(V1,V2,(((0) << 6) | ((2) << 4) | ((1) << 2) | ((0)))); V3 = _mm_shuffle_ps(V3,V4,(((0) << 6) | ((0) << 4) | ((2) << 2) | ((2)))); V3 = _mm_shuffle_ps(V3,V4,(((2) << 6) | ((1) << 4) | ((2) << 2) | ((0))));
                        _mm_stream_ps( reinterpret_cast<float*>(pOutputVector), V1 );
                        _mm_stream_ps( reinterpret_cast<float*>(pOutputVector+16), vTemp );
                        _mm_stream_ps( reinterpret_cast<float*>(pOutputVector+32), V3 );
                        pOutputVector += sizeof(XMFLOAT3)*4;
                        i += 4;
                    }
                }
                else
                {
                    // Packed input, unaligned & packed output
                    for (size_t j = 0; j < four; ++j)
                    {
                        __m128 V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                        __m128 L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                        __m128 L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                        pInputVector += sizeof(XMFLOAT3)*4;

                        // Unpack the 4 vectors (.w components are junk)
                        XMVECTOR V3 = _mm_shuffle_ps(L2,L3,(((0) << 6) | ((0) << 4) | ((3) << 2) | ((2)))); XMVECTOR V2 = _mm_shuffle_ps(L2,V1,(((3) << 6) | ((3) << 4) | ((1) << 2) | ((0)))); V2 = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((0) << 2) | ((2))) ); XMVECTOR V4 = _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );

                        // Result 1
                        XMVECTOR Z = _mm_shuffle_ps( V1, V1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        XMVECTOR Y = _mm_shuffle_ps( V1, V1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        XMVECTOR X = _mm_shuffle_ps( V1, V1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        XMVECTOR vTemp = _mm_mul_ps( Z, row2 );
                        XMVECTOR vTemp2 = _mm_mul_ps( Y, row1 );
                        XMVECTOR vTemp3 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        V1 = _mm_add_ps( vTemp, vTemp3 );

                        // Result 2
                        Z = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V2, V2, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, row2 );
                        vTemp2 = _mm_mul_ps( Y, row1 );
                        vTemp3 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        V2 = _mm_add_ps( vTemp, vTemp3 );

                        // Result 3
                        Z = _mm_shuffle_ps( V3, V3, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V3, V3, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V3, V3, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, row2 );
                        vTemp2 = _mm_mul_ps( Y, row1 );
                        vTemp3 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        V3 = _mm_add_ps( vTemp, vTemp3 );

                        // Result 4
                        Z = _mm_shuffle_ps( V4, V4, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V4, V4, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V4, V4, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, row2 );
                        vTemp2 = _mm_mul_ps( Y, row1 );
                        vTemp3 = _mm_mul_ps( X, row0 );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        V4 = _mm_add_ps( vTemp, vTemp3 );

                        // Pack and store the vectors
                        vTemp = _mm_shuffle_ps(V2,V3,(((1) << 6) | ((0) << 4) | ((2) << 2) | ((1)))); V2 = _mm_shuffle_ps(V2,V1,(((2) << 6) | ((2) << 4) | ((0) << 2) | ((0)))); V1 = _mm_shuffle_ps(V1,V2,(((0) << 6) | ((2) << 4) | ((1) << 2) | ((0)))); V3 = _mm_shuffle_ps(V3,V4,(((0) << 6) | ((0) << 4) | ((2) << 2) | ((2)))); V3 = _mm_shuffle_ps(V3,V4,(((2) << 6) | ((1) << 4) | ((2) << 2) | ((0))));
                        _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), V1 );
                        _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector+16), vTemp );
                        _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector+32), V3 );
                        pOutputVector += sizeof(XMFLOAT3)*4;
                        i += 4;
                    }
                }
            }
            else
            {
                // Packed input, unpacked output
                for (size_t j = 0; j < four; ++j)
                {
                    __m128 V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                    __m128 L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                    __m128 L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                    pInputVector += sizeof(XMFLOAT3)*4;

                    // Unpack the 4 vectors (.w components are junk)
                    XMVECTOR V3 = _mm_shuffle_ps(L2,L3,(((0) << 6) | ((0) << 4) | ((3) << 2) | ((2)))); XMVECTOR V2 = _mm_shuffle_ps(L2,V1,(((3) << 6) | ((3) << 4) | ((1) << 2) | ((0)))); V2 = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((0) << 2) | ((2))) ); XMVECTOR V4 = _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );

                    // Result 1
                    XMVECTOR Z = _mm_shuffle_ps( V1, V1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    XMVECTOR Y = _mm_shuffle_ps( V1, V1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    XMVECTOR X = _mm_shuffle_ps( V1, V1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    XMVECTOR vTemp = _mm_mul_ps( Z, row2 );
                    XMVECTOR vTemp2 = _mm_mul_ps( Y, row1 );
                    XMVECTOR vTemp3 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );

                    XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                    pOutputVector += OutputStride;

                    // Result 2
                    Z = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V2, V2, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, row2 );
                    vTemp2 = _mm_mul_ps( Y, row1 );
                    vTemp3 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );

                    XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                    pOutputVector += OutputStride;

                    // Result 3
                    Z = _mm_shuffle_ps( V3, V3, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V3, V3, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V3, V3, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, row2 );
                    vTemp2 = _mm_mul_ps( Y, row1 );
                    vTemp3 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );

                    XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                    pOutputVector += OutputStride;

                    // Result 4
                    Z = _mm_shuffle_ps( V4, V4, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V4, V4, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V4, V4, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, row2 );
                    vTemp2 = _mm_mul_ps( Y, row1 );
                    vTemp3 = _mm_mul_ps( X, row0 );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );

                    XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                    pOutputVector += OutputStride;

                    i += 4;
                }
            }
        }
    }

    for (; i < VectorCount; i++)
    {
        XMVECTOR V = XMLoadFloat3(reinterpret_cast<const XMFLOAT3*>(pInputVector));
        pInputVector += InputStride;

        XMVECTOR Z = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
        XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
        XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

        XMVECTOR vTemp = _mm_mul_ps( Z, row2 );
        XMVECTOR vTemp2 = _mm_mul_ps( Y, row1 );
        XMVECTOR vTemp3 = _mm_mul_ps( X, row0 );
        vTemp = _mm_add_ps( vTemp, vTemp2 );
        vTemp = _mm_add_ps( vTemp, vTemp3 );

        XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
        pOutputVector += OutputStride;
    }

    _mm_sfence();

    return pOutputStream;
#line 11443 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}





//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3Project
(
    FXMVECTOR V,
    float    ViewportX,
    float    ViewportY,
    float    ViewportWidth,
    float    ViewportHeight,
    float    ViewportMinZ,
    float    ViewportMaxZ,
    FXMMATRIX Projection,
    CXMMATRIX View,
    CXMMATRIX World
)
{
    const float HalfViewportWidth = ViewportWidth * 0.5f;
    const float HalfViewportHeight = ViewportHeight * 0.5f;

    XMVECTOR Scale = XMVectorSet(HalfViewportWidth, -HalfViewportHeight, ViewportMaxZ - ViewportMinZ, 0.0f);
    XMVECTOR Offset = XMVectorSet(ViewportX + HalfViewportWidth, ViewportY + HalfViewportHeight, ViewportMinZ, 0.0f);

    XMMATRIX Transform = XMMatrixMultiply(World, View);
    Transform = XMMatrixMultiply(Transform, Projection);

    XMVECTOR Result = XMVector3TransformCoord(V, Transform);

    Result = XMVectorMultiplyAdd(Result, Scale, Offset);

    return Result;
}

//------------------------------------------------------------------------------







inline XMFLOAT3* __vectorcall XMVector3ProjectStream
(
    XMFLOAT3*       pOutputStream,
    size_t          OutputStride,
    const XMFLOAT3* pInputStream,
    size_t          InputStride,
    size_t          VectorCount,
    float           ViewportX,
    float           ViewportY,
    float           ViewportWidth,
    float           ViewportHeight,
    float           ViewportMinZ,
    float           ViewportMaxZ,
    FXMMATRIX     Projection,
    CXMMATRIX     View,
    CXMMATRIX     World
)
{
    (void)( (!!(pOutputStream != nullptr)) || (_wassert(L"pOutputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(11507)), 0) );
    (void)( (!!(pInputStream != nullptr)) || (_wassert(L"pInputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(11508)), 0) );

    (void)( (!!(InputStride >= sizeof(XMFLOAT3))) || (_wassert(L"InputStride >= sizeof(XMFLOAT3)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(11510)), 0) );
    ;

    (void)( (!!(OutputStride >= sizeof(XMFLOAT3))) || (_wassert(L"OutputStride >= sizeof(XMFLOAT3)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(11513)), 0) );
    ;






























#line 11546 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"









































































































































#line 11684 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    const float HalfViewportWidth = ViewportWidth * 0.5f;
    const float HalfViewportHeight = ViewportHeight * 0.5f;

    XMVECTOR Scale = XMVectorSet(HalfViewportWidth, -HalfViewportHeight, ViewportMaxZ - ViewportMinZ, 1.0f);
    XMVECTOR Offset = XMVectorSet(ViewportX + HalfViewportWidth, ViewportY + HalfViewportHeight, ViewportMinZ, 0.0f);

    XMMATRIX Transform = XMMatrixMultiply(World, View);
    Transform = XMMatrixMultiply(Transform, Projection);

    auto pInputVector = reinterpret_cast<const uint8_t*>(pInputStream);
    auto pOutputVector = reinterpret_cast<uint8_t*>(pOutputStream);

    size_t i = 0;
    size_t four = VectorCount >> 2;
    if ( four > 0 )
    {
        if (InputStride == sizeof(XMFLOAT3))
        {
            if (OutputStride == sizeof(XMFLOAT3))
            {
                if ( !(reinterpret_cast<uintptr_t>(pOutputStream) & 0xF) )
                {
                    // Packed input, aligned & packed output
                    for (size_t j = 0; j < four; ++j)
                    {
                        __m128 V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                        __m128 L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                        __m128 L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                        pInputVector += sizeof(XMFLOAT3)*4;

                        // Unpack the 4 vectors (.w components are junk)
                        XMVECTOR V3 = _mm_shuffle_ps(L2,L3,(((0) << 6) | ((0) << 4) | ((3) << 2) | ((2)))); XMVECTOR V2 = _mm_shuffle_ps(L2,V1,(((3) << 6) | ((3) << 4) | ((1) << 2) | ((0)))); V2 = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((0) << 2) | ((2))) ); XMVECTOR V4 = _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );

                        // Result 1
                        XMVECTOR Z = _mm_shuffle_ps( V1, V1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        XMVECTOR Y = _mm_shuffle_ps( V1, V1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        XMVECTOR X = _mm_shuffle_ps( V1, V1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        XMVECTOR vTemp = _mm_mul_ps( Z, Transform.r[2] );
                        XMVECTOR vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                        XMVECTOR vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        vTemp = _mm_div_ps( vTemp, W );

                        vTemp = _mm_mul_ps( vTemp, Scale );
                        V1 = _mm_add_ps( vTemp, Offset );

                        // Result 2
                        Z = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V2, V2, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, Transform.r[2] );
                        vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                        vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        vTemp = _mm_div_ps( vTemp, W );

                        vTemp = _mm_mul_ps( vTemp, Scale );
                        V2 = _mm_add_ps( vTemp, Offset );

                        // Result 3
                        Z = _mm_shuffle_ps( V3, V3, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V3, V3, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V3, V3, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, Transform.r[2] );
                        vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                        vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        vTemp = _mm_div_ps( vTemp, W );

                        vTemp = _mm_mul_ps( vTemp, Scale );
                        V3 = _mm_add_ps( vTemp, Offset );

                        // Result 4
                        Z = _mm_shuffle_ps( V4, V4, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V4, V4, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V4, V4, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, Transform.r[2] );
                        vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                        vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        vTemp = _mm_div_ps( vTemp, W );

                        vTemp = _mm_mul_ps( vTemp, Scale );
                        V4 = _mm_add_ps( vTemp, Offset );

                        // Pack and store the vectors
                        vTemp = _mm_shuffle_ps(V2,V3,(((1) << 6) | ((0) << 4) | ((2) << 2) | ((1)))); V2 = _mm_shuffle_ps(V2,V1,(((2) << 6) | ((2) << 4) | ((0) << 2) | ((0)))); V1 = _mm_shuffle_ps(V1,V2,(((0) << 6) | ((2) << 4) | ((1) << 2) | ((0)))); V3 = _mm_shuffle_ps(V3,V4,(((0) << 6) | ((0) << 4) | ((2) << 2) | ((2)))); V3 = _mm_shuffle_ps(V3,V4,(((2) << 6) | ((1) << 4) | ((2) << 2) | ((0))));
                        _mm_stream_ps( reinterpret_cast<float*>(pOutputVector), V1 );
                        _mm_stream_ps( reinterpret_cast<float*>(pOutputVector+16), vTemp );
                        _mm_stream_ps( reinterpret_cast<float*>(pOutputVector+32), V3 );
                        pOutputVector += sizeof(XMFLOAT3)*4;
                        i += 4;
                    }
                }
                else
                {
                    // Packed input, unaligned & packed output
                    for (size_t j = 0; j < four; ++j)
                    {
                        __m128 V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                        __m128 L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                        __m128 L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                        pInputVector += sizeof(XMFLOAT3)*4;

                        // Unpack the 4 vectors (.w components are junk)
                        XMVECTOR V3 = _mm_shuffle_ps(L2,L3,(((0) << 6) | ((0) << 4) | ((3) << 2) | ((2)))); XMVECTOR V2 = _mm_shuffle_ps(L2,V1,(((3) << 6) | ((3) << 4) | ((1) << 2) | ((0)))); V2 = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((0) << 2) | ((2))) ); XMVECTOR V4 = _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );

                        // Result 1
                        XMVECTOR Z = _mm_shuffle_ps( V1, V1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        XMVECTOR Y = _mm_shuffle_ps( V1, V1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        XMVECTOR X = _mm_shuffle_ps( V1, V1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        XMVECTOR vTemp = _mm_mul_ps( Z, Transform.r[2] );
                        XMVECTOR vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                        XMVECTOR vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        vTemp = _mm_div_ps( vTemp, W );

                        vTemp = _mm_mul_ps( vTemp, Scale );
                        V1 = _mm_add_ps( vTemp, Offset );

                        // Result 2
                        Z = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V2, V2, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, Transform.r[2] );
                        vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                        vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        vTemp = _mm_div_ps( vTemp, W );

                        vTemp = _mm_mul_ps( vTemp, Scale );
                        V2 = _mm_add_ps( vTemp, Offset );

                        // Result 3
                        Z = _mm_shuffle_ps( V3, V3, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V3, V3, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V3, V3, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, Transform.r[2] );
                        vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                        vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        vTemp = _mm_div_ps( vTemp, W );

                        vTemp = _mm_mul_ps( vTemp, Scale );
                        V3 = _mm_add_ps( vTemp, Offset );

                        // Result 4
                        Z = _mm_shuffle_ps( V4, V4, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V4, V4, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V4, V4, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, Transform.r[2] );
                        vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                        vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        vTemp = _mm_div_ps( vTemp, W );

                        vTemp = _mm_mul_ps( vTemp, Scale );
                        V4 = _mm_add_ps( vTemp, Offset );

                        // Pack and store the vectors
                        vTemp = _mm_shuffle_ps(V2,V3,(((1) << 6) | ((0) << 4) | ((2) << 2) | ((1)))); V2 = _mm_shuffle_ps(V2,V1,(((2) << 6) | ((2) << 4) | ((0) << 2) | ((0)))); V1 = _mm_shuffle_ps(V1,V2,(((0) << 6) | ((2) << 4) | ((1) << 2) | ((0)))); V3 = _mm_shuffle_ps(V3,V4,(((0) << 6) | ((0) << 4) | ((2) << 2) | ((2)))); V3 = _mm_shuffle_ps(V3,V4,(((2) << 6) | ((1) << 4) | ((2) << 2) | ((0))));
                        _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), V1 );
                        _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector+16), vTemp );
                        _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector+32), V3 );
                        pOutputVector += sizeof(XMFLOAT3)*4;
                        i += 4;
                    }
                }
            }
            else
            {
                // Packed input, unpacked output
                for (size_t j = 0; j < four; ++j)
                {
                    __m128 V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                    __m128 L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                    __m128 L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                    pInputVector += sizeof(XMFLOAT3)*4;

                    // Unpack the 4 vectors (.w components are junk)
                    XMVECTOR V3 = _mm_shuffle_ps(L2,L3,(((0) << 6) | ((0) << 4) | ((3) << 2) | ((2)))); XMVECTOR V2 = _mm_shuffle_ps(L2,V1,(((3) << 6) | ((3) << 4) | ((1) << 2) | ((0)))); V2 = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((0) << 2) | ((2))) ); XMVECTOR V4 = _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );

                    // Result 1
                    XMVECTOR Z = _mm_shuffle_ps( V1, V1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    XMVECTOR Y = _mm_shuffle_ps( V1, V1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    XMVECTOR X = _mm_shuffle_ps( V1, V1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    XMVECTOR vTemp = _mm_mul_ps( Z, Transform.r[2] );
                    XMVECTOR vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                    XMVECTOR vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                    vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );

                    XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                    vTemp = _mm_div_ps( vTemp, W );

                    vTemp = _mm_mul_ps( vTemp, Scale );
                    vTemp = _mm_add_ps( vTemp, Offset );

                    XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                    pOutputVector += OutputStride;

                    // Result 2
                    Z = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V2, V2, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, Transform.r[2] );
                    vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                    vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                    vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );

                    W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                    vTemp = _mm_div_ps( vTemp, W );

                    vTemp = _mm_mul_ps( vTemp, Scale );
                    vTemp = _mm_add_ps( vTemp, Offset );

                    XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                    pOutputVector += OutputStride;

                    // Result 3
                    Z = _mm_shuffle_ps( V3, V3, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V3, V3, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V3, V3, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, Transform.r[2] );
                    vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                    vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                    vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );

                    W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                    vTemp = _mm_div_ps( vTemp, W );

                    vTemp = _mm_mul_ps( vTemp, Scale );
                    vTemp = _mm_add_ps( vTemp, Offset );

                    XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                    pOutputVector += OutputStride;

                    // Result 4
                    Z = _mm_shuffle_ps( V4, V4, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V4, V4, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V4, V4, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, Transform.r[2] );
                    vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                    vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                    vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );

                    W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                    vTemp = _mm_div_ps( vTemp, W );

                    vTemp = _mm_mul_ps( vTemp, Scale );
                    vTemp = _mm_add_ps( vTemp, Offset );

                    XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                    pOutputVector += OutputStride;

                    i += 4;
                }
            }
        }
    }

    for (; i < VectorCount; i++)
    {
        XMVECTOR V = XMLoadFloat3(reinterpret_cast<const XMFLOAT3*>(pInputVector));
        pInputVector += InputStride;

        XMVECTOR Z = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
        XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
        XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

        XMVECTOR vTemp = _mm_mul_ps( Z, Transform.r[2] );
        XMVECTOR vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
        XMVECTOR vTemp3 = _mm_mul_ps( X, Transform.r[0] );
        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
        vTemp = _mm_add_ps( vTemp, vTemp2 );
        vTemp = _mm_add_ps( vTemp, vTemp3 );

        XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
        vTemp = _mm_div_ps( vTemp, W );

        vTemp = _mm_mul_ps( vTemp, Scale );
        vTemp = _mm_add_ps( vTemp, Offset );

        XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
        pOutputVector += OutputStride;
    }

    _mm_sfence();

    return pOutputStream;
#line 12026 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}





//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector3Unproject
(
    FXMVECTOR V,
    float     ViewportX,
    float     ViewportY,
    float     ViewportWidth,
    float     ViewportHeight,
    float     ViewportMinZ,
    float     ViewportMaxZ,
    FXMMATRIX Projection,
    CXMMATRIX View,
    CXMMATRIX World
)
{
    static const XMVECTORF32 D = { { { -1.0f, 1.0f, 0.0f, 0.0f } } };

    XMVECTOR Scale = XMVectorSet(ViewportWidth * 0.5f, -ViewportHeight * 0.5f, ViewportMaxZ - ViewportMinZ, 1.0f);
    Scale = XMVectorReciprocal(Scale);

    XMVECTOR Offset = XMVectorSet(-ViewportX, -ViewportY, -ViewportMinZ, 0.0f);
    Offset = XMVectorMultiplyAdd(Scale, Offset, D.v);

    XMMATRIX Transform = XMMatrixMultiply(World, View);
    Transform = XMMatrixMultiply(Transform, Projection);
    Transform = XMMatrixInverse(nullptr, Transform);

    XMVECTOR Result = XMVectorMultiplyAdd(V, Scale, Offset);

    return XMVector3TransformCoord(Result, Transform);
}

//------------------------------------------------------------------------------







inline XMFLOAT3* __vectorcall XMVector3UnprojectStream
(
    XMFLOAT3*       pOutputStream,
    size_t          OutputStride,
    const XMFLOAT3* pInputStream,
    size_t          InputStride,
    size_t          VectorCount,
    float           ViewportX,
    float           ViewportY,
    float           ViewportWidth,
    float           ViewportHeight,
    float           ViewportMinZ,
    float           ViewportMaxZ,
    FXMMATRIX       Projection,
    CXMMATRIX       View,
    CXMMATRIX       World)
{
    (void)( (!!(pOutputStream != nullptr)) || (_wassert(L"pOutputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(12090)), 0) );
    (void)( (!!(pInputStream != nullptr)) || (_wassert(L"pInputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(12091)), 0) );

    (void)( (!!(InputStride >= sizeof(XMFLOAT3))) || (_wassert(L"InputStride >= sizeof(XMFLOAT3)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(12093)), 0) );
    ;

    (void)( (!!(OutputStride >= sizeof(XMFLOAT3))) || (_wassert(L"OutputStride >= sizeof(XMFLOAT3)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(12096)), 0) );
    ;


































#line 12133 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"























































































































































#line 12285 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    static const XMVECTORF32 D = { { { -1.0f, 1.0f, 0.0f, 0.0f } } };

    XMVECTOR Scale = XMVectorSet(ViewportWidth * 0.5f, -ViewportHeight * 0.5f, ViewportMaxZ - ViewportMinZ, 1.0f);
    Scale = XMVectorReciprocal(Scale);

    XMVECTOR Offset = XMVectorSet(-ViewportX, -ViewportY, -ViewportMinZ, 0.0f);
    Offset = _mm_mul_ps(Scale, Offset);
    Offset = _mm_add_ps(Offset, D);

    XMMATRIX Transform = XMMatrixMultiply(World, View);
    Transform = XMMatrixMultiply(Transform, Projection);
    Transform = XMMatrixInverse(nullptr, Transform);

    auto pInputVector = reinterpret_cast<const uint8_t*>(pInputStream);
    auto pOutputVector = reinterpret_cast<uint8_t*>(pOutputStream);

    size_t i = 0;
    size_t four = VectorCount >> 2;
    if ( four > 0 )
    {
        if (InputStride == sizeof(XMFLOAT3))
        {
            if (OutputStride == sizeof(XMFLOAT3))
            {
                if ( !(reinterpret_cast<uintptr_t>(pOutputStream) & 0xF) )
                {
                    // Packed input, aligned & packed output
                    for (size_t j = 0; j < four; ++j)
                    {
                        __m128 V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                        __m128 L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                        __m128 L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                        pInputVector += sizeof(XMFLOAT3)*4;

                        // Unpack the 4 vectors (.w components are junk)
                        XMVECTOR V3 = _mm_shuffle_ps(L2,L3,(((0) << 6) | ((0) << 4) | ((3) << 2) | ((2)))); XMVECTOR V2 = _mm_shuffle_ps(L2,V1,(((3) << 6) | ((3) << 4) | ((1) << 2) | ((0)))); V2 = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((0) << 2) | ((2))) ); XMVECTOR V4 = _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );

                        // Result 1
                        V1 = _mm_mul_ps( V1, Scale );
                        V1 = _mm_add_ps( V1, Offset );

                        XMVECTOR Z = _mm_shuffle_ps( V1, V1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        XMVECTOR Y = _mm_shuffle_ps( V1, V1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        XMVECTOR X = _mm_shuffle_ps( V1, V1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        XMVECTOR vTemp = _mm_mul_ps( Z, Transform.r[2] );
                        XMVECTOR vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                        XMVECTOR vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        V1 = _mm_div_ps( vTemp, W );

                        // Result 2
                        V2 = _mm_mul_ps( V2, Scale );
                        V2 = _mm_add_ps( V2, Offset );

                        Z = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V2, V2, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, Transform.r[2] );
                        vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                        vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        V2 = _mm_div_ps( vTemp, W );

                        // Result 3
                        V3 = _mm_mul_ps( V3, Scale );
                        V3 = _mm_add_ps( V3, Offset );

                        Z = _mm_shuffle_ps( V3, V3, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V3, V3, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V3, V3, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, Transform.r[2] );
                        vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                        vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        V3 = _mm_div_ps( vTemp, W );

                        // Result 4
                        V4 = _mm_mul_ps( V4, Scale );
                        V4 = _mm_add_ps( V4, Offset );

                        Z = _mm_shuffle_ps( V4, V4, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V4, V4, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V4, V4, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, Transform.r[2] );
                        vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                        vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        V4 = _mm_div_ps( vTemp, W );

                        // Pack and store the vectors
                        vTemp = _mm_shuffle_ps(V2,V3,(((1) << 6) | ((0) << 4) | ((2) << 2) | ((1)))); V2 = _mm_shuffle_ps(V2,V1,(((2) << 6) | ((2) << 4) | ((0) << 2) | ((0)))); V1 = _mm_shuffle_ps(V1,V2,(((0) << 6) | ((2) << 4) | ((1) << 2) | ((0)))); V3 = _mm_shuffle_ps(V3,V4,(((0) << 6) | ((0) << 4) | ((2) << 2) | ((2)))); V3 = _mm_shuffle_ps(V3,V4,(((2) << 6) | ((1) << 4) | ((2) << 2) | ((0))));
                        _mm_stream_ps( reinterpret_cast<float*>(pOutputVector), V1 );
                        _mm_stream_ps( reinterpret_cast<float*>(pOutputVector+16), vTemp );
                        _mm_stream_ps( reinterpret_cast<float*>(pOutputVector+32), V3 );
                        pOutputVector += sizeof(XMFLOAT3)*4;
                        i += 4;
                    }
                }
                else
                {
                    // Packed input, unaligned & packed output
                    for (size_t j = 0; j < four; ++j)
                    {
                        __m128 V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                        __m128 L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                        __m128 L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                        pInputVector += sizeof(XMFLOAT3)*4;

                        // Unpack the 4 vectors (.w components are junk)
                        XMVECTOR V3 = _mm_shuffle_ps(L2,L3,(((0) << 6) | ((0) << 4) | ((3) << 2) | ((2)))); XMVECTOR V2 = _mm_shuffle_ps(L2,V1,(((3) << 6) | ((3) << 4) | ((1) << 2) | ((0)))); V2 = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((0) << 2) | ((2))) ); XMVECTOR V4 = _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );

                        // Result 1
                        V1 = _mm_mul_ps( V1, Scale );
                        V1 = _mm_add_ps( V1, Offset );

                        XMVECTOR Z = _mm_shuffle_ps( V1, V1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        XMVECTOR Y = _mm_shuffle_ps( V1, V1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        XMVECTOR X = _mm_shuffle_ps( V1, V1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        XMVECTOR vTemp = _mm_mul_ps( Z, Transform.r[2] );
                        XMVECTOR vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                        XMVECTOR vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        V1 = _mm_div_ps( vTemp, W );

                        // Result 2
                        V2 = _mm_mul_ps( V2, Scale );
                        V2 = _mm_add_ps( V2, Offset );

                        Z = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V2, V2, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, Transform.r[2] );
                        vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                        vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        V2 = _mm_div_ps( vTemp, W );

                        // Result 3
                        V3 = _mm_mul_ps( V3, Scale );
                        V3 = _mm_add_ps( V3, Offset );

                        Z = _mm_shuffle_ps( V3, V3, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V3, V3, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V3, V3, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, Transform.r[2] );
                        vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                        vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        V3 = _mm_div_ps( vTemp, W );

                        // Result 4
                        V4 = _mm_mul_ps( V4, Scale );
                        V4 = _mm_add_ps( V4, Offset );

                        Z = _mm_shuffle_ps( V4, V4, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                        Y = _mm_shuffle_ps( V4, V4, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                        X = _mm_shuffle_ps( V4, V4, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                        vTemp = _mm_mul_ps( Z, Transform.r[2] );
                        vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                        vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                        vTemp = _mm_add_ps( vTemp, vTemp2 );
                        vTemp = _mm_add_ps( vTemp, vTemp3 );

                        W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                        V4 = _mm_div_ps( vTemp, W );

                        // Pack and store the vectors
                        vTemp = _mm_shuffle_ps(V2,V3,(((1) << 6) | ((0) << 4) | ((2) << 2) | ((1)))); V2 = _mm_shuffle_ps(V2,V1,(((2) << 6) | ((2) << 4) | ((0) << 2) | ((0)))); V1 = _mm_shuffle_ps(V1,V2,(((0) << 6) | ((2) << 4) | ((1) << 2) | ((0)))); V3 = _mm_shuffle_ps(V3,V4,(((0) << 6) | ((0) << 4) | ((2) << 2) | ((2)))); V3 = _mm_shuffle_ps(V3,V4,(((2) << 6) | ((1) << 4) | ((2) << 2) | ((0))));
                        _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), V1 );
                        _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector+16), vTemp );
                        _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector+32), V3 );
                        pOutputVector += sizeof(XMFLOAT3)*4;
                        i += 4;
                    }
                }
            }
            else
            {
                // Packed input, unpacked output
                for (size_t j = 0; j < four; ++j)
                {
                    __m128 V1 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                    __m128 L2 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+16) );
                    __m128 L3 = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector+32) );
                    pInputVector += sizeof(XMFLOAT3)*4;

                    // Unpack the 4 vectors (.w components are junk)
                    XMVECTOR V3 = _mm_shuffle_ps(L2,L3,(((0) << 6) | ((0) << 4) | ((3) << 2) | ((2)))); XMVECTOR V2 = _mm_shuffle_ps(L2,V1,(((3) << 6) | ((3) << 4) | ((1) << 2) | ((0)))); V2 = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((0) << 2) | ((2))) ); XMVECTOR V4 = _mm_castsi128_ps( _mm_srli_si128(_mm_castps_si128(L3),32/8) );

                    // Result 1
                    V1 = _mm_mul_ps( V1, Scale );
                    V1 = _mm_add_ps( V1, Offset );

                    XMVECTOR Z = _mm_shuffle_ps( V1, V1, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    XMVECTOR Y = _mm_shuffle_ps( V1, V1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    XMVECTOR X = _mm_shuffle_ps( V1, V1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    XMVECTOR vTemp = _mm_mul_ps( Z, Transform.r[2] );
                    XMVECTOR vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                    XMVECTOR vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                    vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );

                    XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                    vTemp = _mm_div_ps( vTemp, W );

                    XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                    pOutputVector += OutputStride;

                    // Result 2
                    V2 = _mm_mul_ps( V2, Scale );
                    V2 = _mm_add_ps( V2, Offset );

                    Z = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V2, V2, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, Transform.r[2] );
                    vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                    vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                    vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );

                    W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                    vTemp = _mm_div_ps( vTemp, W );

                    XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                    pOutputVector += OutputStride;

                    // Result 3
                    V3 = _mm_mul_ps( V3, Scale );
                    V3 = _mm_add_ps( V3, Offset );

                    Z = _mm_shuffle_ps( V3, V3, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V3, V3, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V3, V3, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, Transform.r[2] );
                    vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                    vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                    vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );

                    W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                    vTemp = _mm_div_ps( vTemp, W );

                    XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                    pOutputVector += OutputStride;

                    // Result 4
                    V4 = _mm_mul_ps( V4, Scale );
                    V4 = _mm_add_ps( V4, Offset );

                    Z = _mm_shuffle_ps( V4, V4, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                    Y = _mm_shuffle_ps( V4, V4, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                    X = _mm_shuffle_ps( V4, V4, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

                    vTemp = _mm_mul_ps( Z, Transform.r[2] );
                    vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
                    vTemp3 = _mm_mul_ps( X, Transform.r[0] );
                    vTemp = _mm_add_ps( vTemp, Transform.r[3] );
                    vTemp = _mm_add_ps( vTemp, vTemp2 );
                    vTemp = _mm_add_ps( vTemp, vTemp3 );

                    W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
                    vTemp = _mm_div_ps( vTemp, W );

                    XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
                    pOutputVector += OutputStride;

                    i += 4;
                }
            }
        }
    }

    for (; i < VectorCount; i++)
    {
        XMVECTOR V = XMLoadFloat3(reinterpret_cast<const XMFLOAT3*>(pInputVector));
        pInputVector += InputStride;

        V = _mm_mul_ps( V, Scale );
        V = _mm_add_ps( V, Offset );

        XMVECTOR Z = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
        XMVECTOR Y = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
        XMVECTOR X = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );

        XMVECTOR vTemp = _mm_mul_ps( Z, Transform.r[2] );
        XMVECTOR vTemp2 = _mm_mul_ps( Y, Transform.r[1] );
        XMVECTOR vTemp3 = _mm_mul_ps( X, Transform.r[0] );
        vTemp = _mm_add_ps( vTemp, Transform.r[3] );
        vTemp = _mm_add_ps( vTemp, vTemp2 );
        vTemp = _mm_add_ps( vTemp, vTemp3 );

        XMVECTOR W = _mm_shuffle_ps( vTemp, vTemp, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
        vTemp = _mm_div_ps( vTemp, W );

        XMStoreFloat3(reinterpret_cast<XMFLOAT3*>(pOutputVector), vTemp);
        pOutputVector += OutputStride;
    }

    _mm_sfence();

    return pOutputStream;
#line 12631 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}





/****************************************************************************
 *
 * 4D Vector
 *
 ****************************************************************************/

//------------------------------------------------------------------------------
// Comparison operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector4Equal
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 12658 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 12663 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpeq_ps(V1,V2);
    return ((_mm_movemask_ps(vTemp)==0x0f) != 0);


#line 12668 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline uint32_t __vectorcall XMVector4EqualR
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{




















#line 12699 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"















#line 12715 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpeq_ps(V1,V2);
    int iTest = _mm_movemask_ps(vTemp);
    uint32_t CR = 0;
    if (iTest==0xf)     // All equal?
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if (iTest==0)  // All not equal?
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
#line 12728 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector4EqualInt
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 12741 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 12746 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
    return ((_mm_movemask_ps(_mm_castsi128_ps(vTemp))==0xf) != 0);


#line 12751 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline uint32_t __vectorcall XMVector4EqualIntR
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


















#line 12780 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"















#line 12796 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
    int iTest = _mm_movemask_ps(_mm_castsi128_ps(vTemp));
    uint32_t CR = 0;
    if (iTest==0xf)     // All equal?
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if (iTest==0)  // All not equal?
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
#line 12809 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

inline bool __vectorcall XMVector4NearEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2,
    FXMVECTOR Epsilon
)
{











#line 12830 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"





#line 12836 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Get the difference
    XMVECTOR vDelta = _mm_sub_ps(V1,V2);
    // Get the absolute value of the difference
    XMVECTOR vTemp = _mm_setzero_ps();
    vTemp = _mm_sub_ps(vTemp,vDelta);
    vTemp = _mm_max_ps(vTemp,vDelta);
    vTemp = _mm_cmple_ps(vTemp,Epsilon);
    return ((_mm_movemask_ps(vTemp)==0xf) != 0);
#line 12845 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector4NotEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 12858 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 12863 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpneq_ps(V1,V2);
    return ((_mm_movemask_ps(vTemp)) != 0);


#line 12868 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector4NotEqualInt
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 12881 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 12886 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    __m128i vTemp = _mm_cmpeq_epi32(_mm_castps_si128(V1),_mm_castps_si128(V2));
    return ((_mm_movemask_ps(_mm_castsi128_ps(vTemp))!=0xF) != 0);


#line 12891 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector4Greater
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 12904 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 12909 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpgt_ps(V1,V2);
    return ((_mm_movemask_ps(vTemp)==0x0f) != 0);


#line 12914 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline uint32_t __vectorcall XMVector4GreaterR
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


















#line 12943 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"















#line 12959 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    uint32_t CR = 0;
    XMVECTOR vTemp = _mm_cmpgt_ps(V1,V2);
    int iTest = _mm_movemask_ps(vTemp);
    if (iTest==0xf) {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if (!iTest)
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
#line 12971 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector4GreaterOrEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 12984 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 12989 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmpge_ps(V1,V2);
    return ((_mm_movemask_ps(vTemp)==0x0f) != 0);


#line 12994 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline uint32_t __vectorcall XMVector4GreaterOrEqualR
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


















#line 13023 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"















#line 13039 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    uint32_t CR = 0;
    XMVECTOR vTemp = _mm_cmpge_ps(V1,V2);
    int iTest = _mm_movemask_ps(vTemp);
    if (iTest==0x0f)
    {
        CR = XM_CRMASK_CR6TRUE;
    }
    else if (!iTest)
    {
        CR = XM_CRMASK_CR6FALSE;
    }
    return CR;
#line 13052 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector4Less
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 13065 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 13070 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmplt_ps(V1,V2);
    return ((_mm_movemask_ps(vTemp)==0x0f) != 0);


#line 13075 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector4LessOrEqual
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{


#line 13088 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 13093 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp = _mm_cmple_ps(V1,V2);
    return ((_mm_movemask_ps(vTemp)==0x0f) != 0);


#line 13098 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector4InBounds
(
    FXMVECTOR V,
    FXMVECTOR Bounds
)
{





#line 13114 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"












#line 13127 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Test if less than or equal
    XMVECTOR vTemp1 = _mm_cmple_ps(V,Bounds);
    // Negate the bounds
    XMVECTOR vTemp2 = _mm_mul_ps(Bounds,g_XMNegativeOne);
    // Test if greater or equal (Reversed)
    vTemp2 = _mm_cmple_ps(vTemp2,V);
    // Blend answers
    vTemp1 = _mm_and_ps(vTemp1,vTemp2);
    // All in bounds?
    return ((_mm_movemask_ps(vTemp1)==0x0f) != 0);


#line 13140 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------


#pragma float_control(push)
#pragma float_control(precise, on)
#line 13148 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

inline bool __vectorcall XMVector4IsNaN
(
    FXMVECTOR V
)
{





#line 13160 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






#line 13167 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Test against itself. NaN is always not equal
    XMVECTOR vTempNan = _mm_cmpneq_ps(V,V);
    // If any are NaN, the mask is non-zero
    return (_mm_movemask_ps(vTempNan)!=0);
#line 13172 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}


#pragma float_control(pop)
#line 13177 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

//------------------------------------------------------------------------------

inline bool __vectorcall XMVector4IsInfinite
(
    FXMVECTOR V
)
{







#line 13193 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"








#line 13202 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Mask off the sign bit
    XMVECTOR vTemp = _mm_and_ps(V,g_XMAbsMask);
    // Compare to infinity
    vTemp = _mm_cmpeq_ps(vTemp,g_XMInfinity);
    // If any are infinity, the signs are true.
    return (_mm_movemask_ps(vTemp) != 0);
#line 13209 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// Computation operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4Dot
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{









#line 13233 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






#line 13240 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#line 13242 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 13246 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR vTemp2 = V2;
    XMVECTOR vTemp = _mm_mul_ps(V1,vTemp2);
    vTemp2 = _mm_shuffle_ps(vTemp2,vTemp,(((1) << 6) | ((0) << 4) | ((0) << 2) | ((0)))); // Copy X to the Z position and Y to the W position
    vTemp2 = _mm_add_ps(vTemp2,vTemp);          // Add Z = X+Z; W = Y+W;
    vTemp = _mm_shuffle_ps(vTemp,vTemp2,(((0) << 6) | ((3) << 4) | ((0) << 2) | ((0))));  // Copy W to the Z position
    vTemp = _mm_add_ps(vTemp,vTemp2);           // Add Z and W together
    return _mm_shuffle_ps( vTemp, vTemp, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );    // Splat Z and return
#line 13254 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4Cross
(
    FXMVECTOR V1,
    FXMVECTOR V2,
    FXMVECTOR V3
)
{
    // [ ((v2.z*v3.w-v2.w*v3.z)*v1.y)-((v2.y*v3.w-v2.w*v3.y)*v1.z)+((v2.y*v3.z-v2.z*v3.y)*v1.w),
    //   ((v2.w*v3.z-v2.z*v3.w)*v1.x)-((v2.w*v3.x-v2.x*v3.w)*v1.z)+((v2.z*v3.x-v2.x*v3.z)*v1.w),
    //   ((v2.y*v3.w-v2.w*v3.y)*v1.x)-((v2.x*v3.w-v2.w*v3.x)*v1.y)+((v2.x*v3.y-v2.y*v3.x)*v1.w),
    //   ((v2.z*v3.y-v2.y*v3.z)*v1.x)-((v2.z*v3.x-v2.x*v3.z)*v1.y)+((v2.y*v3.x-v2.x*v3.y)*v1.z) ]











#line 13281 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
















































































#line 13362 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // V2zwyz * V3wzwy
    XMVECTOR vResult = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((1) << 4) | ((3) << 2) | ((2))) );
    XMVECTOR vTemp3 = _mm_shuffle_ps( V3, V3, (((1) << 6) | ((3) << 4) | ((2) << 2) | ((3))) );
    vResult = _mm_mul_ps(vResult,vTemp3);
    // - V2wzwy * V3zwyz
    XMVECTOR vTemp2 = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((3) << 4) | ((2) << 2) | ((3))) );
    vTemp3 = _mm_shuffle_ps( vTemp3, vTemp3, (((1) << 6) | ((3) << 4) | ((0) << 2) | ((1))) );
    vTemp2 = _mm_mul_ps(vTemp2,vTemp3);
    vResult = _mm_sub_ps(vResult,vTemp2);
    // term1 * V1yxxx
    XMVECTOR vTemp1 = _mm_shuffle_ps( V1, V1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((1))) );
    vResult = _mm_mul_ps(vResult,vTemp1);

    // V2ywxz * V3wxwx
    vTemp2 = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((0) << 4) | ((3) << 2) | ((1))) );
    vTemp3 = _mm_shuffle_ps( V3, V3, (((0) << 6) | ((3) << 4) | ((0) << 2) | ((3))) );
    vTemp3 = _mm_mul_ps(vTemp3,vTemp2);
    // - V2wxwx * V3ywxz
    vTemp2 = _mm_shuffle_ps( vTemp2, vTemp2, (((2) << 6) | ((1) << 4) | ((2) << 2) | ((1))) );
    vTemp1 = _mm_shuffle_ps( V3, V3, (((2) << 6) | ((0) << 4) | ((3) << 2) | ((1))) );
    vTemp2 = _mm_mul_ps(vTemp2,vTemp1);
    vTemp3 = _mm_sub_ps(vTemp3,vTemp2);
    // vResult - temp * V1zzyy
    vTemp1 = _mm_shuffle_ps( V1, V1, (((1) << 6) | ((1) << 4) | ((2) << 2) | ((2))) );
    vTemp1 = _mm_mul_ps(vTemp1,vTemp3);
    vResult = _mm_sub_ps(vResult,vTemp1);

    // V2yzxy * V3zxyx
    vTemp2 = _mm_shuffle_ps( V2, V2, (((1) << 6) | ((0) << 4) | ((2) << 2) | ((1))) );
    vTemp3 = _mm_shuffle_ps( V3, V3, (((0) << 6) | ((1) << 4) | ((0) << 2) | ((2))) );
    vTemp3 = _mm_mul_ps(vTemp3,vTemp2);
    // - V2zxyx * V3yzxy
    vTemp2 = _mm_shuffle_ps( vTemp2, vTemp2, (((2) << 6) | ((0) << 4) | ((2) << 2) | ((1))) );
    vTemp1 = _mm_shuffle_ps( V3, V3, (((1) << 6) | ((0) << 4) | ((2) << 2) | ((1))) );
    vTemp1 = _mm_mul_ps(vTemp1,vTemp2);
    vTemp3 = _mm_sub_ps(vTemp3,vTemp1);
    // vResult + term * V1wwwz
    vTemp1 = _mm_shuffle_ps( V1, V1, (((2) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    vTemp3 = _mm_mul_ps(vTemp3,vTemp1);
    vResult = _mm_add_ps(vResult,vTemp3);
    return vResult;
#line 13404 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4LengthSq
(
    FXMVECTOR V
)
{
    return XMVector4Dot(V, V);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4ReciprocalLengthEst
(
    FXMVECTOR V
)
{









#line 13433 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"









#line 13443 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 13446 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"





#line 13452 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x,y,z and w
    XMVECTOR vLengthSq = _mm_mul_ps(V,V);
    // vTemp has z and w
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))) );
    // x+z, y+w
    vLengthSq = _mm_add_ps(vLengthSq,vTemp);
    // x+z,x+z,x+z,y+w
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((1) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    // ??,??,y+w,y+w
    vTemp = _mm_shuffle_ps(vTemp,vLengthSq,(((3) << 6) | ((3) << 4) | ((0) << 2) | ((0))));
    // ??,??,x+z+y+w,??
    vLengthSq = _mm_add_ps(vLengthSq,vTemp);
    // Splat the length
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    // Get the reciprocal
    vLengthSq = _mm_rsqrt_ps(vLengthSq);
    return vLengthSq;
#line 13470 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4ReciprocalLength
(
    FXMVECTOR V
)
{









#line 13489 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"















#line 13505 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 13509 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






#line 13516 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x,y,z and w
    XMVECTOR vLengthSq = _mm_mul_ps(V,V);
    // vTemp has z and w
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))) );
    // x+z, y+w
    vLengthSq = _mm_add_ps(vLengthSq,vTemp);
    // x+z,x+z,x+z,y+w
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((1) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    // ??,??,y+w,y+w
    vTemp = _mm_shuffle_ps(vTemp,vLengthSq,(((3) << 6) | ((3) << 4) | ((0) << 2) | ((0))));
    // ??,??,x+z+y+w,??
    vLengthSq = _mm_add_ps(vLengthSq,vTemp);
    // Splat the length
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    // Get the reciprocal
    vLengthSq = _mm_sqrt_ps(vLengthSq);
    // Accurate!
    vLengthSq = _mm_div_ps(g_XMOne,vLengthSq);
    return vLengthSq;
#line 13536 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4LengthEst
(
    FXMVECTOR V
)
{









#line 13555 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"













#line 13569 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 13572 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"





#line 13578 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x,y,z and w
    XMVECTOR vLengthSq = _mm_mul_ps(V,V);
    // vTemp has z and w
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))) );
    // x+z, y+w
    vLengthSq = _mm_add_ps(vLengthSq,vTemp);
    // x+z,x+z,x+z,y+w
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((1) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    // ??,??,y+w,y+w
    vTemp = _mm_shuffle_ps(vTemp,vLengthSq,(((3) << 6) | ((3) << 4) | ((0) << 2) | ((0))));
    // ??,??,x+z+y+w,??
    vLengthSq = _mm_add_ps(vLengthSq,vTemp);
    // Splat the length
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    // Get the length
    vLengthSq = _mm_sqrt_ps(vLengthSq);
    return vLengthSq;
#line 13596 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4Length
(
    FXMVECTOR V
)
{









#line 13615 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



















#line 13635 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"


#line 13638 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"





#line 13644 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x,y,z and w
    XMVECTOR vLengthSq = _mm_mul_ps(V,V);
    // vTemp has z and w
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))) );
    // x+z, y+w
    vLengthSq = _mm_add_ps(vLengthSq,vTemp);
    // x+z,x+z,x+z,y+w
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((1) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    // ??,??,y+w,y+w
    vTemp = _mm_shuffle_ps(vTemp,vLengthSq,(((3) << 6) | ((3) << 4) | ((0) << 2) | ((0))));
    // ??,??,x+z+y+w,??
    vLengthSq = _mm_add_ps(vLengthSq,vTemp);
    // Splat the length
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    // Get the length
    vLengthSq = _mm_sqrt_ps(vLengthSq);
    return vLengthSq;
#line 13662 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------
// XMVector4NormalizeEst uses a reciprocal estimate and
// returns QNaN on zero and infinite vectors.

inline XMVECTOR __vectorcall XMVector4NormalizeEst
(
    FXMVECTOR V
)
{







#line 13681 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"










#line 13692 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



#line 13696 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






#line 13703 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x,y,z and w
    XMVECTOR vLengthSq = _mm_mul_ps(V,V);
    // vTemp has z and w
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))) );
    // x+z, y+w
    vLengthSq = _mm_add_ps(vLengthSq,vTemp);
    // x+z,x+z,x+z,y+w
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((1) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    // ??,??,y+w,y+w
    vTemp = _mm_shuffle_ps(vTemp,vLengthSq,(((3) << 6) | ((3) << 4) | ((0) << 2) | ((0))));
    // ??,??,x+z+y+w,??
    vLengthSq = _mm_add_ps(vLengthSq,vTemp);
    // Splat the length
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    // Get the reciprocal
    XMVECTOR vResult = _mm_rsqrt_ps(vLengthSq);
    // Reciprocal mul to perform the normalization
    vResult = _mm_mul_ps(vResult,V);
    return vResult;
#line 13723 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4Normalize
(
    FXMVECTOR V
)
{


















#line 13751 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




















#line 13772 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"



















#line 13792 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






















#line 13815 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Perform the dot product on x,y,z and w
    XMVECTOR vLengthSq = _mm_mul_ps(V,V);
    // vTemp has z and w
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))) );
    // x+z, y+w
    vLengthSq = _mm_add_ps(vLengthSq,vTemp);
    // x+z,x+z,x+z,y+w
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((1) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    // ??,??,y+w,y+w
    vTemp = _mm_shuffle_ps(vTemp,vLengthSq,(((3) << 6) | ((3) << 4) | ((0) << 2) | ((0))));
    // ??,??,x+z+y+w,??
    vLengthSq = _mm_add_ps(vLengthSq,vTemp);
    // Splat the length
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    // Prepare for the division
    XMVECTOR vResult = _mm_sqrt_ps(vLengthSq);
    // Create zero with a single instruction
    XMVECTOR vZeroMask = _mm_setzero_ps();
    // Test for a divide by zero (Must be FP to detect -0.0)
    vZeroMask = _mm_cmpneq_ps(vZeroMask,vResult);
    // Failsafe on zero (Or epsilon) length planes
    // If the length is infinity, set the elements to zero
    vLengthSq = _mm_cmpneq_ps(vLengthSq,g_XMInfinity);
    // Divide to perform the normalization
    vResult = _mm_div_ps(V,vResult);
    // Any that are infinity, set to zero
    vResult = _mm_and_ps(vResult,vZeroMask);
    // Select qnan or result based on infinite length
    XMVECTOR vTemp1 = _mm_andnot_ps(vLengthSq,g_XMQNaN);
    XMVECTOR vTemp2 = _mm_and_ps(vResult,vLengthSq);
    vResult = _mm_or_ps(vTemp1,vTemp2);
    return vResult;
#line 13848 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4ClampLength
(
    FXMVECTOR V,
    float    LengthMin,
    float    LengthMax
)
{
    XMVECTOR ClampMax = XMVectorReplicate(LengthMax);
    XMVECTOR ClampMin = XMVectorReplicate(LengthMin);

    return XMVector4ClampLengthV(V, ClampMin, ClampMax);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4ClampLengthV
(
    FXMVECTOR V,
    FXMVECTOR LengthMin,
    FXMVECTOR LengthMax
)
{
    (void)( (!!((XMVectorGetY(LengthMin) == XMVectorGetX(LengthMin)) && (XMVectorGetZ(LengthMin) == XMVectorGetX(LengthMin)) && (XMVectorGetW(LengthMin) == XMVectorGetX(LengthMin)))) || (_wassert(L"(XMVectorGetY(LengthMin) == XMVectorGetX(LengthMin)) && (XMVectorGetZ(LengthMin) == XMVectorGetX(LengthMin)) && (XMVectorGetW(LengthMin) == XMVectorGetX(LengthMin))", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(13874)), 0) );
    (void)( (!!((XMVectorGetY(LengthMax) == XMVectorGetX(LengthMax)) && (XMVectorGetZ(LengthMax) == XMVectorGetX(LengthMax)) && (XMVectorGetW(LengthMax) == XMVectorGetX(LengthMax)))) || (_wassert(L"(XMVectorGetY(LengthMax) == XMVectorGetX(LengthMax)) && (XMVectorGetZ(LengthMax) == XMVectorGetX(LengthMax)) && (XMVectorGetW(LengthMax) == XMVectorGetX(LengthMax))", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(13875)), 0) );
    (void)( (!!(XMVector4GreaterOrEqual(LengthMin, XMVectorZero()))) || (_wassert(L"XMVector4GreaterOrEqual(LengthMin, XMVectorZero())", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(13876)), 0) );
    (void)( (!!(XMVector4GreaterOrEqual(LengthMax, XMVectorZero()))) || (_wassert(L"XMVector4GreaterOrEqual(LengthMax, XMVectorZero())", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(13877)), 0) );
    (void)( (!!(XMVector4GreaterOrEqual(LengthMax, LengthMin))) || (_wassert(L"XMVector4GreaterOrEqual(LengthMax, LengthMin)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(13878)), 0) );

    XMVECTOR LengthSq = XMVector4LengthSq(V);

    const XMVECTOR Zero = XMVectorZero();

    XMVECTOR RcpLength = XMVectorReciprocalSqrt(LengthSq);

    XMVECTOR InfiniteLength = XMVectorEqualInt(LengthSq, g_XMInfinity.v);
    XMVECTOR ZeroLength = XMVectorEqual(LengthSq, Zero);

    XMVECTOR Normal = XMVectorMultiply(V, RcpLength);

    XMVECTOR Length = XMVectorMultiply(LengthSq, RcpLength);

    XMVECTOR Select = XMVectorEqualInt(InfiniteLength, ZeroLength);
    Length = XMVectorSelect(LengthSq, Length, Select);
    Normal = XMVectorSelect(LengthSq, Normal, Select);

    XMVECTOR ControlMax = XMVectorGreater(Length, LengthMax);
    XMVECTOR ControlMin = XMVectorLess(Length, LengthMin);

    XMVECTOR ClampLength = XMVectorSelect(Length, LengthMax, ControlMax);
    ClampLength = XMVectorSelect(ClampLength, LengthMin, ControlMin);

    XMVECTOR Result = XMVectorMultiply(Normal, ClampLength);

    // Preserve the original vector (with no precision loss) if the length falls within the given range
    XMVECTOR Control = XMVectorEqualInt(ControlMax, ControlMin);
    Result = XMVectorSelect(Result, V, Control);

    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4Reflect
(
    FXMVECTOR Incident,
    FXMVECTOR Normal
)
{
    // Result = Incident - (2 * dot(Incident, Normal)) * Normal

    XMVECTOR Result = XMVector4Dot(Incident, Normal);
    Result = XMVectorAdd(Result, Result);
    Result = XMVectorNegativeMultiplySubtract(Result, Normal, Incident);

    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4Refract
(
    FXMVECTOR Incident,
    FXMVECTOR Normal,
    float    RefractionIndex
)
{
    XMVECTOR Index = XMVectorReplicate(RefractionIndex);
    return XMVector4RefractV(Incident, Normal, Index);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4RefractV
(
    FXMVECTOR Incident,
    FXMVECTOR Normal,
    FXMVECTOR RefractionIndex
)
{




































#line 13988 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

































#line 14022 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    XMVECTOR IDotN = XMVector4Dot(Incident,Normal);

    // R = 1.0f - RefractionIndex * RefractionIndex * (1.0f - IDotN * IDotN)
    XMVECTOR R = _mm_mul_ps(IDotN,IDotN);
    R = _mm_sub_ps(g_XMOne,R);
    R = _mm_mul_ps(R, RefractionIndex);
    R = _mm_mul_ps(R, RefractionIndex);
    R = _mm_sub_ps(g_XMOne,R);

    XMVECTOR vResult = _mm_cmple_ps(R,g_XMZero);
    if (_mm_movemask_ps(vResult)==0x0f)
    {
        // Total internal reflection
        vResult = g_XMZero;
    }
    else
    {
        // R = RefractionIndex * IDotN + sqrt(R)
        R = _mm_sqrt_ps(R);
        vResult = _mm_mul_ps(RefractionIndex, IDotN);
        R = _mm_add_ps(R,vResult);
        // Result = RefractionIndex * Incident - Normal * R
        vResult = _mm_mul_ps(RefractionIndex, Incident);
        R = _mm_mul_ps(R,Normal);
        vResult = _mm_sub_ps(vResult,R);
    }
    return vResult;
#line 14050 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4Orthogonal
(
    FXMVECTOR V
)
{










#line 14070 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 14075 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    static const XMVECTORF32 FlipZW = { { { 1.0f, 1.0f, -1.0f, -1.0f } } };
    XMVECTOR vResult = _mm_shuffle_ps( V, V, (((1) << 6) | ((0) << 4) | ((3) << 2) | ((2))) );
    vResult = _mm_mul_ps(vResult,FlipZW);
    return vResult;
#line 14080 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4AngleBetweenNormalsEst
(
    FXMVECTOR N1,
    FXMVECTOR N2
)
{
    XMVECTOR Result = XMVector4Dot(N1, N2);
    Result = XMVectorClamp(Result, g_XMNegativeOne.v, g_XMOne.v);
    Result = XMVectorACosEst(Result);
    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4AngleBetweenNormals
(
    FXMVECTOR N1,
    FXMVECTOR N2
)
{
    XMVECTOR Result = XMVector4Dot(N1, N2);
    Result = XMVectorClamp(Result, g_XMNegativeOne.v, g_XMOne.v);
    Result = XMVectorACos(Result);
    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4AngleBetweenVectors
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{
    XMVECTOR L1 = XMVector4ReciprocalLength(V1);
    XMVECTOR L2 = XMVector4ReciprocalLength(V2);

    XMVECTOR Dot = XMVector4Dot(V1, V2);

    L1 = XMVectorMultiply(L1, L2);

    XMVECTOR CosAngle = XMVectorMultiply(Dot, L1);
    CosAngle = XMVectorClamp(CosAngle, g_XMNegativeOne.v, g_XMOne.v);

    return XMVectorACos(CosAngle);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMVector4Transform
(
    FXMVECTOR V,
    FXMMATRIX M
)
{









#line 14149 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"






#line 14156 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    // Splat x,y,z and w
    XMVECTOR vTempX = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    XMVECTOR vTempY = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    XMVECTOR vTempZ = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    XMVECTOR vTempW = _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    // Mul by the matrix
    vTempX = _mm_mul_ps(vTempX,M.r[0]);
    vTempY = _mm_mul_ps(vTempY,M.r[1]);
    vTempZ = _mm_mul_ps(vTempZ,M.r[2]);
    vTempW = _mm_mul_ps(vTempW,M.r[3]);
    // Add them all together
    vTempX = _mm_add_ps(vTempX,vTempY);
    vTempZ = _mm_add_ps(vTempZ,vTempW);
    vTempX = _mm_add_ps(vTempX,vTempZ);
    return vTempX;
#line 14172 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

//------------------------------------------------------------------------------

inline XMFLOAT4* __vectorcall XMVector4TransformStream
(
    XMFLOAT4*       pOutputStream,
    size_t          OutputStride,
    const XMFLOAT4* pInputStream,
    size_t          InputStride,
    size_t          VectorCount,
    FXMMATRIX       M
)
{
    (void)( (!!(pOutputStream != nullptr)) || (_wassert(L"pOutputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(14186)), 0) );
    (void)( (!!(pInputStream != nullptr)) || (_wassert(L"pInputStream != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(14187)), 0) );

    (void)( (!!(InputStride >= sizeof(XMFLOAT4))) || (_wassert(L"InputStride >= sizeof(XMFLOAT4)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(14189)), 0) );
    ;

    (void)( (!!(OutputStride >= sizeof(XMFLOAT4))) || (_wassert(L"OutputStride >= sizeof(XMFLOAT4)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl", (unsigned)(14192)), 0) );
    ;









































#line 14236 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

































































































#line 14334 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
    auto pInputVector = reinterpret_cast<const uint8_t*>(pInputStream);
    auto pOutputVector = reinterpret_cast<uint8_t*>(pOutputStream);

    const XMVECTOR row0 = M.r[0];
    const XMVECTOR row1 = M.r[1];
    const XMVECTOR row2 = M.r[2];
    const XMVECTOR row3 = M.r[3];

    if ( !(reinterpret_cast<uintptr_t>(pOutputStream) & 0xF) && !(OutputStride & 0xF) )
    {
        if ( !(reinterpret_cast<uintptr_t>(pInputStream) & 0xF) && !(InputStride & 0xF) )
        {
            // Aligned input, aligned output
            for (size_t i = 0; i < VectorCount; i++)
            {
                __m128 V = _mm_load_ps( reinterpret_cast<const float*>(pInputVector) );
                pInputVector += InputStride;

                XMVECTOR vTempX = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
                XMVECTOR vTempY = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                XMVECTOR vTempZ = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                XMVECTOR vTempW = _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                vTempX = _mm_mul_ps(vTempX,row0);
                vTempY = _mm_mul_ps(vTempY,row1);
                vTempZ = _mm_mul_ps(vTempZ,row2);
                vTempW = _mm_mul_ps(vTempW,row3);

                vTempX = _mm_add_ps(vTempX,vTempY);
                vTempZ = _mm_add_ps(vTempZ,vTempW);
                vTempX = _mm_add_ps(vTempX,vTempZ);

                _mm_stream_ps( reinterpret_cast<float*>(pOutputVector), vTempX );
                pOutputVector += OutputStride;
            }
        }
        else
        {
            // Unaligned input, aligned output
            for (size_t i = 0; i < VectorCount; i++)
            {
                __m128 V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                pInputVector += InputStride;

                XMVECTOR vTempX = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
                XMVECTOR vTempY = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                XMVECTOR vTempZ = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                XMVECTOR vTempW = _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                vTempX = _mm_mul_ps(vTempX,row0);
                vTempY = _mm_mul_ps(vTempY,row1);
                vTempZ = _mm_mul_ps(vTempZ,row2);
                vTempW = _mm_mul_ps(vTempW,row3);

                vTempX = _mm_add_ps(vTempX,vTempY);
                vTempZ = _mm_add_ps(vTempZ,vTempW);
                vTempX = _mm_add_ps(vTempX,vTempZ);

                _mm_stream_ps( reinterpret_cast<float*>(pOutputVector), vTempX );
                pOutputVector += OutputStride;
            }
        }
    }
    else
    {
        if ( !(reinterpret_cast<uintptr_t>(pInputStream) & 0xF) && !(InputStride & 0xF) )
        {
            // Aligned input, unaligned output
            for (size_t i = 0; i < VectorCount; i++)
            {
                __m128 V = _mm_load_ps( reinterpret_cast<const float*>(pInputVector) );
                pInputVector += InputStride;

                XMVECTOR vTempX = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
                XMVECTOR vTempY = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                XMVECTOR vTempZ = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                XMVECTOR vTempW = _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                vTempX = _mm_mul_ps(vTempX,row0);
                vTempY = _mm_mul_ps(vTempY,row1);
                vTempZ = _mm_mul_ps(vTempZ,row2);
                vTempW = _mm_mul_ps(vTempW,row3);

                vTempX = _mm_add_ps(vTempX,vTempY);
                vTempZ = _mm_add_ps(vTempZ,vTempW);
                vTempX = _mm_add_ps(vTempX,vTempZ);

                _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTempX );
                pOutputVector += OutputStride;
            }
        }
        else
        {
            // Unaligned input, unaligned output
            for (size_t i = 0; i < VectorCount; i++)
            {
                __m128 V = _mm_loadu_ps( reinterpret_cast<const float*>(pInputVector) );
                pInputVector += InputStride;

                XMVECTOR vTempX = _mm_shuffle_ps( V, V, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
                XMVECTOR vTempY = _mm_shuffle_ps( V, V, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
                XMVECTOR vTempZ = _mm_shuffle_ps( V, V, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
                XMVECTOR vTempW = _mm_shuffle_ps( V, V, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );

                vTempX = _mm_mul_ps(vTempX,row0);
                vTempY = _mm_mul_ps(vTempY,row1);
                vTempZ = _mm_mul_ps(vTempZ,row2);
                vTempW = _mm_mul_ps(vTempW,row3);

                vTempX = _mm_add_ps(vTempX,vTempY);
                vTempZ = _mm_add_ps(vTempZ,vTempW);
                vTempX = _mm_add_ps(vTempX,vTempZ);

                _mm_storeu_ps( reinterpret_cast<float*>(pOutputVector), vTempX );
                pOutputVector += OutputStride;
            }
        }
    }

    _mm_sfence();

    return pOutputStream;
#line 14457 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"
}

/****************************************************************************
 *
 * XMVECTOR operators
 *
 ****************************************************************************/



//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall operator+ (FXMVECTOR V)
{
    return V;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall operator- (FXMVECTOR V)
{
    return XMVectorNegate(V);
}

//------------------------------------------------------------------------------

inline XMVECTOR& __vectorcall operator+=
(
    XMVECTOR&       V1,
    FXMVECTOR       V2
)
{
    V1 = XMVectorAdd(V1, V2);
    return V1;
}

//------------------------------------------------------------------------------

inline XMVECTOR& __vectorcall operator-=
(
    XMVECTOR&       V1,
    FXMVECTOR       V2
)
{
    V1 = XMVectorSubtract(V1, V2);
    return V1;
}

//------------------------------------------------------------------------------

inline XMVECTOR& __vectorcall operator*=
(
    XMVECTOR&       V1,
    FXMVECTOR       V2
)
{
    V1 = XMVectorMultiply(V1, V2);
    return V1;
}

//------------------------------------------------------------------------------

inline XMVECTOR& __vectorcall operator/=
(
    XMVECTOR&       V1,
    FXMVECTOR       V2
)
{
    V1 = XMVectorDivide(V1,V2);
    return V1;
}

//------------------------------------------------------------------------------

inline XMVECTOR& operator*=
(
    XMVECTOR&   V,
    const float S
)
{
    V = XMVectorScale(V, S);
    return V;
}

//------------------------------------------------------------------------------

inline XMVECTOR& operator/=
(
    XMVECTOR&   V,
    const float S
)
{
    XMVECTOR vS = XMVectorReplicate( S );
    V = XMVectorDivide(V, vS);
    return V;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall operator+
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{
    return XMVectorAdd(V1, V2);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall operator-
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{
    return XMVectorSubtract(V1, V2);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall operator*
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{
    return XMVectorMultiply(V1, V2);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall operator/
(
    FXMVECTOR V1,
    FXMVECTOR V2
)
{
    return XMVectorDivide(V1,V2);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall operator*
(
    FXMVECTOR      V,
    const float    S
)
{
    return XMVectorScale(V, S);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall operator/
(
    FXMVECTOR      V,
    const float    S
)
{
    XMVECTOR vS = XMVectorReplicate( S );
    return XMVectorDivide(V, vS);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall operator*
(
    float           S,
    FXMVECTOR       V
)
{
    return XMVectorScale(V, S);
}

#line 14633 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 14638 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"




#line 14643 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathVector.inl"

#pragma external_header(pop)
#line 2164 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
#pragma external_header(push)
#line 1 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
//-------------------------------------------------------------------------------------
// DirectXMathMatrix.inl -- SIMD C++ Math library
//
// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
//
// http://go.microsoft.com/fwlink/?LinkID=615560
//-------------------------------------------------------------------------------------

#pragma once

/****************************************************************************
 *
 * Matrix
 *
 ****************************************************************************/

//------------------------------------------------------------------------------
// Comparison operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------


#pragma float_control(push)
#pragma float_control(precise, on)
#line 28 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"

// Return true if any entry in the matrix is NaN
inline bool __vectorcall XMMatrixIsNaN
(
    FXMMATRIX M
)
{
















#line 52 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"



















#line 72 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    // Load in registers
    XMVECTOR vX = M.r[0];
    XMVECTOR vY = M.r[1];
    XMVECTOR vZ = M.r[2];
    XMVECTOR vW = M.r[3];
    // Test themselves to check for NaN
    vX = _mm_cmpneq_ps(vX,vX);
    vY = _mm_cmpneq_ps(vY,vY);
    vZ = _mm_cmpneq_ps(vZ,vZ);
    vW = _mm_cmpneq_ps(vW,vW);
    // Or all the results
    vX = _mm_or_ps(vX,vZ);
    vY = _mm_or_ps(vY,vW);
    vX = _mm_or_ps(vX,vY);
    // If any tested true, return true
    return (_mm_movemask_ps(vX)!=0);

#line 90 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}


#pragma float_control(pop)
#line 95 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"

//------------------------------------------------------------------------------

// Return true if any entry in the matrix is +/-INF
inline bool __vectorcall XMMatrixIsInfinite
(
    FXMMATRIX M
)
{















#line 120 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"



















#line 140 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    // Mask off the sign bits
    XMVECTOR vTemp1 = _mm_and_ps(M.r[0],g_XMAbsMask);
    XMVECTOR vTemp2 = _mm_and_ps(M.r[1],g_XMAbsMask);
    XMVECTOR vTemp3 = _mm_and_ps(M.r[2],g_XMAbsMask);
    XMVECTOR vTemp4 = _mm_and_ps(M.r[3],g_XMAbsMask);
    // Compare to infinity
    vTemp1 = _mm_cmpeq_ps(vTemp1,g_XMInfinity);
    vTemp2 = _mm_cmpeq_ps(vTemp2,g_XMInfinity);
    vTemp3 = _mm_cmpeq_ps(vTemp3,g_XMInfinity);
    vTemp4 = _mm_cmpeq_ps(vTemp4,g_XMInfinity);
    // Or the answers together
    vTemp1 = _mm_or_ps(vTemp1,vTemp2);
    vTemp3 = _mm_or_ps(vTemp3,vTemp4);
    vTemp1 = _mm_or_ps(vTemp1,vTemp3);
    // If any are infinity, the signs are true.
    return (_mm_movemask_ps(vTemp1)!=0);
#line 157 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

// Return true if the XMMatrix is equal to identity
inline bool __vectorcall XMMatrixIsIdentity
(
    FXMMATRIX M
)
{





























#line 197 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"











#line 209 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    XMVECTOR vTemp1 = _mm_cmpeq_ps(M.r[0],g_XMIdentityR0);
    XMVECTOR vTemp2 = _mm_cmpeq_ps(M.r[1],g_XMIdentityR1);
    XMVECTOR vTemp3 = _mm_cmpeq_ps(M.r[2],g_XMIdentityR2);
    XMVECTOR vTemp4 = _mm_cmpeq_ps(M.r[3],g_XMIdentityR3);
    vTemp1 = _mm_and_ps(vTemp1,vTemp2);
    vTemp3 = _mm_and_ps(vTemp3,vTemp4);
    vTemp1 = _mm_and_ps(vTemp1,vTemp3);
    return (_mm_movemask_ps(vTemp1)==0x0f);
#line 218 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------
// Computation operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------
// Perform a 4x4 matrix multiply by a 4x4 matrix
inline XMMATRIX __vectorcall XMMatrixMultiply
(
    FXMMATRIX M1,
    CXMMATRIX M2
)
{






































#line 271 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
































#line 304 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    XMMATRIX mResult;
    // Splat the component X,Y,Z then W





#line 312 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    // Use vW to hold the original row
    XMVECTOR vW = M1.r[0];
    XMVECTOR vX = _mm_shuffle_ps( vW, vW, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    XMVECTOR vY = _mm_shuffle_ps( vW, vW, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    XMVECTOR vZ = _mm_shuffle_ps( vW, vW, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    vW = _mm_shuffle_ps( vW, vW, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
#line 319 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    // Perform the operation on the first row
    vX = _mm_mul_ps(vX,M2.r[0]);
    vY = _mm_mul_ps(vY,M2.r[1]);
    vZ = _mm_mul_ps(vZ,M2.r[2]);
    vW = _mm_mul_ps(vW,M2.r[3]);
    // Perform a binary add to reduce cumulative errors
    vX = _mm_add_ps(vX,vZ);
    vY = _mm_add_ps(vY,vW);
    vX = _mm_add_ps(vX,vY);
    mResult.r[0] = vX;
    // Repeat for the other 3 rows





#line 336 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    vW = M1.r[1];
    vX = _mm_shuffle_ps( vW, vW, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    vY = _mm_shuffle_ps( vW, vW, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    vZ = _mm_shuffle_ps( vW, vW, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    vW = _mm_shuffle_ps( vW, vW, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
#line 342 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    vX = _mm_mul_ps(vX,M2.r[0]);
    vY = _mm_mul_ps(vY,M2.r[1]);
    vZ = _mm_mul_ps(vZ,M2.r[2]);
    vW = _mm_mul_ps(vW,M2.r[3]);
    vX = _mm_add_ps(vX,vZ);
    vY = _mm_add_ps(vY,vW);
    vX = _mm_add_ps(vX,vY);
    mResult.r[1] = vX;





#line 356 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    vW = M1.r[2];
    vX = _mm_shuffle_ps( vW, vW, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    vY = _mm_shuffle_ps( vW, vW, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    vZ = _mm_shuffle_ps( vW, vW, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    vW = _mm_shuffle_ps( vW, vW, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
#line 362 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    vX = _mm_mul_ps(vX,M2.r[0]);
    vY = _mm_mul_ps(vY,M2.r[1]);
    vZ = _mm_mul_ps(vZ,M2.r[2]);
    vW = _mm_mul_ps(vW,M2.r[3]);
    vX = _mm_add_ps(vX,vZ);
    vY = _mm_add_ps(vY,vW);
    vX = _mm_add_ps(vX,vY);
    mResult.r[2] = vX;





#line 376 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    vW = M1.r[3];
    vX = _mm_shuffle_ps( vW, vW, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    vY = _mm_shuffle_ps( vW, vW, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    vZ = _mm_shuffle_ps( vW, vW, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    vW = _mm_shuffle_ps( vW, vW, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
#line 382 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    vX = _mm_mul_ps(vX,M2.r[0]);
    vY = _mm_mul_ps(vY,M2.r[1]);
    vZ = _mm_mul_ps(vZ,M2.r[2]);
    vW = _mm_mul_ps(vW,M2.r[3]);
    vX = _mm_add_ps(vX,vZ);
    vY = _mm_add_ps(vY,vW);
    vX = _mm_add_ps(vX,vY);
    mResult.r[3] = vX;
    return mResult;
#line 392 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixMultiplyTranspose
(
    FXMMATRIX M1,
    CXMMATRIX M2
)
{






































#line 441 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"












































#line 486 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    // Splat the component X,Y,Z then W





#line 493 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    // Use vW to hold the original row
    XMVECTOR vW = M1.r[0];
    XMVECTOR vX = _mm_shuffle_ps( vW, vW, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    XMVECTOR vY = _mm_shuffle_ps( vW, vW, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    XMVECTOR vZ = _mm_shuffle_ps( vW, vW, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    vW = _mm_shuffle_ps( vW, vW, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
#line 500 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    // Perform the operation on the first row
    vX = _mm_mul_ps(vX,M2.r[0]);
    vY = _mm_mul_ps(vY,M2.r[1]);
    vZ = _mm_mul_ps(vZ,M2.r[2]);
    vW = _mm_mul_ps(vW,M2.r[3]);
    // Perform a binary add to reduce cumulative errors
    vX = _mm_add_ps(vX,vZ);
    vY = _mm_add_ps(vY,vW);
    vX = _mm_add_ps(vX,vY);
    XMVECTOR r0 = vX;
    // Repeat for the other 3 rows





#line 517 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    vW = M1.r[1];
    vX = _mm_shuffle_ps( vW, vW, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    vY = _mm_shuffle_ps( vW, vW, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    vZ = _mm_shuffle_ps( vW, vW, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    vW = _mm_shuffle_ps( vW, vW, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
#line 523 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    vX = _mm_mul_ps(vX,M2.r[0]);
    vY = _mm_mul_ps(vY,M2.r[1]);
    vZ = _mm_mul_ps(vZ,M2.r[2]);
    vW = _mm_mul_ps(vW,M2.r[3]);
    vX = _mm_add_ps(vX,vZ);
    vY = _mm_add_ps(vY,vW);
    vX = _mm_add_ps(vX,vY);
    XMVECTOR r1 = vX;





#line 537 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    vW = M1.r[2];
    vX = _mm_shuffle_ps( vW, vW, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    vY = _mm_shuffle_ps( vW, vW, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    vZ = _mm_shuffle_ps( vW, vW, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    vW = _mm_shuffle_ps( vW, vW, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
#line 543 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    vX = _mm_mul_ps(vX,M2.r[0]);
    vY = _mm_mul_ps(vY,M2.r[1]);
    vZ = _mm_mul_ps(vZ,M2.r[2]);
    vW = _mm_mul_ps(vW,M2.r[3]);
    vX = _mm_add_ps(vX,vZ);
    vY = _mm_add_ps(vY,vW);
    vX = _mm_add_ps(vX,vY);
    XMVECTOR r2 = vX;





#line 557 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    vW = M1.r[3];
    vX = _mm_shuffle_ps( vW, vW, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    vY = _mm_shuffle_ps( vW, vW, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    vZ = _mm_shuffle_ps( vW, vW, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    vW = _mm_shuffle_ps( vW, vW, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
#line 563 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    vX = _mm_mul_ps(vX,M2.r[0]);
    vY = _mm_mul_ps(vY,M2.r[1]);
    vZ = _mm_mul_ps(vZ,M2.r[2]);
    vW = _mm_mul_ps(vW,M2.r[3]);
    vX = _mm_add_ps(vX,vZ);
    vY = _mm_add_ps(vY,vW);
    vX = _mm_add_ps(vX,vY);
    XMVECTOR r3 = vX;

    // x.x,x.y,y.x,y.y
    XMVECTOR vTemp1 = _mm_shuffle_ps(r0,r1,(((1) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    // x.z,x.w,y.z,y.w
    XMVECTOR vTemp3 = _mm_shuffle_ps(r0,r1,(((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));
    // z.x,z.y,w.x,w.y
    XMVECTOR vTemp2 = _mm_shuffle_ps(r2,r3,(((1) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    // z.z,z.w,w.z,w.w
    XMVECTOR vTemp4 = _mm_shuffle_ps(r2,r3,(((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));

    XMMATRIX mResult;
    // x.x,y.x,z.x,w.x
    mResult.r[0] = _mm_shuffle_ps(vTemp1, vTemp2,(((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));
    // x.y,y.y,z.y,w.y
    mResult.r[1] = _mm_shuffle_ps(vTemp1, vTemp2,(((3) << 6) | ((1) << 4) | ((3) << 2) | ((1))));
    // x.z,y.z,z.z,w.z
    mResult.r[2] = _mm_shuffle_ps(vTemp3, vTemp4,(((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));
    // x.w,y.w,z.w,w.w
    mResult.r[3] = _mm_shuffle_ps(vTemp3, vTemp4,(((3) << 6) | ((1) << 4) | ((3) << 2) | ((1))));
    return mResult;
#line 592 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixTranspose
(
    FXMMATRIX M
)
{






















#line 624 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"












#line 637 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    // x.x,x.y,y.x,y.y
    XMVECTOR vTemp1 = _mm_shuffle_ps(M.r[0],M.r[1],(((1) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    // x.z,x.w,y.z,y.w
    XMVECTOR vTemp3 = _mm_shuffle_ps(M.r[0],M.r[1],(((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));
    // z.x,z.y,w.x,w.y
    XMVECTOR vTemp2 = _mm_shuffle_ps(M.r[2],M.r[3],(((1) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    // z.z,z.w,w.z,w.w
    XMVECTOR vTemp4 = _mm_shuffle_ps(M.r[2],M.r[3],(((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));
    XMMATRIX mResult;

    // x.x,y.x,z.x,w.x
    mResult.r[0] = _mm_shuffle_ps(vTemp1, vTemp2,(((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));
    // x.y,y.y,z.y,w.y
    mResult.r[1] = _mm_shuffle_ps(vTemp1, vTemp2,(((3) << 6) | ((1) << 4) | ((3) << 2) | ((1))));
    // x.z,y.z,z.z,w.z
    mResult.r[2] = _mm_shuffle_ps(vTemp3, vTemp4,(((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));
    // x.w,y.w,z.w,w.w
    mResult.r[3] = _mm_shuffle_ps(vTemp3, vTemp4,(((3) << 6) | ((1) << 4) | ((3) << 2) | ((1))));
    return mResult;
#line 657 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------
// Return the inverse and the determinant of a 4x4 matrix

inline XMMATRIX __vectorcall XMMatrixInverse
(
    XMVECTOR* pDeterminant,
    FXMMATRIX  M
)
{





























































































#line 762 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    XMMATRIX MT = XMMatrixTranspose(M);
    XMVECTOR V00 = _mm_shuffle_ps( MT.r[2], MT.r[2], (((1) << 6) | ((1) << 4) | ((0) << 2) | ((0))) );
    XMVECTOR V10 = _mm_shuffle_ps( MT.r[3], MT.r[3], (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))) );
    XMVECTOR V01 = _mm_shuffle_ps( MT.r[0], MT.r[0], (((1) << 6) | ((1) << 4) | ((0) << 2) | ((0))) );
    XMVECTOR V11 = _mm_shuffle_ps( MT.r[1], MT.r[1], (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))) );
    XMVECTOR V02 = _mm_shuffle_ps(MT.r[2], MT.r[0],(((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));
    XMVECTOR V12 = _mm_shuffle_ps(MT.r[3], MT.r[1],(((3) << 6) | ((1) << 4) | ((3) << 2) | ((1))));

    XMVECTOR D0 = _mm_mul_ps(V00,V10);
    XMVECTOR D1 = _mm_mul_ps(V01,V11);
    XMVECTOR D2 = _mm_mul_ps(V02,V12);

    V00 = _mm_shuffle_ps( MT.r[2], MT.r[2], (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))) );
    V10 = _mm_shuffle_ps( MT.r[3], MT.r[3], (((1) << 6) | ((1) << 4) | ((0) << 2) | ((0))) );
    V01 = _mm_shuffle_ps( MT.r[0], MT.r[0], (((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))) );
    V11 = _mm_shuffle_ps( MT.r[1], MT.r[1], (((1) << 6) | ((1) << 4) | ((0) << 2) | ((0))) );
    V02 = _mm_shuffle_ps(MT.r[2],MT.r[0],(((3) << 6) | ((1) << 4) | ((3) << 2) | ((1))));
    V12 = _mm_shuffle_ps(MT.r[3],MT.r[1],(((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));

    V00 = _mm_mul_ps(V00,V10);
    V01 = _mm_mul_ps(V01,V11);
    V02 = _mm_mul_ps(V02,V12);
    D0 = _mm_sub_ps(D0,V00);
    D1 = _mm_sub_ps(D1,V01);
    D2 = _mm_sub_ps(D2,V02);
    // V11 = D0Y,D0W,D2Y,D2Y
    V11 = _mm_shuffle_ps(D0,D2,(((1) << 6) | ((1) << 4) | ((3) << 2) | ((1))));
    V00 = _mm_shuffle_ps( MT.r[1], MT.r[1], (((1) << 6) | ((0) << 4) | ((2) << 2) | ((1))) );
    V10 = _mm_shuffle_ps(V11,D0,(((0) << 6) | ((3) << 4) | ((0) << 2) | ((2))));
    V01 = _mm_shuffle_ps( MT.r[0], MT.r[0], (((0) << 6) | ((1) << 4) | ((0) << 2) | ((2))) );
    V11 = _mm_shuffle_ps(V11,D0,(((2) << 6) | ((1) << 4) | ((2) << 2) | ((1))));
    // V13 = D1Y,D1W,D2W,D2W
    XMVECTOR V13 = _mm_shuffle_ps(D1,D2,(((3) << 6) | ((3) << 4) | ((3) << 2) | ((1))));
    V02 = _mm_shuffle_ps( MT.r[3], MT.r[3], (((1) << 6) | ((0) << 4) | ((2) << 2) | ((1))) );
    V12 = _mm_shuffle_ps(V13,D1,(((0) << 6) | ((3) << 4) | ((0) << 2) | ((2))));
    XMVECTOR V03 = _mm_shuffle_ps( MT.r[2], MT.r[2], (((0) << 6) | ((1) << 4) | ((0) << 2) | ((2))) );
    V13 = _mm_shuffle_ps(V13,D1,(((2) << 6) | ((1) << 4) | ((2) << 2) | ((1))));

    XMVECTOR C0 = _mm_mul_ps(V00,V10);
    XMVECTOR C2 = _mm_mul_ps(V01,V11);
    XMVECTOR C4 = _mm_mul_ps(V02,V12);
    XMVECTOR C6 = _mm_mul_ps(V03,V13);

    // V11 = D0X,D0Y,D2X,D2X
    V11 = _mm_shuffle_ps(D0,D2,(((0) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    V00 = _mm_shuffle_ps( MT.r[1], MT.r[1], (((2) << 6) | ((1) << 4) | ((3) << 2) | ((2))) );
    V10 = _mm_shuffle_ps(D0,V11,(((2) << 6) | ((1) << 4) | ((0) << 2) | ((3))));
    V01 = _mm_shuffle_ps( MT.r[0], MT.r[0], (((1) << 6) | ((3) << 4) | ((2) << 2) | ((3))) );
    V11 = _mm_shuffle_ps(D0,V11,(((0) << 6) | ((2) << 4) | ((1) << 2) | ((2))));
    // V13 = D1X,D1Y,D2Z,D2Z
    V13 = _mm_shuffle_ps(D1,D2,(((2) << 6) | ((2) << 4) | ((1) << 2) | ((0))));
    V02 = _mm_shuffle_ps( MT.r[3], MT.r[3], (((2) << 6) | ((1) << 4) | ((3) << 2) | ((2))) );
    V12 = _mm_shuffle_ps(D1,V13,(((2) << 6) | ((1) << 4) | ((0) << 2) | ((3))));
    V03 = _mm_shuffle_ps( MT.r[2], MT.r[2], (((1) << 6) | ((3) << 4) | ((2) << 2) | ((3))) );
    V13 = _mm_shuffle_ps(D1,V13,(((0) << 6) | ((2) << 4) | ((1) << 2) | ((2))));

    V00 = _mm_mul_ps(V00,V10);
    V01 = _mm_mul_ps(V01,V11);
    V02 = _mm_mul_ps(V02,V12);
    V03 = _mm_mul_ps(V03,V13);
    C0 = _mm_sub_ps(C0,V00);
    C2 = _mm_sub_ps(C2,V01);
    C4 = _mm_sub_ps(C4,V02);
    C6 = _mm_sub_ps(C6,V03);

    V00 = _mm_shuffle_ps( MT.r[1], MT.r[1], (((0) << 6) | ((3) << 4) | ((0) << 2) | ((3))) );
    // V10 = D0Z,D0Z,D2X,D2Y
    V10 = _mm_shuffle_ps(D0,D2,(((1) << 6) | ((0) << 4) | ((2) << 2) | ((2))));
    V10 = _mm_shuffle_ps( V10, V10, (((0) << 6) | ((2) << 4) | ((3) << 2) | ((0))) );
    V01 = _mm_shuffle_ps( MT.r[0], MT.r[0], (((2) << 6) | ((0) << 4) | ((3) << 2) | ((1))) );
    // V11 = D0X,D0W,D2X,D2Y
    V11 = _mm_shuffle_ps(D0,D2,(((1) << 6) | ((0) << 4) | ((3) << 2) | ((0))));
    V11 = _mm_shuffle_ps( V11, V11, (((2) << 6) | ((1) << 4) | ((0) << 2) | ((3))) );
    V02 = _mm_shuffle_ps( MT.r[3], MT.r[3], (((0) << 6) | ((3) << 4) | ((0) << 2) | ((3))) );
    // V12 = D1Z,D1Z,D2Z,D2W
    V12 = _mm_shuffle_ps(D1,D2,(((3) << 6) | ((2) << 4) | ((2) << 2) | ((2))));
    V12 = _mm_shuffle_ps( V12, V12, (((0) << 6) | ((2) << 4) | ((3) << 2) | ((0))) );
    V03 = _mm_shuffle_ps( MT.r[2], MT.r[2], (((2) << 6) | ((0) << 4) | ((3) << 2) | ((1))) );
    // V13 = D1X,D1W,D2Z,D2W
    V13 = _mm_shuffle_ps(D1,D2,(((3) << 6) | ((2) << 4) | ((3) << 2) | ((0))));
    V13 = _mm_shuffle_ps( V13, V13, (((2) << 6) | ((1) << 4) | ((0) << 2) | ((3))) );

    V00 = _mm_mul_ps(V00,V10);
    V01 = _mm_mul_ps(V01,V11);
    V02 = _mm_mul_ps(V02,V12);
    V03 = _mm_mul_ps(V03,V13);
    XMVECTOR C1 = _mm_sub_ps(C0,V00);
    C0 = _mm_add_ps(C0,V00);
    XMVECTOR C3 = _mm_add_ps(C2,V01);
    C2 = _mm_sub_ps(C2,V01);
    XMVECTOR C5 = _mm_sub_ps(C4,V02);
    C4 = _mm_add_ps(C4,V02);
    XMVECTOR C7 = _mm_add_ps(C6,V03);
    C6 = _mm_sub_ps(C6,V03);

    C0 = _mm_shuffle_ps(C0,C1,(((3) << 6) | ((1) << 4) | ((2) << 2) | ((0))));
    C2 = _mm_shuffle_ps(C2,C3,(((3) << 6) | ((1) << 4) | ((2) << 2) | ((0))));
    C4 = _mm_shuffle_ps(C4,C5,(((3) << 6) | ((1) << 4) | ((2) << 2) | ((0))));
    C6 = _mm_shuffle_ps(C6,C7,(((3) << 6) | ((1) << 4) | ((2) << 2) | ((0))));
    C0 = _mm_shuffle_ps( C0, C0, (((3) << 6) | ((1) << 4) | ((2) << 2) | ((0))) );
    C2 = _mm_shuffle_ps( C2, C2, (((3) << 6) | ((1) << 4) | ((2) << 2) | ((0))) );
    C4 = _mm_shuffle_ps( C4, C4, (((3) << 6) | ((1) << 4) | ((2) << 2) | ((0))) );
    C6 = _mm_shuffle_ps( C6, C6, (((3) << 6) | ((1) << 4) | ((2) << 2) | ((0))) );
    // Get the determinate
    XMVECTOR vTemp = XMVector4Dot(C0,MT.r[0]);
    if (pDeterminant != nullptr)
        *pDeterminant = vTemp;
    vTemp = _mm_div_ps(g_XMOne,vTemp);
    XMMATRIX mResult;
    mResult.r[0] = _mm_mul_ps(C0,vTemp);
    mResult.r[1] = _mm_mul_ps(C2,vTemp);
    mResult.r[2] = _mm_mul_ps(C4,vTemp);
    mResult.r[3] = _mm_mul_ps(C6,vTemp);
    return mResult;
#line 877 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMMatrixDeterminant
(
    FXMMATRIX M
)
{
    static const XMVECTORF32 Sign = { { { 1.0f, -1.0f, 1.0f, -1.0f } } };

    XMVECTOR V0 = XMVectorSwizzle<XM_SWIZZLE_Y, XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_X>(M.r[2]);
    XMVECTOR V1 = XMVectorSwizzle<XM_SWIZZLE_Z, XM_SWIZZLE_Z, XM_SWIZZLE_Y, XM_SWIZZLE_Y>(M.r[3]);
    XMVECTOR V2 = XMVectorSwizzle<XM_SWIZZLE_Y, XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_X>(M.r[2]);
    XMVECTOR V3 = XMVectorSwizzle<XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_Z>(M.r[3]);
    XMVECTOR V4 = XMVectorSwizzle<XM_SWIZZLE_Z, XM_SWIZZLE_Z, XM_SWIZZLE_Y, XM_SWIZZLE_Y>(M.r[2]);
    XMVECTOR V5 = XMVectorSwizzle<XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_Z>(M.r[3]);

    XMVECTOR P0 = XMVectorMultiply(V0, V1);
    XMVECTOR P1 = XMVectorMultiply(V2, V3);
    XMVECTOR P2 = XMVectorMultiply(V4, V5);

    V0 = XMVectorSwizzle<XM_SWIZZLE_Z, XM_SWIZZLE_Z, XM_SWIZZLE_Y, XM_SWIZZLE_Y>(M.r[2]);
    V1 = XMVectorSwizzle<XM_SWIZZLE_Y, XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_X>(M.r[3]);
    V2 = XMVectorSwizzle<XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_Z>(M.r[2]);
    V3 = XMVectorSwizzle<XM_SWIZZLE_Y, XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_X>(M.r[3]);
    V4 = XMVectorSwizzle<XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_Z>(M.r[2]);
    V5 = XMVectorSwizzle<XM_SWIZZLE_Z, XM_SWIZZLE_Z, XM_SWIZZLE_Y, XM_SWIZZLE_Y>(M.r[3]);

    P0 = XMVectorNegativeMultiplySubtract(V0, V1, P0);
    P1 = XMVectorNegativeMultiplySubtract(V2, V3, P1);
    P2 = XMVectorNegativeMultiplySubtract(V4, V5, P2);

    V0 = XMVectorSwizzle<XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_W, XM_SWIZZLE_Z>(M.r[1]);
    V1 = XMVectorSwizzle<XM_SWIZZLE_Z, XM_SWIZZLE_Z, XM_SWIZZLE_Y, XM_SWIZZLE_Y>(M.r[1]);
    V2 = XMVectorSwizzle<XM_SWIZZLE_Y, XM_SWIZZLE_X, XM_SWIZZLE_X, XM_SWIZZLE_X>(M.r[1]);

    XMVECTOR S = XMVectorMultiply(M.r[0], Sign.v);
    XMVECTOR R = XMVectorMultiply(V0, P0);
    R = XMVectorNegativeMultiplySubtract(V1, P1, R);
    R = XMVectorMultiplyAdd(V2, P2, R);

    return XMVector4Dot(S, R);
}






















































inline bool __vectorcall XMMatrixDecompose
(
    XMVECTOR *outScale,
    XMVECTOR *outRotQuat,
    XMVECTOR *outTrans,
    FXMMATRIX M
)
{
    static const XMVECTOR *pvCanonicalBasis[3] = {
        &g_XMIdentityR0.v,
        &g_XMIdentityR1.v,
        &g_XMIdentityR2.v
    };

    (void)( (!!(outScale != nullptr)) || (_wassert(L"outScale != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(989)), 0) );
    (void)( (!!(outRotQuat != nullptr)) || (_wassert(L"outRotQuat != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(990)), 0) );
    (void)( (!!(outTrans != nullptr)) || (_wassert(L"outTrans != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(991)), 0) );

    // Get the translation
    outTrans[0] = M.r[3];

    XMVECTOR *ppvBasis[3];
    XMMATRIX matTemp;
    ppvBasis[0] = &matTemp.r[0];
    ppvBasis[1] = &matTemp.r[1];
    ppvBasis[2] = &matTemp.r[2];

    matTemp.r[0] = M.r[0];
    matTemp.r[1] = M.r[1];
    matTemp.r[2] = M.r[2];
    matTemp.r[3] = g_XMIdentityR3.v;

    auto pfScales = reinterpret_cast<float *>(outScale);

    size_t a, b, c;
    XMVectorGetXPtr(&pfScales[0],XMVector3Length(ppvBasis[0][0]));
    XMVectorGetXPtr(&pfScales[1],XMVector3Length(ppvBasis[1][0]));
    XMVectorGetXPtr(&pfScales[2],XMVector3Length(ppvBasis[2][0]));
    pfScales[3] = 0.f;

    if((pfScales[0]) < (pfScales[1])) { if((pfScales[1]) < (pfScales[2])) { (a) = 2; (b) = 1; (c) = 0; } else { (a) = 1; if((pfScales[0]) < (pfScales[2])) { (b) = 2; (c) = 0; } else { (b) = 0; (c) = 2; } } } else { if((pfScales[0]) < (pfScales[2])) { (a) = 2; (b) = 0; (c) = 1; } else { (a) = 0; if((pfScales[1]) < (pfScales[2])) { (b) = 2; (c) = 1; } else { (b) = 1; (c) = 2; } } }

    if(pfScales[a] < 0.0001f)
    {
        ppvBasis[a][0] = pvCanonicalBasis[a][0];
    }
    ppvBasis[a][0] = XMVector3Normalize(ppvBasis[a][0]);

    if(pfScales[b] < 0.0001f)
    {
        size_t aa, bb, cc;
        float fAbsX, fAbsY, fAbsZ;

        fAbsX = fabsf(XMVectorGetX(ppvBasis[a][0]));
        fAbsY = fabsf(XMVectorGetY(ppvBasis[a][0]));
        fAbsZ = fabsf(XMVectorGetZ(ppvBasis[a][0]));

        if((fAbsX) < (fAbsY)) { if((fAbsY) < (fAbsZ)) { (aa) = 2; (bb) = 1; (cc) = 0; } else { (aa) = 1; if((fAbsX) < (fAbsZ)) { (bb) = 2; (cc) = 0; } else { (bb) = 0; (cc) = 2; } } } else { if((fAbsX) < (fAbsZ)) { (aa) = 2; (bb) = 0; (cc) = 1; } else { (aa) = 0; if((fAbsY) < (fAbsZ)) { (bb) = 2; (cc) = 1; } else { (bb) = 1; (cc) = 2; } } }

        ppvBasis[b][0] = XMVector3Cross(ppvBasis[a][0],pvCanonicalBasis[cc][0]);
    }

    ppvBasis[b][0] = XMVector3Normalize(ppvBasis[b][0]);

    if(pfScales[c] < 0.0001f)
    {
        ppvBasis[c][0] = XMVector3Cross(ppvBasis[a][0],ppvBasis[b][0]);
    }

    ppvBasis[c][0] = XMVector3Normalize(ppvBasis[c][0]);

    float fDet = XMVectorGetX(XMMatrixDeterminant(matTemp));

    // use Kramer's rule to check for handedness of coordinate system
    if(fDet < 0.0f)
    {
        // switch coordinate system by negating the scale and inverting the basis vector on the x-axis
        pfScales[a] = -pfScales[a];
        ppvBasis[a][0] = XMVectorNegate(ppvBasis[a][0]);

        fDet = -fDet;
    }

    fDet -= 1.0f;
    fDet *= fDet;

    if(0.0001f < fDet)
    {
        // Non-SRT matrix encountered
        return false;
    }

    // generate the quaternion from the matrix
    outRotQuat[0] = XMQuaternionRotationMatrix(matTemp);
    return true;
}




//------------------------------------------------------------------------------
// Transformation operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixIdentity()
{
    XMMATRIX M;
    M.r[0] = g_XMIdentityR0.v;
    M.r[1] = g_XMIdentityR1.v;
    M.r[2] = g_XMIdentityR2.v;
    M.r[3] = g_XMIdentityR3.v;
    return M;
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixSet
(
    float m00, float m01, float m02, float m03,
    float m10, float m11, float m12, float m13,
    float m20, float m21, float m22, float m23,
    float m30, float m31, float m32, float m33
)
{
    XMMATRIX M;





#line 1108 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    M.r[0] = XMVectorSet(m00, m01, m02, m03);
    M.r[1] = XMVectorSet(m10, m11, m12, m13);
    M.r[2] = XMVectorSet(m20, m21, m22, m23);
    M.r[3] = XMVectorSet(m30, m31, m32, m33);
#line 1113 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    return M;
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixTranslation
(
    float OffsetX,
    float OffsetY,
    float OffsetZ
)
{
























#line 1150 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    XMMATRIX M;
    M.r[0] = g_XMIdentityR0.v;
    M.r[1] = g_XMIdentityR1.v;
    M.r[2] = g_XMIdentityR2.v;
    M.r[3] = XMVectorSet(OffsetX, OffsetY, OffsetZ, 1.f );
    return M;
#line 1157 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}


//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixTranslationFromVector
(
    FXMVECTOR Offset
)
{
























#line 1192 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    XMMATRIX M;
    M.r[0] = g_XMIdentityR0.v;
    M.r[1] = g_XMIdentityR1.v;
    M.r[2] = g_XMIdentityR2.v;
    M.r[3] = XMVectorSelect( g_XMIdentityR3.v, Offset, g_XMSelect1110.v );
    return M;
#line 1199 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixScaling
(
    float ScaleX,
    float ScaleY,
    float ScaleZ
)
{
























#line 1235 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"







#line 1243 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    XMMATRIX M;
    M.r[0] = _mm_set_ps( 0, 0, 0, ScaleX );
    M.r[1] = _mm_set_ps( 0, 0, ScaleY, 0 );
    M.r[2] = _mm_set_ps( 0, ScaleZ, 0, 0 );
    M.r[3] = g_XMIdentityR3.v;
    return M;
#line 1250 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixScalingFromVector
(
    FXMVECTOR Scale
)
{
























#line 1284 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"






#line 1291 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    XMMATRIX M;
    M.r[0] = _mm_and_ps(Scale,g_XMMaskX);
    M.r[1] = _mm_and_ps(Scale,g_XMMaskY);
    M.r[2] = _mm_and_ps(Scale,g_XMMaskZ);
    M.r[3] = g_XMIdentityR3.v;
    return M;
#line 1298 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixRotationX
(
    float Angle
)
{




























#line 1336 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"


















#line 1355 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    float    SinAngle;
    float    CosAngle;
    XMScalarSinCos(&SinAngle, &CosAngle, Angle);

    XMVECTOR vSin = _mm_set_ss(SinAngle);
    XMVECTOR vCos = _mm_set_ss(CosAngle);
    // x = 0,y = cos,z = sin, w = 0
    vCos = _mm_shuffle_ps(vCos,vSin,(((3) << 6) | ((0) << 4) | ((0) << 2) | ((3))));
    XMMATRIX M;
    M.r[0] = g_XMIdentityR0;
    M.r[1] = vCos;
    // x = 0,y = sin,z = cos, w = 0
    vCos = _mm_shuffle_ps( vCos, vCos, (((3) << 6) | ((1) << 4) | ((2) << 2) | ((0))) );
    // x = 0,y = -sin,z = cos, w = 0
    vCos = _mm_mul_ps(vCos,g_XMNegateY);
    M.r[2] = vCos;
    M.r[3] = g_XMIdentityR3;
    return M;
#line 1374 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixRotationY
(
    float Angle
)
{




























#line 1412 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"


















#line 1431 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    float    SinAngle;
    float    CosAngle;
    XMScalarSinCos(&SinAngle, &CosAngle, Angle);

    XMVECTOR vSin = _mm_set_ss(SinAngle);
    XMVECTOR vCos = _mm_set_ss(CosAngle);
    // x = sin,y = 0,z = cos, w = 0
    vSin = _mm_shuffle_ps(vSin,vCos,(((3) << 6) | ((0) << 4) | ((3) << 2) | ((0))));
    XMMATRIX M;
    M.r[2] = vSin;
    M.r[1] = g_XMIdentityR1;
    // x = cos,y = 0,z = sin, w = 0
    vSin = _mm_shuffle_ps( vSin, vSin, (((3) << 6) | ((0) << 4) | ((1) << 2) | ((2))) );
    // x = cos,y = 0,z = -sin, w = 0
    vSin = _mm_mul_ps(vSin,g_XMNegateZ);
    M.r[0] = vSin;
    M.r[3] = g_XMIdentityR3;
    return M;
#line 1450 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixRotationZ
(
    float Angle
)
{




























#line 1488 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"


















#line 1507 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    float    SinAngle;
    float    CosAngle;
    XMScalarSinCos(&SinAngle, &CosAngle, Angle);

    XMVECTOR vSin = _mm_set_ss(SinAngle);
    XMVECTOR vCos = _mm_set_ss(CosAngle);
    // x = cos,y = sin,z = 0, w = 0
    vCos = _mm_unpacklo_ps(vCos,vSin);
    XMMATRIX M;
    M.r[0] = vCos;
    // x = sin,y = cos,z = 0, w = 0
    vCos = _mm_shuffle_ps( vCos, vCos, (((3) << 6) | ((2) << 4) | ((0) << 2) | ((1))) );
    // x = cos,y = -sin,z = 0, w = 0
    vCos = _mm_mul_ps(vCos,g_XMNegateX);
    M.r[1] = vCos;
    M.r[2] = g_XMIdentityR2;
    M.r[3] = g_XMIdentityR3;
    return M;
#line 1526 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixRotationRollPitchYaw
(
    float Pitch,
    float Yaw,
    float Roll
)
{
    XMVECTOR Angles = XMVectorSet(Pitch, Yaw, Roll, 0.0f);
    return XMMatrixRotationRollPitchYawFromVector(Angles);
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixRotationRollPitchYawFromVector
(
    FXMVECTOR Angles // <Pitch, Yaw, Roll, undefined>
)
{
    XMVECTOR Q = XMQuaternionRotationRollPitchYawFromVector(Angles);
    return XMMatrixRotationQuaternion(Q);
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixRotationNormal
(
    FXMVECTOR NormalAxis,
    float     Angle
)
{



































#line 1596 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    float    fSinAngle;
    float    fCosAngle;
    XMScalarSinCos(&fSinAngle, &fCosAngle, Angle);

    XMVECTOR C2 = _mm_set_ps1(1.0f - fCosAngle);
    XMVECTOR C1 = _mm_set_ps1(fCosAngle);
    XMVECTOR C0 = _mm_set_ps1(fSinAngle);

    XMVECTOR N0 = _mm_shuffle_ps( NormalAxis, NormalAxis, (((3) << 6) | ((0) << 4) | ((2) << 2) | ((1))) );
    XMVECTOR N1 = _mm_shuffle_ps( NormalAxis, NormalAxis, (((3) << 6) | ((1) << 4) | ((0) << 2) | ((2))) );

    XMVECTOR V0 = _mm_mul_ps(C2, N0);
    V0 = _mm_mul_ps(V0, N1);

    XMVECTOR R0 = _mm_mul_ps(C2, NormalAxis);
    R0 = _mm_mul_ps(R0, NormalAxis);
    R0 = _mm_add_ps(R0, C1);

    XMVECTOR R1 = _mm_mul_ps(C0, NormalAxis);
    R1 = _mm_add_ps(R1, V0);
    XMVECTOR R2 = _mm_mul_ps(C0, NormalAxis);
    R2 = _mm_sub_ps(V0,R2);

    V0 = _mm_and_ps(R0,g_XMMask3);
    XMVECTOR V1 = _mm_shuffle_ps(R1,R2,(((2) << 6) | ((1) << 4) | ((2) << 2) | ((0))));
    V1 = _mm_shuffle_ps( V1, V1, (((0) << 6) | ((3) << 4) | ((2) << 2) | ((1))) );
    XMVECTOR V2 = _mm_shuffle_ps(R1,R2,(((0) << 6) | ((0) << 4) | ((1) << 2) | ((1))));
    V2 = _mm_shuffle_ps( V2, V2, (((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))) );

    R2 = _mm_shuffle_ps(V0,V1,(((1) << 6) | ((0) << 4) | ((3) << 2) | ((0))));
    R2 = _mm_shuffle_ps( R2, R2, (((1) << 6) | ((3) << 4) | ((2) << 2) | ((0))) );

    XMMATRIX M;
    M.r[0] = R2;

    R2 = _mm_shuffle_ps(V0,V1,(((3) << 6) | ((2) << 4) | ((3) << 2) | ((1))));
    R2 = _mm_shuffle_ps( R2, R2, (((1) << 6) | ((3) << 4) | ((0) << 2) | ((2))) );
    M.r[1] = R2;

    V2 = _mm_shuffle_ps(V2,V0,(((3) << 6) | ((2) << 4) | ((1) << 2) | ((0))));
    M.r[2] = V2;
    M.r[3] = g_XMIdentityR3.v;
    return M;
#line 1640 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixRotationAxis
(
    FXMVECTOR Axis,
    float     Angle
)
{
    (void)( (!!(!XMVector3Equal(Axis, XMVectorZero()))) || (_wassert(L"!XMVector3Equal(Axis, XMVectorZero())", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(1650)), 0) );
    (void)( (!!(!XMVector3IsInfinite(Axis))) || (_wassert(L"!XMVector3IsInfinite(Axis)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(1651)), 0) );

    XMVECTOR Normal = XMVector3Normalize(Axis);
    return XMMatrixRotationNormal(Normal, Angle);
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixRotationQuaternion
(
    FXMVECTOR Quaternion
)
{

































#line 1698 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    static const XMVECTORF32  Constant1110 = { { { 1.0f, 1.0f, 1.0f, 0.0f } } };

    XMVECTOR Q0 = _mm_add_ps(Quaternion,Quaternion);
    XMVECTOR Q1 = _mm_mul_ps(Quaternion,Q0);

    XMVECTOR V0 = _mm_shuffle_ps( Q1, Q1, (((3) << 6) | ((0) << 4) | ((0) << 2) | ((1))) );
    V0 = _mm_and_ps(V0,g_XMMask3);
    XMVECTOR V1 = _mm_shuffle_ps( Q1, Q1, (((3) << 6) | ((1) << 4) | ((2) << 2) | ((2))) );
    V1 = _mm_and_ps(V1,g_XMMask3);
    XMVECTOR R0 = _mm_sub_ps(Constant1110,V0);
    R0 = _mm_sub_ps(R0, V1);

    V0 = _mm_shuffle_ps( Quaternion, Quaternion, (((3) << 6) | ((1) << 4) | ((0) << 2) | ((0))) );
    V1 = _mm_shuffle_ps( Q0, Q0, (((3) << 6) | ((2) << 4) | ((1) << 2) | ((2))) );
    V0 = _mm_mul_ps(V0, V1);

    V1 = _mm_shuffle_ps( Quaternion, Quaternion, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    XMVECTOR V2 = _mm_shuffle_ps( Q0, Q0, (((3) << 6) | ((0) << 4) | ((2) << 2) | ((1))) );
    V1 = _mm_mul_ps(V1, V2);

    XMVECTOR R1 = _mm_add_ps(V0, V1);
    XMVECTOR R2 = _mm_sub_ps(V0, V1);

    V0 = _mm_shuffle_ps(R1,R2,(((1) << 6) | ((0) << 4) | ((2) << 2) | ((1))));
    V0 = _mm_shuffle_ps( V0, V0, (((1) << 6) | ((3) << 4) | ((2) << 2) | ((0))) );
    V1 = _mm_shuffle_ps(R1,R2,(((2) << 6) | ((2) << 4) | ((0) << 2) | ((0))));
    V1 = _mm_shuffle_ps( V1, V1, (((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))) );

    Q1 = _mm_shuffle_ps(R0,V0,(((1) << 6) | ((0) << 4) | ((3) << 2) | ((0))));
    Q1 = _mm_shuffle_ps( Q1, Q1, (((1) << 6) | ((3) << 4) | ((2) << 2) | ((0))) );

    XMMATRIX M;
    M.r[0] = Q1;

    Q1 = _mm_shuffle_ps(R0,V0,(((3) << 6) | ((2) << 4) | ((3) << 2) | ((1))));
    Q1 = _mm_shuffle_ps( Q1, Q1, (((1) << 6) | ((3) << 4) | ((0) << 2) | ((2))) );
    M.r[1] = Q1;

    Q1 = _mm_shuffle_ps(V1,R0,(((3) << 6) | ((2) << 4) | ((1) << 2) | ((0))));
    M.r[2] = Q1;
    M.r[3] = g_XMIdentityR3;
    return M;
#line 1741 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixTransformation2D
(
    FXMVECTOR ScalingOrigin,
    float     ScalingOrientation,
    FXMVECTOR Scaling,
    FXMVECTOR RotationOrigin,
    float     Rotation,
    GXMVECTOR Translation
)
{
    // M = Inverse(MScalingOrigin) * Transpose(MScalingOrientation) * MScaling * MScalingOrientation *
    //         MScalingOrigin * Inverse(MRotationOrigin) * MRotation * MRotationOrigin * MTranslation;

    XMVECTOR VScalingOrigin       = XMVectorSelect(g_XMSelect1100.v, ScalingOrigin, g_XMSelect1100.v);
    XMVECTOR NegScalingOrigin     = XMVectorNegate(VScalingOrigin);

    XMMATRIX MScalingOriginI      = XMMatrixTranslationFromVector(NegScalingOrigin);
    XMMATRIX MScalingOrientation  = XMMatrixRotationZ(ScalingOrientation);
    XMMATRIX MScalingOrientationT = XMMatrixTranspose(MScalingOrientation);
    XMVECTOR VScaling             = XMVectorSelect(g_XMOne.v, Scaling, g_XMSelect1100.v);
    XMMATRIX MScaling             = XMMatrixScalingFromVector(VScaling);
    XMVECTOR VRotationOrigin      = XMVectorSelect(g_XMSelect1100.v, RotationOrigin, g_XMSelect1100.v);
    XMMATRIX MRotation            = XMMatrixRotationZ(Rotation);
    XMVECTOR VTranslation         = XMVectorSelect(g_XMSelect1100.v, Translation,g_XMSelect1100.v);

    XMMATRIX M = XMMatrixMultiply(MScalingOriginI, MScalingOrientationT);
    M      = XMMatrixMultiply(M, MScaling);
    M      = XMMatrixMultiply(M, MScalingOrientation);
    M.r[3] = XMVectorAdd(M.r[3], VScalingOrigin);
    M.r[3] = XMVectorSubtract(M.r[3], VRotationOrigin);
    M      = XMMatrixMultiply(M, MRotation);
    M.r[3] = XMVectorAdd(M.r[3], VRotationOrigin);
    M.r[3] = XMVectorAdd(M.r[3], VTranslation);

    return M;
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixTransformation
(
    FXMVECTOR ScalingOrigin,
    FXMVECTOR ScalingOrientationQuaternion,
    FXMVECTOR Scaling,
    GXMVECTOR RotationOrigin,
    HXMVECTOR RotationQuaternion,
    HXMVECTOR Translation
)
{
    // M = Inverse(MScalingOrigin) * Transpose(MScalingOrientation) * MScaling * MScalingOrientation *
    //         MScalingOrigin * Inverse(MRotationOrigin) * MRotation * MRotationOrigin * MTranslation;

    XMVECTOR VScalingOrigin       = XMVectorSelect(g_XMSelect1110.v, ScalingOrigin, g_XMSelect1110.v);
    XMVECTOR NegScalingOrigin     = XMVectorNegate(ScalingOrigin);

    XMMATRIX MScalingOriginI      = XMMatrixTranslationFromVector(NegScalingOrigin);
    XMMATRIX MScalingOrientation  = XMMatrixRotationQuaternion(ScalingOrientationQuaternion);
    XMMATRIX MScalingOrientationT = XMMatrixTranspose(MScalingOrientation);
    XMMATRIX MScaling             = XMMatrixScalingFromVector(Scaling);
    XMVECTOR VRotationOrigin      = XMVectorSelect(g_XMSelect1110.v, RotationOrigin, g_XMSelect1110.v);
    XMMATRIX MRotation            = XMMatrixRotationQuaternion(RotationQuaternion);
    XMVECTOR VTranslation         = XMVectorSelect(g_XMSelect1110.v, Translation, g_XMSelect1110.v);

    XMMATRIX M;
    M      = XMMatrixMultiply(MScalingOriginI, MScalingOrientationT);
    M      = XMMatrixMultiply(M, MScaling);
    M      = XMMatrixMultiply(M, MScalingOrientation);
    M.r[3] = XMVectorAdd(M.r[3], VScalingOrigin);
    M.r[3] = XMVectorSubtract(M.r[3], VRotationOrigin);
    M      = XMMatrixMultiply(M, MRotation);
    M.r[3] = XMVectorAdd(M.r[3], VRotationOrigin);
    M.r[3] = XMVectorAdd(M.r[3], VTranslation);
    return M;
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixAffineTransformation2D
(
    FXMVECTOR Scaling,
    FXMVECTOR RotationOrigin,
    float     Rotation,
    FXMVECTOR Translation
)
{
    // M = MScaling * Inverse(MRotationOrigin) * MRotation * MRotationOrigin * MTranslation;

    XMVECTOR VScaling        = XMVectorSelect(g_XMOne.v, Scaling, g_XMSelect1100.v);
    XMMATRIX MScaling        = XMMatrixScalingFromVector(VScaling);
    XMVECTOR VRotationOrigin = XMVectorSelect(g_XMSelect1100.v, RotationOrigin, g_XMSelect1100.v);
    XMMATRIX MRotation       = XMMatrixRotationZ(Rotation);
    XMVECTOR VTranslation    = XMVectorSelect(g_XMSelect1100.v, Translation,g_XMSelect1100.v);

    XMMATRIX M;
    M      = MScaling;
    M.r[3] = XMVectorSubtract(M.r[3], VRotationOrigin);
    M      = XMMatrixMultiply(M, MRotation);
    M.r[3] = XMVectorAdd(M.r[3], VRotationOrigin);
    M.r[3] = XMVectorAdd(M.r[3], VTranslation);
    return M;
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixAffineTransformation
(
    FXMVECTOR Scaling,
    FXMVECTOR RotationOrigin,
    FXMVECTOR RotationQuaternion,
    GXMVECTOR Translation
)
{
    // M = MScaling * Inverse(MRotationOrigin) * MRotation * MRotationOrigin * MTranslation;

    XMMATRIX MScaling        = XMMatrixScalingFromVector(Scaling);
    XMVECTOR VRotationOrigin = XMVectorSelect(g_XMSelect1110.v, RotationOrigin,g_XMSelect1110.v);
    XMMATRIX MRotation       = XMMatrixRotationQuaternion(RotationQuaternion);
    XMVECTOR VTranslation    = XMVectorSelect(g_XMSelect1110.v, Translation,g_XMSelect1110.v);

    XMMATRIX M;
    M      = MScaling;
    M.r[3] = XMVectorSubtract(M.r[3], VRotationOrigin);
    M      = XMMatrixMultiply(M, MRotation);
    M.r[3] = XMVectorAdd(M.r[3], VRotationOrigin);
    M.r[3] = XMVectorAdd(M.r[3], VTranslation);
    return M;
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixReflect
(
    FXMVECTOR ReflectionPlane
)
{
    (void)( (!!(!XMVector3Equal(ReflectionPlane, XMVectorZero()))) || (_wassert(L"!XMVector3Equal(ReflectionPlane, XMVectorZero())", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(1880)), 0) );
    (void)( (!!(!XMPlaneIsInfinite(ReflectionPlane))) || (_wassert(L"!XMPlaneIsInfinite(ReflectionPlane)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(1881)), 0) );

    static const XMVECTORF32 NegativeTwo = { { { -2.0f, -2.0f, -2.0f, 0.0f } } };

    XMVECTOR P = XMPlaneNormalize(ReflectionPlane);
    XMVECTOR S = XMVectorMultiply(P, NegativeTwo);

    XMVECTOR A = XMVectorSplatX(P);
    XMVECTOR B = XMVectorSplatY(P);
    XMVECTOR C = XMVectorSplatZ(P);
    XMVECTOR D = XMVectorSplatW(P);

    XMMATRIX M;
    M.r[0] = XMVectorMultiplyAdd(A, S, g_XMIdentityR0.v);
    M.r[1] = XMVectorMultiplyAdd(B, S, g_XMIdentityR1.v);
    M.r[2] = XMVectorMultiplyAdd(C, S, g_XMIdentityR2.v);
    M.r[3] = XMVectorMultiplyAdd(D, S, g_XMIdentityR3.v);
    return M;
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixShadow
(
    FXMVECTOR ShadowPlane,
    FXMVECTOR LightPosition
)
{
    static const XMVECTORU32 Select0001 = { { { XM_SELECT_0, XM_SELECT_0, XM_SELECT_0, XM_SELECT_1 } } };

    (void)( (!!(!XMVector3Equal(ShadowPlane, XMVectorZero()))) || (_wassert(L"!XMVector3Equal(ShadowPlane, XMVectorZero())", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(1911)), 0) );
    (void)( (!!(!XMPlaneIsInfinite(ShadowPlane))) || (_wassert(L"!XMPlaneIsInfinite(ShadowPlane)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(1912)), 0) );

    XMVECTOR P = XMPlaneNormalize(ShadowPlane);
    XMVECTOR Dot = XMPlaneDot(P, LightPosition);
    P = XMVectorNegate(P);
    XMVECTOR D = XMVectorSplatW(P);
    XMVECTOR C = XMVectorSplatZ(P);
    XMVECTOR B = XMVectorSplatY(P);
    XMVECTOR A = XMVectorSplatX(P);
    Dot = XMVectorSelect(Select0001.v, Dot, Select0001.v);

    XMMATRIX M;
    M.r[3] = XMVectorMultiplyAdd(D, LightPosition, Dot);
    Dot = XMVectorRotateLeft(Dot, 1);
    M.r[2] = XMVectorMultiplyAdd(C, LightPosition, Dot);
    Dot = XMVectorRotateLeft(Dot, 1);
    M.r[1] = XMVectorMultiplyAdd(B, LightPosition, Dot);
    Dot = XMVectorRotateLeft(Dot, 1);
    M.r[0] = XMVectorMultiplyAdd(A, LightPosition, Dot);
    return M;
}

//------------------------------------------------------------------------------
// View and projection initialization operations
//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixLookAtLH
(
    FXMVECTOR EyePosition,
    FXMVECTOR FocusPosition,
    FXMVECTOR UpDirection
)
{
    XMVECTOR EyeDirection = XMVectorSubtract(FocusPosition, EyePosition);
    return XMMatrixLookToLH(EyePosition, EyeDirection, UpDirection);
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixLookAtRH
(
    FXMVECTOR EyePosition,
    FXMVECTOR FocusPosition,
    FXMVECTOR UpDirection
)
{
    XMVECTOR NegEyeDirection = XMVectorSubtract(EyePosition, FocusPosition);
    return XMMatrixLookToLH(EyePosition, NegEyeDirection, UpDirection);
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixLookToLH
(
    FXMVECTOR EyePosition,
    FXMVECTOR EyeDirection,
    FXMVECTOR UpDirection
)
{
    (void)( (!!(!XMVector3Equal(EyeDirection, XMVectorZero()))) || (_wassert(L"!XMVector3Equal(EyeDirection, XMVectorZero())", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(1971)), 0) );
    (void)( (!!(!XMVector3IsInfinite(EyeDirection))) || (_wassert(L"!XMVector3IsInfinite(EyeDirection)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(1972)), 0) );
    (void)( (!!(!XMVector3Equal(UpDirection, XMVectorZero()))) || (_wassert(L"!XMVector3Equal(UpDirection, XMVectorZero())", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(1973)), 0) );
    (void)( (!!(!XMVector3IsInfinite(UpDirection))) || (_wassert(L"!XMVector3IsInfinite(UpDirection)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(1974)), 0) );

    XMVECTOR R2 = XMVector3Normalize(EyeDirection);

    XMVECTOR R0 = XMVector3Cross(UpDirection, R2);
    R0 = XMVector3Normalize(R0);

    XMVECTOR R1 = XMVector3Cross(R2, R0);

    XMVECTOR NegEyePosition = XMVectorNegate(EyePosition);

    XMVECTOR D0 = XMVector3Dot(R0, NegEyePosition);
    XMVECTOR D1 = XMVector3Dot(R1, NegEyePosition);
    XMVECTOR D2 = XMVector3Dot(R2, NegEyePosition);

    XMMATRIX M;
    M.r[0] = XMVectorSelect(D0, R0, g_XMSelect1110.v);
    M.r[1] = XMVectorSelect(D1, R1, g_XMSelect1110.v);
    M.r[2] = XMVectorSelect(D2, R2, g_XMSelect1110.v);
    M.r[3] = g_XMIdentityR3.v;

    M = XMMatrixTranspose(M);

    return M;
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixLookToRH
(
    FXMVECTOR EyePosition,
    FXMVECTOR EyeDirection,
    FXMVECTOR UpDirection
)
{
    XMVECTOR NegEyeDirection = XMVectorNegate(EyeDirection);
    return XMMatrixLookToLH(EyePosition, NegEyeDirection, UpDirection);
}

//------------------------------------------------------------------------------






inline XMMATRIX __vectorcall XMMatrixPerspectiveLH
(
    float ViewWidth,
    float ViewHeight,
    float NearZ,
    float FarZ
)
{
    (void)( (!!(NearZ > 0.f && FarZ > 0.f)) || (_wassert(L"NearZ > 0.f && FarZ > 0.f", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2028)), 0) );
    (void)( (!!(!XMScalarNearEqual(ViewWidth, 0.0f, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(ViewWidth, 0.0f, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2029)), 0) );
    (void)( (!!(!XMScalarNearEqual(ViewHeight, 0.0f, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(ViewHeight, 0.0f, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2030)), 0) );
    (void)( (!!(!XMScalarNearEqual(FarZ, NearZ, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(FarZ, NearZ, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2031)), 0) );




























#line 2061 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"









#line 2071 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    XMMATRIX M;
    float TwoNearZ = NearZ + NearZ;
    float fRange = FarZ / (FarZ - NearZ);
    // Note: This is recorded on the stack
    XMVECTOR rMem = {
        TwoNearZ / ViewWidth,
        TwoNearZ / ViewHeight,
        fRange,
        -fRange * NearZ
    };
    // Copy from memory to SSE register
    XMVECTOR vValues = rMem;
    XMVECTOR vTemp = _mm_setzero_ps();
    // Copy x only
    vTemp = _mm_move_ss(vTemp,vValues);
    // TwoNearZ / ViewWidth,0,0,0
    M.r[0] = vTemp;
    // 0,TwoNearZ / ViewHeight,0,0
    vTemp = vValues;
    vTemp = _mm_and_ps(vTemp,g_XMMaskY);
    M.r[1] = vTemp;
    // x=fRange,y=-fRange * NearZ,0,1.0f
    vValues = _mm_shuffle_ps(vValues,g_XMIdentityR3,(((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));
    // 0,0,fRange,1.0f
    vTemp = _mm_setzero_ps();
    vTemp = _mm_shuffle_ps(vTemp,vValues,(((3) << 6) | ((0) << 4) | ((0) << 2) | ((0))));
    M.r[2] = vTemp;
    // 0,0,-fRange * NearZ,0
    vTemp = _mm_shuffle_ps(vTemp,vValues,(((2) << 6) | ((1) << 4) | ((0) << 2) | ((0))));
    M.r[3] = vTemp;
    return M;
#line 2103 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixPerspectiveRH
(
    float ViewWidth,
    float ViewHeight,
    float NearZ,
    float FarZ
)
{
    (void)( (!!(NearZ > 0.f && FarZ > 0.f)) || (_wassert(L"NearZ > 0.f && FarZ > 0.f", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2115)), 0) );
    (void)( (!!(!XMScalarNearEqual(ViewWidth, 0.0f, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(ViewWidth, 0.0f, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2116)), 0) );
    (void)( (!!(!XMScalarNearEqual(ViewHeight, 0.0f, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(ViewHeight, 0.0f, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2117)), 0) );
    (void)( (!!(!XMScalarNearEqual(FarZ, NearZ, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(FarZ, NearZ, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2118)), 0) );




























#line 2148 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"










#line 2159 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    XMMATRIX M;
    float TwoNearZ = NearZ + NearZ;
    float fRange = FarZ / (NearZ-FarZ);
    // Note: This is recorded on the stack
    XMVECTOR rMem = {
        TwoNearZ / ViewWidth,
        TwoNearZ / ViewHeight,
        fRange,
        fRange * NearZ
    };
    // Copy from memory to SSE register
    XMVECTOR vValues = rMem;
    XMVECTOR vTemp = _mm_setzero_ps();
    // Copy x only
    vTemp = _mm_move_ss(vTemp,vValues);
    // TwoNearZ / ViewWidth,0,0,0
    M.r[0] = vTemp;
    // 0,TwoNearZ / ViewHeight,0,0
    vTemp = vValues;
    vTemp = _mm_and_ps(vTemp,g_XMMaskY);
    M.r[1] = vTemp;
    // x=fRange,y=-fRange * NearZ,0,-1.0f
    vValues = _mm_shuffle_ps(vValues,g_XMNegIdentityR3,(((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));
    // 0,0,fRange,-1.0f
    vTemp = _mm_setzero_ps();
    vTemp = _mm_shuffle_ps(vTemp,vValues,(((3) << 6) | ((0) << 4) | ((0) << 2) | ((0))));
    M.r[2] = vTemp;
    // 0,0,-fRange * NearZ,0
    vTemp = _mm_shuffle_ps(vTemp,vValues,(((2) << 6) | ((1) << 4) | ((0) << 2) | ((0))));
    M.r[3] = vTemp;
    return M;
#line 2191 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixPerspectiveFovLH
(
    float FovAngleY,
    float AspectRatio,
    float NearZ,
    float FarZ
)
{
    (void)( (!!(NearZ > 0.f && FarZ > 0.f)) || (_wassert(L"NearZ > 0.f && FarZ > 0.f", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2203)), 0) );
    (void)( (!!(!XMScalarNearEqual(FovAngleY, 0.0f, 0.00001f * 2.0f))) || (_wassert(L"!XMScalarNearEqual(FovAngleY, 0.0f, 0.00001f * 2.0f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2204)), 0) );
    (void)( (!!(!XMScalarNearEqual(AspectRatio, 0.0f, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(AspectRatio, 0.0f, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2205)), 0) );
    (void)( (!!(!XMScalarNearEqual(FarZ, NearZ, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(FarZ, NearZ, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2206)), 0) );

































#line 2241 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"















#line 2257 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    float    SinFov;
    float    CosFov;
    XMScalarSinCos(&SinFov, &CosFov, 0.5f * FovAngleY);

    float fRange = FarZ / (FarZ-NearZ);
    // Note: This is recorded on the stack
    float Height = CosFov / SinFov;
    XMVECTOR rMem = {
        Height / AspectRatio,
        Height,
        fRange,
        -fRange * NearZ
    };
    // Copy from memory to SSE register
    XMVECTOR vValues = rMem;
    XMVECTOR vTemp = _mm_setzero_ps();
    // Copy x only
    vTemp = _mm_move_ss(vTemp,vValues);
    // CosFov / SinFov,0,0,0
    XMMATRIX M;
    M.r[0] = vTemp;
    // 0,Height / AspectRatio,0,0
    vTemp = vValues;
    vTemp = _mm_and_ps(vTemp,g_XMMaskY);
    M.r[1] = vTemp;
    // x=fRange,y=-fRange * NearZ,0,1.0f
    vTemp = _mm_setzero_ps();
    vValues = _mm_shuffle_ps(vValues,g_XMIdentityR3,(((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));
    // 0,0,fRange,1.0f
    vTemp = _mm_shuffle_ps(vTemp,vValues,(((3) << 6) | ((0) << 4) | ((0) << 2) | ((0))));
    M.r[2] = vTemp;
    // 0,0,-fRange * NearZ,0.0f
    vTemp = _mm_shuffle_ps(vTemp,vValues,(((2) << 6) | ((1) << 4) | ((0) << 2) | ((0))));
    M.r[3] = vTemp;
    return M;
#line 2293 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixPerspectiveFovRH
(
    float FovAngleY,
    float AspectRatio,
    float NearZ,
    float FarZ
)
{
    (void)( (!!(NearZ > 0.f && FarZ > 0.f)) || (_wassert(L"NearZ > 0.f && FarZ > 0.f", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2305)), 0) );
    (void)( (!!(!XMScalarNearEqual(FovAngleY, 0.0f, 0.00001f * 2.0f))) || (_wassert(L"!XMScalarNearEqual(FovAngleY, 0.0f, 0.00001f * 2.0f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2306)), 0) );
    (void)( (!!(!XMScalarNearEqual(AspectRatio, 0.0f, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(AspectRatio, 0.0f, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2307)), 0) );
    (void)( (!!(!XMScalarNearEqual(FarZ, NearZ, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(FarZ, NearZ, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2308)), 0) );

































#line 2343 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"














#line 2358 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    float    SinFov;
    float    CosFov;
    XMScalarSinCos(&SinFov, &CosFov, 0.5f * FovAngleY);
    float fRange = FarZ / (NearZ-FarZ);
    // Note: This is recorded on the stack
    float Height = CosFov / SinFov;
    XMVECTOR rMem = {
        Height / AspectRatio,
        Height,
        fRange,
        fRange * NearZ
    };
    // Copy from memory to SSE register
    XMVECTOR vValues = rMem;
    XMVECTOR vTemp = _mm_setzero_ps();
    // Copy x only
    vTemp = _mm_move_ss(vTemp,vValues);
    // CosFov / SinFov,0,0,0
    XMMATRIX M;
    M.r[0] = vTemp;
    // 0,Height / AspectRatio,0,0
    vTemp = vValues;
    vTemp = _mm_and_ps(vTemp,g_XMMaskY);
    M.r[1] = vTemp;
    // x=fRange,y=-fRange * NearZ,0,-1.0f
    vTemp = _mm_setzero_ps();
    vValues = _mm_shuffle_ps(vValues,g_XMNegIdentityR3,(((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));
    // 0,0,fRange,-1.0f
    vTemp = _mm_shuffle_ps(vTemp,vValues,(((3) << 6) | ((0) << 4) | ((0) << 2) | ((0))));
    M.r[2] = vTemp;
    // 0,0,fRange * NearZ,0.0f
    vTemp = _mm_shuffle_ps(vTemp,vValues,(((2) << 6) | ((1) << 4) | ((0) << 2) | ((0))));
    M.r[3] = vTemp;
    return M;
#line 2393 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixPerspectiveOffCenterLH
(
    float ViewLeft,
    float ViewRight,
    float ViewBottom,
    float ViewTop,
    float NearZ,
    float FarZ
)
{
    (void)( (!!(NearZ > 0.f && FarZ > 0.f)) || (_wassert(L"NearZ > 0.f && FarZ > 0.f", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2407)), 0) );
    (void)( (!!(!XMScalarNearEqual(ViewRight, ViewLeft, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(ViewRight, ViewLeft, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2408)), 0) );
    (void)( (!!(!XMScalarNearEqual(ViewTop, ViewBottom, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(ViewTop, ViewBottom, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2409)), 0) );
    (void)( (!!(!XMScalarNearEqual(FarZ, NearZ, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(FarZ, NearZ, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2410)), 0) );






























#line 2442 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"















#line 2458 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    XMMATRIX M;
    float TwoNearZ = NearZ+NearZ;
    float ReciprocalWidth = 1.0f / (ViewRight - ViewLeft);
    float ReciprocalHeight = 1.0f / (ViewTop - ViewBottom);
    float fRange = FarZ / (FarZ-NearZ);
    // Note: This is recorded on the stack
    XMVECTOR rMem = {
        TwoNearZ*ReciprocalWidth,
        TwoNearZ*ReciprocalHeight,
        -fRange * NearZ,
        0
    };
    // Copy from memory to SSE register
    XMVECTOR vValues = rMem;
    XMVECTOR vTemp = _mm_setzero_ps();
    // Copy x only
    vTemp = _mm_move_ss(vTemp,vValues);
    // TwoNearZ*ReciprocalWidth,0,0,0
    M.r[0] = vTemp;
    // 0,TwoNearZ*ReciprocalHeight,0,0
    vTemp = vValues;
    vTemp = _mm_and_ps(vTemp,g_XMMaskY);
    M.r[1] = vTemp;
    // 0,0,fRange,1.0f
    M.r[2] = XMVectorSet( -(ViewLeft + ViewRight) * ReciprocalWidth,
                          -(ViewTop + ViewBottom) * ReciprocalHeight,
                          fRange,
                          1.0f );
    // 0,0,-fRange * NearZ,0.0f
    vValues = _mm_and_ps(vValues,g_XMMaskZ);
    M.r[3] = vValues;
    return M;
#line 2491 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixPerspectiveOffCenterRH
(
    float ViewLeft,
    float ViewRight,
    float ViewBottom,
    float ViewTop,
    float NearZ,
    float FarZ
)
{
    (void)( (!!(NearZ > 0.f && FarZ > 0.f)) || (_wassert(L"NearZ > 0.f && FarZ > 0.f", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2505)), 0) );
    (void)( (!!(!XMScalarNearEqual(ViewRight, ViewLeft, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(ViewRight, ViewLeft, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2506)), 0) );
    (void)( (!!(!XMScalarNearEqual(ViewTop, ViewBottom, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(ViewTop, ViewBottom, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2507)), 0) );
    (void)( (!!(!XMScalarNearEqual(FarZ, NearZ, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(FarZ, NearZ, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2508)), 0) );






























#line 2540 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"















#line 2556 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    XMMATRIX M;
    float TwoNearZ = NearZ+NearZ;
    float ReciprocalWidth = 1.0f / (ViewRight - ViewLeft);
    float ReciprocalHeight = 1.0f / (ViewTop - ViewBottom);
    float fRange = FarZ / (NearZ-FarZ);
    // Note: This is recorded on the stack
    XMVECTOR rMem = {
        TwoNearZ*ReciprocalWidth,
        TwoNearZ*ReciprocalHeight,
        fRange * NearZ,
        0
    };
    // Copy from memory to SSE register
    XMVECTOR vValues = rMem;
    XMVECTOR vTemp = _mm_setzero_ps();
    // Copy x only
    vTemp = _mm_move_ss(vTemp,vValues);
    // TwoNearZ*ReciprocalWidth,0,0,0
    M.r[0] = vTemp;
    // 0,TwoNearZ*ReciprocalHeight,0,0
    vTemp = vValues;
    vTemp = _mm_and_ps(vTemp,g_XMMaskY);
    M.r[1] = vTemp;
    // 0,0,fRange,1.0f
    M.r[2] = XMVectorSet( (ViewLeft + ViewRight) * ReciprocalWidth,
                          (ViewTop + ViewBottom) * ReciprocalHeight,
                          fRange,
                          -1.0f );
    // 0,0,-fRange * NearZ,0.0f
    vValues = _mm_and_ps(vValues,g_XMMaskZ);
    M.r[3] = vValues;
    return M;
#line 2589 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixOrthographicLH
(
    float ViewWidth,
    float ViewHeight,
    float NearZ,
    float FarZ
)
{
    (void)( (!!(!XMScalarNearEqual(ViewWidth, 0.0f, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(ViewWidth, 0.0f, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2601)), 0) );
    (void)( (!!(!XMScalarNearEqual(ViewHeight, 0.0f, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(ViewHeight, 0.0f, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2602)), 0) );
    (void)( (!!(!XMScalarNearEqual(FarZ, NearZ, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(FarZ, NearZ, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2603)), 0) );



























#line 2632 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"









#line 2642 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    XMMATRIX M;
    float fRange = 1.0f / (FarZ-NearZ);
    // Note: This is recorded on the stack
    XMVECTOR rMem = {
        2.0f / ViewWidth,
        2.0f / ViewHeight,
        fRange,
        -fRange * NearZ
    };
    // Copy from memory to SSE register
    XMVECTOR vValues = rMem;
    XMVECTOR vTemp = _mm_setzero_ps();
    // Copy x only
    vTemp = _mm_move_ss(vTemp,vValues);
    // 2.0f / ViewWidth,0,0,0
    M.r[0] = vTemp;
    // 0,2.0f / ViewHeight,0,0
    vTemp = vValues;
    vTemp = _mm_and_ps(vTemp,g_XMMaskY);
    M.r[1] = vTemp;
    // x=fRange,y=-fRange * NearZ,0,1.0f
    vTemp = _mm_setzero_ps();
    vValues = _mm_shuffle_ps(vValues,g_XMIdentityR3,(((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));
    // 0,0,fRange,0.0f
    vTemp = _mm_shuffle_ps(vTemp,vValues,(((2) << 6) | ((0) << 4) | ((0) << 2) | ((0))));
    M.r[2] = vTemp;
    // 0,0,-fRange * NearZ,1.0f
    vTemp = _mm_shuffle_ps(vTemp,vValues,(((3) << 6) | ((1) << 4) | ((0) << 2) | ((0))));
    M.r[3] = vTemp;
    return M;
#line 2673 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixOrthographicRH
(
    float ViewWidth,
    float ViewHeight,
    float NearZ,
    float FarZ
)
{
    (void)( (!!(!XMScalarNearEqual(ViewWidth, 0.0f, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(ViewWidth, 0.0f, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2685)), 0) );
    (void)( (!!(!XMScalarNearEqual(ViewHeight, 0.0f, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(ViewHeight, 0.0f, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2686)), 0) );
    (void)( (!!(!XMScalarNearEqual(FarZ, NearZ, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(FarZ, NearZ, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2687)), 0) );



























#line 2716 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"









#line 2726 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    XMMATRIX M;
    float fRange = 1.0f / (NearZ-FarZ);
    // Note: This is recorded on the stack
    XMVECTOR rMem = {
        2.0f / ViewWidth,
        2.0f / ViewHeight,
        fRange,
        fRange * NearZ
    };
    // Copy from memory to SSE register
    XMVECTOR vValues = rMem;
    XMVECTOR vTemp = _mm_setzero_ps();
    // Copy x only
    vTemp = _mm_move_ss(vTemp,vValues);
    // 2.0f / ViewWidth,0,0,0
    M.r[0] = vTemp;
    // 0,2.0f / ViewHeight,0,0
    vTemp = vValues;
    vTemp = _mm_and_ps(vTemp,g_XMMaskY);
    M.r[1] = vTemp;
    // x=fRange,y=fRange * NearZ,0,1.0f
    vTemp = _mm_setzero_ps();
    vValues = _mm_shuffle_ps(vValues,g_XMIdentityR3,(((3) << 6) | ((2) << 4) | ((3) << 2) | ((2))));
    // 0,0,fRange,0.0f
    vTemp = _mm_shuffle_ps(vTemp,vValues,(((2) << 6) | ((0) << 4) | ((0) << 2) | ((0))));
    M.r[2] = vTemp;
    // 0,0,fRange * NearZ,1.0f
    vTemp = _mm_shuffle_ps(vTemp,vValues,(((3) << 6) | ((1) << 4) | ((0) << 2) | ((0))));
    M.r[3] = vTemp;
    return M;
#line 2757 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixOrthographicOffCenterLH
(
    float ViewLeft,
    float ViewRight,
    float ViewBottom,
    float ViewTop,
    float NearZ,
    float FarZ
)
{
    (void)( (!!(!XMScalarNearEqual(ViewRight, ViewLeft, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(ViewRight, ViewLeft, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2771)), 0) );
    (void)( (!!(!XMScalarNearEqual(ViewTop, ViewBottom, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(ViewTop, ViewBottom, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2772)), 0) );
    (void)( (!!(!XMScalarNearEqual(FarZ, NearZ, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(FarZ, NearZ, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2773)), 0) );





























#line 2804 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"













#line 2818 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    XMMATRIX M;
    float fReciprocalWidth = 1.0f / (ViewRight - ViewLeft);
    float fReciprocalHeight = 1.0f / (ViewTop - ViewBottom);
    float fRange = 1.0f / (FarZ-NearZ);
    // Note: This is recorded on the stack
    XMVECTOR rMem = {
        fReciprocalWidth,
        fReciprocalHeight,
        fRange,
        1.0f
    };
    XMVECTOR rMem2 = {
        -(ViewLeft + ViewRight),
        -(ViewTop + ViewBottom),
        -NearZ,
        1.0f
    };
    // Copy from memory to SSE register
    XMVECTOR vValues = rMem;
    XMVECTOR vTemp = _mm_setzero_ps();
    // Copy x only
    vTemp = _mm_move_ss(vTemp,vValues);
    // fReciprocalWidth*2,0,0,0
    vTemp = _mm_add_ss(vTemp,vTemp);
    M.r[0] = vTemp;
    // 0,fReciprocalHeight*2,0,0
    vTemp = vValues;
    vTemp = _mm_and_ps(vTemp,g_XMMaskY);
    vTemp = _mm_add_ps(vTemp,vTemp);
    M.r[1] = vTemp;
    // 0,0,fRange,0.0f
    vTemp = vValues;
    vTemp = _mm_and_ps(vTemp,g_XMMaskZ);
    M.r[2] = vTemp;
    // -(ViewLeft + ViewRight)*fReciprocalWidth,-(ViewTop + ViewBottom)*fReciprocalHeight,fRange*-NearZ,1.0f
    vValues = _mm_mul_ps(vValues,rMem2);
    M.r[3] = vValues;
    return M;
#line 2857 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMatrixOrthographicOffCenterRH
(
    float ViewLeft,
    float ViewRight,
    float ViewBottom,
    float ViewTop,
    float NearZ,
    float FarZ
)
{
    (void)( (!!(!XMScalarNearEqual(ViewRight, ViewLeft, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(ViewRight, ViewLeft, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2871)), 0) );
    (void)( (!!(!XMScalarNearEqual(ViewTop, ViewBottom, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(ViewTop, ViewBottom, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2872)), 0) );
    (void)( (!!(!XMScalarNearEqual(FarZ, NearZ, 0.00001f))) || (_wassert(L"!XMScalarNearEqual(FarZ, NearZ, 0.00001f)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2873)), 0) );





























#line 2904 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"













#line 2918 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    XMMATRIX M;
    float fReciprocalWidth = 1.0f / (ViewRight - ViewLeft);
    float fReciprocalHeight = 1.0f / (ViewTop - ViewBottom);
    float fRange = 1.0f / (NearZ-FarZ);
    // Note: This is recorded on the stack
    XMVECTOR rMem = {
        fReciprocalWidth,
        fReciprocalHeight,
        fRange,
        1.0f
    };
    XMVECTOR rMem2 = {
        -(ViewLeft + ViewRight),
        -(ViewTop + ViewBottom),
        NearZ,
        1.0f
    };
    // Copy from memory to SSE register
    XMVECTOR vValues = rMem;
    XMVECTOR vTemp = _mm_setzero_ps();
    // Copy x only
    vTemp = _mm_move_ss(vTemp,vValues);
    // fReciprocalWidth*2,0,0,0
    vTemp = _mm_add_ss(vTemp,vTemp);
    M.r[0] = vTemp;
    // 0,fReciprocalHeight*2,0,0
    vTemp = vValues;
    vTemp = _mm_and_ps(vTemp,g_XMMaskY);
    vTemp = _mm_add_ps(vTemp,vTemp);
    M.r[1] = vTemp;
    // 0,0,fRange,0.0f
    vTemp = vValues;
    vTemp = _mm_and_ps(vTemp,g_XMMaskZ);
    M.r[2] = vTemp;
    // -(ViewLeft + ViewRight)*fReciprocalWidth,-(ViewTop + ViewBottom)*fReciprocalHeight,fRange*-NearZ,1.0f
    vValues = _mm_mul_ps(vValues,rMem2);
    M.r[3] = vValues;
    return M;
#line 2957 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}





/****************************************************************************
 *
 * XMMATRIX operators and methods
 *
 ****************************************************************************/

//------------------------------------------------------------------------------

inline XMMATRIX::XMMATRIX
(
    float m00, float m01, float m02, float m03,
    float m10, float m11, float m12, float m13,
    float m20, float m21, float m22, float m23,
    float m30, float m31, float m32, float m33
)
{
    r[0] = XMVectorSet(m00, m01, m02, m03);
    r[1] = XMVectorSet(m10, m11, m12, m13);
    r[2] = XMVectorSet(m20, m21, m22, m23);
    r[3] = XMVectorSet(m30, m31, m32, m33);
}

//------------------------------------------------------------------------------

inline XMMATRIX::XMMATRIX
(
    const float* pArray
)
{
    (void)( (!!(pArray != nullptr)) || (_wassert(L"pArray != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(2992)), 0) );
    r[0] = XMLoadFloat4(reinterpret_cast<const XMFLOAT4*>(pArray));
    r[1] = XMLoadFloat4(reinterpret_cast<const XMFLOAT4*>(pArray + 4));
    r[2] = XMLoadFloat4(reinterpret_cast<const XMFLOAT4*>(pArray + 8));
    r[3] = XMLoadFloat4(reinterpret_cast<const XMFLOAT4*>(pArray + 12));
}

//------------------------------------------------------------------------------

inline XMMATRIX XMMATRIX::operator- () const
{
    XMMATRIX R;
    R.r[0] = XMVectorNegate( r[0] );
    R.r[1] = XMVectorNegate( r[1] );
    R.r[2] = XMVectorNegate( r[2] );
    R.r[3] = XMVectorNegate( r[3] );
    return R;
}

//------------------------------------------------------------------------------

inline XMMATRIX& __vectorcall XMMATRIX::operator+= (FXMMATRIX M)
{
    r[0] = XMVectorAdd( r[0], M.r[0] );
    r[1] = XMVectorAdd( r[1], M.r[1] );
    r[2] = XMVectorAdd( r[2], M.r[2] );
    r[3] = XMVectorAdd( r[3], M.r[3] );
    return *this;
}

//------------------------------------------------------------------------------

inline XMMATRIX& __vectorcall XMMATRIX::operator-= (FXMMATRIX M)
{
    r[0] = XMVectorSubtract( r[0], M.r[0] );
    r[1] = XMVectorSubtract( r[1], M.r[1] );
    r[2] = XMVectorSubtract( r[2], M.r[2] );
    r[3] = XMVectorSubtract( r[3], M.r[3] );
    return *this;
}

//------------------------------------------------------------------------------

inline XMMATRIX& __vectorcall XMMATRIX::operator*=(FXMMATRIX M)
{
    *this = XMMatrixMultiply( *this, M );
    return *this;
}

//------------------------------------------------------------------------------

inline XMMATRIX& XMMATRIX::operator*= (float S)
{
    r[0] = XMVectorScale( r[0], S );
    r[1] = XMVectorScale( r[1], S );
    r[2] = XMVectorScale( r[2], S );
    r[3] = XMVectorScale( r[3], S );
    return *this;
}

//------------------------------------------------------------------------------

inline XMMATRIX& XMMATRIX::operator/= (float S)
{







#line 3064 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"





















#line 3086 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    __m128 vS = _mm_set_ps1( S );
    r[0] = _mm_div_ps( r[0], vS );
    r[1] = _mm_div_ps( r[1], vS );
    r[2] = _mm_div_ps( r[2], vS );
    r[3] = _mm_div_ps( r[3], vS );
    return *this;
#line 3093 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMATRIX::operator+ (FXMMATRIX M) const
{
    XMMATRIX R;
    R.r[0] = XMVectorAdd( r[0], M.r[0] );
    R.r[1] = XMVectorAdd( r[1], M.r[1] );
    R.r[2] = XMVectorAdd( r[2], M.r[2] );
    R.r[3] = XMVectorAdd( r[3], M.r[3] );
    return R;
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMATRIX::operator- (FXMMATRIX M) const
{
    XMMATRIX R;
    R.r[0] = XMVectorSubtract( r[0], M.r[0] );
    R.r[1] = XMVectorSubtract( r[1], M.r[1] );
    R.r[2] = XMVectorSubtract( r[2], M.r[2] );
    R.r[3] = XMVectorSubtract( r[3], M.r[3] );
    return R;
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall XMMATRIX::operator*(FXMMATRIX M) const
{
    return XMMatrixMultiply(*this, M);
}

//------------------------------------------------------------------------------

inline XMMATRIX XMMATRIX::operator* (float S) const
{
    XMMATRIX R;
    R.r[0] = XMVectorScale( r[0], S );
    R.r[1] = XMVectorScale( r[1], S );
    R.r[2] = XMVectorScale( r[2], S );
    R.r[3] = XMVectorScale( r[3], S );
    return R;
}

//------------------------------------------------------------------------------

inline XMMATRIX XMMATRIX::operator/ (float S) const
{








#line 3151 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"























#line 3175 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
    __m128 vS = _mm_set_ps1( S );
    XMMATRIX R;
    R.r[0] = _mm_div_ps( r[0], vS );
    R.r[1] = _mm_div_ps( r[1], vS );
    R.r[2] = _mm_div_ps( r[2], vS );
    R.r[3] = _mm_div_ps( r[3], vS );
    return R;
#line 3183 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl"
}

//------------------------------------------------------------------------------

inline XMMATRIX __vectorcall operator*
(
    float S,
    FXMMATRIX M
)
{
    XMMATRIX R;
    R.r[0] = XMVectorScale( M.r[0], S );
    R.r[1] = XMVectorScale( M.r[1], S );
    R.r[2] = XMVectorScale( M.r[2], S );
    R.r[3] = XMVectorScale( M.r[3], S );
    return R;
}

/****************************************************************************
 *
 * XMFLOAT3X3 operators
 *
 ****************************************************************************/

//------------------------------------------------------------------------------

inline XMFLOAT3X3::XMFLOAT3X3
(
    const float* pArray
)
{
    (void)( (!!(pArray != nullptr)) || (_wassert(L"pArray != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(3214)), 0) );
    for (size_t Row = 0; Row < 3; Row++)
    {
        for (size_t Column = 0; Column < 3; Column++)
        {
            m[Row][Column] = pArray[Row * 3 + Column];
        }
    }
}

/****************************************************************************
 *
 * XMFLOAT4X3 operators
 *
 ****************************************************************************/

//------------------------------------------------------------------------------

inline XMFLOAT4X3::XMFLOAT4X3
(
    const float* pArray
)
{
    (void)( (!!(pArray != nullptr)) || (_wassert(L"pArray != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(3237)), 0) );

    m[0][0] = pArray[0];
    m[0][1] = pArray[1];
    m[0][2] = pArray[2];

    m[1][0] = pArray[3];
    m[1][1] = pArray[4];
    m[1][2] = pArray[5];

    m[2][0] = pArray[6];
    m[2][1] = pArray[7];
    m[2][2] = pArray[8];

    m[3][0] = pArray[9];
    m[3][1] = pArray[10];
    m[3][2] = pArray[11];
}

/****************************************************************************
*
* XMFLOAT3X4 operators
*
****************************************************************************/

//------------------------------------------------------------------------------

inline XMFLOAT3X4::XMFLOAT3X4
(
    const float* pArray
)
{
    (void)( (!!(pArray != nullptr)) || (_wassert(L"pArray != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(3269)), 0) );

    m[0][0] = pArray[0];
    m[0][1] = pArray[1];
    m[0][2] = pArray[2];
    m[0][3] = pArray[3];

    m[1][0] = pArray[4];
    m[1][1] = pArray[5];
    m[1][2] = pArray[6];
    m[1][3] = pArray[7];

    m[2][0] = pArray[8];
    m[2][1] = pArray[9];
    m[2][2] = pArray[10];
    m[2][3] = pArray[11];
}

/****************************************************************************
 *
 * XMFLOAT4X4 operators
 *
 ****************************************************************************/

//------------------------------------------------------------------------------

inline XMFLOAT4X4::XMFLOAT4X4
(
    const float* pArray
)
{
    (void)( (!!(pArray != nullptr)) || (_wassert(L"pArray != nullptr", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMatrix.inl", (unsigned)(3300)), 0) );

    m[0][0] = pArray[0];
    m[0][1] = pArray[1];
    m[0][2] = pArray[2];
    m[0][3] = pArray[3];

    m[1][0] = pArray[4];
    m[1][1] = pArray[5];
    m[1][2] = pArray[6];
    m[1][3] = pArray[7];

    m[2][0] = pArray[8];
    m[2][1] = pArray[9];
    m[2][2] = pArray[10];
    m[2][3] = pArray[11];

    m[3][0] = pArray[12];
    m[3][1] = pArray[13];
    m[3][2] = pArray[14];
    m[3][3] = pArray[15];
}

#pragma external_header(pop)
#line 2165 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"
#pragma external_header(push)
#line 1 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
//-------------------------------------------------------------------------------------
// DirectXMathMisc.inl -- SIMD C++ Math library
//
// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
//
// http://go.microsoft.com/fwlink/?LinkID=615560
//-------------------------------------------------------------------------------------

#pragma once

/****************************************************************************
 *
 * Quaternion
 *
 ****************************************************************************/

//------------------------------------------------------------------------------
// Comparison operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline bool __vectorcall XMQuaternionEqual
(
    FXMVECTOR Q1,
    FXMVECTOR Q2
)
{
    return XMVector4Equal(Q1, Q2);
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMQuaternionNotEqual
(
    FXMVECTOR Q1,
    FXMVECTOR Q2
)
{
    return XMVector4NotEqual(Q1, Q2);
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMQuaternionIsNaN
(
    FXMVECTOR Q
)
{
    return XMVector4IsNaN(Q);
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMQuaternionIsInfinite
(
    FXMVECTOR Q
)
{
    return XMVector4IsInfinite(Q);
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMQuaternionIsIdentity
(
    FXMVECTOR Q
)
{
    return XMVector4Equal(Q, g_XMIdentityR3.v);
}

//------------------------------------------------------------------------------
// Computation operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionDot
(
    FXMVECTOR Q1,
    FXMVECTOR Q2
)
{
    return XMVector4Dot(Q1, Q2);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionMultiply
(
    FXMVECTOR Q1,
    FXMVECTOR Q2
)
{
    // Returns the product Q2*Q1 (which is the concatenation of a rotation Q1 followed by the rotation Q2)

    // [ (Q2.w * Q1.x) + (Q2.x * Q1.w) + (Q2.y * Q1.z) - (Q2.z * Q1.y),
    //   (Q2.w * Q1.y) - (Q2.x * Q1.z) + (Q2.y * Q1.w) + (Q2.z * Q1.x),
    //   (Q2.w * Q1.z) + (Q2.x * Q1.y) - (Q2.y * Q1.x) + (Q2.z * Q1.w),
    //   (Q2.w * Q1.w) - (Q2.x * Q1.x) - (Q2.y * Q1.y) - (Q2.z * Q1.z) ]









#line 113 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"





























#line 143 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
    static const XMVECTORF32 ControlWZYX = { { { 1.0f, -1.0f, 1.0f, -1.0f } } };
    static const XMVECTORF32 ControlZWXY = { { { 1.0f, 1.0f, -1.0f, -1.0f } } };
    static const XMVECTORF32 ControlYXWZ = { { { -1.0f, 1.0f, 1.0f, -1.0f } } };
    // Copy to SSE registers and use as few as possible for x86
    XMVECTOR Q2X = Q2;
    XMVECTOR Q2Y = Q2;
    XMVECTOR Q2Z = Q2;
    XMVECTOR vResult = Q2;
    // Splat with one instruction
    vResult = _mm_shuffle_ps( vResult, vResult, (((3) << 6) | ((3) << 4) | ((3) << 2) | ((3))) );
    Q2X = _mm_shuffle_ps( Q2X, Q2X, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    Q2Y = _mm_shuffle_ps( Q2Y, Q2Y, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    Q2Z = _mm_shuffle_ps( Q2Z, Q2Z, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );
    // Retire Q1 and perform Q1*Q2W
    vResult = _mm_mul_ps(vResult,Q1);
    XMVECTOR Q1Shuffle = Q1;
    // Shuffle the copies of Q1
    Q1Shuffle = _mm_shuffle_ps( Q1Shuffle, Q1Shuffle, (((0) << 6) | ((1) << 4) | ((2) << 2) | ((3))) );
    // Mul by Q1WZYX
    Q2X = _mm_mul_ps(Q2X,Q1Shuffle);
    Q1Shuffle = _mm_shuffle_ps( Q1Shuffle, Q1Shuffle, (((2) << 6) | ((3) << 4) | ((0) << 2) | ((1))) );
    // Flip the signs on y and z
    Q2X = _mm_mul_ps(Q2X,ControlWZYX);
    // Mul by Q1ZWXY
    Q2Y = _mm_mul_ps(Q2Y,Q1Shuffle);
    Q1Shuffle = _mm_shuffle_ps( Q1Shuffle, Q1Shuffle, (((0) << 6) | ((1) << 4) | ((2) << 2) | ((3))) );
    // Flip the signs on z and w
    Q2Y = _mm_mul_ps(Q2Y,ControlZWXY);
    // Mul by Q1YXWZ
    Q2Z = _mm_mul_ps(Q2Z,Q1Shuffle);
    vResult = _mm_add_ps(vResult,Q2X);
    // Flip the signs on x and w
    Q2Z = _mm_mul_ps(Q2Z,ControlYXWZ);
    Q2Y = _mm_add_ps(Q2Y,Q2Z);
    vResult = _mm_add_ps(vResult,Q2Y);
    return vResult;
#line 180 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionLengthSq
(
    FXMVECTOR Q
)
{
    return XMVector4LengthSq(Q);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionReciprocalLength
(
    FXMVECTOR Q
)
{
    return XMVector4ReciprocalLength(Q);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionLength
(
    FXMVECTOR Q
)
{
    return XMVector4Length(Q);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionNormalizeEst
(
    FXMVECTOR Q
)
{
    return XMVector4NormalizeEst(Q);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionNormalize
(
    FXMVECTOR Q
)
{
    return XMVector4Normalize(Q);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionConjugate
(
    FXMVECTOR Q
)
{








#line 248 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"


#line 251 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
    static const XMVECTORF32 NegativeOne3 = { { { -1.0f, -1.0f, -1.0f, 1.0f } } };
    return _mm_mul_ps(Q,NegativeOne3);
#line 254 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionInverse
(
    FXMVECTOR Q
)
{
    const XMVECTOR  Zero = XMVectorZero();

    XMVECTOR L = XMVector4LengthSq(Q);
    XMVECTOR Conjugate = XMQuaternionConjugate(Q);

    XMVECTOR Control = XMVectorLessOrEqual(L, g_XMEpsilon.v);

    XMVECTOR Result = XMVectorDivide(Conjugate, L);

    Result = XMVectorSelect(Result, Zero, Control);

    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionLn
(
    FXMVECTOR Q
)
{
    static const XMVECTORF32 OneMinusEpsilon = { { { 1.0f - 0.00001f, 1.0f - 0.00001f, 1.0f - 0.00001f, 1.0f - 0.00001f } } };

    XMVECTOR QW = XMVectorSplatW(Q);
    XMVECTOR Q0 = XMVectorSelect(g_XMSelect1110.v, Q, g_XMSelect1110.v);

    XMVECTOR ControlW = XMVectorInBounds(QW, OneMinusEpsilon.v);

    XMVECTOR Theta = XMVectorACos(QW);
    XMVECTOR SinTheta = XMVectorSin(Theta);

    XMVECTOR S = XMVectorDivide(Theta,SinTheta);

    XMVECTOR Result = XMVectorMultiply(Q0, S);
    Result = XMVectorSelect(Q0, Result, ControlW);

    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionExp
(
    FXMVECTOR Q
)
{
    XMVECTOR Theta = XMVector3Length(Q);

    XMVECTOR SinTheta, CosTheta;
    XMVectorSinCos(&SinTheta, &CosTheta, Theta);

    XMVECTOR S = XMVectorDivide(SinTheta, Theta);

    XMVECTOR Result = XMVectorMultiply(Q, S);

    const XMVECTOR Zero = XMVectorZero();
    XMVECTOR Control = XMVectorNearEqual(Theta, Zero, g_XMEpsilon.v);
    Result = XMVectorSelect(Result, Q, Control);

    Result = XMVectorSelect(CosTheta, Result, g_XMSelect1110.v);

    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionSlerp
(
    FXMVECTOR Q0,
    FXMVECTOR Q1,
    float    t
)
{
    XMVECTOR T = XMVectorReplicate(t);
    return XMQuaternionSlerpV(Q0, Q1, T);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionSlerpV
(
    FXMVECTOR Q0,
    FXMVECTOR Q1,
    FXMVECTOR T
)
{
    (void)( (!!((XMVectorGetY(T) == XMVectorGetX(T)) && (XMVectorGetZ(T) == XMVectorGetX(T)) && (XMVectorGetW(T) == XMVectorGetX(T)))) || (_wassert(L"(XMVectorGetY(T) == XMVectorGetX(T)) && (XMVectorGetZ(T) == XMVectorGetX(T)) && (XMVectorGetW(T) == XMVectorGetX(T))", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(349)), 0) );

    // Result = Q0 * sin((1.0 - t) * Omega) / sin(Omega) + Q1 * sin(t * Omega) / sin(Omega)












































#line 397 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
    static const XMVECTORF32 OneMinusEpsilon = { { { 1.0f - 0.00001f, 1.0f - 0.00001f, 1.0f - 0.00001f, 1.0f - 0.00001f } } };
    static const XMVECTORU32 SignMask2       = { { { 0x80000000, 0x00000000, 0x00000000, 0x00000000 } } };

    XMVECTOR CosOmega = XMQuaternionDot(Q0, Q1);

    const XMVECTOR Zero = XMVectorZero();
    XMVECTOR Control = XMVectorLess(CosOmega, Zero);
    XMVECTOR Sign = XMVectorSelect(g_XMOne, g_XMNegativeOne, Control);

    CosOmega = _mm_mul_ps(CosOmega, Sign);

    Control = XMVectorLess(CosOmega, OneMinusEpsilon);

    XMVECTOR SinOmega = _mm_mul_ps(CosOmega,CosOmega);
    SinOmega = _mm_sub_ps(g_XMOne,SinOmega);
    SinOmega = _mm_sqrt_ps(SinOmega);

    XMVECTOR Omega = XMVectorATan2(SinOmega, CosOmega);

    XMVECTOR V01 = _mm_shuffle_ps( T, T, (((2) << 6) | ((3) << 4) | ((0) << 2) | ((1))) );
    V01 = _mm_and_ps(V01,g_XMMaskXY);
    V01 = _mm_xor_ps(V01,SignMask2);
    V01 = _mm_add_ps(g_XMIdentityR0, V01);

    XMVECTOR S0 = _mm_mul_ps(V01, Omega);
    S0 = XMVectorSin(S0);
    S0 = _mm_div_ps(S0, SinOmega);

    S0 = XMVectorSelect(V01, S0, Control);

    XMVECTOR S1 = XMVectorSplatY(S0);
    S0 = XMVectorSplatX(S0);

    S1 = _mm_mul_ps(S1, Sign);
    XMVECTOR Result = _mm_mul_ps(Q0, S0);
    S1 = _mm_mul_ps(S1, Q1);
    Result = _mm_add_ps(Result,S1);
    return Result;
#line 436 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionSquad
(
    FXMVECTOR Q0,
    FXMVECTOR Q1,
    FXMVECTOR Q2,
    GXMVECTOR Q3,
    float    t
)
{
    XMVECTOR T = XMVectorReplicate(t);
    return XMQuaternionSquadV(Q0, Q1, Q2, Q3, T);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionSquadV
(
    FXMVECTOR Q0,
    FXMVECTOR Q1,
    FXMVECTOR Q2,
    GXMVECTOR Q3,
    HXMVECTOR T
)
{
    (void)( (!!((XMVectorGetY(T) == XMVectorGetX(T)) && (XMVectorGetZ(T) == XMVectorGetX(T)) && (XMVectorGetW(T) == XMVectorGetX(T)))) || (_wassert(L"(XMVectorGetY(T) == XMVectorGetX(T)) && (XMVectorGetZ(T) == XMVectorGetX(T)) && (XMVectorGetW(T) == XMVectorGetX(T))", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(464)), 0) );

    XMVECTOR TP = T;
    const XMVECTOR Two = XMVectorSplatConstant(2, 0);

    XMVECTOR Q03 = XMQuaternionSlerpV(Q0, Q3, T);
    XMVECTOR Q12 = XMQuaternionSlerpV(Q1, Q2, T);

    TP = XMVectorNegativeMultiplySubtract(TP, TP, TP);
    TP = XMVectorMultiply(TP, Two);

    XMVECTOR Result = XMQuaternionSlerpV(Q03, Q12, TP);

    return Result;
}

//------------------------------------------------------------------------------

inline void __vectorcall XMQuaternionSquadSetup
(
    XMVECTOR* pA,
    XMVECTOR* pB,
    XMVECTOR* pC,
    FXMVECTOR  Q0,
    FXMVECTOR  Q1,
    FXMVECTOR  Q2,
    GXMVECTOR  Q3
)
{
    (void)( (!!(pA)) || (_wassert(L"pA", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(493)), 0) );
    (void)( (!!(pB)) || (_wassert(L"pB", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(494)), 0) );
    (void)( (!!(pC)) || (_wassert(L"pC", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(495)), 0) );

    XMVECTOR LS12 = XMQuaternionLengthSq(XMVectorAdd(Q1, Q2));
    XMVECTOR LD12 = XMQuaternionLengthSq(XMVectorSubtract(Q1, Q2));
    XMVECTOR SQ2 = XMVectorNegate(Q2);

    XMVECTOR Control1 = XMVectorLess(LS12, LD12);
    SQ2 = XMVectorSelect(Q2, SQ2, Control1);

    XMVECTOR LS01 = XMQuaternionLengthSq(XMVectorAdd(Q0, Q1));
    XMVECTOR LD01 = XMQuaternionLengthSq(XMVectorSubtract(Q0, Q1));
    XMVECTOR SQ0 = XMVectorNegate(Q0);

    XMVECTOR LS23 = XMQuaternionLengthSq(XMVectorAdd(SQ2, Q3));
    XMVECTOR LD23 = XMQuaternionLengthSq(XMVectorSubtract(SQ2, Q3));
    XMVECTOR SQ3 = XMVectorNegate(Q3);

    XMVECTOR Control0 = XMVectorLess(LS01, LD01);
    XMVECTOR Control2 = XMVectorLess(LS23, LD23);

    SQ0 = XMVectorSelect(Q0, SQ0, Control0);
    SQ3 = XMVectorSelect(Q3, SQ3, Control2);

    XMVECTOR InvQ1 = XMQuaternionInverse(Q1);
    XMVECTOR InvQ2 = XMQuaternionInverse(SQ2);

    XMVECTOR LnQ0 = XMQuaternionLn(XMQuaternionMultiply(InvQ1, SQ0));
    XMVECTOR LnQ2 = XMQuaternionLn(XMQuaternionMultiply(InvQ1, SQ2));
    XMVECTOR LnQ1 = XMQuaternionLn(XMQuaternionMultiply(InvQ2, Q1));
    XMVECTOR LnQ3 = XMQuaternionLn(XMQuaternionMultiply(InvQ2, SQ3));

    const XMVECTOR NegativeOneQuarter = XMVectorSplatConstant(-1, 2);

    XMVECTOR ExpQ02 = XMVectorMultiply(XMVectorAdd(LnQ0, LnQ2), NegativeOneQuarter);
    XMVECTOR ExpQ13 = XMVectorMultiply(XMVectorAdd(LnQ1, LnQ3), NegativeOneQuarter);
    ExpQ02 = XMQuaternionExp(ExpQ02);
    ExpQ13 = XMQuaternionExp(ExpQ13);

    *pA = XMQuaternionMultiply(Q1, ExpQ02);
    *pB = XMQuaternionMultiply(SQ2, ExpQ13);
    *pC = SQ2;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionBaryCentric
(
    FXMVECTOR Q0,
    FXMVECTOR Q1,
    FXMVECTOR Q2,
    float    f,
    float    g
)
{
    float s = f + g;

    XMVECTOR Result;
    if ((s < 0.00001f) && (s > -0.00001f))
    {
        Result = Q0;
    }
    else
    {
        XMVECTOR Q01 = XMQuaternionSlerp(Q0, Q1, s);
        XMVECTOR Q02 = XMQuaternionSlerp(Q0, Q2, s);

        Result = XMQuaternionSlerp(Q01, Q02, g / s);
    }

    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionBaryCentricV
(
    FXMVECTOR Q0,
    FXMVECTOR Q1,
    FXMVECTOR Q2,
    GXMVECTOR F,
    HXMVECTOR G
)
{
    (void)( (!!((XMVectorGetY(F) == XMVectorGetX(F)) && (XMVectorGetZ(F) == XMVectorGetX(F)) && (XMVectorGetW(F) == XMVectorGetX(F)))) || (_wassert(L"(XMVectorGetY(F) == XMVectorGetX(F)) && (XMVectorGetZ(F) == XMVectorGetX(F)) && (XMVectorGetW(F) == XMVectorGetX(F))", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(578)), 0) );
    (void)( (!!((XMVectorGetY(G) == XMVectorGetX(G)) && (XMVectorGetZ(G) == XMVectorGetX(G)) && (XMVectorGetW(G) == XMVectorGetX(G)))) || (_wassert(L"(XMVectorGetY(G) == XMVectorGetX(G)) && (XMVectorGetZ(G) == XMVectorGetX(G)) && (XMVectorGetW(G) == XMVectorGetX(G))", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(579)), 0) );

    const XMVECTOR Epsilon = XMVectorSplatConstant(1, 16);

    XMVECTOR S = XMVectorAdd(F, G);

    XMVECTOR Result;
    if (XMVector4InBounds(S, Epsilon))
    {
        Result = Q0;
    }
    else
    {
        XMVECTOR Q01 = XMQuaternionSlerpV(Q0, Q1, S);
        XMVECTOR Q02 = XMQuaternionSlerpV(Q0, Q2, S);
        XMVECTOR GS = XMVectorReciprocal(S);
        GS = XMVectorMultiply(G, GS);

        Result = XMQuaternionSlerpV(Q01, Q02, GS);
    }

    return Result;
}

//------------------------------------------------------------------------------
// Transformation operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionIdentity()
{
    return g_XMIdentityR3.v;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionRotationRollPitchYaw
(
    float Pitch,
    float Yaw,
    float Roll
)
{
    XMVECTOR Angles = XMVectorSet(Pitch, Yaw, Roll, 0.0f);
    XMVECTOR Q = XMQuaternionRotationRollPitchYawFromVector(Angles);
    return Q;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionRotationRollPitchYawFromVector
(
    FXMVECTOR Angles // <Pitch, Yaw, Roll, 0>
)
{
    static const XMVECTORF32  Sign = { { { 1.0f, -1.0f, -1.0f, 1.0f } } };

    XMVECTOR HalfAngles = XMVectorMultiply(Angles, g_XMOneHalf.v);

    XMVECTOR SinAngles, CosAngles;
    XMVectorSinCos(&SinAngles, &CosAngles, HalfAngles);

    XMVECTOR P0 = XMVectorPermute<XM_PERMUTE_0X, XM_PERMUTE_1X, XM_PERMUTE_1X, XM_PERMUTE_1X>(SinAngles, CosAngles);
    XMVECTOR Y0 = XMVectorPermute<XM_PERMUTE_1Y, XM_PERMUTE_0Y, XM_PERMUTE_1Y, XM_PERMUTE_1Y>(SinAngles, CosAngles);
    XMVECTOR R0 = XMVectorPermute<XM_PERMUTE_1Z, XM_PERMUTE_1Z, XM_PERMUTE_0Z, XM_PERMUTE_1Z>(SinAngles, CosAngles);
    XMVECTOR P1 = XMVectorPermute<XM_PERMUTE_0X, XM_PERMUTE_1X, XM_PERMUTE_1X, XM_PERMUTE_1X>(CosAngles, SinAngles);
    XMVECTOR Y1 = XMVectorPermute<XM_PERMUTE_1Y, XM_PERMUTE_0Y, XM_PERMUTE_1Y, XM_PERMUTE_1Y>(CosAngles, SinAngles);
    XMVECTOR R1 = XMVectorPermute<XM_PERMUTE_1Z, XM_PERMUTE_1Z, XM_PERMUTE_0Z, XM_PERMUTE_1Z>(CosAngles, SinAngles);

    XMVECTOR Q1 = XMVectorMultiply(P1, Sign.v);
    XMVECTOR Q0 = XMVectorMultiply(P0, Y0);
    Q1 = XMVectorMultiply(Q1, Y1);
    Q0 = XMVectorMultiply(Q0, R0);
    XMVECTOR Q = XMVectorMultiplyAdd(Q1, R1, Q0);

    return Q;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionRotationNormal
(
    FXMVECTOR NormalAxis,
    float    Angle
)
{









#line 676 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
    XMVECTOR N = _mm_and_ps(NormalAxis,g_XMMask3);
    N = _mm_or_ps(N,g_XMIdentityR3);
    XMVECTOR Scale = _mm_set_ps1(0.5f * Angle);
    XMVECTOR vSine;
    XMVECTOR vCosine;
    XMVectorSinCos(&vSine,&vCosine,Scale);
    Scale = _mm_and_ps(vSine,g_XMMask3);
    vCosine = _mm_and_ps(vCosine,g_XMMaskW);
    Scale = _mm_or_ps(Scale,vCosine);
    N = _mm_mul_ps(N,Scale);
    return N;
#line 688 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionRotationAxis
(
    FXMVECTOR Axis,
    float    Angle
)
{
    (void)( (!!(!XMVector3Equal(Axis, XMVectorZero()))) || (_wassert(L"!XMVector3Equal(Axis, XMVectorZero())", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(698)), 0) );
    (void)( (!!(!XMVector3IsInfinite(Axis))) || (_wassert(L"!XMVector3IsInfinite(Axis)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(699)), 0) );

    XMVECTOR Normal = XMVector3Normalize(Axis);
    XMVECTOR Q = XMQuaternionRotationNormal(Normal, Angle);
    return Q;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMQuaternionRotationMatrix
(
    FXMMATRIX M
)
{




















































#line 766 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"






















































































#line 853 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
    static const XMVECTORF32 XMPMMP = { { { +1.0f, -1.0f, -1.0f, +1.0f } } };
    static const XMVECTORF32 XMMPMP = { { { -1.0f, +1.0f, -1.0f, +1.0f } } };
    static const XMVECTORF32 XMMMPP = { { { -1.0f, -1.0f, +1.0f, +1.0f } } };

    XMVECTOR r0 = M.r[0];  // (r00, r01, r02, 0)
    XMVECTOR r1 = M.r[1];  // (r10, r11, r12, 0)
    XMVECTOR r2 = M.r[2];  // (r20, r21, r22, 0)

    // (r00, r00, r00, r00)
    XMVECTOR r00 = _mm_shuffle_ps( r0, r0, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    // (r11, r11, r11, r11)
    XMVECTOR r11 = _mm_shuffle_ps( r1, r1, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    // (r22, r22, r22, r22)
    XMVECTOR r22 = _mm_shuffle_ps( r2, r2, (((2) << 6) | ((2) << 4) | ((2) << 2) | ((2))) );

    // x^2 >= y^2 equivalent to r11 - r00 <= 0
    // (r11 - r00, r11 - r00, r11 - r00, r11 - r00)
    XMVECTOR r11mr00 = _mm_sub_ps(r11, r00);
    XMVECTOR x2gey2 = _mm_cmple_ps(r11mr00, g_XMZero);

    // z^2 >= w^2 equivalent to r11 + r00 <= 0
    // (r11 + r00, r11 + r00, r11 + r00, r11 + r00)
    XMVECTOR r11pr00 = _mm_add_ps(r11, r00);
    XMVECTOR z2gew2 = _mm_cmple_ps(r11pr00, g_XMZero);

    // x^2 + y^2 >= z^2 + w^2 equivalent to r22 <= 0
    XMVECTOR x2py2gez2pw2 = _mm_cmple_ps(r22, g_XMZero);

    // (+r00, -r00, -r00, +r00)
    XMVECTOR t0 = _mm_mul_ps(XMPMMP, r00);

    // (-r11, +r11, -r11, +r11)
    XMVECTOR t1 = _mm_mul_ps(XMMPMP, r11);

    // (-r22, -r22, +r22, +r22)
    XMVECTOR t2 = _mm_mul_ps(XMMMPP, r22);

    // (4*x^2, 4*y^2, 4*z^2, 4*w^2)
    XMVECTOR x2y2z2w2 = _mm_add_ps(t0, t1);
    x2y2z2w2 = _mm_add_ps(t2, x2y2z2w2);
    x2y2z2w2 = _mm_add_ps(x2y2z2w2, g_XMOne);

    // (r01, r02, r12, r11)
    t0 = _mm_shuffle_ps(r0, r1, (((1) << 6) | ((2) << 4) | ((2) << 2) | ((1))));
    // (r10, r10, r20, r21)
    t1 = _mm_shuffle_ps(r1, r2, (((1) << 6) | ((0) << 4) | ((0) << 2) | ((0))));
    // (r10, r20, r21, r10)
    t1 = _mm_shuffle_ps( t1, t1, (((1) << 6) | ((3) << 4) | ((2) << 2) | ((0))) );
    // (4*x*y, 4*x*z, 4*y*z, unused)
    XMVECTOR xyxzyz = _mm_add_ps(t0, t1);

    // (r21, r20, r10, r10)
    t0 = _mm_shuffle_ps(r2, r1, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((1))));
    // (r12, r12, r02, r01)
    t1 = _mm_shuffle_ps(r1, r0, (((1) << 6) | ((2) << 4) | ((2) << 2) | ((2))));
    // (r12, r02, r01, r12)
    t1 = _mm_shuffle_ps( t1, t1, (((1) << 6) | ((3) << 4) | ((2) << 2) | ((0))) );
    // (4*x*w, 4*y*w, 4*z*w, unused)
    XMVECTOR xwywzw = _mm_sub_ps(t0, t1);
    xwywzw = _mm_mul_ps(XMMPMP, xwywzw);

    // (4*x^2, 4*y^2, 4*x*y, unused)
    t0 = _mm_shuffle_ps(x2y2z2w2, xyxzyz, (((0) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    // (4*z^2, 4*w^2, 4*z*w, unused)
    t1 = _mm_shuffle_ps(x2y2z2w2, xwywzw, (((0) << 6) | ((2) << 4) | ((3) << 2) | ((2))));
    // (4*x*z, 4*y*z, 4*x*w, 4*y*w)
    t2 = _mm_shuffle_ps(xyxzyz, xwywzw, (((1) << 6) | ((0) << 4) | ((2) << 2) | ((1))));

    // (4*x*x, 4*x*y, 4*x*z, 4*x*w)
    XMVECTOR tensor0 = _mm_shuffle_ps(t0, t2, (((2) << 6) | ((0) << 4) | ((2) << 2) | ((0))));
    // (4*y*x, 4*y*y, 4*y*z, 4*y*w)
    XMVECTOR tensor1 = _mm_shuffle_ps(t0, t2, (((3) << 6) | ((1) << 4) | ((1) << 2) | ((2))));
    // (4*z*x, 4*z*y, 4*z*z, 4*z*w)
    XMVECTOR tensor2 = _mm_shuffle_ps(t2, t1, (((2) << 6) | ((0) << 4) | ((1) << 2) | ((0))));
    // (4*w*x, 4*w*y, 4*w*z, 4*w*w)
    XMVECTOR tensor3 = _mm_shuffle_ps(t2, t1, (((1) << 6) | ((2) << 4) | ((3) << 2) | ((2))));

    // Select the row of the tensor-product matrix that has the largest
    // magnitude.
    t0 = _mm_and_ps(x2gey2, tensor0);
    t1 = _mm_andnot_ps(x2gey2, tensor1);
    t0 = _mm_or_ps(t0, t1);
    t1 = _mm_and_ps(z2gew2, tensor2);
    t2 = _mm_andnot_ps(z2gew2, tensor3);
    t1 = _mm_or_ps(t1, t2);
    t0 = _mm_and_ps(x2py2gez2pw2, t0);
    t1 = _mm_andnot_ps(x2py2gez2pw2, t1);
    t2 = _mm_or_ps(t0, t1);

    // Normalize the row.  No division by zero is possible because the
    // quaternion is unit-length (and the row is a nonzero multiple of
    // the quaternion).
    t0 = XMVector4Length(t2);
    return _mm_div_ps(t2, t0);
#line 948 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
}

//------------------------------------------------------------------------------
// Conversion operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline void __vectorcall XMQuaternionToAxisAngle
(
    XMVECTOR* pAxis,
    float*    pAngle,
    FXMVECTOR  Q
)
{
    (void)( (!!(pAxis)) || (_wassert(L"pAxis", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(963)), 0) );
    (void)( (!!(pAngle)) || (_wassert(L"pAngle", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(964)), 0) );

    *pAxis = Q;

    *pAngle = 2.0f * XMScalarACos(XMVectorGetW(Q));
}

/****************************************************************************
 *
 * Plane
 *
 ****************************************************************************/

//------------------------------------------------------------------------------
// Comparison operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline bool __vectorcall XMPlaneEqual
(
    FXMVECTOR P1,
    FXMVECTOR P2
)
{
    return XMVector4Equal(P1, P2);
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMPlaneNearEqual
(
    FXMVECTOR P1,
    FXMVECTOR P2,
    FXMVECTOR Epsilon
)
{
    XMVECTOR NP1 = XMPlaneNormalize(P1);
    XMVECTOR NP2 = XMPlaneNormalize(P2);
    return XMVector4NearEqual(NP1, NP2, Epsilon);
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMPlaneNotEqual
(
    FXMVECTOR P1,
    FXMVECTOR P2
)
{
    return XMVector4NotEqual(P1, P2);
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMPlaneIsNaN
(
    FXMVECTOR P
)
{
    return XMVector4IsNaN(P);
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMPlaneIsInfinite
(
    FXMVECTOR P
)
{
    return XMVector4IsInfinite(P);
}

//------------------------------------------------------------------------------
// Computation operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMPlaneDot
(
    FXMVECTOR P,
    FXMVECTOR V
)
{
    return XMVector4Dot(P, V);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMPlaneDotCoord
(
    FXMVECTOR P,
    FXMVECTOR V
)
{
    // Result = P[0] * V[0] + P[1] * V[1] + P[2] * V[2] + P[3]

    XMVECTOR V3 = XMVectorSelect(g_XMOne.v, V, g_XMSelect1110.v);
    XMVECTOR Result = XMVector4Dot(P, V3);
    return Result;
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMPlaneDotNormal
(
    FXMVECTOR P,
    FXMVECTOR V
)
{
    return XMVector3Dot(P, V);
}

//------------------------------------------------------------------------------
// XMPlaneNormalizeEst uses a reciprocal estimate and
// returns QNaN on zero and infinite vectors.

inline XMVECTOR __vectorcall XMPlaneNormalizeEst
(
    FXMVECTOR P
)
{





#line 1093 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"



#line 1097 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
    // Perform the dot product
    XMVECTOR vDot = _mm_mul_ps(P,P);
    // x=Dot.y, y=Dot.z
    XMVECTOR vTemp = _mm_shuffle_ps( vDot, vDot, (((2) << 6) | ((1) << 4) | ((2) << 2) | ((1))) );
    // Result.x = x+y
    vDot = _mm_add_ss(vDot,vTemp);
    // x=Dot.z
    vTemp = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    // Result.x = (x+y)+z
    vDot = _mm_add_ss(vDot,vTemp);
    // Splat x
    vDot = _mm_shuffle_ps( vDot, vDot, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    // Get the reciprocal
    vDot = _mm_rsqrt_ps(vDot);
    // Get the reciprocal
    vDot = _mm_mul_ps(vDot,P);
    return vDot;
#line 1115 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMPlaneNormalize
(
    FXMVECTOR P
)
{














#line 1139 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"


#line 1142 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"











#line 1154 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
    // Perform the dot product on x,y and z only
    XMVECTOR vLengthSq = _mm_mul_ps(P,P);
    XMVECTOR vTemp = _mm_shuffle_ps( vLengthSq, vLengthSq, (((2) << 6) | ((1) << 4) | ((2) << 2) | ((1))) );
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    vTemp = _mm_shuffle_ps( vTemp, vTemp, (((1) << 6) | ((1) << 4) | ((1) << 2) | ((1))) );
    vLengthSq = _mm_add_ss(vLengthSq,vTemp);
    vLengthSq = _mm_shuffle_ps( vLengthSq, vLengthSq, (((0) << 6) | ((0) << 4) | ((0) << 2) | ((0))) );
    // Prepare for the division
    XMVECTOR vResult = _mm_sqrt_ps(vLengthSq);
    // Failsafe on zero (Or epsilon) length planes
    // If the length is infinity, set the elements to zero
    vLengthSq = _mm_cmpneq_ps(vLengthSq,g_XMInfinity);
    // Reciprocal mul to perform the normalization
    vResult = _mm_div_ps(P,vResult);
    // Any that are infinity, set to zero
    vResult = _mm_and_ps(vResult,vLengthSq);
    return vResult;
#line 1172 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMPlaneIntersectLine
(
    FXMVECTOR P,
    FXMVECTOR LinePoint1,
    FXMVECTOR LinePoint2
)
{
    XMVECTOR V1 = XMVector3Dot(P, LinePoint1);
    XMVECTOR V2 = XMVector3Dot(P, LinePoint2);
    XMVECTOR D = XMVectorSubtract(V1, V2);

    XMVECTOR VT = XMPlaneDotCoord(P, LinePoint1);
    VT = XMVectorDivide(VT, D);

    XMVECTOR Point = XMVectorSubtract(LinePoint2, LinePoint1);
    Point = XMVectorMultiplyAdd(Point, VT, LinePoint1);

    const XMVECTOR Zero = XMVectorZero();
    XMVECTOR Control = XMVectorNearEqual(D, Zero, g_XMEpsilon.v);

    return XMVectorSelect(Point, g_XMQNaN.v, Control);
}

//------------------------------------------------------------------------------

inline void __vectorcall XMPlaneIntersectPlane
(
    XMVECTOR* pLinePoint1,
    XMVECTOR* pLinePoint2,
    FXMVECTOR  P1,
    FXMVECTOR  P2
)
{
    (void)( (!!(pLinePoint1)) || (_wassert(L"pLinePoint1", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(1209)), 0) );
    (void)( (!!(pLinePoint2)) || (_wassert(L"pLinePoint2", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(1210)), 0) );

    XMVECTOR V1 = XMVector3Cross(P2, P1);

    XMVECTOR LengthSq = XMVector3LengthSq(V1);

    XMVECTOR V2 = XMVector3Cross(P2, V1);

    XMVECTOR P1W = XMVectorSplatW(P1);
    XMVECTOR Point = XMVectorMultiply(V2, P1W);

    XMVECTOR V3 = XMVector3Cross(V1, P1);

    XMVECTOR P2W = XMVectorSplatW(P2);
    Point = XMVectorMultiplyAdd(V3, P2W, Point);

    XMVECTOR LinePoint1 = XMVectorDivide(Point, LengthSq);

    XMVECTOR LinePoint2 = XMVectorAdd(LinePoint1, V1);

    XMVECTOR Control = XMVectorLessOrEqual(LengthSq, g_XMEpsilon.v);
    *pLinePoint1 = XMVectorSelect(LinePoint1,g_XMQNaN.v, Control);
    *pLinePoint2 = XMVectorSelect(LinePoint2,g_XMQNaN.v, Control);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMPlaneTransform
(
    FXMVECTOR P,
    FXMMATRIX M
)
{
    XMVECTOR W = XMVectorSplatW(P);
    XMVECTOR Z = XMVectorSplatZ(P);
    XMVECTOR Y = XMVectorSplatY(P);
    XMVECTOR X = XMVectorSplatX(P);

    XMVECTOR Result = XMVectorMultiply(W, M.r[3]);
    Result = XMVectorMultiplyAdd(Z, M.r[2], Result);
    Result = XMVectorMultiplyAdd(Y, M.r[1], Result);
    Result = XMVectorMultiplyAdd(X, M.r[0], Result);
    return Result;
}

//------------------------------------------------------------------------------

inline XMFLOAT4* __vectorcall XMPlaneTransformStream
(
    XMFLOAT4*       pOutputStream,
    size_t          OutputStride,
    const XMFLOAT4* pInputStream,
    size_t          InputStride,
    size_t          PlaneCount,
    FXMMATRIX       M
)
{
    return XMVector4TransformStream(pOutputStream,
                                    OutputStride,
                                    pInputStream,
                                    InputStride,
                                    PlaneCount,
                                    M);
}

//------------------------------------------------------------------------------
// Conversion operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMPlaneFromPointNormal
(
    FXMVECTOR Point,
    FXMVECTOR Normal
)
{
    XMVECTOR W = XMVector3Dot(Point, Normal);
    W = XMVectorNegate(W);
    return XMVectorSelect(W, Normal, g_XMSelect1110.v);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMPlaneFromPoints
(
    FXMVECTOR Point1,
    FXMVECTOR Point2,
    FXMVECTOR Point3
)
{
    XMVECTOR V21 = XMVectorSubtract(Point1, Point2);
    XMVECTOR V31 = XMVectorSubtract(Point1, Point3);

    XMVECTOR N = XMVector3Cross(V21, V31);
    N = XMVector3Normalize(N);

    XMVECTOR D = XMPlaneDotNormal(N, Point1);
    D = XMVectorNegate(D);

    XMVECTOR Result = XMVectorSelect(D, N, g_XMSelect1110.v);

    return Result;
}

/****************************************************************************
 *
 * Color
 *
 ****************************************************************************/

//------------------------------------------------------------------------------
// Comparison operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline bool __vectorcall XMColorEqual
(
    FXMVECTOR C1,
    FXMVECTOR C2
)
{
    return XMVector4Equal(C1, C2);
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMColorNotEqual
(
    FXMVECTOR C1,
    FXMVECTOR C2
)
{
    return XMVector4NotEqual(C1, C2);
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMColorGreater
(
    FXMVECTOR C1,
    FXMVECTOR C2
)
{
    return XMVector4Greater(C1, C2);
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMColorGreaterOrEqual
(
    FXMVECTOR C1,
    FXMVECTOR C2
)
{
    return XMVector4GreaterOrEqual(C1, C2);
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMColorLess
(
    FXMVECTOR C1,
    FXMVECTOR C2
)
{
    return XMVector4Less(C1, C2);
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMColorLessOrEqual
(
    FXMVECTOR C1,
    FXMVECTOR C2
)
{
    return XMVector4LessOrEqual(C1, C2);
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMColorIsNaN
(
    FXMVECTOR C
)
{
    return XMVector4IsNaN(C);
}

//------------------------------------------------------------------------------

inline bool __vectorcall XMColorIsInfinite
(
    FXMVECTOR C
)
{
    return XMVector4IsInfinite(C);
}

//------------------------------------------------------------------------------
// Computation operations
//------------------------------------------------------------------------------

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMColorNegative
(
    FXMVECTOR vColor
)
{








#line 1431 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"


#line 1434 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
    // Negate only x,y and z.
    XMVECTOR vTemp = _mm_xor_ps(vColor,g_XMNegate3);
    // Add 1,1,1,0 to -x,-y,-z,w
    return _mm_add_ps(vTemp,g_XMOne3);
#line 1439 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMColorModulate
(
    FXMVECTOR C1,
    FXMVECTOR C2
)
{
    return XMVectorMultiply(C1, C2);
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMColorAdjustSaturation
(
    FXMVECTOR vColor,
    float    fSaturation
)
{
    // Luminance = 0.2125f * C[0] + 0.7154f * C[1] + 0.0721f * C[2];
    // Result = (C - Luminance) * Saturation + Luminance;

    const XMVECTORF32 gvLuminance = { { { 0.2125f, 0.7154f, 0.0721f, 0.0f } } };








#line 1473 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"




#line 1478 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
    XMVECTOR vLuminance = XMVector3Dot( vColor, gvLuminance );
// Splat fSaturation
    XMVECTOR vSaturation = _mm_set_ps1(fSaturation);
// vResult = ((vColor-vLuminance)*vSaturation)+vLuminance;
    XMVECTOR vResult = _mm_sub_ps(vColor,vLuminance);
    vResult = _mm_mul_ps(vResult,vSaturation);
    vResult = _mm_add_ps(vResult,vLuminance);
// Retain w from the source color
    vLuminance = _mm_shuffle_ps(vResult,vColor,(((3) << 6) | ((2) << 4) | ((2) << 2) | ((2))));   // x = vResult.z,y = vResult.z,z = vColor.z,w=vColor.w
    vResult = _mm_shuffle_ps(vResult,vLuminance,(((3) << 6) | ((0) << 4) | ((1) << 2) | ((0))));  // x = vResult.x,y = vResult.y,z = vResult.z,w=vColor.w
    return vResult;
#line 1490 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMColorAdjustContrast
(
    FXMVECTOR vColor,
    float    fContrast
)
{
    // Result = (vColor - 0.5f) * fContrast + 0.5f;









#line 1511 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"



#line 1515 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
    XMVECTOR vScale = _mm_set_ps1(fContrast);           // Splat the scale
    XMVECTOR vResult = _mm_sub_ps(vColor,g_XMOneHalf);  // Subtract 0.5f from the source (Saving source)
    vResult = _mm_mul_ps(vResult,vScale);               // Mul by scale
    vResult = _mm_add_ps(vResult,g_XMOneHalf);          // Add 0.5f
// Retain w from the source color
    vScale = _mm_shuffle_ps(vResult,vColor,(((3) << 6) | ((2) << 4) | ((2) << 2) | ((2))));   // x = vResult.z,y = vResult.z,z = vColor.z,w=vColor.w
    vResult = _mm_shuffle_ps(vResult,vScale,(((3) << 6) | ((0) << 4) | ((1) << 2) | ((0))));  // x = vResult.x,y = vResult.y,z = vResult.z,w=vColor.w
    return vResult;
#line 1524 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMColorRGBToHSL( FXMVECTOR rgb )
{
    XMVECTOR r = XMVectorSplatX( rgb );
    XMVECTOR g = XMVectorSplatY( rgb );
    XMVECTOR b = XMVectorSplatZ( rgb );

    XMVECTOR min = XMVectorMin( r, XMVectorMin( g, b ) );
    XMVECTOR max = XMVectorMax( r, XMVectorMax( g, b ) );

    XMVECTOR l = XMVectorMultiply( XMVectorAdd( min, max ), g_XMOneHalf );

    XMVECTOR d = XMVectorSubtract( max, min );

    XMVECTOR la = XMVectorSelect( rgb, l, g_XMSelect1110 );

    if ( XMVector3Less( d, g_XMEpsilon ) )
    {
        // Achromatic, assume H and S of 0
        return XMVectorSelect( la, g_XMZero, g_XMSelect1100 );
    }
    else
    {
        XMVECTOR s, h;

        XMVECTOR d2 = XMVectorAdd( min, max );

        if ( XMVector3Greater( l, g_XMOneHalf ) )
        {
            // d / (2-max-min)
            s = XMVectorDivide( d, XMVectorSubtract( g_XMTwo, d2 ) );
        }
        else
        {
            // d / (max+min)
            s = XMVectorDivide( d, d2 );
        }

        if ( XMVector3Equal( r, max ) )
        {
            // Red is max
            h = XMVectorDivide( XMVectorSubtract( g, b ), d );
        }
        else if ( XMVector3Equal( g, max ) )
        {
            // Green is max
            h = XMVectorDivide( XMVectorSubtract( b, r ), d );
            h = XMVectorAdd( h, g_XMTwo );
        }
        else
        {
            // Blue is max
            h = XMVectorDivide( XMVectorSubtract( r, g ), d );
            h = XMVectorAdd( h, g_XMFour );
        }

        h = XMVectorDivide( h, g_XMSix );

        if ( XMVector3Less( h, g_XMZero ) )
            h = XMVectorAdd( h, g_XMOne );

        XMVECTOR lha = XMVectorSelect( la, h, g_XMSelect1100 );
        return XMVectorSelect( s, lha, g_XMSelect1011 );
    }
}

//------------------------------------------------------------------------------

namespace Internal
{

inline XMVECTOR __vectorcall XMColorHue2Clr( FXMVECTOR p, FXMVECTOR q, FXMVECTOR h )
{
    static const XMVECTORF32 oneSixth = { { { 1.0f / 6.0f, 1.0f / 6.0f, 1.0f / 6.0f, 1.0f / 6.0f } } };
    static const XMVECTORF32 twoThirds = { { { 2.0f / 3.0f, 2.0f / 3.0f, 2.0f / 3.0f, 2.0f / 3.0f } } };

    XMVECTOR t = h;

    if ( XMVector3Less( t, g_XMZero ) )
        t = XMVectorAdd( t, g_XMOne );

    if ( XMVector3Greater( t, g_XMOne ) )
        t = XMVectorSubtract( t, g_XMOne );

    if ( XMVector3Less( t, oneSixth ) )
    {
        // p + (q - p) * 6 * t
        XMVECTOR t1 = XMVectorSubtract( q, p );
        XMVECTOR t2 = XMVectorMultiply( g_XMSix, t );
        return XMVectorMultiplyAdd( t1, t2, p );
    }

    if ( XMVector3Less( t, g_XMOneHalf ) )
        return q;

    if ( XMVector3Less( t, twoThirds ) )
    {
        // p + (q - p) * 6 * (2/3 - t)
        XMVECTOR t1 = XMVectorSubtract( q, p );
        XMVECTOR t2 = XMVectorMultiply( g_XMSix, XMVectorSubtract( twoThirds, t ) );
        return XMVectorMultiplyAdd( t1, t2, p );
    }

    return p;
}

} // namespace Internal

inline XMVECTOR __vectorcall XMColorHSLToRGB( FXMVECTOR hsl )
{
    static const XMVECTORF32 oneThird = { { { 1.0f / 3.0f, 1.0f / 3.0f, 1.0f / 3.0f, 1.0f / 3.0f } } };

    XMVECTOR s = XMVectorSplatY( hsl );
    XMVECTOR l = XMVectorSplatZ( hsl );

    if ( XMVector3NearEqual( s, g_XMZero, g_XMEpsilon ) )
    {
        // Achromatic
        return XMVectorSelect( hsl, l, g_XMSelect1110 );
    }
    else
    {
        XMVECTOR h = XMVectorSplatX( hsl );

        XMVECTOR q;
        if ( XMVector3Less( l, g_XMOneHalf ) )
        {
            q = XMVectorMultiply( l, XMVectorAdd ( g_XMOne, s ) );
        }
        else
        {
            q = XMVectorSubtract( XMVectorAdd( l, s ), XMVectorMultiply( l, s ) );
        }

        XMVECTOR p = XMVectorSubtract( XMVectorMultiply( g_XMTwo, l ), q );

        XMVECTOR r = DirectX::Internal::XMColorHue2Clr( p, q, XMVectorAdd( h, oneThird ) );
        XMVECTOR g = DirectX::Internal::XMColorHue2Clr( p, q, h );
        XMVECTOR b = DirectX::Internal::XMColorHue2Clr( p, q, XMVectorSubtract( h, oneThird ) );

        XMVECTOR rg = XMVectorSelect( g, r, g_XMSelect1000 );
        XMVECTOR ba = XMVectorSelect( hsl, b, g_XMSelect1110 );

        return XMVectorSelect( ba, rg, g_XMSelect1100 );
    }
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMColorRGBToHSV( FXMVECTOR rgb )
{
    XMVECTOR r = XMVectorSplatX( rgb );
    XMVECTOR g = XMVectorSplatY( rgb );
    XMVECTOR b = XMVectorSplatZ( rgb );

    XMVECTOR min = XMVectorMin( r, XMVectorMin( g, b ) );
    XMVECTOR v = XMVectorMax( r, XMVectorMax( g, b ) );

    XMVECTOR d = XMVectorSubtract( v, min );

    XMVECTOR s = ( XMVector3NearEqual( v, g_XMZero, g_XMEpsilon ) ) ? g_XMZero : XMVectorDivide( d, v );

    if ( XMVector3Less( d, g_XMEpsilon ) )
    {
        // Achromatic, assume H of 0
        XMVECTOR hv = XMVectorSelect( v, g_XMZero, g_XMSelect1000 );
        XMVECTOR hva = XMVectorSelect( rgb, hv, g_XMSelect1110 );
        return XMVectorSelect( s, hva, g_XMSelect1011 );
    }
    else
    {
        XMVECTOR h;

        if ( XMVector3Equal( r, v ) )
        {
            // Red is max
            h = XMVectorDivide( XMVectorSubtract( g, b ), d );

            if ( XMVector3Less( g, b ) )
                h = XMVectorAdd( h, g_XMSix );
        }
        else if ( XMVector3Equal( g, v ) )
        {
            // Green is max
            h = XMVectorDivide( XMVectorSubtract( b, r ), d );
            h = XMVectorAdd( h, g_XMTwo );
        }
        else
        {
            // Blue is max
            h = XMVectorDivide( XMVectorSubtract( r, g ), d );
            h = XMVectorAdd( h, g_XMFour );
        }

        h = XMVectorDivide( h, g_XMSix );

        XMVECTOR hv = XMVectorSelect( v, h, g_XMSelect1000 );
        XMVECTOR hva = XMVectorSelect( rgb, hv, g_XMSelect1110 );
        return XMVectorSelect( s, hva, g_XMSelect1011 );
    }
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMColorHSVToRGB( FXMVECTOR hsv )
{
    XMVECTOR h = XMVectorSplatX( hsv );
    XMVECTOR s = XMVectorSplatY( hsv );
    XMVECTOR v = XMVectorSplatZ( hsv );

    XMVECTOR h6 = XMVectorMultiply( h, g_XMSix );

    XMVECTOR i = XMVectorFloor( h6 );
    XMVECTOR f = XMVectorSubtract( h6, i );

    // p = v* (1-s)
    XMVECTOR p = XMVectorMultiply( v, XMVectorSubtract( g_XMOne, s ) );

    // q = v*(1-f*s)
    XMVECTOR q = XMVectorMultiply( v, XMVectorSubtract( g_XMOne, XMVectorMultiply( f, s ) ) );

    // t = v*(1 - (1-f)*s)
    XMVECTOR t = XMVectorMultiply( v, XMVectorSubtract( g_XMOne, XMVectorMultiply( XMVectorSubtract( g_XMOne, f ), s ) ) );

    auto ii = static_cast<int>( XMVectorGetX( XMVectorMod( i, g_XMSix ) ) );

    XMVECTOR _rgb;

    switch (ii)
    {
    case 0: // rgb = vtp
        {
            XMVECTOR vt = XMVectorSelect( t, v, g_XMSelect1000 );
            _rgb = XMVectorSelect( p, vt, g_XMSelect1100 );
        }
        break;
    case 1: // rgb = qvp
        {
            XMVECTOR qv = XMVectorSelect( v, q, g_XMSelect1000 );
            _rgb = XMVectorSelect( p, qv, g_XMSelect1100 );
        }
        break;
    case 2: // rgb = pvt
        {
            XMVECTOR pv = XMVectorSelect( v, p, g_XMSelect1000 );
            _rgb = XMVectorSelect( t, pv, g_XMSelect1100 );
        }
        break;
    case 3: // rgb = pqv
        {
            XMVECTOR pq = XMVectorSelect( q, p, g_XMSelect1000 );
            _rgb = XMVectorSelect( v, pq, g_XMSelect1100 );
        }
        break;
    case 4: // rgb = tpv
        {
            XMVECTOR tp = XMVectorSelect( p, t, g_XMSelect1000 );
            _rgb = XMVectorSelect( v, tp, g_XMSelect1100 );
        }
        break;
    default: // rgb = vpq
        {
            XMVECTOR vp = XMVectorSelect( p, v, g_XMSelect1000 );
            _rgb = XMVectorSelect( q, vp, g_XMSelect1100 );
        }
        break;
    }

    return XMVectorSelect( hsv, _rgb, g_XMSelect1110 );
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMColorRGBToYUV( FXMVECTOR rgb )
{
    static const XMVECTORF32 Scale0 = { { { 0.299f, -0.147f, 0.615f, 0.0f } } };
    static const XMVECTORF32 Scale1 = { { { 0.587f, -0.289f, -0.515f, 0.0f } } };
    static const XMVECTORF32 Scale2 = { { { 0.114f, 0.436f, -0.100f, 0.0f } } };

    XMMATRIX M( Scale0, Scale1, Scale2, g_XMZero );
    XMVECTOR clr = XMVector3Transform( rgb, M );

    return XMVectorSelect( rgb, clr, g_XMSelect1110 );
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMColorYUVToRGB( FXMVECTOR yuv )
{
    static const XMVECTORF32 Scale1 = { { { 0.0f, -0.395f, 2.032f, 0.0f } } };
    static const XMVECTORF32 Scale2 = { { { 1.140f, -0.581f, 0.0f, 0.0f } } };

    XMMATRIX M( g_XMOne, Scale1, Scale2, g_XMZero );
    XMVECTOR clr = XMVector3Transform( yuv, M );

    return XMVectorSelect( yuv, clr, g_XMSelect1110 );
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMColorRGBToYUV_HD( FXMVECTOR rgb )
{
    static const XMVECTORF32 Scale0 = { { { 0.2126f, -0.0997f, 0.6150f, 0.0f } } };
    static const XMVECTORF32 Scale1 = { { { 0.7152f, -0.3354f, -0.5586f, 0.0f } } };
    static const XMVECTORF32 Scale2 = { { { 0.0722f, 0.4351f, -0.0564f, 0.0f } } };

    XMMATRIX M( Scale0, Scale1, Scale2, g_XMZero );
    XMVECTOR clr = XMVector3Transform( rgb, M );

    return XMVectorSelect( rgb, clr, g_XMSelect1110 );
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMColorYUVToRGB_HD( FXMVECTOR yuv )
{
    static const XMVECTORF32 Scale1 = { { { 0.0f, -0.2153f, 2.1324f, 0.0f } } };
    static const XMVECTORF32 Scale2 = { { { 1.2803f, -0.3806f, 0.0f, 0.0f } } };

    XMMATRIX M( g_XMOne, Scale1, Scale2, g_XMZero );
    XMVECTOR clr = XMVector3Transform( yuv, M );

    return XMVectorSelect( yuv, clr, g_XMSelect1110 );
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMColorRGBToXYZ( FXMVECTOR rgb )
{
    static const XMVECTORF32 Scale0 = { { { 0.4887180f, 0.1762044f, 0.0000000f, 0.0f } } };
    static const XMVECTORF32 Scale1 = { { { 0.3106803f, 0.8129847f, 0.0102048f, 0.0f } } };
    static const XMVECTORF32 Scale2 = { { { 0.2006017f, 0.0108109f, 0.9897952f, 0.0f } } };
    static const XMVECTORF32 Scale  = { { { 1.f / 0.17697f, 1.f / 0.17697f, 1.f / 0.17697f, 0.0f } } };

    XMMATRIX M( Scale0, Scale1, Scale2, g_XMZero );
    XMVECTOR clr = XMVectorMultiply( XMVector3Transform( rgb, M ), Scale );

    return XMVectorSelect( rgb, clr, g_XMSelect1110 );
}

inline XMVECTOR __vectorcall XMColorXYZToRGB( FXMVECTOR xyz )
{
    static const XMVECTORF32 Scale0 = { { { 2.3706743f, -0.5138850f, 0.0052982f, 0.0f } } };
    static const XMVECTORF32 Scale1 = { { { -0.9000405f, 1.4253036f, -0.0146949f, 0.0f } } };
    static const XMVECTORF32 Scale2 = { { { -0.4706338f, 0.0885814f, 1.0093968f, 0.0f } } };
    static const XMVECTORF32 Scale  = { { { 0.17697f, 0.17697f, 0.17697f, 0.0f } } };

    XMMATRIX M( Scale0, Scale1, Scale2, g_XMZero );
    XMVECTOR clr = XMVector3Transform( XMVectorMultiply( xyz, Scale ), M );

    return XMVectorSelect( xyz, clr, g_XMSelect1110 );
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMColorXYZToSRGB( FXMVECTOR xyz )
{
    static const XMVECTORF32 Scale0 = { { { 3.2406f, -0.9689f, 0.0557f, 0.0f } } };
    static const XMVECTORF32 Scale1 = { { { -1.5372f, 1.8758f, -0.2040f, 0.0f } } };
    static const XMVECTORF32 Scale2 = { { { -0.4986f, 0.0415f, 1.0570f, 0.0f } } };
    static const XMVECTORF32 Cutoff = { { { 0.0031308f, 0.0031308f, 0.0031308f, 0.0f } } };
    static const XMVECTORF32 Exp    = { { { 1.0f / 2.4f, 1.0f / 2.4f, 1.0f / 2.4f, 1.0f } } };

    XMMATRIX M( Scale0, Scale1, Scale2, g_XMZero );
    XMVECTOR lclr = XMVector3Transform( xyz, M );

    XMVECTOR sel = XMVectorGreater( lclr, Cutoff );

    // clr = 12.92 * lclr for lclr <= 0.0031308f
    XMVECTOR smallC = XMVectorMultiply( lclr, g_XMsrgbScale );

    // clr = (1+a)*pow(lclr, 1/2.4) - a for lclr > 0.0031308 (where a = 0.055)
    XMVECTOR largeC = XMVectorSubtract( XMVectorMultiply( g_XMsrgbA1, XMVectorPow( lclr, Exp ) ), g_XMsrgbA );

    XMVECTOR clr = XMVectorSelect( smallC, largeC, sel );

    return XMVectorSelect( xyz, clr, g_XMSelect1110 );
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMColorSRGBToXYZ( FXMVECTOR srgb )
{
    static const XMVECTORF32 Scale0 = { { { 0.4124f, 0.2126f, 0.0193f, 0.0f } } };
    static const XMVECTORF32 Scale1 = { { { 0.3576f, 0.7152f, 0.1192f, 0.0f } } };
    static const XMVECTORF32 Scale2 = { { { 0.1805f, 0.0722f, 0.9505f, 0.0f } } };
    static const XMVECTORF32 Cutoff = { { { 0.04045f, 0.04045f, 0.04045f, 0.0f } } };
    static const XMVECTORF32 Exp    = { { { 2.4f, 2.4f, 2.4f, 1.0f } } };

    XMVECTOR sel = XMVectorGreater( srgb, Cutoff );

    // lclr = clr / 12.92
    XMVECTOR smallC = XMVectorDivide( srgb, g_XMsrgbScale );

    // lclr = pow( (clr + a) / (1+a), 2.4 )
    XMVECTOR largeC = XMVectorPow( XMVectorDivide( XMVectorAdd( srgb, g_XMsrgbA ), g_XMsrgbA1 ), Exp );

    XMVECTOR lclr = XMVectorSelect( smallC, largeC, sel );

    XMMATRIX M( Scale0, Scale1, Scale2, g_XMZero );
    XMVECTOR clr = XMVector3Transform( lclr, M );

    return XMVectorSelect( srgb, clr, g_XMSelect1110 );
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMColorRGBToSRGB( FXMVECTOR rgb )
{
    static const XMVECTORF32 Cutoff   = { { { 0.0031308f, 0.0031308f, 0.0031308f, 1.f } } };
    static const XMVECTORF32 Linear   = { { { 12.92f, 12.92f, 12.92f, 1.f } } };
    static const XMVECTORF32 Scale    = { { { 1.055f, 1.055f, 1.055f, 1.f } } };
    static const XMVECTORF32 Bias     = { { { 0.055f, 0.055f, 0.055f, 0.f } } };
    static const XMVECTORF32 InvGamma = { { { 1.0f / 2.4f, 1.0f / 2.4f, 1.0f / 2.4f, 1.f } } };

    XMVECTOR V = XMVectorSaturate(rgb);
    XMVECTOR V0 = XMVectorMultiply( V, Linear );
    XMVECTOR V1 = XMVectorSubtract( XMVectorMultiply( Scale, XMVectorPow( V, InvGamma ) ), Bias );
    XMVECTOR select = XMVectorLess( V, Cutoff );
    V = XMVectorSelect( V1, V0, select );
    return XMVectorSelect( rgb, V, g_XMSelect1110 );
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMColorSRGBToRGB( FXMVECTOR srgb )
{
    static const XMVECTORF32 Cutoff  = { { { 0.04045f, 0.04045f, 0.04045f, 1.f } } };
    static const XMVECTORF32 ILinear = { { { 1.f / 12.92f, 1.f / 12.92f, 1.f / 12.92f, 1.f } } };
    static const XMVECTORF32 Scale   = { { { 1.f / 1.055f, 1.f / 1.055f, 1.f / 1.055f, 1.f } } };
    static const XMVECTORF32 Bias    = { { { 0.055f, 0.055f, 0.055f, 0.f } } };
    static const XMVECTORF32 Gamma   = { { { 2.4f, 2.4f, 2.4f, 1.f } } };

    XMVECTOR V = XMVectorSaturate(srgb);
    XMVECTOR V0 = XMVectorMultiply( V, ILinear );
    XMVECTOR V1 = XMVectorPow( XMVectorMultiply( XMVectorAdd( V, Bias ), Scale ), Gamma );
    XMVECTOR select = XMVectorGreater( V, Cutoff );
    V = XMVectorSelect( V0, V1, select );
    return XMVectorSelect( srgb, V, g_XMSelect1110 );
}

/****************************************************************************
 *
 * Miscellaneous
 *
 ****************************************************************************/

//------------------------------------------------------------------------------

inline bool XMVerifyCPUSupport()
{

    int CPUInfo[4] = { -1 };



    __cpuid(CPUInfo, 0);
#line 1985 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"





    if (CPUInfo[0] < 1)
        return false;
#line 1993 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"




    __cpuid(CPUInfo, 1);
#line 1999 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"





#line 2005 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"


#line 2008 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"


#line 2011 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"


#line 2014 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"


#line 2017 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"


#line 2020 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"


#line 2023 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"

    // The x64 processor model requires SSE2 support, but no harm in checking
    if ((CPUInfo[3] & 0x6000000) != 0x6000000)
        return false; // No SSE2/SSE support









#line 2037 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"

    return true;






#line 2046 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
}

//------------------------------------------------------------------------------

inline XMVECTOR __vectorcall XMFresnelTerm
(
    FXMVECTOR CosIncidentAngle,
    FXMVECTOR RefractionIndex
)
{
    (void)( (!!(!XMVector4IsInfinite(CosIncidentAngle))) || (_wassert(L"!XMVector4IsInfinite(CosIncidentAngle)", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(2056)), 0) );

    // Result = 0.5f * (g - c)^2 / (g + c)^2 * ((c * (g + c) - 1)^2 / (c * (g - c) + 1)^2 + 1) where
    // c = CosIncidentAngle
    // g = sqrt(c^2 + RefractionIndex^2 - 1)






























#line 2092 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
    // G = sqrt(abs((RefractionIndex^2-1) + CosIncidentAngle^2))
    XMVECTOR G = _mm_mul_ps(RefractionIndex,RefractionIndex);
    XMVECTOR vTemp = _mm_mul_ps(CosIncidentAngle,CosIncidentAngle);
    G = _mm_sub_ps(G,g_XMOne);
    vTemp = _mm_add_ps(vTemp,G);
    // max((0-vTemp),vTemp) == abs(vTemp)
    // The abs is needed to deal with refraction and cosine being zero
    G = _mm_setzero_ps();
    G = _mm_sub_ps(G,vTemp);
    G = _mm_max_ps(G,vTemp);
    // Last operation, the sqrt()
    G = _mm_sqrt_ps(G);

    // Calc G-C and G+C
    XMVECTOR GAddC = _mm_add_ps(G,CosIncidentAngle);
    XMVECTOR GSubC = _mm_sub_ps(G,CosIncidentAngle);
    // Perform the term (0.5f *(g - c)^2) / (g + c)^2
    XMVECTOR vResult = _mm_mul_ps(GSubC,GSubC);
    vTemp = _mm_mul_ps(GAddC,GAddC);
    vResult = _mm_mul_ps(vResult,g_XMOneHalf);
    vResult = _mm_div_ps(vResult,vTemp);
    // Perform the term ((c * (g + c) - 1)^2 / (c * (g - c) + 1)^2 + 1)
    GAddC = _mm_mul_ps(GAddC,CosIncidentAngle);
    GSubC = _mm_mul_ps(GSubC,CosIncidentAngle);
    GAddC = _mm_sub_ps(GAddC,g_XMOne);
    GSubC = _mm_add_ps(GSubC,g_XMOne);
    GAddC = _mm_mul_ps(GAddC,GAddC);
    GSubC = _mm_mul_ps(GSubC,GSubC);
    GAddC = _mm_div_ps(GAddC,GSubC);
    GAddC = _mm_add_ps(GAddC,g_XMOne);
    // Multiply the two term parts
    vResult = _mm_mul_ps(vResult,GAddC);
    // Clamp to 0.0 - 1.0f
    vResult = _mm_max_ps(vResult,g_XMZero);
    vResult = _mm_min_ps(vResult,g_XMOne);
    return vResult;
#line 2129 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl"
}

//------------------------------------------------------------------------------

inline bool XMScalarNearEqual
(
    float S1,
    float S2,
    float Epsilon
)
{
    float Delta = S1 - S2;
    return (fabsf(Delta) <= Epsilon);
}

//------------------------------------------------------------------------------
// Modulo the range of the given angle such that -XM_PI <= Angle < XM_PI
inline float XMScalarModAngle
(
    float Angle
)
{
    // Note: The modulo is performed with unsigned math only to work
    // around a precision error on numbers that are close to PI

    // Normalize the range from 0.0f to XM_2PI
    Angle = Angle + XM_PI;
    // Perform the modulo, unsigned
    float fTemp = fabsf(Angle);
    fTemp = fTemp - (XM_2PI * static_cast<float>(static_cast<int32_t>(fTemp/XM_2PI)));
    // Restore the number to the range of -XM_PI to XM_PI-epsilon
    fTemp = fTemp - XM_PI;
    // If the modulo'd value was negative, restore negation
    if (Angle<0.0f) {
        fTemp = -fTemp;
    }
    return fTemp;
}

//------------------------------------------------------------------------------

inline float XMScalarSin
(
    float Value
)
{
    // Map Value to y in [-pi,pi], x = 2*pi*quotient + remainder.
    float quotient = XM_1DIV2PI*Value;
    if (Value >= 0.0f)
    {
        quotient = static_cast<float>(static_cast<int>(quotient + 0.5f));
    }
    else
    {
        quotient = static_cast<float>(static_cast<int>(quotient - 0.5f));
    }
    float y = Value - XM_2PI*quotient;

    // Map y to [-pi/2,pi/2] with sin(y) = sin(Value).
    if (y > XM_PIDIV2)
    {
        y = XM_PI - y;
    }
    else if (y < -XM_PIDIV2)
    {
        y = -XM_PI - y;
    }

    // 11-degree minimax approximation
    float y2 = y * y;
    return ( ( ( ( (-2.3889859e-08f * y2 + 2.7525562e-06f) * y2 - 0.00019840874f ) * y2 + 0.0083333310f ) * y2 - 0.16666667f ) * y2 + 1.0f ) * y;
}

//------------------------------------------------------------------------------

inline float XMScalarSinEst
(
    float Value
)
{
    // Map Value to y in [-pi,pi], x = 2*pi*quotient + remainder.
    float quotient = XM_1DIV2PI*Value;
    if (Value >= 0.0f)
    {
        quotient = static_cast<float>(static_cast<int>(quotient + 0.5f));
    }
    else
    {
        quotient = static_cast<float>(static_cast<int>(quotient - 0.5f));
    }
    float y = Value - XM_2PI*quotient;

    // Map y to [-pi/2,pi/2] with sin(y) = sin(Value).
    if (y > XM_PIDIV2)
    {
        y = XM_PI - y;
    }
    else if (y < -XM_PIDIV2)
    {
        y = -XM_PI - y;
    }

    // 7-degree minimax approximation
    float y2 = y * y;
    return ( ( ( -0.00018524670f * y2 + 0.0083139502f ) * y2 - 0.16665852f ) * y2 + 1.0f ) * y;
}

//------------------------------------------------------------------------------

inline float XMScalarCos
(
    float Value
)
{
    // Map Value to y in [-pi,pi], x = 2*pi*quotient + remainder.
    float quotient = XM_1DIV2PI*Value;
    if (Value >= 0.0f)
    {
        quotient = static_cast<float>(static_cast<int>(quotient + 0.5f));
    }
    else
    {
        quotient = static_cast<float>(static_cast<int>(quotient - 0.5f));
    }
    float y = Value - XM_2PI*quotient;

    // Map y to [-pi/2,pi/2] with cos(y) = sign*cos(x).
    float sign;
    if (y > XM_PIDIV2)
    {
        y = XM_PI - y;
        sign = -1.0f;
    }
    else if (y < -XM_PIDIV2)
    {
        y = -XM_PI - y;
        sign = -1.0f;
    }
    else
    {
        sign = +1.0f;
    }

    // 10-degree minimax approximation
    float y2 = y*y;
    float p = ( ( ( ( -2.6051615e-07f * y2 + 2.4760495e-05f ) * y2 - 0.0013888378f ) * y2 + 0.041666638f ) * y2 - 0.5f ) * y2 + 1.0f;
    return sign*p;
}

//------------------------------------------------------------------------------

inline float XMScalarCosEst
(
    float Value
)
{
    // Map Value to y in [-pi,pi], x = 2*pi*quotient + remainder.
    float quotient = XM_1DIV2PI*Value;
    if (Value >= 0.0f)
    {
        quotient = static_cast<float>(static_cast<int>(quotient + 0.5f));
    }
    else
    {
        quotient = static_cast<float>(static_cast<int>(quotient - 0.5f));
    }
    float y = Value - XM_2PI*quotient;

    // Map y to [-pi/2,pi/2] with cos(y) = sign*cos(x).
    float sign;
    if (y > XM_PIDIV2)
    {
        y = XM_PI - y;
        sign = -1.0f;
    }
    else if (y < -XM_PIDIV2)
    {
        y = -XM_PI - y;
        sign = -1.0f;
    }
    else
    {
        sign = +1.0f;
    }

    // 6-degree minimax approximation
    float y2 = y * y;
    float p = ( ( -0.0012712436f * y2 + 0.041493919f ) * y2 - 0.49992746f ) * y2 + 1.0f;
    return sign*p;
}

//------------------------------------------------------------------------------


inline void XMScalarSinCos
(
    float* pSin,
    float* pCos,
    float  Value
)
{
    (void)( (!!(pSin)) || (_wassert(L"pSin", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(2330)), 0) );
    (void)( (!!(pCos)) || (_wassert(L"pCos", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(2331)), 0) );

    // Map Value to y in [-pi,pi], x = 2*pi*quotient + remainder.
    float quotient = XM_1DIV2PI*Value;
    if (Value >= 0.0f)
    {
        quotient = static_cast<float>(static_cast<int>(quotient + 0.5f));
    }
    else
    {
        quotient = static_cast<float>(static_cast<int>(quotient - 0.5f));
    }
    float y = Value - XM_2PI*quotient;

    // Map y to [-pi/2,pi/2] with sin(y) = sin(Value).
    float sign;
    if (y > XM_PIDIV2)
    {
        y = XM_PI - y;
        sign = -1.0f;
    }
    else if (y < -XM_PIDIV2)
    {
        y = -XM_PI - y;
        sign = -1.0f;
    }
    else
    {
        sign = +1.0f;
    }

    float y2 = y * y;

    // 11-degree minimax approximation
    *pSin = ( ( ( ( (-2.3889859e-08f * y2 + 2.7525562e-06f) * y2 - 0.00019840874f ) * y2 + 0.0083333310f ) * y2 - 0.16666667f ) * y2 + 1.0f ) * y;

    // 10-degree minimax approximation
    float p = ( ( ( ( -2.6051615e-07f * y2 + 2.4760495e-05f ) * y2 - 0.0013888378f ) * y2 + 0.041666638f ) * y2 - 0.5f ) * y2 + 1.0f;
    *pCos = sign*p;
}

//------------------------------------------------------------------------------


inline void XMScalarSinCosEst
(
    float* pSin,
    float* pCos,
    float  Value
)
{
    (void)( (!!(pSin)) || (_wassert(L"pSin", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(2382)), 0) );
    (void)( (!!(pCos)) || (_wassert(L"pCos", L"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMathMisc.inl", (unsigned)(2383)), 0) );

    // Map Value to y in [-pi,pi], x = 2*pi*quotient + remainder.
    float quotient = XM_1DIV2PI*Value;
    if (Value >= 0.0f)
    {
        quotient = static_cast<float>(static_cast<int>(quotient + 0.5f));
    }
    else
    {
        quotient = static_cast<float>(static_cast<int>(quotient - 0.5f));
    }
    float y = Value - XM_2PI*quotient;

    // Map y to [-pi/2,pi/2] with sin(y) = sin(Value).
    float sign;
    if (y > XM_PIDIV2)
    {
        y = XM_PI - y;
        sign = -1.0f;
    }
    else if (y < -XM_PIDIV2)
    {
        y = -XM_PI - y;
        sign = -1.0f;
    }
    else
    {
        sign = +1.0f;
    }

    float y2 = y * y;

    // 7-degree minimax approximation
    *pSin = ( ( ( -0.00018524670f * y2 + 0.0083139502f ) * y2 - 0.16665852f ) * y2 + 1.0f ) * y;

    // 6-degree minimax approximation
    float p = ( ( -0.0012712436f * y2 + 0.041493919f ) * y2 - 0.49992746f ) * y2 + 1.0f;
    *pCos = sign*p;
}

//------------------------------------------------------------------------------

inline float XMScalarASin
(
    float Value
)
{
    // Clamp input to [-1,1].
    bool nonnegative = (Value >= 0.0f);
    float x = fabsf(Value);
    float omx = 1.0f - x;
    if (omx < 0.0f)
    {
        omx = 0.0f;
    }
    float root = sqrtf(omx);

    // 7-degree minimax approximation
    float result = ( ( ( ( ( ( -0.0012624911f * x + 0.0066700901f ) * x - 0.0170881256f ) * x + 0.0308918810f ) * x - 0.0501743046f ) * x + 0.0889789874f ) * x - 0.2145988016f ) * x + 1.5707963050f;
    result *= root;  // acos(|x|)

    // acos(x) = pi - acos(-x) when x < 0, asin(x) = pi/2 - acos(x)
    return (nonnegative ? XM_PIDIV2 - result : result - XM_PIDIV2);
}

//------------------------------------------------------------------------------

inline float XMScalarASinEst
(
    float Value
)
{
    // Clamp input to [-1,1].
    bool nonnegative = (Value >= 0.0f);
    float x = fabsf(Value);
    float omx = 1.0f - x;
    if (omx < 0.0f)
    {
        omx = 0.0f;
    }
    float root = sqrtf(omx);

    // 3-degree minimax approximation
    float result = ((-0.0187293f*x+0.0742610f)*x-0.2121144f)*x+1.5707288f;
    result *= root;  // acos(|x|)

    // acos(x) = pi - acos(-x) when x < 0, asin(x) = pi/2 - acos(x)
    return (nonnegative ? XM_PIDIV2 - result : result - XM_PIDIV2);
}

//------------------------------------------------------------------------------

inline float XMScalarACos
(
    float Value
)
{
    // Clamp input to [-1,1].
    bool nonnegative = (Value >= 0.0f);
    float x = fabsf(Value);
    float omx = 1.0f - x;
    if (omx < 0.0f)
    {
        omx = 0.0f;
    }
    float root = sqrtf(omx);

    // 7-degree minimax approximation
    float result = ( ( ( ( ( ( -0.0012624911f * x + 0.0066700901f ) * x - 0.0170881256f ) * x + 0.0308918810f ) * x - 0.0501743046f ) * x + 0.0889789874f ) * x - 0.2145988016f ) * x + 1.5707963050f;
    result *= root;

    // acos(x) = pi - acos(-x) when x < 0
    return (nonnegative ? result : XM_PI - result);
}

//------------------------------------------------------------------------------

inline float XMScalarACosEst
(
    float Value
)
{
    // Clamp input to [-1,1].
    bool nonnegative = (Value >= 0.0f);
    float x = fabsf(Value);
    float omx = 1.0f - x;
    if (omx < 0.0f)
    {
        omx = 0.0f;
    }
    float root = sqrtf(omx);

    // 3-degree minimax approximation
    float result = ( ( -0.0187293f * x + 0.0742610f ) * x - 0.2121144f ) * x + 1.5707288f;
    result *= root;

    // acos(x) = pi - acos(-x) when x < 0
    return (nonnegative ? result : XM_PI - result);
}

#pragma external_header(pop)
#line 2166 "C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\\DirectXMath.h"





#pragma warning(pop)

} // namespace DirectX

#pragma external_header(pop)
#line 4 "C:\\Users\\dboja\\source\\Electricity2\\Electricity2\\Math.h"

typedef DirectX::XMFLOAT2 Float2;
typedef DirectX::XMFLOAT2A Float2A;

typedef DirectX::XMFLOAT3 Float3;
typedef DirectX::XMFLOAT2A Float3A;

typedef DirectX::XMFLOAT4 Float4;
typedef DirectX::XMFLOAT4A Float4A;

typedef Float2 Vec2;
typedef Float3 Vec3;
typedef Float4 Vec4;
typedef DirectX::XMVECTOR FastVec;



typedef DirectX::XMMATRIX FASTMAT4;

typedef DirectX::XMFLOAT3X3 MAT3;
typedef DirectX::XMFLOAT4X4 MAT4;
typedef FASTMAT4 Transform;




namespace Electricity::Math
{
	template <typename Type>
	inline Type Min( const Type A, const Type B )
	{
		return ( A > B ? B : A );
	}

	template <typename Type>
	inline Type Max( const Type A, const Type B )
	{
		return ( A > B ? A : B );
	}
}
#line 4 "C:\\Users\\dboja\\source\\Electricity2\\Electricity2\\Vertex.h"
struct VertexPositionOnly
{
	Float3 m_Position;
	
	VertexPositionOnly() noexcept = default;
	VertexPositionOnly( const VertexPositionOnly& ) noexcept = default;
	VertexPositionOnly( float x, float y, float z ) noexcept;
	VertexPositionOnly( const Float3& position ) noexcept;

	void SetPosition( float x, float y, float z ) noexcept;
	void SetPosition( const Float3& pos ) noexcept;
};
#line 2 "C:\\Users\\dboja\\source\\Electricity2\\Electricity2\\Vertex.cpp"

VertexPositionOnly::VertexPositionOnly( float x, float y, float z ) noexcept :
	m_Position( x, y, z )
{
}

VertexPositionOnly::VertexPositionOnly( const Float3& position ) noexcept :
	m_Position( position )
{
}

void
VertexPositionOnly::SetPosition( float x, float y, float z ) noexcept
{
	m_Position.x = x;
	m_Position.y = y;
	m_Position.z = z;
}

void
VertexPositionOnly::SetPosition( const Float3& pos ) noexcept
{
	m_Position = pos;
}
